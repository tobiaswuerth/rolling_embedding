{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = './arxiv_downloads_processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pkl',\n",
       "  '2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pkl',\n",
       "  '2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pkl'],\n",
       " 100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir(preprocessed_data)\n",
    "files[:3], len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rolling.paper import Paper, load_paper, print_paper\n",
    "\n",
    "load_n = 100\n",
    "\n",
    "papers:list[Paper] = []\n",
    "for i in range(load_n):\n",
    "    papers.append(load_paper(os.path.join(preprocessed_data, files[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: 2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf\n",
      "> [ 1.09270895  1.30160093 -2.66840649] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA. where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq. (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp. 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab. (LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n",
      "  > [ 1.51249397  1.30708337 -2.13811707] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA. where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq.\n",
      "    > [ 0.5607357   0.48944172 -2.37554383] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq.\n",
      "      > [ 0.59995002  1.83745313 -3.12929296] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters.\n",
      "        > [ 1.47802949  1.3865068  -3.02602625] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs.\n",
      "          > [ 1.41769803  1.27047706 -3.15513825] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a.\n",
      "          > [ 1.99488759  0.7405777  -4.12540531] local cluster) surrounding vs in time roughly linear with the size of Cs.\n",
      "          > [ 0.35239524  1.89398146 -3.05035019] This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs.\n",
      "          > [ 0.21401146  1.60931432 -3.01792622] However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs.\n",
      "        > [ 0.98459125  1.14680135 -3.5964818 ] To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation.\n",
      "          > [ 0.83557868  1.45382118 -3.3706789 ] To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality.\n",
      "          > [ 0.44359392  1.02308512 -3.88586831] To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes.\n",
      "          > [ 0.93769324  1.4336102  -3.63604832] Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality.\n",
      "          > [ 0.89692289  0.72259998 -2.98592877] The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation.\n",
      "        > [ 1.55274737  1.97804773 -3.58837652] Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more.\n",
      "          > [ 1.38456714  1.33846092 -3.55438757] Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster.\n",
      "          > [ 0.89499038  1.33556437 -3.81551647] Index Termsâ€”local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a.\n",
      "          > [ 1.32031989  1.06898713 -3.78155184] local cluster) pertinent to a given seed node by exploring a small region around it in the input graph.\n",
      "          > [ 0.56314409  2.55118036 -3.55792594] Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more.\n",
      "        > [ 0.58675414  1.97431254 -3.59304786] In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters.\n",
      "          > [ 0.09591432  1.73751116 -3.17208862] In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13].\n",
      "          > [ 0.72806567  1.34387183 -4.01423788] Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges.\n",
      "          > [ 0.44286263  1.86974394 -3.11720133] This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20].\n",
      "          > [ 0.94925308  1.91508937 -3.42663479] To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters.\n",
      "      > [ 0.14417896  0.36159837 -3.1875999 ] Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions.\n",
      "        > [ 0.66280317  1.15901268 -3.28384137] Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V).\n",
      "          > [ 0.06269412  1.66945899 -3.33678007] Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency.\n",
      "          > [ 0.35274819  1.10541832 -3.42873716] Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23].\n",
      "          > [ 1.26517344  1.02646637 -4.0949297 ] Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp.\n",
      "          > [ 1.30154848  1.05918956 -3.75689507] Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V).\n",
      "        > [ 0.02052707  1.2205559  -3.05985165] As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs.\n",
      "          > [-0.3230086   1.20907462 -3.33162689] As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering.\n",
      "          > [ 0.91186255  0.50403953 -3.63398719] However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges.\n",
      "          > [ 0.24938962  1.20348501 -2.65910745] Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly.\n",
      "          > [ 0.86817539  1.66749334 -3.57692099] In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs.\n",
      "        > [ 1.24319875  1.23686421 -3.44839525] These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints.\n",
      "          > [ 1.83776855  1.11541748 -3.94506836] These are termed attributed graphs.\n",
      "          > [ 0.08765912  1.30436993 -3.6067338 ] Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks.\n",
      "          > [ 0.12829979  1.2452774  -3.75549793] Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality.\n",
      "          > [ 1.46819329  0.95442349 -3.53446627] Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints.\n",
      "        > [ 0.91526288  1.11322737 -3.36430073] However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions.\n",
      "          > [ 0.55834186  0.51185775 -3.93968272] However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections.\n",
      "          > [ 0.82721591  1.72266269 -2.94459558] To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency.\n",
      "          > [ 1.06409144  1.44789267 -3.8814373 ] Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs.\n",
      "          > [ 0.08955733  0.15701222 -3.61845088] The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions.\n",
      "      > [ 0.23884243  1.0666219  -2.75948238] Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges.\n",
      "        > [ 0.75951689  1.51906788 -3.41158533] Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph.\n",
      "          > [ 0.83603394  0.99682504 -3.55803394] Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t.\n",
      "          > [ 0.4821851   1.97027266 -3.53461862] vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes.\n",
      "          > [ 1.1812439   1.63779879 -3.5802002 ] This is non-local and infeasible for large graphs.\n",
      "          > [ 0.98563766  1.25804305 -3.0569458 ] In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph.\n",
      "        > [ 1.20175004  0.74455404 -3.457618  ] First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance.\n",
      "          > [ 0.8048352   0.79389262 -3.28426647] First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35].\n",
      "          > [ 0.98802674  0.9571743  -3.27971625] This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs.\n",
      "          > [ 1.18743169  0.49821597 -3.67599559] On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion.\n",
      "          > [ 1.37247968  0.93544096 -3.58053041] It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance.\n",
      "        > [ 1.25763011  1.54354763 -2.97842169] Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor.\n",
      "          > [ 0.80721867  1.16819608 -2.88103461] Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD.\n",
      "          > [ 1.32515669  1.95499682 -2.81065512] We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38].\n",
      "          > [ 1.36913323  1.17630935 -3.14730835] Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost.\n",
      "          > [ 1.37972367  0.69885409 -3.60182858] In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor.\n",
      "        > [ 0.3144837   0.73731202 -2.90282869] II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges.\n",
      "          > [ 0.76526749 -0.11198903 -3.13219213] II. P ROBLEM FORMULATION A.\n",
      "          > [ 0.53023309  0.20357326 -3.49417782] Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a.\n",
      "          > [-0.05841827 -0.07971954 -3.83825684] network), where V = {v1, v2, .\n",
      "          > [ 0.61812657  0.63529885 -3.14277816] . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges.\n",
      "      > [ 0.29290217  0.41245431 -2.66963315] For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq.\n",
      "        > [ 0.03341744  0.12760648 -2.89785337] For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations.\n",
      "          > [ 0.80250031 -0.33329964 -3.23660421] For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other.\n",
      "          > [ 0.46119595 -0.34275627 -3.38879395] We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree.\n",
      "          > [-0.32677069  0.36428669 -2.37970877] Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0.\n",
      "          > [-0.27734748  0.28928205 -4.12075424] The TABLE I: Frequently used notations.\n",
      "        > [-0.04447474  0.36848432 -3.13175464] Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively.\n",
      "          > [ 0.24672806  0.17788059 -3.26032972] Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively.\n",
      "          > [ 0.11167698  0.95086646 -3.33404922] n, m, d The numbers of nodes, edges, and distinct attributes, respectively.\n",
      "          > [ 0.25268415 -0.23056969 -3.53345776] N(vi), d(vi) The set of neighbors and degree of node vi, respectively.\n",
      "          > [-0.38645291  0.64805168 -3.38963699] A, D, P The adjacency, degree, and transition matrices of the graph G, respectively.\n",
      "        > [ 0.74762011  0.82183641 -3.51766348] vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq.\n",
      "          > [ 1.09465754  1.02506113 -3.79631972] vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi).\n",
      "          > [ 0.74755859  0.98074406 -2.76336288] supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}.\n",
      "          > [ 0.16045247 -0.49128771 -3.8243475 ] Î± The restart factor in RWR, Î± âˆˆ (0, 1).\n",
      "          > [ 0.6443367   0.86215413 -3.15854168] s(vi, vj) The SNAS defined by Eq.\n",
      "        > [ 0.65165007 -0.53651792 -3.57360649] (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq.\n",
      "          > [-0.01172906 -0.56641972 -3.54607677] (1). Ï€(vx, vy) The RWR score of vy w.r.t.\n",
      "          > [ 1.13128507  0.27703586 -3.07102442] vx (See Eq.\n",
      "          > [ 0.47350025 -0.52021885 -4.21026611] (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t.\n",
      "          > [ 1.19686139  0.01140662 -3.6999681 ] vs respectively (See definition in Eq.\n",
      "    > [ 1.73766041  0.58441114 -3.00418472] (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA.\n",
      "      > [ 0.55003071 -0.1340666  -3.36296916] (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors.\n",
      "        > [-0.02340592  0.8871963  -3.43438983] (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0.\n",
      "          > [ 0.28276572 -0.31028086 -3.55085707] (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq.\n",
      "          > [ 0.68674636  1.58101046 -3.76734161] (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo.\n",
      "          > [-0.10637227  0.35976177 -2.60294294] 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi).\n",
      "          > [-0.49848139  0.46334773 -2.8799839 ] The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0.\n",
      "        > [ 0.42188513  1.03491139 -3.10797811] Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i).\n",
      "          > [ 0.22770482  0.4803558  -3.09930182] Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj.\n",
      "          > [ 0.79961795  0.81011707 -3.26888967] A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1.\n",
      "          > [-0.60732114 -0.45512205 -3.06730509] Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively.\n",
      "          > [ 0.60090542  0.65007561 -2.58050156] The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i).\n",
      "        > [ 0.93527031  0.99508464 -3.33765602] The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t.\n",
      "          > [ 1.21679425  1.03055906 -3.39389467] The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi).\n",
      "          > [ 1.09098935  1.10492969 -3.14768839] Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi).\n",
      "          > [ 0.43509385  0.33414552 -3.76289129] Table I lists notations frequently used throughout this paper.\n",
      "          > [ 0.5184617   0.21172693 -3.67309403] A local cluster Cs w.r.t.\n",
      "        > [ 1.23321843  0.96841085 -3.34796429] a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors.\n",
      "          > [ 0.59763211  0.98897254 -3.39422703] a seed node vs in graph G is defined as a subset of V containing vs.\n",
      "          > [ 0.75782895  0.99267364 -3.18419552] Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity.\n",
      "          > [ 1.11412776  1.60323763 -3.09550953] Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs).\n",
      "          > [ 0.6870023   0.10982413 -3.64090323] B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors.\n",
      "      > [ 1.15430987  0.56024688 -2.4537704 ] The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq.\n",
      "        > [ 0.87034488  0.55731124 -3.05895257] The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj.\n",
      "          > [ 0.71110129  0.6572544  -3.41104388] The denominator in Eq.\n",
      "          > [ 0.5914405  -1.11535442 -3.60754657] (1) is to ensure the s(vi, vj) values w.r.t.\n",
      "          > [ 0.76701611  0.47285283 -3.19977689] any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section.\n",
      "          > [ 0.68674266  0.30092597 -3.31122613] Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj.\n",
      "        > [ 1.16966665  0.24963592 -4.0424037 ] We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£!\n",
      "          > [ 0.98433125 -0.26325461 -3.39598036] We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39].\n",
      "          > [ 0.73317331  0.80569589 -3.71658659] When f(Â·, Â·) 2ð‘£!\n",
      "          > [ 0.39453185  0.49879792 -3.75992298] ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£!\n",
      "          > [ 0.64557242  1.15075839 -4.2001729 ] ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£!\n",
      "        > [ 0.56822306  1.9586823  -3.84752584] ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD.\n",
      "          > [ 0.60402536  0.77252018 -3.5744257 ] ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£!\n",
      "          > [-0.04179356  1.41722262 -3.88531637] ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£!\n",
      "          > [ 0.683801    0.99197447 -3.74423242] ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig.\n",
      "          > [ 0.05110189  1.09293771 -3.37217045] 1: Figurative Illustration of BDD.\n",
      "        > [ 1.30419338  0.09255464 -3.51538897] is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq.\n",
      "          > [ 1.14921641 -0.38499841 -3.43740058] is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1.\n",
      "          > [ 0.85628819  0.57066834 -3.13706875] Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) .\n",
      "          > [ 1.21105039 -0.03403091 -3.68773031] (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor.\n",
      "          > [ 0.67663187  0.40588567 -2.45592999] Essentially, Eq.\n",
      "      > [ 0.82290375  0.64571691 -2.88998938] (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA.\n",
      "        > [ 0.26019332  0.56637049 -3.53243828] (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t.\n",
      "          > [ 0.55859864  0.98862213 -3.63084936] (4) can be deemed as a variant of the softmax function.\n",
      "          > [ 0.24313423  0.53070772 -3.66999292] C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster.\n",
      "          > [ 0.46428284  0.68174952 -3.28625321] In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target.\n",
      "          > [-0.05700332  0.25088677 -3.3092804 ] More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t.\n",
      "        > [ 0.01800476  0.18423207 -3.46553016] node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp.\n",
      "          > [ 0.59735912  0.27518982 -2.96123743] node vx.\n",
      "          > [-0.77241814  0.28888896 -3.50773525] At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±.\n",
      "          > [ 1.03210962  0.15339705 -2.61265182] In essence, Ï€(vs, vi) (resp.\n",
      "          > [ 0.51119143 -0.22021636 -3.47441196] Ï€(vt, vj)) is the probability that an RWR originating from vs (resp.\n",
      "        > [ 0.65261447  0.25781167 -3.61584663] vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively.\n",
      "          > [-0.05163765 -0.31461716 -3.75415039] vt) termi- nates at node vi (resp.\n",
      "          > [ 0.86829019 -0.10746651 -2.39857364] vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp.\n",
      "          > [ 1.22055578 -0.07979427 -3.20298576] Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp.\n",
      "          > [-0.98373413  0.69329071 -3.92333984] vt) to all the nodes in G via random walks, respectively.\n",
      "        > [ 1.2302264   0.96766317 -3.53858089] Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA.\n",
      "          > [ 0.85594118  0.77942127 -3.30767727] Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt.\n",
      "          > [ 0.83625633 -0.14738619 -3.31586671] The BDD âˆ’ â†’Ï t of (vs, vt) in Eq.\n",
      "          > [ 0.67049152  0.50456041 -4.23610783] (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig.\n",
      "          > [ 0.81557465  0.29410535 -3.74405551] 2: Basic Idea of LACA.\n",
      "      > [ 1.84927881  0.48515752 -2.82868505] the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA.\n",
      "        > [ 0.58151263  1.09326875 -3.16802073] the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs.\n",
      "          > [ 0.02905995  0.98761547 -3.20822883] the overall SNAS of all such ending pairs, as illustrated in Fig.\n",
      "          > [ 0.09005471  0.92831391 -3.31260252] 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links.\n",
      "          > [ 0.93562537  0.66758251 -3.75476551] Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph.\n",
      "          > [ 0.73550671  1.21885097 -3.14642739] D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs.\n",
      "        > [ 0.71199942  1.23981225 -3.11676478] In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t.\n",
      "          > [ 1.01082027  0.73655438 -2.95534205] In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€².\n",
      "          > [ 0.87581772  1.5183717  -3.12418532] More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality.\n",
      "          > [ 1.16308951  0.0869081  -3.0289433 ] Notice that the direct computation of BDD âˆ’ â†’Ï in Eq.\n",
      "          > [-0.28447318 -0.76185858 -3.69948816] (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t.\n",
      "        > [ 0.72083396  0.01921706 -3.27665353] the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows.\n",
      "          > [ 0.2241246  -0.1296168  -3.24641299] the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t.\n",
      "          > [ 0.26225907  0.09360355 -3.35873413] all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging.\n",
      "          > [ 1.2877059  -0.8871963  -2.77863669] Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€².\n",
      "          > [ 0.99438691  0.83678526 -3.28752589] III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows.\n",
      "        > [ 1.13343763  0.59449768 -3.3015089 ] A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA.\n",
      "          > [ 1.25502729  1.11962199 -2.9997921 ] A. Problem Transformation Firstly, by the definition of BDD in Eq.\n",
      "          > [ 0.2395691  -0.3651495  -3.71914387] (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t.\n",
      "          > [ 0.95109534  0.57574004 -3.65571737] the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig.\n",
      "          > [ 1.01638091  0.76257563 -3.66085815] 3: Overview of LACA.\n",
      "    > [ 1.35306954  1.01135981 -2.46613955] where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f .\n",
      "      > [ 0.12600404  1.17455184 -2.78734565] where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq.\n",
      "        > [ 0.7448054   0.87141961 -3.81685233] where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq.\n",
      "          > [ 1.43318266e-01 -1.81001425e-03 -3.81508422e+00] where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t.\n",
      "          > [ 0.48748916 -0.04274962 -3.24471951] vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi).\n",
      "          > [ 0.3230651   0.53321737 -3.12940216] (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq.\n",
      "          > [ 0.7351737   0.84278053 -3.89028931] (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq.\n",
      "        > [ 0.58824986  0.73963124 -3.34604383] (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq.\n",
      "          > [ 1.15787232  1.24798894 -4.04457903] (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold.\n",
      "          > [ 0.23003221  0.08968966 -3.01243138] However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq.\n",
      "          > [ 0.53454411  0.69965929 -2.36991739] (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM).\n",
      "          > [ 0.14407711  0.51116008 -3.5285368 ] In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq.\n",
      "        > [ 0.54383248 -0.00716944 -2.90171123] (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i.\n",
      "          > [ 0.12270992 -0.28603318 -3.33391595] (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t.\n",
      "          > [ 1.28470445 -0.04374065 -3.15493536] seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq.\n",
      "          > [ 0.36045659  0.7662769  -2.82454872] (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}.\n",
      "          > [ 1.32350886  0.46457708 -2.83199072] (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i.\n",
      "        > [ 0.13461693  1.50377047 -3.51422739] If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq.\n",
      "          > [ 0.39130193  1.82626355 -3.57414794] If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ).\n",
      "          > [ 0.71756577  1.66802144 -4.21825552] As illustrated in Fig.\n",
      "          > [-0.17042515  0.16523299 -3.2461555 ] 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig.\n",
      "          > [-0.05084448  0.36916161 -3.39444065] 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq.\n",
      "      > [ 0.68076247  0.71272761 -3.30518103] (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq.\n",
      "        > [ 1.1409719   0.80555946 -3.87593627] (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€².\n",
      "          > [ 0.55764961  0.59446049 -3.94973922] (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€².\n",
      "          > [ 0.64668292 -0.4072935  -4.05012465] ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig.\n",
      "          > [ 1.13798738  0.8636862  -3.81484938] 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1.\n",
      "          > [ 0.80668384  0.71909124 -3.45332026] B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€².\n",
      "        > [ 0.28144002 -0.09420399 -3.30023074] Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion.\n",
      "          > [ 1.24772239 -0.38271055 -2.9647646 ] Since âˆ’ â†’Ï•â€² can be properly computed using Eq.\n",
      "          > [ 0.10806362  0.1269078  -3.2352829 ] (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€².\n",
      "          > [-0.37409228 -0.49932617 -2.81197596] Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else.\n",
      "          > [ 0.49061868  0.30897495 -3.77686667] The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion.\n",
      "        > [ 0.64345145  0.81054652 -3.42548847] This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo.\n",
      "          > [ 0.78122908  1.26131237 -3.70296907] This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency.\n",
      "          > [ 0.16210741  0.15387894 -2.93013549] As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig.\n",
      "          > [ 0.96252334  0.71865726 -2.80095387] 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq.\n",
      "          > [ 0.34565988  1.43140733 -3.34636998] (11) (Algo.\n",
      "        > [ 0.46525237  0.71208346 -3.86179352] 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq.\n",
      "          > [ 0.42640954  0.38730773 -3.67102766] 3), and a three-step scheme for the online approximation of BDD vector.\n",
      "          > [ 0.57119113  0.87905902 -3.82133245] Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo.\n",
      "          > [ 0.0035902  -0.19281167 -2.90946507] 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq.\n",
      "          > [-0.33921349  0.76851416 -3.58760381] (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq.\n",
      "      > [ 1.01895988  1.05444849 -3.05875778] (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7).\n",
      "        > [ 0.96994025  0.58634877 -3.5244801 ] (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo.\n",
      "          > [ 1.04638815  0.99325907 -3.56450987] (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo.\n",
      "          > [ 0.12885788  1.34282136 -3.70933342] 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo.\n",
      "          > [ 4.69546497e-01  3.19588184e-03 -3.31094217e+00] 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo.\n",
      "          > [ 0.99487078  0.5800367  -3.06656504] 3 for constructing TNAM Z and the complete LACA algorithm (Algo.\n",
      "        > [ 0.75511098  0.58279777 -3.01114082] 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively.\n",
      "          > [ 0.70755368  0.28793025 -3.42364597] 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime.\n",
      "          > [ 0.68587643  0.43945751 -3.3478632 ] IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq.\n",
      "          > [ 0.36181307 -0.48308784 -2.82759356] (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq.\n",
      "          > [ 0.56560075 -0.16106971 -2.911165  ] (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively.\n",
      "        > [ 0.84576422  1.25655854 -3.2236557 ] Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f .\n",
      "          > [ 0.64384168  1.8836776  -3.3536377 ] Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency.\n",
      "          > [ 1.10326385  1.43025494 -3.25036621] Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme.\n",
      "          > [ 1.36988401  1.56927872 -3.9699564 ] A. The GreedyDiffuse Approach Algo.\n",
      "          > [ 0.63406694  0.5409801  -3.35493326] 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f .\n",
      "        > [ 0.75565398  1.59134173 -3.73165059] Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7).\n",
      "          > [ 1.11503887  2.31315446 -3.48644018] Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee.\n",
      "          > [ 0.20519452  1.24525261 -3.39409304] Given the transition matrix P of the input graph G and the diffusion threshold, Algo.\n",
      "          > [ 0.84504306 -1.22221494 -3.74405384] 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1.\n",
      "          > [ 1.10619211 -0.76233643 -3.73017359] Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7).\n",
      "      > [ 1.16991436 -0.39488143 -2.86808348] Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f .\n",
      "        > [ 1.14999521 -0.41287237 -3.96730781] Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors.\n",
      "          > [ 1.02234244 -0.96272641 -3.99752164] Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion.\n",
      "          > [ 0.51552796 -0.08309352 -3.0414083 ] More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise.\n",
      "          > [ 0.58835143 -0.7189157  -3.7111783 ] (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6).\n",
      "          > [ 1.57289839 -0.41110176 -3.49412632] âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors.\n",
      "        > [ 0.91047233  0.49732202 -3.58924532] That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7).\n",
      "          > [ 0.80860585  0.25589383 -3.63966894] That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig.\n",
      "          > [ 0.84638768  1.15093517 -4.07330132] 5: Greedy v.s.\n",
      "          > [ 1.33700812  2.2626636  -3.79492903] Non-greedy.\n",
      "          > [ 0.90934622 -0.51558024 -3.41917133] These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7).\n",
      "        > [ 0.95871735  0.86253494 -3.78720975] Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time.\n",
      "          > [ 1.01045632  0.12330114 -3.89100885] Theorem IV .1.\n",
      "          > [ 0.60024929  0.12372153 -3.74005938] Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo.\n",
      "          > [ 0.88642085 -0.30235234 -3.32753563] 1 outputs a diffused vector âˆ’ â†’q satisfying Eq.\n",
      "          > [ 0.23245955  1.48987198 -3.35464478] (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time.\n",
      "        > [ 1.62704039  0.08741125 -3.23457146] Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f .\n",
      "          > [ 1.93568361  0.99315476 -3.25438762] Proof. All missing proofs can be found in Appendix A. Algo.\n",
      "          > [ 1.23841882 -0.71612287 -2.98950434] 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq.\n",
      "          > [ 0.65372944  0.66914988 -3.48164272] (15). Eventually, Algo.\n",
      "          > [ 1.32822907 -0.93317723 -3.66086388] 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f .\n",
      "    > [ 1.53783441  1.28753912 -2.14712596] Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq.\n",
      "      > [ 1.64209533  0.78471893 -1.72256958] Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq.\n",
      "        > [ 1.32768321  1.62496114 -3.8099432 ] Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig.\n",
      "          > [ 1.42142594  0.96766907 -3.5054698 ] Theorem IV .1 establishes the approximation accuracy guarantees of Algo.\n",
      "          > [ 1.3193624   1.25084519 -4.0019989 ] 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively.\n",
      "          > [ 0.58772725  0.75796968 -4.09123421] A Running Example.\n",
      "          > [ 0.70380032  1.17039979 -2.56342983] Consider the example G in Fig.\n",
      "        > [ 0.92501491  0.44991007 -3.74562764] 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly.\n",
      "          > [ 0.34860802  0.16349149 -3.16616821] 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6.\n",
      "          > [ 0.85862809  1.22815132 -3.97523785] We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1.\n",
      "          > [ 1.10983562 -1.07138646 -3.85217071] Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig.\n",
      "          > [ 1.01895583  0.53190631 -3.96449113] 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly.\n",
      "        > [ 0.96581835 -0.10243485 -4.25369883] Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively.\n",
      "          > [ 1.08261883 -1.01129854 -4.18765354] Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual.\n",
      "          > [ 0.89770293 -0.6540606  -3.69351625] Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ.\n",
      "          > [ 1.06386781  0.24205969 -3.65438104] Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4.\n",
      "          > [ 0.62125796 -0.7110315  -4.22048044] A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively.\n",
      "        > [ 1.49292231  0.42686206 -3.02779746] The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq.\n",
      "          > [ 1.12689078 -1.4137876  -4.02288914] The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1.\n",
      "          > [ 1.76238704  1.07025123 -3.40028453] GreedyDiffuse then terminates and returns the reserve values as the result.\n",
      "          > [ 1.31485355  1.03318226 -3.01492405] B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo.\n",
      "          > [ 0.10074004 -0.10792398 -3.19378185] 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq.\n",
      "      > [ 1.4149816   1.31204093 -2.99769044] (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations.\n",
      "        > [ 1.26404381  1.46172965 -3.17216349] (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq.\n",
      "          > [ 0.52575916  0.40408432 -3.63866949] (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo.\n",
      "          > [ 0.83539528  1.51654124 -3.73502827] 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq.\n",
      "          > [ 1.15345454  0.4116942  -3.31303668] (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo.\n",
      "          > [ 0.92074537  1.16198421 -3.40716267] 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq.\n",
      "        > [ 0.98518831  0.20681043 -3.63734794] (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration.\n",
      "          > [-0.02037021  0.40002352 -3.96426296] (17)) on PubMed and ArXiv datasets (Table III).\n",
      "          > [ 1.29225647  0.52870697 -3.15882349] âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq.\n",
      "          > [ 0.20866118  1.72717381 -3.58844995] (16), non-greedy opera- tions in Eq.\n",
      "          > [ 1.07276797 -0.71740413 -4.16845894] (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration.\n",
      "        > [ 1.27452087  1.60444295 -3.27404237] From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq.\n",
      "          > [ 1.63100123  0.65710336 -3.95896029] From Fig.\n",
      "          > [ 1.07463861  0.65251875 -3.41494632] 5, we can observe that Algo.\n",
      "          > [ 0.95521414  1.21989441 -3.51160622] 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency.\n",
      "          > [ 1.28220248  1.59799719 -3.61505961] The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq.\n",
      "        > [ 1.43091083  0.56932807 -3.71613812] (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations.\n",
      "          > [ 1.35328317 -0.14397123 -4.00717354] (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched.\n",
      "          > [ 1.22974288  0.51212329 -3.16625595] Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q .\n",
      "          > [ 1.13802266  0.70572364 -3.52668762] As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy.\n",
      "          > [ 1.399526    1.26087618 -3.21226549] In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations.\n",
      "      > [ 1.411955    1.04128909 -3.07405853] Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11.\n",
      "        > [ 1.25685453  0.94122171 -3.22331238] Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7).\n",
      "          > [ 1.48630476 -0.36388153 -3.54514003] Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination.\n",
      "          > [ 1.18481517  2.23721385 -2.96677542] However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs.\n",
      "          > [ 0.89696681  1.02796054 -3.26626587] C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality.\n",
      "          > [ 0.49289     0.30876487 -3.99906182] TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7).\n",
      "        > [ 1.18519557  1.26231825 -3.37418675] Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo.\n",
      "          > [ 0.78724194  1.72363281 -4.203125  ] Dataset Global avg.\n",
      "          > [ 0.76176363  1.21003342 -3.51139784] degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo.\n",
      "          > [ 0.63686377  0.44772315 -3.28571272] 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations.\n",
      "          > [ 0.37998962  1.86474609 -3.26879883] In contrast to Algo.\n",
      "        > [ 1.23912597  0.83361107 -2.97817945] 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees).\n",
      "          > [ 0.67929506  0.01918228 -2.70213437] 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq.\n",
      "          > [-0.37358633  0.06006427 -3.71051979] (15) at Line 3, Algo.\n",
      "          > [ 0.20412064  0.59950638 -3.57000732] 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions.\n",
      "          > [ 2.16417599  0.94660473 -3.43744421] The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees).\n",
      "        > [ 1.31426823  1.15326762 -3.29599738] To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11.\n",
      "          > [ 1.10616469  1.00673044 -3.6214838 ] To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq.\n",
      "          > [ 0.92489547  1.58710551 -3.03800941] (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ .\n",
      "          > [ 1.05792058  1.98313403 -3.16175556] Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted.\n",
      "          > [ 0.84596753 -0.36335561 -3.30156231] When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11.\n",
      "      > [ 1.17643046  1.38499463 -2.82851672] Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq.\n",
      "        > [ 0.94866973  1.75132573 -3.23909521] Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo.\n",
      "          > [ 0.9439708   1.53378284 -3.20972395] Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo.\n",
      "          > [ 0.21142861 -0.60724521 -3.05223751] 2 outputs a vector âˆ’ â†’q such that Eq.\n",
      "          > [ 0.17816664  1.23716283 -3.41620588] (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time.\n",
      "          > [ 0.97225916  1.62970448 -2.47199535] In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo.\n",
      "        > [ 0.12802574  1.02628946 -2.1384244 ] 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1.\n",
      "          > [ 0.3356328   0.8310703  -2.22705722] 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo.\n",
      "          > [-0.14125936 -0.21533005 -3.97949386] 2, respectively.\n",
      "          > [ 0.41533798  1.04350746 -2.76868439] Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2.\n",
      "          > [ 0.45327029  0.59082651 -3.23613548] In particular, when Ïƒ â‰¥ 1, Î² = 1.\n",
      "        > [ 1.21804571  0.36308527 -3.35720682] V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo.\n",
      "          > [ 1.41447055  0.09601451 -3.46905851] V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC.\n",
      "          > [ 0.54214859  2.42163086 -3.26916504] We first elucidate the algorithmic details of Algo.\n",
      "          > [-0.07641271  0.51953053 -3.36273646] 3 for constructing TNAM Z.\n",
      "          > [ 1.60610318  1.45094752 -2.95674801] The complete algorithmic details of LACA (Algo.\n",
      "        > [ 0.63657337  0.33423132 -2.67020202] 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq.\n",
      "          > [ 1.08923578  0.10804992 -2.92850447] 4) and related analyses are provided in Section V-B.\n",
      "          > [ 0.9513604   1.19147301 -2.72421622] Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38].\n",
      "          > [ 0.38428521  0.01605849 -3.26173592] A. Construction of TNAM Z Basic Idea.\n",
      "          > [ 0.71342868 -0.05881568 -2.83287907] To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq.\n",
      "  > [ 0.95979726  0.77378416 -2.64053869] (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395.\n",
      "    > [ 1.55392766  1.4662869  -2.33687663] (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix.\n",
      "      > [ 0.38303214  0.94094282 -2.02572656] (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1).\n",
      "        > [ 0.34478119  0.05132717 -2.34861231] (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq.\n",
      "          > [ 0.29555064  0.21730159 -2.93789721] (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j).\n",
      "          > [ 0.8552624   0.23373452 -3.03256583] Accordingly, Eq.\n",
      "          > [ 0.08471966 -0.65994108 -2.57651997] (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, .\n",
      "          > [ 1.04559326  0.43471241 -2.72338867] . . , k}; 9 Compute Y ; â–· Eq.\n",
      "        > [-0.15861832  0.22853272 -3.05466676] (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively.\n",
      "          > [ 0.46120763  0.41896629 -3.29919434] (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq.\n",
      "          > [ 0.55760014 -0.3257041  -3.06268311] (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq.\n",
      "          > [ 0.28300661  0.20772909 -3.48326159] (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— .\n",
      "          > [-0.26901776  0.33361182 -3.39431763] (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively.\n",
      "        > [ 0.38615653  1.00403249 -2.80405426] Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details.\n",
      "          > [ 0.56351495  0.5378834  -3.0465014 ] Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y .\n",
      "          > [ 0.29427382  0.92181116 -2.72157311] Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X.\n",
      "          > [-0.17402405  1.13678694 -2.53710032] Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2.\n",
      "          > [ 0.99462891  0.62744141 -4.47265625] Details.\n",
      "        > [ 0.25118959  1.10177469 -2.67371368] In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1).\n",
      "          > [ 0.49260378  1.85832906 -3.49087501] In Algo.\n",
      "          > [ 0.14811626  0.74957585 -2.81323433] 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X.\n",
      "          > [ 0.7041626   1.68969727 -2.37194824] That is, Algo.\n",
      "          > [ 0.16811375 -0.05905931 -2.97332883] 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1).\n",
      "      > [ 0.91166401 -0.32214254 -2.22600198] UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11).\n",
      "        > [ 0.39450151 -0.39816201 -2.88224363] UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo.\n",
      "          > [ 0.82857096 -0.29339683 -2.63996339] UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4).\n",
      "          > [ 0.99655652 -0.24435723 -2.96049452] However, when f(Â·, Â·) is the exponential cosine similarity function (Eq.\n",
      "          > [ 0.43124557  0.57833523 -3.05748987] (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs.\n",
      "          > [ 0.03026884 -0.47628888 -3.08508134] As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo.\n",
      "        > [ 0.09067842 -0.06248374 -2.91206312] 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq.\n",
      "          > [ 0.3545008  -0.30984727 -3.41948414] 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7.\n",
      "          > [-0.40043122 -0.78866053 -3.30928469] This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44].\n",
      "          > [ 0.26941219  0.61809963 -2.98603082] Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq.\n",
      "          > [ 0.10552977  0.48727292 -2.59025764] (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq.\n",
      "        > [ 0.85373509 -0.24153823 -2.95617461] (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2.\n",
      "          > [ 0.56001335  0.13399142 -3.33801389] (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d.\n",
      "          > [ 0.93169594  0.60531473 -3.36157227] from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed.\n",
      "          > [ 0.66898513 -0.7249403  -2.64912772] Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices.\n",
      "          > [ 0.77013636 -0.44326475 -3.63479018] Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2.\n",
      "        > [ 0.29215294  0.05072393 -2.84134269] E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11).\n",
      "          > [ 0.50824207 -0.38012719 -3.08675313] E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq.\n",
      "          > [ 0.12618928 -0.16343167 -3.37440252] (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo.\n",
      "          > [ 0.08793569 -0.22154811 -2.83146024] 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq.\n",
      "          > [-0.05098623 -0.7249766  -4.02928925] (18) (Lines 10- 11).\n",
      "      > [ 1.39363003  1.63787329 -2.77810073] The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7.\n",
      "        > [ 1.14701569  1.75392592 -2.61041546] The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd).\n",
      "          > [ 1.33926654  2.22703409 -2.96344376] The total processing cost entailed by Algo.\n",
      "          > [ 0.9887104   0.80456758 -2.48550987] 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3.\n",
      "          > [ 0.39549595  2.47647119 -3.47220325] The runtime cost of Algo.\n",
      "          > [ 0.30256212  0.41894448 -3.33765864] 3 is O(nd).\n",
      "        > [ 1.27616513  1.18869543 -3.31860518] B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo.\n",
      "          > [ 0.97280526  1.53579843 -3.08274317] B. Complete Algorithm and Analysis In Algo.\n",
      "          > [ 1.18236446  0.76142621 -3.2759223 ] 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ.\n",
      "          > [ 1.04323113  1.04391026 -3.36067247] In the first place, Algo.\n",
      "          > [ 0.59951591  0.26325336 -3.78059626] 4 invokes AdaptiveDiffuse (Algo.\n",
      "        > [ 0.25151554  0.90135449 -2.65173244] 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq.\n",
      "          > [-0.22518668 -0.71521294 -3.11226606] 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2).\n",
      "          > [ 0.70408219  1.61162949 -2.73771167] By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 .\n",
      "          > [ 0.62212998 -0.2554014  -3.25326443] Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4.\n",
      "          > [ 0.27176216  0.07962849 -2.80135012] In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq.\n",
      "        > [ 1.71907735  0.81904674 -3.461308  ] (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7.\n",
      "          > [ 1.22084379  1.51967752 -3.11765409] (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 .\n",
      "          > [ 1.55605388  0.40229601 -3.49398375] After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5).\n",
      "          > [ 1.02678204 -0.42759323 -3.42272949] Let âˆ’ â†’Ï â€² be the output of the above diffusion process.\n",
      "          > [ 1.62055385  0.1054054  -3.14995217] LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7.\n",
      "      > [-0.75986737 -0.3746933  -1.8354739 ] On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix.\n",
      "        > [ 1.34717333  0.3529028  -2.95732141] On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ.\n",
      "          > [ 2.14167166  0.05404542 -3.53338671] On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4.\n",
      "          > [ 0.69741774  0.17936331 -2.88026643] When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq.\n",
      "          > [ 0.3733142   0.74385405 -3.15170479] (10), âˆ’ â†’Ï â€² output by Algo.\n",
      "          > [ 0.12848814  0.10769761 -3.11472511] 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ.\n",
      "        > [ 1.58727837  1.78928578 -3.06211376] Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA.\n",
      "          > [ 1.02468073  1.40382159 -3.12591863] Volume and Complexity Analysis.\n",
      "          > [ 1.55881977 -0.05540812 -3.69581556] Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1.\n",
      "          > [ 0.77806926  1.88381243 -2.52289605] Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3.\n",
      "          > [ 1.61225474  1.79772305 -3.54886413] Next, we analyze the time complexity of LACA.\n",
      "        > [ 0.39360416  1.89196122 -3.00474691] First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 .\n",
      "          > [-0.12760861  0.0311572  -3.67667389] First, Line 2 invokes Algo.\n",
      "          > [ 0.17939368  0.27652287 -3.70337176] 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2.\n",
      "          > [ 0.82213765  1.46965647 -2.97207069] The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3.\n",
      "          > [ 0.65351468  1.86456823 -3.1390903 ] Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 .\n",
      "        > [ 1.30521381  1.07517481 -3.00804806] Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix.\n",
      "          > [ 1.33906782  1.34072256 -3.02442002] Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€².\n",
      "          > [ 0.8507179   0.80849838 -3.39396548] C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5.\n",
      "          > [ 1.67136049  0.06031044 -3.85280538] Definition V .5 (Graph Signal Denoising [45]) .\n",
      "          > [ 0.26591611 -0.25510716 -2.49254537] Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix.\n",
      "    > [ 1.11146104  1.19094777 -2.66541481] The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets.\n",
      "      > [ 0.79504514  0.66657698 -2.43512201] The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}.\n",
      "        > [ 1.25779963  0.04060837 -3.26979184] The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization.\n",
      "          > [ 0.7834084  -0.02507786 -3.41814828] The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm.\n",
      "          > [ 1.25763714  0.51610076 -3.16730499] The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq.\n",
      "          > [ 0.6477204   0.05223788 -3.37273145] (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar.\n",
      "          > [ 2.22008967  0.4395276  -3.40004826] The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization.\n",
      "        > [ 0.61170483  0.41912839 -2.50469971] Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq.\n",
      "          > [ 0.55937958 -0.09149551 -3.38525391] Lemma V .6.\n",
      "          > [ 0.77111602  0.60923547 -2.5474782 ] The closed-form solution to Eq.\n",
      "          > [ 0.1831356   0.17094781 -2.70771646] (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 .\n",
      "          > [ 1.17457891  1.12114549 -3.38507843] By applying the gradient descent to solve Eq.\n",
      "        > [ 0.28361592  0.89393866 -3.18540883] (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq.\n",
      "          > [ 0.47324467  0.33635101 -3.24542236] (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets.\n",
      "          > [-0.51338363  1.00523055 -3.40758204] Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47].\n",
      "          > [ 0.06831074  0.10972121 -3.03491211] If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a.\n",
      "          > [ 0.72121477  0.86718225 -2.90118313] embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq.\n",
      "        > [ 0.56047255  0.59171444 -2.85300541] (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}.\n",
      "          > [ 0.67887044  0.66999048 -3.6895442 ] (10) holds, combining Eq.\n",
      "          > [ 0.64062768  0.4360773  -3.76222301] (5) and Eq.\n",
      "          > [ 1.19877577 -0.02180272 -3.18638396] (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤.\n",
      "          > [ 0.04421113  0.88718957 -2.99366093] In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}.\n",
      "      > [ 1.40914142  1.79476285 -2.7854526 ] Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods.\n",
      "        > [ 1.52148402  2.05003023 -2.64105034] Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B.\n",
      "          > [ 1.56753993  1.62435663 -2.63319659] Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI.\n",
      "          > [ 0.80796051  1.24656403 -3.2474947 ] E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency.\n",
      "          > [ 0.35870835  2.9598999  -3.46956015] All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory.\n",
      "          > [ 1.99896824  0.93420041 -2.89054155] Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B.\n",
      "        > [ 0.91083151  0.85542792 -3.59140062] For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments.\n",
      "          > [ 0.82970589  0.85667503 -3.04130507] For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github.\n",
      "          > [ 1.06538606  0.08583467 -2.70402575] com/HaoranZ99/laca.\n",
      "          > [-0.2968663   0.83157456 -4.53616142] A. Experimental Setup Datasets.\n",
      "          > [ 0.14747913  1.19835579 -3.66044545] Table III lists the statistics of the datasets used in the experiments.\n",
      "        > [ 0.53618199  0.84329247 -3.34852242] The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication.\n",
      "          > [ 0.42691395  0.58830446 -3.28902721] The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively.\n",
      "          > [ 0.73991334  0.81925356 -3.8625598 ] |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph.\n",
      "          > [ 0.1310645   0.81253082 -3.47897339] Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively.\n",
      "          > [-0.02924892  0.23463111 -3.15723586] The attributes of each node are bag-of-words embed- dings of the corresponding publication.\n",
      "        > [ 0.29998979  0.48431766 -3.101758  ] The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods.\n",
      "          > [ 0.72896451  0.14063013 -3.79014587] The ground-truth local cluster Ys of each publication contains the publications in its same subject areas.\n",
      "          > [ 0.09225523  0.8965438  -3.56378627] BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively.\n",
      "          > [-0.45713833  0.55838299 -2.95411253] Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups.\n",
      "          > [ 0.18273306 -0.4305293  -2.93256569] Yelp and Reddit 8TABLE IV: Evaluated methods.\n",
      "      > [ 0.94758701  0.99083483 -2.85063338] Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes.\n",
      "        > [ 0.5807128   1.29696453 -3.50836754] Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters.\n",
      "          > [ 0.39974338  1.31624615 -3.50447226] Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ !\n",
      "          > [ 0.37337908  0.54376626 -3.29584408] WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52].\n",
      "          > [-0.4151414   0.58205783 -3.89033651] Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters.\n",
      "          > [ 0.43637109  0.80371523 -3.04061365] Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters.\n",
      "        > [ 0.68323559  0.72530127 -3.50160217] Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively.\n",
      "          > [-0.33635941  1.09818709 -3.71931148] Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together.\n",
      "          > [ 0.19018139  1.00756621 -3.75769043] The ground-truth local clusters are generated based on the categories of products.\n",
      "          > [-0.01364136  0.71240234 -4.81445312] Competitors.\n",
      "          > [ 1.85211384 -0.14903159 -3.26124787] We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively.\n",
      "        > [ 1.16295695  1.10759652 -3.43473077] We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods.\n",
      "          > [ 0.70767301  0.93325406 -3.06818724] We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62].\n",
      "          > [ 1.1893512   1.06678092 -3.89208055] Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms.\n",
      "          > [ 1.17651784  0.57647341 -3.35802317] APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33].\n",
      "          > [ 0.17590249  0.8021996  -3.91538787] Groups 2)-4) comprise all global methods.\n",
      "        > [ 0.75483263  0.64328235 -3.05364275] Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes.\n",
      "          > [ 0.53045154  0.15983963 -3.29467773] Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively.\n",
      "          > [ 0.56597823  0.71788293 -3.61630988] The local clusters are then generated by sorting all nodes according to the similarity scores.\n",
      "          > [ 0.82134724  0.54156411 -3.18291593] The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors.\n",
      "          > [ 0.36294544  1.14903116 -2.74726605] Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes.\n",
      "      > [ 0.77413625  1.37514949 -3.091501  ] Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets.\n",
      "        > [ 0.59526914  1.1852603  -3.34436321] Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank.\n",
      "          > [ 0.50907767  1.72399116 -3.66699171] Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters.\n",
      "          > [ 0.20054244  0.45569551 -3.40953827] Implementations and Parameter Settings.\n",
      "          > [ 0.84539962 -0.2809839  -3.04525208] For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63].\n",
      "          > [ 0.19100121  0.8697468  -2.96837425] We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank.\n",
      "        > [ 0.94447583  1.22004938 -3.07492065] As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers.\n",
      "          > [ 0.20322987  0.25039545 -3.0059917 ] As for other competitors, we obtain their source codes from the respective authors.\n",
      "          > [ 0.02236881  1.03204954 -3.28683233] All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia.\n",
      "          > [ 1.46908212  0.68806881 -3.13612103] For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E).\n",
      "          > [ 1.0112915   1.16726685 -3.19506836] The parameters in global methods are set as suggested in their respective papers.\n",
      "        > [ 1.00240541  0.57555926 -3.00787067] On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|.\n",
      "          > [ 0.11902261  1.39268398 -3.51116729] On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks.\n",
      "          > [ 0.2976166   0.20354182 -3.48998737] All evaluation results reported in the experiments are averaged over seed nodes in S. B.\n",
      "          > [ 0.85621238  0.32681721 -3.65487671] Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters.\n",
      "          > [ 1.25752711 -0.04127071 -2.7266717 ] More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|.\n",
      "        > [ 1.12619817  0.28606328 -3.61476398] Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets.\n",
      "          > [ 0.59798968 -0.3471806  -3.51622057] Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets.\n",
      "          > [ 1.24750757  1.04998565 -3.3489542 ] The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined.\n",
      "          > [ 1.11616504  1.32499385 -3.40782452] We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours.\n",
      "          > [ 0.9022125  -0.15920572 -3.83962679] Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets.\n",
      "    > [ 0.63471013 -0.31325439 -4.15703678] Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16].\n",
      "      > [ 1.93630588  1.09508657 -2.86771536] Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA.\n",
      "        > [ 0.87150764  1.14433253 -4.06922865] Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision.\n",
      "          > [ 0.62977213  0.85146636 -3.76132035] Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin.\n",
      "          > [ 1.15523076  0.5953635  -4.02560902] For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively.\n",
      "          > [ 0.98142391  1.38101661 -3.43777394] Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively.\n",
      "          > [ 1.52202928  0.68057638 -3.76598763] Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision.\n",
      "        > [ 0.4834398   0.53414488 -3.2271018 ] Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%.\n",
      "          > [ 1.0618279   0.01509938 -3.41527629] Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth.\n",
      "          > [ 1.11532819  0.35866499 -3.71865559] Best is bolded and best baseline underlined .\n",
      "          > [ 0.91830283  0.5687831  -2.37660336] Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig.\n",
      "          > [ 0.49538153  0.52290988 -4.25693607] 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%.\n",
      "        > [ 1.29330301  1.22715783 -3.47191381] The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets.\n",
      "          > [ 0.5209229   1.36214614 -3.57316923] The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures.\n",
      "          > [ 1.44407392  1.33275557 -3.24788165] LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information.\n",
      "          > [ 1.62068295  1.21863174 -3.0847497 ] This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs.\n",
      "          > [ 1.19863892  0.52332973 -3.54553223] In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets.\n",
      "        > [ 1.5407598   1.01722527 -3.17641401] The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA.\n",
      "          > [ 1.21901894  0.36110628 -3.27562928] The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C).\n",
      "          > [ 1.04243779  0.96548694 -3.84920454] 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied.\n",
      "          > [ 1.50662172  1.1174314  -3.50138521] For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ).\n",
      "          > [ 1.89072406  0.15840046 -2.18046713] We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA.\n",
      "      > [ 1.42019403  1.72675073 -2.95141077] For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets.\n",
      "        > [ 0.91389722  0.66350651 -3.48316479] For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets.\n",
      "          > [ 0.89641368  1.20229518 -2.96565437] For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8.\n",
      "          > [ 0.60160679 -0.04469127 -3.75139308] For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|.\n",
      "          > [ 1.27623832  1.44155335 -3.78890657] Intuitively, the smaller Ïµ is, the higher the recall should be.\n",
      "          > [ 0.26015085  0.48470923 -4.03511286] Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets.\n",
      "        > [ 1.3298043   0.36433002 -3.54531503] The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ.\n",
      "          > [ 0.66817701  0.46405628 -3.66781116] The x-axis and y-axis represent Ïµ and the average recall, respectively.\n",
      "          > [ 1.63100123  0.65710336 -3.95896029] From Fig.\n",
      "          > [ 0.16460949  0.14146733 -3.84017897] 6, we can make the following observations.\n",
      "          > [ 1.47040558  0.30199766 -3.47888184] In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ.\n",
      "        > [ 1.0697422   1.18035781 -3.58107305] In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively.\n",
      "          > [ 1.21176279  1.35619462 -3.17161369] In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored).\n",
      "          > [ 0.63641822  0.51678932 -4.18603039] When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig.\n",
      "          > [ 0.39403856  1.30928433 -4.53845549] 7: Running times.\n",
      "          > [ 1.08116817  0.35258088 -3.43129849] Best method and competitor (in terms of precision) are bolded and underlined , respectively.\n",
      "        > [ 1.01608396  1.48100698 -3.10776567] except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets.\n",
      "          > [ 0.22182055  0.59898424 -3.13875389] except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably.\n",
      "          > [ 0.62570417  0.89499277 -3.60487795] Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5.\n",
      "          > [ 1.76776123  1.7825706  -3.55942822] The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV).\n",
      "          > [ 1.45504022  0.8166436  -2.61793208] Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets.\n",
      "      > [ 1.51590133  1.44843674 -3.09981441] This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV.\n",
      "        > [ 1.03751397  1.17149079 -3.40654826] This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V).\n",
      "          > [ 1.12773073  1.51302373 -3.56310534] This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed.\n",
      "          > [ 0.04143925  0.57734632 -3.64467335] Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a.\n",
      "          > [ 0.38314319  0.00640694 -3.71966076] WCSS) of nodes in local clusters output by all methods.\n",
      "          > [ 1.61952436  0.65249699 -3.22718   ] We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V).\n",
      "        > [ 0.92391789  1.32994044 -3.51270819] In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average.\n",
      "          > [ 0.47783059  1.09571791 -3.76094556] In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach.\n",
      "          > [ 0.80361491  1.13537073 -3.70774031] The y-axis represents the running time in seconds on a log scale.\n",
      "          > [ 1.39057159  0.6464982  -3.71922469] The first observation we can make from Fig.\n",
      "          > [ 0.81147826  1.00697732 -3.65442681] 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average.\n",
      "        > [ 1.28045833  1.60520649 -3.61948943] However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task.\n",
      "          > [ 0.86527997  1.19745767 -3.61223531] However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds.\n",
      "          > [ 1.38973773  1.88544703 -3.87167907] On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions.\n",
      "          > [ 0.58478767  1.31233406 -4.29185343] On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average.\n",
      "          > [ 1.09098577  1.4067651  -3.31332588] Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task.\n",
      "        > [ 1.64101553  1.09182501 -3.50471044] For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV.\n",
      "          > [ 1.63771975  1.25036037 -3.50008559] For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo.\n",
      "          > [-4.46378112e-01 -3.39169800e-03 -4.21918440e+00] 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage).\n",
      "          > [ 1.38486648  0.70670646 -4.30817413] In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503.\n",
      "          > [ 1.5379647   0.74669123 -3.43857837] The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV.\n",
      "      > [ 1.09764493  1.14630711 -3.1875391 ] On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16].\n",
      "        > [ 1.0651412   1.2092514  -3.60996389] On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network.\n",
      "          > [ 0.84756178 -0.02957096 -3.66622972] On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time.\n",
      "          > [ 1.79932499  1.63977051 -3.27449799] In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality.\n",
      "          > [ 0.19175833  1.02323771 -3.76088572] D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig.\n",
      "          > [-0.13348523  0.95704782 -3.89729691] 8: A real-world scenario on an academic network.\n",
      "        > [ 0.61796451  1.06264329 -3.06645274] detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)).\n",
      "          > [ 0.57156515  0.66806626 -4.05576944] detection) [17], [66], [67], and academic collaboration net- works [68].\n",
      "          > [ 0.53198588  0.9855091  -3.26624823] A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests.\n",
      "          > [ 0.51730096  1.38370514 -3.23949718] Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig.\n",
      "          > [-0.41203901  0.19699118 -3.52778578] 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)).\n",
      "        > [ 1.19404721  0.89468437 -3.15611911] In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph.\n",
      "          > [ 0.38107985  0.12131334 -3.21341276] In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig.\n",
      "          > [ 0.828637    0.87515014 -2.93162417] 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist.\n",
      "          > [-0.19406354  1.50893426 -4.04473877] VII. R ELATED WORK A.\n",
      "          > [ 1.32583404  1.74654424 -3.90436602] Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph.\n",
      "        > [-0.01955049  1.02909553 -3.8888495 ] The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16].\n",
      "          > [-0.01473215  0.41756624 -3.62305641] The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster.\n",
      "          > [ 0.47191793  1.45705128 -4.19873047] Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey).\n",
      "          > [-0.32140493  0.64317471 -4.02174473] Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank.\n",
      "          > [ 0.60939276  1.03641105 -3.49279404] Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16].\n",
      "    > [ 1.04971802  0.80967033 -3.25799775] The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395.\n",
      "      > [ 1.36579418  1.44972849 -3.17380571] The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss.\n",
      "        > [ 1.64180815  0.79838693 -3.88270283] The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality.\n",
      "          > [ 0.82652318  0.12749165 -3.93971157] The flow-based methods include [20]â€“[22], [33], [72].\n",
      "          > [ 2.06506109  0.90188414 -3.87464666] CRD [20] converts capacity diffusion into a maximum-flow problem.\n",
      "          > [ 0.71059835  0.58092666 -3.53179502] p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values.\n",
      "          > [ 1.44922447  0.8441847  -3.76423621] These methods usually provide theoretical guarantees on running time and approximation quality.\n",
      "        > [ 1.11697066  0.98563707 -3.3697381 ] However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization.\n",
      "          > [-1.11977148  1.55141628 -2.94206738] However, these guar- antees may not be useful in practical efficacy.\n",
      "          > [ 1.21944034  1.05617607 -3.69865918] Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach.\n",
      "          > [ 1.45926929  0.29986954 -3.87320709] [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs.\n",
      "          > [ 1.26316428  1.08474517 -3.00232387] Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization.\n",
      "        > [ 0.51824421  1.82549059 -3.6999402 ] B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed.\n",
      "          > [ 0.3764745   1.62828827 -3.52313232] B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74].\n",
      "          > [ 1.31624758  1.81638992 -3.49621677] Similar to local clustering, community search is query-dependent and does not need to compute the graph globally.\n",
      "          > [-0.34070331  1.82315719 -3.6673429 ] Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80].\n",
      "          > [ 0.71903402  1.25955665 -3.94464087] To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed.\n",
      "        > [ 0.06317718  1.2747879  -3.132653  ] These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss.\n",
      "          > [ 0.16653232  0.52809197 -3.16773653] These methods usually consider keywords as the attributes of the nodes.\n",
      "          > [ 0.19931781  1.58614922 -3.38839769] Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community.\n",
      "          > [ 0.48458165  0.73410022 -3.13762474] Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity.\n",
      "          > [ 0.53009504  0.78777421 -2.95502162] Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss.\n",
      "      > [ 1.25050545  1.45967615 -2.92900515] Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y .\n",
      "        > [ 0.72412479  1.58007205 -3.11390114] Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search.\n",
      "          > [ 0.81113482  1.57143474 -3.4379189 ] Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes.\n",
      "          > [ 0.54203933  1.30199444 -3.41071129] Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32].\n",
      "          > [ 0.58034831  0.84902632 -2.95479298] In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices.\n",
      "          > [ 1.00141239  1.54107225 -3.15155959] Therefore, the problem setting of our work is orthogonal to that of community search.\n",
      "        > [ 1.62580943  1.02303541 -3.42708325] VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs.\n",
      "          > [ 1.78835511  0.73156297 -3.34585118] VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs.\n",
      "          > [ 1.72913468  1.15741575 -3.57034278] LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation.\n",
      "          > [ 1.00316894  0.87231928 -3.40979767] The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency.\n",
      "          > [ 0.74175805  1.27818716 -3.41971135] Regarding future work, we plan to study the local clustering on heterophilic graphs.\n",
      "        > [ 0.09526316  1.35494566 -3.166991  ] 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596.\n",
      "          > [ 0.00942802  1.11195612 -3.34488535] 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) .\n",
      "          > [-0.64905107  1.10464585 -3.22269702] IEEE, 2022, pp.\n",
      "          > [ 0.29253361  0.99326706 -2.90137863] 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp.\n",
      "          > [-0.88124871 -0.20482805 -3.25056744] 5586â€“5596.\n",
      "        > [-0.21072885  0.37617397 -2.91459846] [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y .\n",
      "          > [-0.30728945  0.60632718 -3.07229686] [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining .\n",
      "          > [-0.20433998  1.43713379 -3.36254883] IEEE, 2009, pp.\n",
      "          > [-0.54809701  1.2727803  -3.15782118] 237â€“242.\n",
      "          > [-0.28322807 -0.11488842 -3.280164  ] [4] K. V oevodski, S.-H. Teng, and Y .\n",
      "      > [ 0.58250415  1.31282759 -3.51007795] Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099.\n",
      "        > [ 0.74929678  0.78308851 -3.35883427] Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009.\n",
      "          > [ 1.09884894  1.11517775 -3.30289602] Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol.\n",
      "          > [ 0.21987885  0.65022171 -3.17427373] 10, pp. 1â€“14, 2009.\n",
      "          > [ 0.66422749  0.45289439 -3.21613312] [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol.\n",
      "          > [-0.56403732  0.52500343 -2.46011353] 25, pp. i253â€“i258, 2009.\n",
      "        > [ 0.88250095  1.28953815 -3.3658247 ] [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527.\n",
      "          > [ 0.15480855 -0.54861295 -3.882411  ] [6] J. Li, J.\n",
      "          > [-0.37512469 -0.86878908 -2.8619597 ] He, and Y .\n",
      "          > [ 0.81321925  1.41502714 -3.60751081] Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp.\n",
      "          > [ 0.23254013 -0.40048027 -3.53198242] 519â€“527.\n",
      "        > [ 0.03280616  0.56344408 -3.67080688] [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724.\n",
      "          > [-0.35637283 -0.88664865 -3.97781229] [7] Y . Zhu, J. Li, J.\n",
      "          > [ 0.44055966 -0.57706439 -2.99761105] He, B. L. Quanz, and A.\n",
      "          > [ 0.301357    0.63086373 -3.48614979] A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp.\n",
      "          > [-0.87548828 -0.03449631 -3.68286133] 3718â€“3724.\n",
      "        > [-0.01703776  0.69164068 -3.76306677] [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099.\n",
      "          > [-0.31734869 -0.23787555 -3.88965416] [8] H. Yang, Y .\n",
      "          > [-0.52182263 -0.88331193 -3.10950017] Zhu, and J.\n",
      "          > [ 0.07935552  1.00775707 -3.68267941] He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp.\n",
      "          > [-0.00517845  0.1027832  -4.04248047] 2091â€“2099.\n",
      "      > [ 0.74402535  0.61740345 -2.96722412] [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395.\n",
      "        > [ 0.3067039   0.2036801  -2.87884021] [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368.\n",
      "          > [ 0.66106975  0.62518549 -2.88952756] [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol.\n",
      "          > [-0.56267953 -0.0756043  -2.96193242] 15, no. 13, pp. 3883â€“3896, 2022.\n",
      "          > [ 0.53407449  0.26123855 -3.55394101] [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp.\n",
      "          > [-0.1266861  -0.29516602 -3.51318359] 359â€“368.\n",
      "        > [ 0.88777977  0.55465376 -2.83841085] [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol.\n",
      "          > [ 0.06417687  0.85237759 -3.0808332 ] [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp.\n",
      "          > [-0.20318604 -0.29872894 -3.47973633] 2120â€“2129.\n",
      "          > [ 1.13446379  0.74233514 -2.85036993] [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011.\n",
      "          > [ 0.60718787 -0.14733431 -3.23833084] [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol.\n",
      "        > [ 0.76673579  0.78670126 -3.61616039] 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) .\n",
      "          > [-0.22587606  0.26329491 -3.36292577] 13, no. 77, pp. 2339â€“2365, 2012.\n",
      "          > [ 1.05906403  0.64560252 -3.55764961] [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol.\n",
      "          > [-0.83875656 -0.2650032  -3.79992676] 42, no. 1, pp. 1â€“26, 2013.\n",
      "          > [ 0.51235539  0.53155804 -3.6053772 ] [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) .\n",
      "        > [ 0.84335375  0.93298846 -3.79853439] IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395.\n",
      "          > [-0.02382278  0.98406982 -3.65454102] IEEE, 2006, pp.\n",
      "          > [-0.42588702  0.11862507 -3.65011287] 475â€“486.\n",
      "          > [ 0.98214775  0.66450971 -3.65154362] [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp.\n",
      "          > [ 0.33772278  0.01596832 -3.92651367] 1386â€“1395.\n",
      "  > [ 1.031376    2.10364819 -2.06967926] [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp. 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab. (LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0.\n",
      "    > [ 0.21087125  0.99555886 -2.71285987] [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp.\n",
      "      > [ 0.65233958  1.12580693 -3.54339099] [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020.\n",
      "        > [ 0.29170796  0.90681034 -3.83151031] [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp.\n",
      "          > [ 0.3785533   0.77961224 -3.60281467] [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp.\n",
      "          > [ 0.31501007 -0.18812847 -3.51489258] 1339â€“1356.\n",
      "          > [ 0.01451126  0.83261681 -3.88307524] [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol.\n",
      "          > [ 0.84590632  0.43009275 -2.7141304 ] 716, pp.\n",
      "        > [ 0.215747    1.04052031 -3.23243332] 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning .\n",
      "          > [-0.499897   -0.1489315  -4.39257812] 1â€“58, 2017.\n",
      "          > [ 0.40421122  0.59647655 -3.19885015] [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol.\n",
      "          > [ 0.30140659 -0.45504299 -3.60604358] 91, 2015.\n",
      "          > [ 0.14665815  1.22656775 -3.19071269] [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning .\n",
      "        > [ 0.710549    1.191679   -3.36923242] PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp.\n",
      "          > [-0.04663843 -0.57790577 -3.13523936] PMLR, 2017, pp.\n",
      "          > [-0.35317913 -0.32157078 -3.27221537] 3598â€“3607.\n",
      "          > [ 0.63905847  1.19285965 -3.57376528] [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning .\n",
      "          > [-0.09196955 -0.61187911 -2.6610415 ] PMLR, 2020, pp.\n",
      "        > [ 0.90099651  0.91312462 -3.65077925] 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020.\n",
      "          > [ 0.23356628  0.06836891 -3.2800293 ] 3222â€“3232.\n",
      "          > [-0.30234459 -1.0609206  -3.89595699] [22] A. Jung and Y .\n",
      "          > [ 1.02294469  0.99033272 -3.53092146] SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol.\n",
      "          > [-0.61035514 -0.29367015 -2.8947196 ] 28, pp. 106â€“110, 2020.\n",
      "      > [ 1.08512151  1.31383049 -3.16215777] [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol.\n",
      "        > [ 0.67858899  1.02634537 -3.60408068] [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564.\n",
      "          > [ 0.45553398  1.01517868 -3.44616699] [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol.\n",
      "          > [ 0.31847143 -0.46393967 -3.81970215] 2, no. 1-46, p. 4, 1993.\n",
      "          > [ 0.80705845  1.10621262 -3.55044556] [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp.\n",
      "          > [-0.22099091 -0.31133023 -3.20778871] 555â€“564.\n",
      "        > [ 0.77413243  1.53400755 -3.2378943 ] [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) .\n",
      "          > [-1.0430603   0.88225937 -3.69744873] [25] D. Fu, D. Zhou, and J.\n",
      "          > [ 0.3021974   1.33484221 -3.31604505] He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp.\n",
      "          > [ 0.67415076  0.03737507 -4.23264551] 390â€“400.\n",
      "          > [ 0.73106462  1.31937945 -3.11058807] [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) .\n",
      "        > [ 0.83347499  1.17740607 -3.09998488] SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y .\n",
      "          > [-0.11240862  0.75696093 -3.48836637] SIAM, 2023, pp.\n",
      "          > [ 0.80224001  1.25693822 -3.44105744] 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol.\n",
      "          > [-0.08675348 -0.48936582 -3.35003281] 18, no. 3, p. 183404, 2024.\n",
      "          > [-0.32506543 -0.66186213 -3.61771441] [28] C. Huang, H. Li, Y .\n",
      "        > [ 1.03331828  1.01027131 -3.18544793] Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol.\n",
      "          > [ 1.16889572  0.89738667 -3.12867355] Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp.\n",
      "          > [-0.51086426 -0.0477953  -3.48168945] 803â€“814.\n",
      "          > [ 0.70957911  0.72725976 -3.6879015 ] [29] L. Liao, X.\n",
      "          > [ 0.12964229  0.99313009 -3.15968323] He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol.\n",
      "      > [ 1.0154202   1.26623142 -3.39298964] 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J.\n",
      "        > [ 0.84854692  1.22417331 -3.91103196] 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015. [31] S. Freitas, N. Cao, Y .\n",
      "          > [-0.05858854  0.36168551 -3.55252028] 30, no. 12, pp. 2257â€“2270, 2018.\n",
      "          > [ 1.04601097  1.14050436 -4.02922344] [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol.\n",
      "          > [-0.83441395 -0.24955855 -2.95311618] 3, no. 3, pp. 408â€“444, 2015.\n",
      "          > [ 0.09227675 -0.54903394 -3.57171106] [31] S. Freitas, N. Cao, Y .\n",
      "        > [ 0.61976826  1.20483565 -3.83132744] Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y .\n",
      "          > [ 0.52538061  1.18104708 -3.81156921] Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data).\n",
      "          > [-0.62781906  0.99234772 -3.53320312] IEEE, 2018, pp.\n",
      "          > [ 0.11593437  0.28095245 -3.99926758] 1001â€“1008.\n",
      "          > [ 0.2430788   0.35547367 -3.70091915] [32] Y .\n",
      "        > [ 0.53606379  1.17402995 -3.05993485] Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817.\n",
      "          > [-0.14446013  0.09471326 -2.81845927] Niu, Y .\n",
      "          > [ 0.52181047  1.21904325 -3.11164761] Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) .\n",
      "          > [-0.64905107  1.10464585 -3.22269702] IEEE, 2022, pp.\n",
      "          > [-0.29428101  0.27716637 -3.26586914] 2805â€“2817.\n",
      "        > [ 0.96258593  1.11332476 -3.33623838] [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J.\n",
      "          > [ 1.05403137  1.10873067 -3.31420755] [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol.\n",
      "          > [ 0.37922683  0.20853752 -2.38860059] 202, 2023, pp.\n",
      "          > [ 0.03757477  0.27275848 -3.57739258] 39 252â€“39 276.\n",
      "          > [ 9.42245051e-02 -2.36130692e-03 -3.27459335e+00] [34] N. Halko, P.-G. Martinsson, and J.\n",
      "      > [-0.01777313  0.86837953 -3.23414993] A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp.\n",
      "        > [ 0.08819249  0.45794398 -3.10998869] A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol. 29, 2016.\n",
      "          > [ 0.49316403  0.43825203 -3.27513361] A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol.\n",
      "          > [-0.51695251 -0.64958191 -3.15704346] 53, no. 2, pp. 217â€“288, 2011.\n",
      "          > [-0.22828132  0.45701021 -3.17533565] [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol.\n",
      "          > [ 0.25403419 -0.15308884 -3.91349435] 29, 2016.\n",
      "        > [-0.04012275  0.95252347 -3.16278243] [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp.\n",
      "          > [ 0.03354191  0.06738422 -3.27845573] [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y .\n",
      "          > [-0.15834357  0.96680647 -3.61774015] Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp.\n",
      "          > [ 0.07930541 -0.53768116 -3.63393903] 505â€“514.\n",
      "          > [ 0.4234263   1.36419749 -3.26857471] [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp.\n",
      "        > [ 0.32123625  1.10508442 -3.75702333] 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034.\n",
      "          > [ 0.64876765 -0.44767898 -3.54583478] 308â€“318.\n",
      "          > [ 0.46753228  0.95556754 -3.88604355] [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol.\n",
      "          > [ 0.879776    0.30902863 -3.28076172] 30, 2017, pp.\n",
      "          > [ 0.38951111 -0.26905823 -3.76245117] 1024â€“1034.\n",
      "        > [ 0.13695791  0.66218466 -3.72894788] [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp.\n",
      "          > [ 0.16555667  0.68151617 -3.26451492] [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022.\n",
      "          > [-0.20439328 -0.18509746 -3.3215239 ] [40] H. Tong, C. Faloutsos, and J.-Y .\n",
      "          > [ 0.14624281  0.72590381 -4.47727299] Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06).\n",
      "          > [-0.02382278  0.98406982 -3.65454102] IEEE, 2006, pp.\n",
      "    > [ 0.53387612  1.79757917 -3.19454789] 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab. (LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol.\n",
      "      > [-0.00944829  0.97411305 -2.77201629] 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226.\n",
      "        > [ 0.09912057  1.2653476  -3.12121296] 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp.\n",
      "          > [-0.20471573  0.0320282  -3.45690918] 613â€“622.\n",
      "          > [ 0.04813793  1.35476482 -3.1036067 ] [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp.\n",
      "          > [-0.05824189 -0.18987888 -3.62937093] 271â€“ 279.\n",
      "          > [ 0.61468828  0.65607822 -3.49472451] [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp.\n",
      "        > [-0.2281158   0.83968979 -3.46043444] 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176.\n",
      "          > [-0.08283234  0.08981991 -3.76330566] 1392â€“ 1402.\n",
      "          > [-0.02576863  0.6846686  -3.48036718] [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 .\n",
      "          > [-0.43566895  0.15791702 -3.40258789] Springer, 2015, pp.\n",
      "          > [-0.07416721  0.38260478 -3.98535204] 164â€“176.\n",
      "        > [ 0.72251058 -0.38011912 -3.61737061] [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y .\n",
      "          > [ 0.85324007 -0.25434768 -3.84788847] [44] R. J. Muirhead, Aspects of multivariate statistical theory .\n",
      "          > [-0.29891381  0.32695904 -3.31364894] John Wiley & Sons, 2009.\n",
      "          > [ 0.47715542 -0.10065159 -4.05197525] [45] Y .\n",
      "          > [ 0.15646626 -0.26889935 -3.24112344] Ma, X. Liu, T. Zhao, Y .\n",
      "        > [ 0.3868725   1.04564774 -2.60850859] Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226.\n",
      "          > [ 0.67818451  0.74929309 -2.82509518] Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp.\n",
      "          > [ 0.3529892   0.79602051 -3.41113281] 1202â€“1211.\n",
      "          > [-0.18113115  1.52943897 -2.44897723] 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp.\n",
      "          > [ 0.6340332   0.24146461 -3.35742188] 1215â€“1226.\n",
      "      > [ 0.19284745  1.33676386 -2.6753509 ] [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp.\n",
      "        > [ 0.33868852  1.40608752 -3.11124301] [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008.\n",
      "          > [ 0.42105338  1.62364292 -3.18042731] [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp.\n",
      "          > [-0.47509766 -0.02072144 -3.45996094] 2464â€“2473.\n",
      "          > [ 0.06451739  0.75687468 -3.10440922] [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol.\n",
      "          > [ 0.19174004 -0.14296627 -2.97338867] 29, no. 3, pp. 93â€“93, 2008.\n",
      "        > [ 0.29387248  0.82570922 -3.03846002] [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739.\n",
      "          > [ 0.68604523  0.44518715 -3.11612177] [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp.\n",
      "          > [-0.45772552  0.14134979 -3.421875  ] 817â€“826.\n",
      "          > [ 0.17872912  1.25829864 -2.9351759 ] [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp.\n",
      "          > [ 0.60068321 -0.55285645 -3.72338867] 731â€“739.\n",
      "        > [ 0.33640435  1.20935678 -3.12359023] [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V .\n",
      "          > [-1.21990848 -0.59434313 -3.53237367] [51] W. Hu, M. Fey, M. Zitnik, Y .\n",
      "          > [ 0.474594    0.96438837 -3.23215294] Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol.\n",
      "          > [-0.07642215 -0.12218636 -2.87986827] 33, pp. 22 118â€“22 133, 2020.\n",
      "          > [-0.19326988  0.29382896 -3.18437004] [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V .\n",
      "        > [ 0.16818975  1.19333017 -2.79329586] K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp.\n",
      "          > [ 0.33356899  0.90108806 -3.00344896] K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023.\n",
      "          > [ 0.24835177  0.00721778 -3.00664258] [53] W.-L. Chiang, X. Liu, S. Si, Y .\n",
      "          > [ 0.04568337  0.29665256 -2.44298172] Li, S. Bengio, and C.-J.\n",
      "          > [ 0.01527928  1.45644653 -3.28346968] Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp.\n",
      "      > [-0.12365283  1.19631886 -3.17378402] 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol.\n",
      "        > [-0.16893667  0.27662003 -3.37465954] 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp.\n",
      "          > [ 0.55664158  0.7081598  -3.87609053] 257â€“266.\n",
      "          > [-0.33309081  0.38024646 -3.23409438] [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp.\n",
      "          > [-0.46895504 -0.17524385 -3.38903809] 556â€“559.\n",
      "          > [ 0.53501213  0.18925497 -3.54369974] [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp.\n",
      "        > [-0.77028865  0.9707129  -3.36159468] 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1.\n",
      "          > [-0.36174011 -0.66043091 -3.56994629] 538â€“ 543.\n",
      "          > [-0.880669    1.24699843 -3.65754461] [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010.\n",
      "          > [ 0.45615071  0.1766877  -2.92007971] [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol.\n",
      "          > [-0.24651277 -0.28217244 -3.25200844] 4, no. 1, 2012, p. 1.\n",
      "        > [ 0.49078226  1.34924769 -2.84371591] [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y .\n",
      "          > [ 0.21228598 -0.1727457  -3.69412422] [58] C.-C. Hsu, Y .-A.\n",
      "          > [ 0.4964388   1.1379323  -3.26327252] Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017.\n",
      "          > [ 0.27653149  1.22203171 -3.11857891] [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016.\n",
      "          > [-0.23513104  0.10009164 -3.3169713 ] [60] R. Yang, J. Shi, X. Xiao, Y .\n",
      "        > [ 0.85990584  2.39996338 -3.5583055 ] Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol.\n",
      "          > [ 1.29348958  2.65600014 -3.51772881] Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol.\n",
      "          > [-0.6532802  -0.13573647 -3.29376221] 14, no. 1, pp. 37â€“49, 2020.\n",
      "          > [-0.22873405  0.28988045 -3.39781761] [61] R. Yang, J. Shi, X. Xiao, Y .\n",
      "          > [ 0.30006897  1.45296168 -3.45902967] Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol.\n",
      "      > [ 0.72318339  1.68967235 -3.30099344] 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab. (LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol.\n",
      "        > [ 0.64173239  1.34182549 -3.25570083] 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018.\n",
      "          > [ 0.12200884  0.63838339 -2.99780035] 32, pp. 1237â€“1262, 2023.\n",
      "          > [-0.29292744  0.24863674 -3.65135193] [62] G. Pan, Y .\n",
      "          > [ 0.34932357  1.45216393 -3.35869741] Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021.\n",
      "          > [ 0.48638749  0.49217737 -3.70286751] [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018.\n",
      "        > [ 0.35524696  1.1163801  -3.36980677] [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab. (LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol.\n",
      "          > [ 0.00811289  1.43765509 -3.39664173] [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab.\n",
      "          > [ 0.79001796  0.75723928 -3.72958589] (LANL), Los Alamos, NM (United States), Tech.\n",
      "          > [ 0.29361501  0.34136909 -3.27236843] Rep., 2008.\n",
      "          > [ 1.07881546  0.64106369 -3.6930542 ] [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol.\n",
      "        > [ 1.08213329  1.57144582 -3.64936805] 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol.\n",
      "          > [ 0.54940999 -0.07968718 -2.91709471] 104, no.\n",
      "          > [ 0.78576601 -0.29992518 -2.82167816] 50, pp. 19 735â€“19 740, 2007.\n",
      "          > [ 0.16149274  0.32171741 -4.10592031] [66] Y .\n",
      "          > [ 0.95375878  1.65800405 -3.72314072] Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol.\n",
      "        > [ 0.5119738   1.678092   -3.42067933] 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol.\n",
      "          > [ 0.54730982  0.84374374 -3.35440445] 9. Association for Computing Machinery (ACM), 2016, pp.\n",
      "          > [ 0.17454529  0.35948181 -3.84277344] 1233â€“1244.\n",
      "          > [ 0.30666927  0.08891109 -4.11654854] [67] Y .\n",
      "          > [ 0.47470373  1.92758727 -3.51830101] Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol.\n",
      "    > [ 0.75226337  1.41805172 -2.57206583] 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      "      > [ 0.34962225  1.29501653 -3.24991989] 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V .\n",
      "        > [ 0.09774137  1.43625021 -3.51017785] 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol.\n",
      "          > [-0.41961712  0.07066396 -3.44709373] 31, no. 11, pp. 2093â€“2107, 2018.\n",
      "          > [ 0.18409555  0.06020011 -3.94798088] [68] Y .\n",
      "          > [ 1.23191071  0.37649205 -3.85348773] Fang, Y .\n",
      "          > [ 0.25177246  1.7558254  -3.53291202] Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol.\n",
      "        > [ 0.66311342  0.98115551 -3.45683813] 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp.\n",
      "          > [ 0.68704718  0.73115742 -3.52272677] 13, no. 6. VLDB Endowment, 2020, pp.\n",
      "          > [ 0.29721832  0.47351837 -3.5592041 ] 854â€“867.\n",
      "          > [ 0.38777217 -1.10024488 -3.59576106] [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y .\n",
      "          > [ 0.1640002   1.3971591  -3.39301085] Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp.\n",
      "        > [ 0.47042045  0.47858348 -3.73951387] 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y .\n",
      "          > [ 0.39620972 -0.6230011  -3.69335938] 1445â€“1454.\n",
      "          > [ 0.48740086  0.40379554 -3.53803396] [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol.\n",
      "          > [-0.23284747  0.36926654 -2.63298059] 10, pp. 110 701â€“110 726, 2022.\n",
      "          > [-0.13674513 -0.24639551 -3.34894896] [71] R. Andersen, S. O. Gharan, Y .\n",
      "        > [ 0.066083    1.10685194 -3.17932129] Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V .\n",
      "          > [ 0.31390134  1.49271822 -3.14981151] Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol.\n",
      "          > [-0.45283508 -0.08031178 -3.24621582] 63, no. 2, pp. 1â€“31, 2016.\n",
      "          > [ 0.20170468  0.7174114  -3.17114019] [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023.\n",
      "          > [ 0.65410972 -0.02719082 -4.05499458] [73] X. Huang, L. V .\n",
      "      > [ 0.30587927  2.20627999 -3.16847587] Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y .\n",
      "        > [ 0.30186495  1.97586524 -3.19191694] Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454.\n",
      "          > [ 0.36928213  2.05331707 -2.95873547] Lakshmanan, and J. Xu, Community Search over Big Graphs.\n",
      "          > [ 0.13824469 -0.38714069 -3.9348805 ] Morgan & Claypool Publishers, 2019.\n",
      "          > [ 0.22529955  1.93478227 -3.37100315] [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp.\n",
      "          > [ 0.25158501 -0.72988892 -3.69262695] 1451â€“1454.\n",
      "        > [ 0.3398644   2.12641382 -3.01080823] [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol.\n",
      "          > [-0.26396263  1.64226699 -3.00827146] [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010.\n",
      "          > [ 0.44102132 -0.29273957 -3.85566664] [76] W. Cui, Y .\n",
      "          > [ 0.30311868  1.74460399 -3.42403841] Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014.\n",
      "          > [ 0.68202889  1.53134847 -3.33327746] [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol.\n",
      "        > [ 0.13091762  1.22519553 -3.46615171] 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol.\n",
      "          > [-0.43376672 -0.09749819 -2.76139545] 29, pp. 1406â€“1433, 2015.\n",
      "          > [ 0.18285553  1.74080443 -3.74891067] [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014.\n",
      "          > [ 0.56249988  0.20313583 -3.95854568] [79] X. Huang, L. V .\n",
      "          > [-0.04621084  0.69376558 -3.42363024] S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol.\n",
      "        > [ 0.13865034  0.57086635 -2.86998606] 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y .\n",
      "          > [ 0.28120154  0.76493979 -3.12023473] 9. Association for Computing Machinery (ACM), 2015, pp.\n",
      "          > [-0.29375359  0.66305864 -3.54116154] 276â€“287.\n",
      "          > [-0.05504864  0.7187264  -3.55452871] [80] Q. Liu, Y .\n",
      "          > [-0.34022152 -0.51225036 -3.28065252] Zhu, M. Zhao, X. Huang, J. Xu, and Y .\n",
      "      > [ 0.68407631  1.3850404  -3.27230024] Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786.\n",
      "        > [ 0.86155683  1.46614814 -3.84346247] Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol.\n",
      "          > [ 0.37518686  1.47749925 -3.94033718] Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020.\n",
      "          > [ 0.07261854  0.55221057 -3.75244331] [81] Y .\n",
      "          > [ 0.96258914 -0.2268572  -3.32938671] Fang, R. Cheng, Y .\n",
      "          > [ 0.76503748  0.71249622 -3.53755546] Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol.\n",
      "        > [ 0.71216112  1.12295711 -3.17344236] 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp.\n",
      "          > [-0.63493276  0.05278307 -3.49433231] 26, pp. 803â€“828, 2017.\n",
      "          > [ 0.28534353 -0.22402608 -4.12258291] [82] X. Huang and L. V .\n",
      "          > [ 0.79640651  0.82320613 -3.22748566] S. Lakshmanan, â€œAttribute-driven community search,â€ vol.\n",
      "          > [ 0.33720803  0.74779743 -3.63678789] 10. Association for Computing Machinery (ACM), 2017, pp.\n",
      "        > [ 0.28437775  0.81361389 -3.00088739] 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996.\n",
      "          > [ 0.27715701  0.31857982 -4.16564894] 949â€“960.\n",
      "          > [ 0.30454275  0.21258906 -3.71767211] [83] Y .\n",
      "          > [ 0.00380794  0.80094624 -3.06076431] Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020.\n",
      "          > [ 0.72129291  0.38385111 -3.42399263] [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996.\n",
      "        > [ 0.53502047  0.19230726 -3.12066436] [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786.\n",
      "          > [ 1.00492513  0.29714212 -3.20596385] [85] G. Strang, Introduction to linear algebra .\n",
      "          > [-0.6623174   0.21148479 -3.56864142] SIAM, 2022.\n",
      "          > [ 0.34225377  0.48153615 -3.20968843] [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp.\n",
      "          > [-0.99359131 -0.2655735  -3.22167969] 2778â€“2786.\n",
      "      > [ 0.60264534  1.00644159 -3.25741625] [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      "        > [ 1.39548922 -0.40999791 -3.44000483] [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297.\n",
      "          > [ 0.77503538  0.58371657 -3.97545958] [87] J. MacQueen et al.\n",
      "          > [ 1.2371943  -0.49116063 -3.58205128] , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol.\n",
      "          > [ 1.00824988  1.08575416 -3.99647427] 1, no. 14. Oakland, CA, USA, 1967, pp.\n",
      "          > [ 0.47818652  0.30509588 -3.74711204] 281â€“297.\n",
      "        > [ 0.11848589  1.17026031 -3.40755486] [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp.\n",
      "          > [ 0.57514191  0.97147751 -3.68041992] [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012.\n",
      "          > [ 0.00899615 -0.11822467 -3.37601423] [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y .\n",
      "          > [-0.13072282  1.32642353 -3.6455245 ] Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence .\n",
      "          > [-0.72211576  1.93572092 -2.64110804] International Joint Conferences on Artificial Intelligence Organization, 2015, pp.\n",
      "        > [ 0.46252495  1.3796072  -3.24372506] 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp.\n",
      "          > [-0.36370897  0.61727905 -3.67578125] 2111â€“2117.\n",
      "          > [ 0.66210049  1.16033685 -3.70462728] [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol.\n",
      "          > [-0.42225012  0.73686594 -3.68352532] Not available.\n",
      "          > [ 0.40741357  0.61903125 -3.6915679 ] Society for Industrial and Applied Mathematics, 2017, pp.\n",
      "        > [ 0.33397266  1.45576608 -3.22466969] 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      "          > [ 0.329422    0.69476318 -3.65380859] 633â€“641.\n",
      "          > [ 0.44029069  1.13461781 -3.26409721] [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019.\n",
      "          > [ 0.28641173  1.18676579 -3.02675962] [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      "          > [ 0.15796554  1.61540616 -3.54741955] [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      "    > [ 1.35109174  2.29778504 -2.13137174] [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0.\n",
      "      > [ 0.64134699  0.7009483  -2.69080901] [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi).\n",
      "        > [-0.07255742  0.49757093 -3.28148532] [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119.\n",
      "          > [-0.43363106  0.03376308 -3.45837212] [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J.\n",
      "          > [ 0.2307055   0.18838003 -3.5014236 ] Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol.\n",
      "          > [ 0.13253784  0.35259247 -2.83630371] 26, 2013, pp.\n",
      "          > [-0.37836456 -0.0859642  -3.72875977] 3111â€“3119.\n",
      "        > [ 0.91962636 -0.23063408 -2.69385815] 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“.\n",
      "          > [ 0.22172928  0.22666931 -3.09960938] 14APPENDIX A.\n",
      "          > [ 1.34915161  0.70385742 -3.6875    ] Theoretical Proofs 1) Proof of Theorem IV .1: Proof.\n",
      "          > [ 1.07445478 -0.87175149 -2.95042419] We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration.\n",
      "          > [ 0.5064677   0.08618581 -2.63732338] Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“.\n",
      "        > [-0.04627847  0.87274134 -2.60772038] (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· .\n",
      "          > [-0.38810086  0.07859719 -3.14276624] (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, .\n",
      "          > [ 0.10201441  0.72820657 -2.84401846] . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, .\n",
      "          > [ 1.28117156 -0.1087402  -2.66457915] . . ,âˆ’ â†’Î³ âˆž}.\n",
      "          > [ 0.41792011  0.87533158 -2.97667813] By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· .\n",
      "        > [ 0.23323537  0.57787728 -1.99708915] Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi).\n",
      "          > [ 1.35127139  0.1327365  -2.50097585] Then, Eq.\n",
      "          > [ 0.614384    0.37190717 -2.41522288] (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“.\n",
      "          > [ 0.49144098  0.52612507 -2.86269665] (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5).\n",
      "          > [-0.03578183  0.6234141  -2.83435082] Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi).\n",
      "      > [ 0.72604275  0.32058072 -2.35678363] Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1.\n",
      "        > [ 0.0048353   0.57274497 -2.34328938] Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq.\n",
      "          > [ 0.45645106  0.33195654 -3.77733088] Together with Eq.\n",
      "          > [ 0.89585364 -0.14563784 -2.56059718] (22) and the matrix definition of Ï€(vi, vj) defined in Eq.\n",
      "          > [-0.27710852  0.72523648 -3.0757637 ] (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt).\n",
      "          > [-0.16603044  1.00898612 -2.82301641] By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq.\n",
      "        > [ 1.04441357  2.25214624 -2.92848897] (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“.\n",
      "          > [ 0.59746766  2.95010138 -3.85605478] (14). In what follows, we analyze the time complexity of Algo.\n",
      "          > [ 0.6469906   0.94136161 -3.51359272] (1). First, suppose that Lines 3-7 in Algo.\n",
      "          > [ 0.68840766 -0.86460751 -3.02397656] 1 are executed for L iterations.\n",
      "          > [ 0.86790532  0.26624593 -2.57242155] Now, we consider any iteration â„“.\n",
      "        > [ 0.30370745  0.1655094  -3.1515975 ] For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction.\n",
      "          > [ 0.7974053  -0.56874955 -3.25100183] For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively.\n",
      "          > [-0.67233795  0.79029506 -3.10377693] Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ.\n",
      "          > [ 9.33843195e-01  2.06824392e-03 -2.98551726e+00] First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1.\n",
      "          > [-0.08112752  1.45935619 -3.94573545] (23) We prove this by induction.\n",
      "        > [ 0.69644099  0.3361088  -2.8804822 ] For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1.\n",
      "          > [ 1.34085393  0.34132603 -2.71082473] For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq.\n",
      "          > [ 0.13730213  0.61058861 -4.15443897] (23) holds.\n",
      "          > [ 1.39426279  0.18342291 -3.1353972 ] Next, we assume Eq.\n",
      "          > [ 0.55361235  0.07500093 -3.36541367] (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1.\n",
      "      > [ 1.23550534  1.62751389 -2.17189646] According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1.\n",
      "        > [ 0.52100992  0.81196404 -2.33454633] According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq.\n",
      "          > [ 0.55977976  0.75962293 -2.72763562] According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P.\n",
      "          > [ 0.61193722  0.66191339 -3.14451361] As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1.\n",
      "          > [ 0.40950945  0.59943604 -2.88310814] (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq.\n",
      "          > [ 1.02606082  0.41711336 -3.1940794 ] (24) equals 0, namely, Eq.\n",
      "        > [ 0.75653493  0.55339652 -2.77341652] (23) still holds. As per Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq.\n",
      "          > [ 0.94515991  0.29795456 -4.18286133] (23) still holds.\n",
      "          > [ 0.52238625  0.0564643  -3.06966925] As per Line 6, in each â„“-th iteration, Algo.\n",
      "          > [ 0.61275917  0.12108608 -2.86251664] 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“.\n",
      "          > [ 1.41183782 -0.17711437 -2.48396373] After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq.\n",
      "        > [ 0.37638521  1.19438279 -2.25198245] (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned.\n",
      "          > [ 0.31706047  0.83178711 -3.78991699] (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1.\n",
      "          > [-0.01077236  1.03209579 -2.51851749] We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ .\n",
      "          > [ 0.03332167  0.56553495 -2.65238476] Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P.\n",
      "          > [ 1.32783604  1.48643231 -2.43904543] Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned.\n",
      "        > [ 1.00031519  1.47690785 -2.70633602] In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1.\n",
      "          > [ 0.35460752  0.47515243 -2.58640337] In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“.\n",
      "          > [ 1.65539145  1.42722201 -2.28767419] Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well.\n",
      "          > [ 0.52182621  1.26947522 -3.025177  ] As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) .\n",
      "          > [ 1.13576365  0.47719458 -2.73530412] In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1.\n",
      "      > [ 1.21686935  2.65872002 -2.729568  ] Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0.\n",
      "        > [ 0.75879729  2.80988669 -3.15631056] Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof.\n",
      "          > [ 0.52708024  2.84500837 -3.56569815] Hence, the overall time complexity of Algo.\n",
      "          > [-0.03373285  1.33613396 -3.00260472] 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 .\n",
      "          > [ 2.09450674  1.58798134 -3.80266261] The theorem is proved.\n",
      "          > [ 1.14259279  0.24493892 -3.12182426] 152) Proof of Theorem IV .2: Proof.\n",
      "        > [ 1.60872233 -0.21629137 -2.57282686] Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6.\n",
      "          > [ 1.50748444 -0.12192726 -2.56594849] Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo.\n",
      "          > [ 0.18591213 -0.63814163 -3.62634277] 2. By Lines 6 and Lines 8-11, Eq.\n",
      "          > [ 1.20977783 -0.3196106  -4.22460938] (22) still holds.\n",
      "          > [ 1.34344625 -0.22638984 -2.63040209] Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6.\n",
      "        > [ 0.69157666  2.05948138 -2.87065816] As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo.\n",
      "          > [ 0.44739056  0.45227814 -2.55996704] As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq.\n",
      "          > [ 1.33164215  0.06759644 -3.09375   ] (14) follows as in the proof of Theorem IV .1.\n",
      "          > [ 0.61732948  2.8415761  -3.51119304] Next, we analyze the time complexity of Algo.\n",
      "          > [ 0.79255062  0.35190538 -3.36121368] 2. Notice that according to Line 4 in Algo.\n",
      "        > [ 1.06493866  1.76440847 -2.87859178] (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0.\n",
      "          > [ 0.83894336  1.56364131 -3.12245393] (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 .\n",
      "          > [ 0.34085816  1.41836524 -3.30760884] As for greedy operations (Lines 8-11), they will be conducted as in Algo.\n",
      "          > [ 0.86016482  0.78125989 -3.00096083] (1). Note that Algo.\n",
      "          > [ 1.41821241 -0.50606132 -2.37108445] 2 also terminates when âˆ’ â†’Î³ is 0.\n",
      "  > [ 1.09270895  1.30160093 -2.66840649] Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n",
      "    > [ 0.72970587  0.71107984 -1.56995893] Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq.\n",
      "      > [ 0.99800676  1.97315073 -2.86735892] Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ.\n",
      "        > [ 1.02753007  2.29556036 -3.00187731] Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof.\n",
      "          > [ 1.16125202  2.02263403 -3.48317552] Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1).\n",
      "          > [ 0.5467847   2.83882499 -3.16068649] In sum, the overall complexity of Algo.\n",
      "          > [ 0.15271211  1.18968594 -3.0404892 ] 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 .\n",
      "          > [ 1.12632179  0.64270568 -2.97219086] 3) Proof of Lemma IV .3: Proof.\n",
      "        > [ 0.5110755   0.00906449 -2.85649157] We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, .\n",
      "          > [ 1.55226576  0.62605387 -2.97598124] We assume Algo.\n",
      "          > [-0.16586125 -0.61748087 -3.49983215] 2 conducts Lines 3-11 for L it- erations.\n",
      "          > [ 1.07049704 -0.03462601 -3.26499033] and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1.\n",
      "          > [-0.31168321  0.01381164 -3.15557575] Further, we assume that in â„“1-th, â„“2-th, .\n",
      "        > [ 0.5172801   0.29540607 -2.90850329] . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, .\n",
      "          > [ 0.87626362 -0.16863917 -3.2826457 ] . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11.\n",
      "          > [-0.25520518  0.36238778 -3.38165212] Based on Inequal- ity (25) and Eq.\n",
      "          > [-0.3200244   0.7030983  -3.19066978] (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ .\n",
      "          > [ 0.49092817 -0.35382888 -2.63897085] Let L = {1, 2, .\n",
      "        > [-0.07562731  0.22870998 -2.73660088] . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ.\n",
      "          > [-0.12659726 -0.13588747 -3.13111806] . . , L} and T = {â„“1, â„“2, .\n",
      "          > [ 0.68817049  0.42337543 -2.91242838] . . , â„“T }. As for the L \\ Titerations, Algo.\n",
      "          > [-0.3375563  -0.69242162 -4.24808979] (2) conducts Lines 5-6.\n",
      "          > [ 0.04889941  0.31701601 -2.80298114] Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ.\n",
      "      > [ 0.56912935  0.6916855  -1.84872377] According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof.\n",
      "        > [ 0.31665629  0.30356866 -2.95139265] According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof.\n",
      "          > [ 0.34142691 -0.09624711 -2.84726334] According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1.\n",
      "          > [ 0.25975275  0.64843017 -3.07108283] Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ.\n",
      "          > [ 0.51986927 -0.23509961 -3.18650532] When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set.\n",
      "          > [ 0.47734708  1.12203288 -2.56502008] The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof.\n",
      "        > [ 0.73783392  0.66337395 -3.68348169] 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X.\n",
      "          > [ 0.95587164  0.68380857 -3.32250595] 4) Proof of Lemma V .1: Proof.\n",
      "          > [ 0.65024424  1.44409621 -3.42291546] We first need the following theorem.\n",
      "          > [ 0.51696241  0.53970087 -3.88153553] Theorem A.1 (Eckartâ€“Young Theorem [84]).\n",
      "          > [ 0.80156845  0.59375226 -3.26122618] Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X.\n",
      "        > [ 0.36861116  0.9693529  -2.34631705] Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative.\n",
      "          > [ 0.29432854  0.47730035 -2.76787066] Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤.\n",
      "          > [ 0.1407977   1.05198109 -2.31329966] Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤.\n",
      "          > [ 0.1187899   0.30667013 -2.78979206] According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤.\n",
      "          > [ 0.90710104  0.31995255 -2.53899956] Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative.\n",
      "        > [ 0.12654503  0.63635409 -2.45401144] Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof.\n",
      "          > [-0.10047781  0.81743526 -2.38058019] Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1.\n",
      "          > [ 0.02946187  0.43079329 -2.69428325] Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤.\n",
      "          > [ 1.02690482  0.71442544 -2.66323328] Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done.\n",
      "          > [ 1.87538266  0.66936982 -3.6308198 ] 5) Proof of Theorem V .2: Proof.\n",
      "      > [ 1.21565962  0.82270169 -2.85656738] Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo.\n",
      "        > [ 0.80587971 -0.39564258 -2.64824557] Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ !\n",
      "          > [ 1.17827344 -0.25070339 -2.72414351] Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1.\n",
      "          > [-0.05632605 -0.63348514 -2.62215519] Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4].\n",
      "          > [ 0.83326948 -0.23769081 -3.32303357] On its basis, f(vi, vj) in Eq.\n",
      "          > [ 0.53721255  0.34545699 -3.45236778] (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ !\n",
      "        > [ 1.35799527  0.01917798 -2.67715859] = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq.\n",
      "          > [ 1.30190516  0.34699446 -3.09500146] = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 .\n",
      "          > [ 0.93982458 -0.01668661 -2.60886192] (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)).\n",
      "          > [ 0.90998572  0.16957954 -3.12666273] Plugging the definitions of f(vi, vj) in Eq.\n",
      "          > [ 0.13282827 -0.07224442 -3.82330012] (26) and Y in Eq.\n",
      "        > [ 1.41796076  1.49023151 -3.19844532] (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq.\n",
      "          > [ 1.61850071  1.06869042 -3.67086744] (19) into the above equation proves the theorem.\n",
      "          > [ 1.22476172  1.09563279 -2.9769094 ] 166) Proof of Lemma V .3: Proof.\n",
      "          > [ 0.69893324  0.74761832 -3.76424813] According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time.\n",
      "          > [ 1.7512908   1.39222407 -2.71938181] Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq.\n",
      "        > [ 0.86247492  0.37884119 -3.35761619] (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo.\n",
      "          > [-0.10964227 -0.1264789  -3.58256721] (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time.\n",
      "          > [ 1.02817297 -0.25867403 -3.34313416] Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time.\n",
      "          > [ 1.48976409 -0.09270306 -3.31535292] In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively.\n",
      "          > [ 0.52213997  2.70393682 -3.61336136] In turn, the overall time complexity of Algo.\n",
      "      > [ 0.78492844  1.01824975 -1.84450126] 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq.\n",
      "        > [ 1.19523048  0.77044511 -2.48142242] 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo.\n",
      "          > [ 1.02817237  0.71658456 -2.53152299] 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant.\n",
      "          > [ 1.81566501  1.0373112  -3.68985271] 7) Proof of Theorem V .4: Proof.\n",
      "          > [ 0.80987138 -0.238584   -3.01957798] Let âˆ’ â†’Ï â—¦ be the vector returned by Algo.\n",
      "          > [ 0.22676198  0.14938064 -3.43616676] 2 invoked at Line 5 in Algo.\n",
      "        > [ 0.22874178  0.29738668 -2.45403886] 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo.\n",
      "          > [ 0.53505123  0.11516577 -2.74012852] 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ.\n",
      "          > [-0.00509164  0.20868139 -2.46440887] By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ.\n",
      "          > [-0.17097552  0.4339616  -2.62776089] Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ.\n",
      "          > [ 1.1177063   0.39913294 -2.97451091] Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo.\n",
      "        > [ 0.56850767  0.50494486 -2.73581123] 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem.\n",
      "          > [ 0.83760977  0.15100512 -2.99487019] 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2.\n",
      "          > [ 0.1346145   0.38343358 -2.75279522] We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj).\n",
      "          > [ 0.92063344  0.66361177 -2.63755345] Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi).\n",
      "          > [ 0.02129052  0.44631279 -3.12651634] Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem.\n",
      "        > [ 0.87422603  0.54785502 -2.73845172] 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq.\n",
      "          > [ 0.97990382  0.66762376 -3.09006882] 8) Proof of Lemma V .6: Proof.\n",
      "          > [ 0.9645915   0.41222441 -3.07287431] By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦.\n",
      "          > [ 0.4508611   0.28146803 -2.65840816] (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ .\n",
      "          > [ 0.68947512  1.16319025 -3.5562458 ] Plugging it into Eq.\n",
      "    > [ 1.75447631  0.72275937 -2.6103301 ] (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88].\n",
      "      > [ 1.77390981  0.0798886  -3.04203796] (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters.\n",
      "        > [ 0.91527557  0.71616614 -3.36128569] (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs.\n",
      "          > [ 1.21348703  1.02482295 -3.74014902] (27) completes the proof.\n",
      "          > [ 0.84741569  0.29437777 -3.28766346] B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors.\n",
      "          > [ 0.6605919   0.45982721 -2.95897388] For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed.\n",
      "          > [ 0.67150879  0.82582092 -3.42102051] Varying Î±. Figs.\n",
      "        > [ 1.54269028  0.44956747 -3.16224599] 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8.\n",
      "          > [ 1.40468121  0.27530533 -3.33887339] 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1.\n",
      "          > [ 1.10529149 -0.34538236 -3.42677331] It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±.\n",
      "          > [ 1.60283899  0.47673827 -3.43886757] That is, the precision scores increase conspicuously with Î± increasing.\n",
      "          > [ 0.90536588  1.03042066 -2.97502613] The only exception is on BlogCL, where the best result is attained when Î± = 0.8.\n",
      "        > [ 0.99516106  1.45359004 -3.15280867] Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse.\n",
      "          > [ 0.84606934  1.2666626  -3.85253906] Recall that in Algo.\n",
      "          > [ 0.89096516  0.68743736 -3.61303759] 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality.\n",
      "          > [ 0.88098508  0.20495838 -3.25670385] Varying Ïƒ.\n",
      "          > [ 0.8539362   1.10521829 -3.40749931] Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse.\n",
      "        > [ 1.94214737  0.78944361 -3.53105235] Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters.\n",
      "          > [ 1.3533783   1.58163357 -3.29522705] Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1.\n",
      "          > [ 1.80133748  0.23830324 -3.281106  ] Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets.\n",
      "          > [ 1.51199639 -0.16628547 -3.58913326] We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig.\n",
      "          > [ 1.31067514  0.27315095 -3.7323904 ] 9: Precision when varying parameters.\n",
      "      > [ 1.6738615   0.2491456  -2.87142086] sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes.\n",
      "        > [ 1.4364624   0.80447012 -3.47327471] sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}.\n",
      "          > [ 1.05722141  0.56022751 -3.52702332] sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward.\n",
      "          > [ 1.21637011  1.36255944 -3.45759749] LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A.\n",
      "          > [ 0.63178253  1.02435303 -3.73706055] Varying k. Figs.\n",
      "          > [ 1.49888754 -0.03576803 -3.36497617] 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}.\n",
      "        > [ 1.25224352  0.95349509 -3.52586222] On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components.\n",
      "          > [ 1.23385644  0.36397964 -3.51028466] On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8.\n",
      "          > [ 0.51302797  1.44799995 -3.42768741] In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively.\n",
      "          > [ 1.02196264  1.77535987 -3.70944595] The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information.\n",
      "          > [ 1.04631555  0.29830191 -2.7824893 ] Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components.\n",
      "        > [ 1.18293464  0.49066135 -3.03382945] The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component.\n",
      "          > [ 0.06729899 -0.02775292 -3.22023344] The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32.\n",
      "          > [ 0.9668901   0.54100859 -2.67197633] 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq.\n",
      "          > [ 1.6191808   0.40757829 -3.85065603] (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively.\n",
      "          > [ 1.35265481  0.64896554 -3.20120072] Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component.\n",
      "        > [ 1.84263325  0.21195787 -3.17415142] Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes.\n",
      "          > [ 1.16352654  0.24691914 -3.72999763] Table VI reports the best precision scores attained by each method on 8 datasets.\n",
      "          > [ 1.94527149  0.84762388 -3.03525591] We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS.\n",
      "          > [ 1.55515075  0.75256252 -3.01654267] The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon.\n",
      "          > [ 0.71236652  0.01162583 -4.06177473] This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes.\n",
      "      > [ 1.07206559  1.20125639 -3.08660126] From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS.\n",
      "        > [ 1.6296097   0.97540742 -3.09335089] From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability.\n",
      "          > [ 1.29880393  1.11589575 -3.57637286] From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C.\n",
      "          > [ 1.3623178   0.53688949 -3.99414587] Consistent with the observations from Figs.\n",
      "          > [ 0.56258863  1.23643422 -3.68917036] 9(e) and 9(f), the k-SVD in Algo.\n",
      "          > [ 1.53028047 -0.01092611 -3.30554223] 3 improves the performance of LACA due to its denoising ability.\n",
      "        > [ 1.54671705  0.15965903 -2.80848122] TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs.\n",
      "          > [ 1.93490052  0.53631097 -3.14281559] TABLE VI: Ablation study.\n",
      "          > [ 0.56588745  0.02231598 -3.29418945] Darker shades indicate better results.\n",
      "          > [ 0.59818548  0.73049873 -3.58779645] Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors.\n",
      "          > [ 0.71966136 -0.11958014 -3.86580133] Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs.\n",
      "        > [ 0.73913503  0.49373645 -4.12441206] Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets.\n",
      "          > [ 0.58239359  0.54354537 -4.46923637] Intuitively, lower conductance and WCSS indicate a higher clustering quality.\n",
      "          > [ 0.55428797  0.06142577 -4.00235939] The conductance and WCSS values of all methods on the eight datasets are reported in Table VII.\n",
      "          > [ 0.82992542  0.17969641 -3.06287813] We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality.\n",
      "          > [ 0.66912222  0.94057363 -3.34232092] From Table VII, we can see that none of the evaluated methods perform best on all datasets.\n",
      "        > [ 0.93053329  1.064026   -3.64940381] But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS.\n",
      "          > [ 0.75103533  0.72040391 -3.58689451] But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity.\n",
      "          > [ 0.74256158  0.20421572 -4.21235037] Another observation is that compared to WCSS, conductance values by different methods vary markedly.\n",
      "          > [ 1.20870662  1.06614232 -3.90873575] This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance.\n",
      "          > [ 0.56737566  1.04157162 -3.78892684] 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS.\n",
      "      > [ 1.76236701  0.72181517 -2.60439086] Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88].\n",
      "        > [ 1.68086886  1.34219289 -2.52579618] Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B.\n",
      "          > [ 1.57093859  0.51716167 -2.46325922] Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig.\n",
      "          > [ 0.21155098  1.99300528 -3.72319078] 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig.\n",
      "          > [ 0.63130546  0.18251252 -3.81777692] 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively.\n",
      "          > [ 1.69061494  1.36665368 -3.12911725] Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B.\n",
      "        > [ 1.54781353  1.88151824 -3.7006135 ] Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo.\n",
      "          > [ 1.77760208  0.95791209 -3.89745021] Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes.\n",
      "          > [ 1.248191    1.61954725 -3.56776714] Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns.\n",
      "          > [ 1.14931524  2.42532682 -3.29533744] For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4.\n",
      "          > [ 0.46333939  1.50718153 -3.29803181] The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo.\n",
      "        > [ 0.74961799  1.50073266 -3.31384897] 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes.\n",
      "          > [-0.68334085  0.89102697 -3.57197475] 2 inefficient.\n",
      "          > [-0.07265732  1.29867756 -3.01055622] As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency.\n",
      "          > [ 1.16555476  1.55691266 -3.57822967] In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large.\n",
      "          > [ 0.4814584   0.53725982 -3.27538323] TABLE VIII: Statistics of Datasets without Attributes.\n",
      "        > [ 1.33208227  0.99436796 -3.23767352] Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88].\n",
      "          > [ 0.69879055  0.99490887 -3.92046261] Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth.\n",
      "          > [ 1.11532819  0.35866499 -3.71865559] Best is bolded and best baseline underlined .\n",
      "          > [ 1.26721275  0.96570641 -3.43587184] Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B.\n",
      "          > [ 0.72716063  0.8913064  -3.15282941] Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88].\n",
      "    > [ 1.03636241  1.29825556 -2.76014853] In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n",
      "      > [-0.15700844  0.47941008 -3.03450942] In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps.\n",
      "        > [ 0.70332259  1.59768248 -3.6234324 ] In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories.\n",
      "          > [ 0.77344215  1.09356952 -3.23961115] In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors.\n",
      "          > [ 0.20577691  0.71343434 -3.86753559] The ground- truth clusters are formed based on the publication venue.\n",
      "          > [ 0.09007151  2.08723903 -3.46653986] com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items.\n",
      "          > [ 0.50159204  1.20516491 -3.80021358] 19The ground-truth clusters are determined by product cate- gories.\n",
      "        > [ 0.33866808  0.62957364 -3.77324748] com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets.\n",
      "          > [-0.34213114  0.53670055 -3.70687509] com-Orkut is a social network where users establish friendships and join groups.\n",
      "          > [-0.11968482  1.34287786 -3.57625484] The groups created by users are considered the ground-truth clusters.\n",
      "          > [ 1.23381698  0.65223497 -3.66404152] The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX.\n",
      "          > [ 1.28406405  0.5754987  -3.59975123] It can be observed that LACA consistently delivers the best performance across all three datasets.\n",
      "        > [ 1.21587622  0.22355573 -2.6210444 ] Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures.\n",
      "          > [ 1.09210885  1.27211237 -3.51652384] Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively.\n",
      "          > [ 1.40071702  1.19849932 -3.44816232] On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%.\n",
      "          > [ 0.68200415  0.62000167 -3.34345746] The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations.\n",
      "          > [ 0.92417616  0.61704397 -2.27360916] Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures.\n",
      "        > [ 0.91631061  0.78115124 -3.36760688] Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps.\n",
      "          > [ 0.97379082  0.8445974  -3.49289751] Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested.\n",
      "          > [ 0.13032192  0.49862936 -3.34112525] Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps.\n",
      "          > [ 0.23216926  0.36029059 -3.28788376] For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj).\n",
      "          > [-0.17608783  0.54697031 -3.93199778] 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps.\n",
      "      > [ 1.1246742   1.1698451  -3.00808144] For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors.\n",
      "        > [ 0.12014078  0.572155   -3.4822135 ] For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD.\n",
      "          > [ 0.25977093  0.36296093 -3.28371119] For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj).\n",
      "          > [-0.03851635  0.50807989 -4.00798321] 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps.\n",
      "          > [ 0.29279923  0.36267018 -3.25446367] For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj).\n",
      "          > [ 0.58236206  0.12607548 -3.70994997] and have compared them against our original BDD.\n",
      "        > [ 1.13069797  0.94752228 -3.6797328 ] Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks.\n",
      "          > [ 0.99888897  0.54058057 -3.71176529] Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions.\n",
      "          > [ 0.64293468  0.42978689 -3.39394045] It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets.\n",
      "          > [ 1.40090287  0.92980456 -3.67650151] For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively.\n",
      "          > [ 0.4902513   1.43125927 -3.5596118 ] The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks.\n",
      "        > [ 1.34279692  0.21672994 -3.10973215] In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%.\n",
      "          > [ 1.52219427  0.21905389 -3.0295527 ] In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets.\n",
      "          > [ 0.8627184   0.5002538  -3.05335712] Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs.\n",
      "          > [ 0.67080963  0.68855023 -3.69090176] As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains.\n",
      "          > [ 1.90863252  0.20923078 -3.16227293] For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%.\n",
      "        > [ 0.71371126  1.40267205 -3.40364528] D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors.\n",
      "          > [ 0.88157707  1.41582882 -3.40385199] D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information.\n",
      "          > [ 0.21845046  1.94828379 -3.15094399] The obtained embeddings can be used in many downstream tasks, including graph clustering.\n",
      "          > [ 0.36799133  0.82562572 -3.65832114] ANE methods can be categorized into two types: factorization- based and learning-based.\n",
      "          > [ 0.80396235  0.30382177 -3.13118649] Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors.\n",
      "      > [ 0.19851846  0.78798032 -2.75470901] TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n",
      "        > [-0.08323112  0.75795418 -2.73817635] TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss.\n",
      "          > [-0.31157374  0.789361   -2.96389771] TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique.\n",
      "          > [ 0.69174004  1.53045034 -2.80860567] However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix.\n",
      "          > [ 0.36781853 -0.17182258 -3.36832809] Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories.\n",
      "          > [ 0.35397649  0.72722316 -3.13569045] Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss.\n",
      "        > [ 0.66051447  0.65522194 -2.91625929] Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n",
      "          > [ 0.32838699  1.05331981 -2.98008251] Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94].\n",
      "          > [ 0.58858091  0.20116812 -3.1958046 ] CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model.\n",
      "          > [ 0.78130078  0.79800248 -3.3121388 ] In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n"
     ]
    }
   ],
   "source": [
    "print_paper(papers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_color():\n",
    "    return (random.random(), random.random(), random.random())  # RGB tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64706"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_embeddings = np.stack([emb for paper in papers for emb in paper.embeddings])\n",
    "len(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Collect all embeddings and corresponding PDF labels\n",
    "all_embeddings = np.stack([emb for paper in papers for emb in paper.embeddings])\n",
    "labels = [paper.title for paper in papers for _ in paper.embeddings]\n",
    "\n",
    "# Reduce to 3D using PCA\n",
    "reducer = PCA(n_components=3)\n",
    "low_dim_embeddings = reducer.fit_transform(all_embeddings)\n",
    "\n",
    "color_map = {title: random_color() for title in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\n",
    "import mplcursors\n",
    "\n",
    "# Your 3D plotting code\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "start = 0\n",
    "scatter_plots = []\n",
    "for paper in papers:\n",
    "    num_points = len(paper.embeddings)\n",
    "    subset = low_dim_embeddings[start:start+num_points]\n",
    "    \n",
    "    # Scatter plot for each PDF in 3D\n",
    "    scatter = ax.scatter(subset[:, 0], subset[:, 1], subset[:, 2], \n",
    "                         color=color_map[paper.title], label=paper.title, alpha=0.6)\n",
    "    \n",
    "    # Add the scatter plot and corresponding text parts for hover\n",
    "    scatter_plots.append((scatter, paper.texts))  # Store the scatter and the corresponding text parts\n",
    "    \n",
    "    start += num_points\n",
    "\n",
    "# Add the legend and labels\n",
    "ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.1))\n",
    "ax.set_title(\"3D Embeddings Visualization\")\n",
    "ax.set_xlabel(\"UMAP Dim 1\")\n",
    "ax.set_ylabel(\"UMAP Dim 2\")\n",
    "ax.set_zlabel(\"UMAP Dim 3\")\n",
    "\n",
    "# Use mplcursors to display text on hover\n",
    "cursor = mplcursors.cursor(hover=True)\n",
    "\n",
    "# Define the callback to show and hide annotations\n",
    "@cursor.connect(\"add\")\n",
    "def on_add(sel):\n",
    "    # Get the index of the hovered point\n",
    "    index = sel.index\n",
    "    # Find the corresponding part of the text\n",
    "    part_text = paper.texts[index]\n",
    "    # Set the annotation text to the text part\n",
    "    sel.annotation.set_text(part_text)\n",
    "\n",
    "@cursor.connect(\"remove\")\n",
    "def on_remove(sel):\n",
    "    # Hide the annotation when the mouse moves off\n",
    "    sel.annotation.set_text(\"\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Prepare flat lists\n",
    "all_embeddings = []\n",
    "all_texts = []\n",
    "all_titles = []\n",
    "paper_indices = []  # map each embedding to its corpus index\n",
    "\n",
    "for paper_idx, paper in enumerate(papers):\n",
    "    all_embeddings.extend(paper.embeddings)\n",
    "    all_texts.extend(paper.texts)\n",
    "    all_titles.extend([paper.title] * len(paper.embeddings))\n",
    "    paper_indices.extend([paper_idx] * len(paper.embeddings))\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "# Fit NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=10, algorithm='auto').fit(all_embeddings)\n",
    "distances, indices = nbrs.kneighbors(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match (Distance: 10.00047):\n",
      "Paper 1:     2503.20698v1.MMMORRF__Multimodal_Multilingual_Modularized_Reciprocal_Rank_Fusion.pdf\n",
      "Paper 2:     2503.20630v1._Î²__GNN__A_Robust_Ensemble_Approach_Against_Graph_Structure_Perturbation.pdf\n",
      "Embedding 1: [-0.31322026  0.32337621 -2.7252872  -0.08644752  0.83922893]...\n",
      "Embedding 2: [ 0.25969315  0.25360304 -2.9244976  -0.30857641  1.14607215]...\n",
      "Text 1:      arXiv:1611.09268 [cs.CL] https://arxiv.org/abs/1611.09268 [4] Meng Cao, Haoran Tang, Jinfa Huang, Peng Jin, Can Zhang, Ruyang Liu, Long Chen, Xiaodan Liang, Li Yuan, and Ge Li.\n",
      "Text 2:      arXiv:1609.02907 http://arxiv.org/abs/1609.02907 [13] Y. Li, W. Jin, H. Xu, and J. Tang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00090):\n",
      "Paper 1:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Paper 2:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Embedding 1: [-0.76816636 -0.40030932 -3.75645161  0.00444262 -1.09072435]...\n",
      "Embedding 2: [-0.85856473 -0.05290981 -3.44286585  0.33531082 -0.46727812]...\n",
      "Text 1:      3 [40] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, and Zhaoyang Lv.\n",
      "Text 2:      3 [45] Jiawei Ren, Cheng Xie, Ashkan Mirzaei, Karsten Kreis, Zi- wei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, Huan Ling, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00105):\n",
      "Paper 1:     2503.20672v1.BizGen__Advancing_Article_level_Visual_Text_Rendering_for_Infographics_Generation.pdf\n",
      "Paper 2:     2503.20701v1.UniEDU__A_Unified_Language_and_Vision_Assistant_for_Education_Applications.pdf\n",
      "Embedding 1: [-0.84272152  0.41310707 -3.43686366 -0.77808887  0.68887198]...\n",
      "Embedding 2: [-0.26615274  0.27263296 -3.9682312  -0.59843922  0.52997398]...\n",
      "Text 1:      arXiv preprint arXiv:2310.15144, 2023. 1 [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 4 [20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\n",
      "Text 2:      Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruc- tion tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00105):\n",
      "Paper 1:     2503.20701v1.UniEDU__A_Unified_Language_and_Vision_Assistant_for_Education_Applications.pdf\n",
      "Paper 2:     2503.20672v1.BizGen__Advancing_Article_level_Visual_Text_Rendering_for_Infographics_Generation.pdf\n",
      "Embedding 1: [-0.26615274  0.27263296 -3.9682312  -0.59843922  0.52997398]...\n",
      "Embedding 2: [-0.84272152  0.41310707 -3.43686366 -0.77808887  0.68887198]...\n",
      "Text 1:      Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruc- tion tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning.\n",
      "Text 2:      arXiv preprint arXiv:2310.15144, 2023. 1 [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 4 [20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00124):\n",
      "Paper 1:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Paper 2:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Embedding 1: [-0.76385856  0.46243095 -3.22211933 -0.39503106  1.02693009]...\n",
      "Embedding 2: [-0.79716486 -0.06419816 -3.16164184  0.7342869   0.28415883]...\n",
      "Text 1:      1, 2, 3, 10 [66] Ludan Ruan, Y Ma, Huan Yang, Huiguo He, Bei Liu, Jian- long Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo.\n",
      "Text 2:      2, 3, 5, 8 [50] Xinyu Liu, Jinlong Li, Jin Ma, Huiming Sun, Zhigang Xu, Tianyun Zhang, and Hongkai Yu.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00238):\n",
      "Paper 1:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Paper 2:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Embedding 1: [-0.90331805 -0.12819886 -3.36623383 -0.2121639  -0.34727886]...\n",
      "Embedding 2: [-0.88630038 -0.00944226 -3.424438   -0.28795075  0.22402428]...\n",
      "Text 1:      5 [31] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.\n",
      "Text 2:      5 [39] Levon Khachatryan, Andranik Movsisyan, Vahram Tade- vosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00238):\n",
      "Paper 1:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Paper 2:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Embedding 1: [-0.88630038 -0.00944226 -3.424438   -0.28795075  0.22402428]...\n",
      "Embedding 2: [-0.90331805 -0.12819886 -3.36623383 -0.2121639  -0.34727886]...\n",
      "Text 1:      5 [39] Levon Khachatryan, Andranik Movsisyan, Vahram Tade- vosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi.\n",
      "Text 2:      5 [31] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00256):\n",
      "Paper 1:     2503.20630v1._Î²__GNN__A_Robust_Ensemble_Approach_Against_Graph_Structure_Perturbation.pdf\n",
      "Paper 2:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Embedding 1: [-0.17985758  0.99286252 -2.64047956 -1.22088754  1.07165313]...\n",
      "Embedding 2: [-1.03473425  1.17336202 -3.00379848 -0.63641232  1.29367614]...\n",
      "Text 1:      In Proceedings of the Twenty- Eighth International Joint Conference on Artificial Intelligence, IJCAI-19 .\n",
      "Text 2:      In Proceed- ings of the Thirtieth International Joint Conference on Ar- tificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021 , pages 4839â€“4843.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00256):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20630v1._Î²__GNN__A_Robust_Ensemble_Approach_Against_Graph_Structure_Perturbation.pdf\n",
      "Embedding 1: [-1.03473425  1.17336202 -3.00379848 -0.63641232  1.29367614]...\n",
      "Embedding 2: [-0.17985758  0.99286252 -2.64047956 -1.22088754  1.07165313]...\n",
      "Text 1:      In Proceed- ings of the Thirtieth International Joint Conference on Ar- tificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021 , pages 4839â€“4843.\n",
      "Text 2:      In Proceedings of the Twenty- Eighth International Joint Conference on Artificial Intelligence, IJCAI-19 .\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00273):\n",
      "Paper 1:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Paper 2:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Embedding 1: [-0.55702281  0.28228849 -3.400774    0.63421404  0.89778799]...\n",
      "Embedding 2: [-0.97764313  0.01223509 -3.63167191  0.31481618  0.31851038]...\n",
      "Text 1:      2, 3, 4, 5, 6, 15, 18 [103] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, and Achuta Kadambi.\n",
      "Text 2:      3 [14] Chaoya Jiang, Jia Hongrui, Haiyang Xu, Wei Ye, Mengfan Dong, Ming Yan, Ji Zhang, Fei Huang, and Shikun Zhang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00287):\n",
      "Paper 1:     2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "Paper 2:     2503.20676v1.Inductive_Link_Prediction_on_N_ary_Relational_Facts_via_Semantic_Hypergraph_Reasoning.pdf\n",
      "Embedding 1: [-0.26407242  0.12544024 -3.36957955  0.41642034 -0.26200402]...\n",
      "Embedding 2: [-0.6453259   0.02283143 -3.30595708 -0.03973304  0.19991767]...\n",
      "Text 1:      [42] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.\n",
      "Text 2:      [44] Gongzhu Yin, Xing Wang, Hongli Zhang, Chao Meng, Yuchen Yang, Kun Lu, and Yi Luo.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00302):\n",
      "Paper 1:     2503.20676v1.Inductive_Link_Prediction_on_N_ary_Relational_Facts_via_Semantic_Hypergraph_Reasoning.pdf\n",
      "Paper 2:     2503.20697v1.Semi_supervised_Node_Importance_Estimation_with_Informative_Distribution_Modeling_for_Uncertainty_Regularization.pdf\n",
      "Embedding 1: [-1.16733134 -0.08720005 -3.36913157 -0.94676387  0.04051014]...\n",
      "Embedding 2: [-1.24897766  0.33469677 -3.51879883 -0.2708025  -0.04416466]...\n",
      "Text 1:      [25] Haoran Luo, Haihong E, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, and Wei Lin.\n",
      "Text 2:      [25] Dongyuan Li, Zhen Wang, Yankai Chen, Renhe Jiang, Weiping Ding, and Manabu Okumura.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00357):\n",
      "Paper 1:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Paper 2:     2503.20719v1.Learning_Straight_Flows_by_Learning_Curved_Interpolants.pdf\n",
      "Embedding 1: [ 0.4210313   0.39043295 -3.63690758 -1.07537138  1.1915946 ]...\n",
      "Embedding 2: [ 0.5242964   0.15371916 -2.84144139  0.00913331  0.86660677]...\n",
      "Text 1:      B. Denoising Diffusion Probabilistic Models Diffusion models are a generative deep learning technique that leverage an approach for data synthesis.\n",
      "Text 2:      Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00360):\n",
      "Paper 1:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Paper 2:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Embedding 1: [-1.03622735  0.3329587  -3.50044727 -0.3811869  -0.32566828]...\n",
      "Embedding 2: [-0.9097625   0.36923003 -3.25577497  0.09444313  0.13176672]...\n",
      "Text 1:      5 [12] Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang.\n",
      "Text 2:      2 [12] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wen- zheng Chen, and Baoquan Chen.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00402):\n",
      "Paper 1:     2503.20576v1.Optimizing_Case_Based_Reasoning_System_for_Functional_Test_Script_Generation_with_Large_Language_Models.pdf\n",
      "Paper 2:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Embedding 1: [-0.92541611  0.24384409 -3.78731155  0.06930272 -0.10919204]...\n",
      "Embedding 2: [-0.68471253  0.18979128 -3.49268532  0.28081715 -0.13896376]...\n",
      "Text 1:      [32] Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Haotian Hui, Weichuan Liu, Zhiyuan Liu, et al.\n",
      "Text 2:      6 [32] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00402):\n",
      "Paper 1:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Paper 2:     2503.20576v1.Optimizing_Case_Based_Reasoning_System_for_Functional_Test_Script_Generation_with_Large_Language_Models.pdf\n",
      "Embedding 1: [-0.68471253  0.18979128 -3.49268532  0.28081715 -0.13896376]...\n",
      "Embedding 2: [-0.92541611  0.24384409 -3.78731155  0.06930272 -0.10919204]...\n",
      "Text 1:      6 [32] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al.\n",
      "Text 2:      [32] Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Haotian Hui, Weichuan Liu, Zhiyuan Liu, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00417):\n",
      "Paper 1:     2503.20749v1.Beyond_Believability__Accurate_Human_Behavior_Simulation_with_Fine_Tuned_LLMs.pdf\n",
      "Paper 2:     2503.20527v1.StableToolBench_MirrorAPI__Modeling_Tool_Environments_as_Mirrors_of_7_000__Real_World_APIs.pdf\n",
      "Embedding 1: [-0.11837462  0.46750614 -2.93045712  0.49106118  0.63530815]...\n",
      "Embedding 2: [-1.14279819  0.75479728 -2.79952335  0.48071304  0.4732708 ]...\n",
      "Text 1:      K. Li, Y . Q. Wang, Y . X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yu- jia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y . X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.\n",
      "Text 2:      Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00423):\n",
      "Paper 1:     2503.20744v1.High_Quality_Diffusion_Distillation_on_a_Single_GPU_with_Relative_and_Absolute_Position_Matching.pdf\n",
      "Paper 2:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Embedding 1: [ 0.40733823  0.42384604 -3.30691218 -0.49437711  0.41100261]...\n",
      "Embedding 2: [ 0.74098778 -0.22127867 -3.01928711 -0.19178009  1.03062487]...\n",
      "Text 1:      Improved denoising diffusion probabilistic models. arXiv preprint arXiv:2102.09672, 2021. [25] A. Nichol, P. Dharwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. GLIDE: Towards Photorealistic image generation and editing with text-guided diffusion models.\n",
      "Text 2:      [21] A. Nichol and P. Dhariwal, â€œImproved Denoising Diffusion Probabilistic Models,â€ 2 2021.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00441):\n",
      "Paper 1:     2503.20588v1.Synthetic_Data_Augmentation_for_Cross_domain_Implicit_Discourse_Relation_Recognition.pdf\n",
      "Paper 2:     2503.20715v1.From_Annotation_to_Adaptation__Metrics__Synthetic_Data__and_Aspect_Extraction_for_Aspect_Based_Sentiment_Analysis_with_Large_Language_Models.pdf\n",
      "Embedding 1: [ 0.23947439  0.51162755 -2.86092997 -1.52896059  1.20393717]...\n",
      "Embedding 2: [-0.1626699   0.75690097 -2.78527975 -1.29855883  0.33066133]...\n",
      "Text 1:      In Proceedings of the 18th Confer- ence of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 179â€“192, St. Julianâ€™s, Malta.\n",
      "Text 2:      In Proceedings of the 52nd annual meeting of the association for computational linguistics (vol- ume 2: Short papers), pages 49â€“54.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00495):\n",
      "Paper 1:     2503.20682v1.GLRD__Global_Local_Collaborative_Reason_and_Debate_with_PSL_for_3D_Open_Vocabulary_Detection.pdf\n",
      "Paper 2:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Embedding 1: [ 0.02204378  1.44320953 -3.71918035 -1.39806962  1.42111814]...\n",
      "Embedding 2: [ 0.30231211  1.152089   -3.97101116 -0.2117447   0.98284841]...\n",
      "Text 1:      : Grounding dino: Marrying dino with grounded pre- training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023) [36] Liu, X., Ji, K., Fu, Y ., Tam, W., Du, Z., Yang, Z., Tang, J.: P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (V olume 2: Short Papers) (2022) [37] Liu, X., Ji, K., Fu, Y ., Tam, W.L., Du, Z., Yang, Z., Tang, J.: P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602 (2021) [38] Liu, Z., Zhang, Z., Cao, Y ., Hu, H., Tong, X.: Group-free 3d object de- tection via transformers.\n",
      "Text 2:      Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. 5 [50] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00495):\n",
      "Paper 1:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Paper 2:     2503.20682v1.GLRD__Global_Local_Collaborative_Reason_and_Debate_with_PSL_for_3D_Open_Vocabulary_Detection.pdf\n",
      "Embedding 1: [ 0.30231211  1.152089   -3.97101116 -0.2117447   0.98284841]...\n",
      "Embedding 2: [ 0.02204378  1.44320953 -3.71918035 -1.39806962  1.42111814]...\n",
      "Text 1:      Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. 5 [50] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control.\n",
      "Text 2:      : Grounding dino: Marrying dino with grounded pre- training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023) [36] Liu, X., Ji, K., Fu, Y ., Tam, W., Du, Z., Yang, Z., Tang, J.: P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (V olume 2: Short Papers) (2022) [37] Liu, X., Ji, K., Fu, Y ., Tam, W.L., Du, Z., Yang, Z., Tang, J.: P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602 (2021) [38] Liu, Z., Zhang, Z., Cao, Y ., Hu, H., Tong, X.: Group-free 3d object de- tection via transformers.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00535):\n",
      "Paper 1:     2503.20719v1.Learning_Straight_Flows_by_Learning_Curved_Interpolants.pdf\n",
      "Paper 2:     2503.20725v1.Continual_learning_via_probabilistic_exchangeable_sequence_modelling.pdf\n",
      "Embedding 1: [ 1.0509783   1.23758459 -3.17813993 -0.65102887  1.27719021]...\n",
      "Embedding 2: [ 1.51086283  1.10165417 -2.72437143 -1.02369642  0.82346833]...\n",
      "Text 1:      Chen, R. T., Rubanova, Y ., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations.\n",
      "Text 2:      Neural ordinary differential equations.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00542):\n",
      "Paper 1:     2503.20519v1.MAR_3D__Progressive_Masked_Auto_regressor_for_High_Resolution_3D_Generation.pdf\n",
      "Paper 2:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Embedding 1: [ 0.50999892  0.84907144 -3.55169654 -1.22992289 -0.13008453]...\n",
      "Embedding 2: [ 1.06979918  0.93050182 -3.43796563 -0.80914533 -0.00811747]...\n",
      "Text 1:      3d gaussian splatting for real-time radiance field rendering. ACM TOG, 2023. 2 [18] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model.\n",
      "Text 2:      3d gaussian splatting for real-time radiance field rendering.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00542):\n",
      "Paper 1:     2503.20519v1.MAR_3D__Progressive_Masked_Auto_regressor_for_High_Resolution_3D_Generation.pdf\n",
      "Paper 2:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Embedding 1: [ 0.50999892  0.84907144 -3.55169654 -1.22992289 -0.13008453]...\n",
      "Embedding 2: [ 1.06979918  0.93050182 -3.43796563 -0.80914533 -0.00811747]...\n",
      "Text 1:      3d gaussian splatting for real-time radiance field rendering. ACM TOG, 2023. 2 [18] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model.\n",
      "Text 2:      3d gaussian splatting for real-time radiance field rendering.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00542):\n",
      "Paper 1:     2503.20519v1.MAR_3D__Progressive_Masked_Auto_regressor_for_High_Resolution_3D_Generation.pdf\n",
      "Paper 2:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Embedding 1: [ 0.50999892  0.84907144 -3.55169654 -1.22992289 -0.13008453]...\n",
      "Embedding 2: [ 1.06979918  0.93050182 -3.43796563 -0.80914533 -0.00811747]...\n",
      "Text 1:      3d gaussian splatting for real-time radiance field rendering. ACM TOG, 2023. 2 [18] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model.\n",
      "Text 2:      3d gaussian splatting for real-time radiance field rendering.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00596):\n",
      "Paper 1:     2503.20496v1.Enhancing_Depression_Detection_via_Question_wise_Modality_Fusion.pdf\n",
      "Paper 2:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Embedding 1: [-1.11847425  0.79319972 -3.17771268 -1.32906795  0.96865976]...\n",
      "Embedding 2: [-1.26320982  0.73467922 -3.15986633 -0.63470989  0.91755342]...\n",
      "Text 1:      Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171â€“4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n",
      "Text 2:      Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00596):\n",
      "Paper 1:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Paper 2:     2503.20496v1.Enhancing_Depression_Detection_via_Question_wise_Modality_Fusion.pdf\n",
      "Embedding 1: [-1.26320982  0.73467922 -3.15986633 -0.63470989  0.91755342]...\n",
      "Embedding 2: [-1.11847425  0.79319972 -3.17771268 -1.32906795  0.96865976]...\n",
      "Text 1:      Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization.\n",
      "Text 2:      Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171â€“4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00684):\n",
      "Paper 1:     2503.20698v1.MMMORRF__Multimodal_Multilingual_Modularized_Reciprocal_Rank_Fusion.pdf\n",
      "Paper 2:     2503.20781v1.BASKET__A_Large_Scale_Video_Dataset_for_Fine_Grained_Skill_Estimation.pdf\n",
      "Embedding 1: [ 0.62616569  0.676925   -3.02507925 -0.61383307  0.08364763]...\n",
      "Embedding 2: [ 0.28230226  1.18540442 -3.01204681 -0.97885954  0.176009  ]...\n",
      "Text 1:      To bridge the modality gap between textual queries and video frames, we use SigLIP [40], a variant of CLIP that replaces the standard contrastive learning softmax normalization with a sigmoid loss function.\n",
      "Text 2:      X-CLIP [24] extends CLIP to video-text retrieval with multi-grained contrastive learning, aligning coarse- grained and fine-grained visual features. SigLIP [45] uses a pairwise sigmoid loss to learn visual representations from large-scale image-language data. To extend SigLIP to video, we apply temporal pooling on individually extracted frame-level features. LLaVA-OneVision [16] introduces a large vision-language model that achieves impressive cross- scenario generalization transfer across many image and video tasks.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00684):\n",
      "Paper 1:     2503.20781v1.BASKET__A_Large_Scale_Video_Dataset_for_Fine_Grained_Skill_Estimation.pdf\n",
      "Paper 2:     2503.20698v1.MMMORRF__Multimodal_Multilingual_Modularized_Reciprocal_Rank_Fusion.pdf\n",
      "Embedding 1: [ 0.28230226  1.18540442 -3.01204681 -0.97885954  0.176009  ]...\n",
      "Embedding 2: [ 0.62616569  0.676925   -3.02507925 -0.61383307  0.08364763]...\n",
      "Text 1:      X-CLIP [24] extends CLIP to video-text retrieval with multi-grained contrastive learning, aligning coarse- grained and fine-grained visual features. SigLIP [45] uses a pairwise sigmoid loss to learn visual representations from large-scale image-language data. To extend SigLIP to video, we apply temporal pooling on individually extracted frame-level features. LLaVA-OneVision [16] introduces a large vision-language model that achieves impressive cross- scenario generalization transfer across many image and video tasks.\n",
      "Text 2:      To bridge the modality gap between textual queries and video frames, we use SigLIP [40], a variant of CLIP that replaces the standard contrastive learning softmax normalization with a sigmoid loss function.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00733):\n",
      "Paper 1:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Paper 2:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Embedding 1: [-0.38535309 -0.65451211 -3.06295228 -0.0250945   0.65944332]...\n",
      "Embedding 2: [-0.3077507  -0.44322371 -3.12826538 -0.25081432  0.5201478 ]...\n",
      "Text 1:      2, 3 [8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Â´e JÂ´egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\n",
      "Text 2:      3 [7] Timoth Â´ee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00735):\n",
      "Paper 1:     2503.20756v1.ADS_Edit__A_Multimodal_Knowledge_Editing_Dataset_for_Autonomous_Driving_Systems.pdf\n",
      "Paper 2:     2503.20641v1.Unlocking_Efficient_Long_to_Short_LLM_Reasoning_with_Model_Merging.pdf\n",
      "Embedding 1: [-0.49965549  0.9159717  -3.16695118  0.55632675  0.49091563]...\n",
      "Embedding 2: [-0.68888688  0.64382964 -2.86730289  0.19127829  0.54357785]...\n",
      "Text 1:      Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang.\n",
      "Text 2:      Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, and Cheng-Lin Liu.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00737):\n",
      "Paper 1:     2503.20508v1.Explainable_ICD_Coding_via_Entity_Linking.pdf\n",
      "Paper 2:     2503.20757v1.MCTS_RAG__Enhancing_Retrieval_Augmented_Generation_with_Monte_Carlo_Tree_Search.pdf\n",
      "Embedding 1: [-0.8406927   1.27333045 -2.8696456   0.27122545  0.09989339]...\n",
      "Embedding 2: [-0.86551964  0.62670678 -2.76883602  0.17671356  0.34486702]...\n",
      "Text 1:      Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhi- fang Sui.\n",
      "Text 2:      Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00737):\n",
      "Paper 1:     2503.20757v1.MCTS_RAG__Enhancing_Retrieval_Augmented_Generation_with_Monte_Carlo_Tree_Search.pdf\n",
      "Paper 2:     2503.20508v1.Explainable_ICD_Coding_via_Entity_Linking.pdf\n",
      "Embedding 1: [-0.86551964  0.62670678 -2.76883602  0.17671356  0.34486702]...\n",
      "Embedding 2: [-0.8406927   1.27333045 -2.8696456   0.27122545  0.09989339]...\n",
      "Text 1:      Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou.\n",
      "Text 2:      Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhi- fang Sui.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00855):\n",
      "Paper 1:     2503.20744v1.High_Quality_Diffusion_Distillation_on_a_Single_GPU_with_Relative_and_Absolute_Position_Matching.pdf\n",
      "Paper 2:     2503.20502v1.MLLM_Selector__Necessity_and_Diversity_driven_High_Value_Data_Selection_for_Enhanced_Visual_Instruction_Tuning.pdf\n",
      "Embedding 1: [-0.08327165  0.89244163 -2.3162775  -1.16949737  0.6078527 ]...\n",
      "Embedding 2: [-0.68145365  0.52476317 -2.64683604 -1.45847499  0.85154068]...\n",
      "Text 1:      arXiv preprint arXiv:2211.17091 [cs.CV], 2022. [16] D. Kim, C.-H. Lai, W.-H. Liao, N. Murata, Y . Takida, T. Uesaka, Y . He, Y . Mitsufuji, and S. Ermon.\n",
      "Text 2:      arXiv preprint arXiv:2308.10792 (2023) [45] Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.W., Tay, Y., Zhou, D., Le, Q.V., Zoph, B., Wei, J., et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00925):\n",
      "Paper 1:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Paper 2:     2503.20673v1.Mitigating_Low_Level_Visual_Hallucinations_Requires_Self_Awareness__Database__Model_and_Training_Strategy.pdf\n",
      "Embedding 1: [ 0.79204375  0.29869705 -2.94697809 -0.64255208  1.23725033]...\n",
      "Embedding 2: [ 0.54960817  0.43789077 -2.76246023 -0.66140711  0.78718615]...\n",
      "Text 1:      [35] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, â€œImage quality assessment: From error visibility to structural similarity,â€ IEEE Transactions on Image Processing , vol.\n",
      "Text 2:      [6] Y . Gao, X. Min, Y . Cao, X. Liu, and G. Zhai, â€œNo-reference image quality assessment: Obtain mos from image quality score distribution,â€ IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2024. [7] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli, â€œImage quality assessment: Unifying structure and texture similarity,â€IEEE transactions on pattern analysis and machine intelligence (TPAMI) , vol. 44, no. 5, pp. 2567â€“2581, 2020.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00925):\n",
      "Paper 1:     2503.20673v1.Mitigating_Low_Level_Visual_Hallucinations_Requires_Self_Awareness__Database__Model_and_Training_Strategy.pdf\n",
      "Paper 2:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Embedding 1: [ 0.54960817  0.43789077 -2.76246023 -0.66140711  0.78718615]...\n",
      "Embedding 2: [ 0.79204375  0.29869705 -2.94697809 -0.64255208  1.23725033]...\n",
      "Text 1:      [6] Y . Gao, X. Min, Y . Cao, X. Liu, and G. Zhai, â€œNo-reference image quality assessment: Obtain mos from image quality score distribution,â€ IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2024. [7] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli, â€œImage quality assessment: Unifying structure and texture similarity,â€IEEE transactions on pattern analysis and machine intelligence (TPAMI) , vol. 44, no. 5, pp. 2567â€“2581, 2020.\n",
      "Text 2:      [35] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, â€œImage quality assessment: From error visibility to structural similarity,â€ IEEE Transactions on Image Processing , vol.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00926):\n",
      "Paper 1:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Paper 2:     2503.20783v1.Understanding_R1_Zero_Like_Training__A_Critical_Perspective.pdf\n",
      "Embedding 1: [-0.79976487  0.42520118 -3.18665361  0.84199464  0.4681257 ]...\n",
      "Embedding 2: [-0.75691414  0.63283849 -3.01701355  1.21545601  1.26791859]...\n",
      "Text 1:      Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Diederik P Kingma, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun.\n",
      "Text 2:      Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01017):\n",
      "Paper 1:     2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "Paper 2:     2503.20505v1.Riemannian_Optimization_on_Relaxed_Indicator_Matrix_Manifold.pdf\n",
      "Embedding 1: [-1.15853477  0.20527029 -3.65785313 -0.16562131 -0.54943335]...\n",
      "Embedding 2: [-0.53020424 -0.2337538  -3.67873907  0.61905956 -0.07440683]...\n",
      "Text 1:      [32] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang.\n",
      "Text 2:      [69] Shuai Li, Yingjie Zhang, Hongtu Zhu, Christina Wang, Hai Shu, Ziqi Chen, Zhuoran Sun, and Yanfeng Yang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01017):\n",
      "Paper 1:     2503.20505v1.Riemannian_Optimization_on_Relaxed_Indicator_Matrix_Manifold.pdf\n",
      "Paper 2:     2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "Embedding 1: [-0.53020424 -0.2337538  -3.67873907  0.61905956 -0.07440683]...\n",
      "Embedding 2: [-1.15853477  0.20527029 -3.65785313 -0.16562131 -0.54943335]...\n",
      "Text 1:      [69] Shuai Li, Yingjie Zhang, Hongtu Zhu, Christina Wang, Hai Shu, Ziqi Chen, Zhuoran Sun, and Yanfeng Yang.\n",
      "Text 2:      [32] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01057):\n",
      "Paper 1:     2503.20641v1.Unlocking_Efficient_Long_to_Short_LLM_Reasoning_with_Model_Merging.pdf\n",
      "Paper 2:     2503.20527v1.StableToolBench_MirrorAPI__Modeling_Tool_Environments_as_Mirrors_of_7_000__Real_World_APIs.pdf\n",
      "Embedding 1: [ 0.15030776  1.09910607 -3.10773706 -1.54364491  1.35493636]...\n",
      "Embedding 2: [ 0.50989199  1.10934448 -3.01998901 -1.57685089  1.24127197]...\n",
      "Text 1:      17167â€“ 17186. Association for Computational Linguistics, 2024. URL https://aclanthology. org/2024.findings-emnlp.1000.\n",
      "Text 2:      In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 7346â€“7356, Mi- ami, Florida, USA.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01070):\n",
      "Paper 1:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Paper 2:     2503.20748v1.UniSTD__Towards_Unified_Spatio_Temporal_Learning_across_Diverse_Disciplines.pdf\n",
      "Embedding 1: [-0.67765909 -0.21583897 -3.59526134 -0.63158739 -0.29429278]...\n",
      "Embedding 2: [-0.89311856  0.13556072 -3.85847807 -0.20742305 -0.27961078]...\n",
      "Text 1:      2, 6, 7 [42] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon- tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al.\n",
      "Text 2:      3 [24] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,Vaishaal Shankar, Hongseok Namkoong, John Miller, Han- naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01070):\n",
      "Paper 1:     2503.20748v1.UniSTD__Towards_Unified_Spatio_Temporal_Learning_across_Diverse_Disciplines.pdf\n",
      "Paper 2:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Embedding 1: [-0.89311856  0.13556072 -3.85847807 -0.20742305 -0.27961078]...\n",
      "Embedding 2: [-0.67765909 -0.21583897 -3.59526134 -0.63158739 -0.29429278]...\n",
      "Text 1:      3 [24] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,Vaishaal Shankar, Hongseok Namkoong, John Miller, Han- naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.\n",
      "Text 2:      2, 6, 7 [42] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon- tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01125):\n",
      "Paper 1:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Paper 2:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Embedding 1: [-5.78380585e-01 -6.04595989e-02 -3.78781724e+00 -8.91968608e-04\n",
      "  3.27735364e-01]...\n",
      "Embedding 2: [-0.70480245 -0.04608679 -3.89190197  0.23959728 -0.40968567]...\n",
      "Text 1:      2 [18] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.\n",
      "Text 2:      2 [24] Wenyi Hong, Ming Ding, Wendi Chen, Woonhyuk Baek Zhang, Zhuoyi Yang, Xiaojie Xu, Junyuan Wang, Chang Zhou, Hongxia Yang, and Jie Tang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01235):\n",
      "Paper 1:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Paper 2:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Embedding 1: [-1.08269179  0.23464607 -3.52810025  0.70787549  0.07131348]...\n",
      "Embedding 2: [-1.28039467 -0.26093975 -3.35694408  0.38049185  0.52905715]...\n",
      "Text 1:      3 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siaro- hin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al.\n",
      "Text 2:      2, 3 [41] Dan Kondratyuk, Lijun Yu, Xiuye Gu, JosÃ© Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01245):\n",
      "Paper 1:     2503.20641v1.Unlocking_Efficient_Long_to_Short_LLM_Reasoning_with_Model_Merging.pdf\n",
      "Paper 2:     2503.20724v1.Dynamic_Motion_Blending_for_Versatile_Motion_Editing.pdf\n",
      "Embedding 1: [ 0.09441435  1.04933691 -3.75085449 -1.06126404  1.25140381]...\n",
      "Embedding 2: [-0.50995237  0.5631178  -3.22738552 -1.25661457  1.46463251]...\n",
      "Text 1:      In The Twelfth International Conference on Learning Rep- resentations, ICLR 2024, Vienna, Austria, May 7-11, 2024 .\n",
      "Text 2:      In International Conference on Learning Repre- sentations (ICLR), 2022.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01314):\n",
      "Paper 1:     2503.20502v1.MLLM_Selector__Necessity_and_Diversity_driven_High_Value_Data_Selection_for_Enhanced_Visual_Instruction_Tuning.pdf\n",
      "Paper 2:     2503.20662v1.AutoRad_Lung__A_Radiomic_Guided_Prompting_Autoregressive_Vision_Language_Model_for_Lung_Nodule_Malignancy_Prediction.pdf\n",
      "Embedding 1: [-0.41678432  1.06291139 -3.10052824 -1.25204873  0.49744952]...\n",
      "Embedding 2: [-0.17182703  1.18555188 -3.40343547 -0.90960801  0.91050529]...\n",
      "Text 1:      To further extend these capa- bilities into the realm of visual understanding, a series of pioneering studies [14â€“17] have inte- grated visual encoders into LLMs, leading to the development of multimodal large language models (MLLMs).\n",
      "Text 2:      A further breakthrough in the field has been driven by the emergence of Vision- Language Models (VLMs) [20], inspired by advancements in Large Language Models (LLMs) [7].\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01314):\n",
      "Paper 1:     2503.20662v1.AutoRad_Lung__A_Radiomic_Guided_Prompting_Autoregressive_Vision_Language_Model_for_Lung_Nodule_Malignancy_Prediction.pdf\n",
      "Paper 2:     2503.20502v1.MLLM_Selector__Necessity_and_Diversity_driven_High_Value_Data_Selection_for_Enhanced_Visual_Instruction_Tuning.pdf\n",
      "Embedding 1: [-0.17182703  1.18555188 -3.40343547 -0.90960801  0.91050529]...\n",
      "Embedding 2: [-0.41678432  1.06291139 -3.10052824 -1.25204873  0.49744952]...\n",
      "Text 1:      A further breakthrough in the field has been driven by the emergence of Vision- Language Models (VLMs) [20], inspired by advancements in Large Language Models (LLMs) [7].\n",
      "Text 2:      To further extend these capa- bilities into the realm of visual understanding, a series of pioneering studies [14â€“17] have inte- grated visual encoders into LLMs, leading to the development of multimodal large language models (MLLMs).\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01361):\n",
      "Paper 1:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Paper 2:     2503.20527v1.StableToolBench_MirrorAPI__Modeling_Tool_Environments_as_Mirrors_of_7_000__Real_World_APIs.pdf\n",
      "Embedding 1: [-0.71550363  0.54225439 -2.87493801  0.4975338   0.48939863]...\n",
      "Embedding 2: [-0.90097702  0.61948252 -3.12543511  0.47973961  0.75292367]...\n",
      "Text 1:      Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al.\n",
      "Text 2:      Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, and Feng Zhao.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01395):\n",
      "Paper 1:     2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf\n",
      "Paper 2:     2503.20768v1.An_Empirical_Study_of_the_Impact_of_Federated_Learning_on_Machine_Learning_Model_Accuracy.pdf\n",
      "Embedding 1: [ 0.89737856  0.70978546 -3.56553483 -0.84487885 -0.30352181]...\n",
      "Embedding 2: [ 0.43660757  0.8859306  -3.54330182 -0.51569206  0.2755861 ]...\n",
      "Text 1:      Hierarchical text-conditional image gener- ation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 3 [21] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj Â¨orn Ommer. High-resolution image synthesis with latent diffusion models.\n",
      "Text 2:      CoRR, abs/2003.00295, 2020. [56] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. InIEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 10674â€“10685.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01399):\n",
      "Paper 1:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Paper 2:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Embedding 1: [ 0.42793173  0.74717212 -3.25557232 -0.15300289 -0.06007813]...\n",
      "Embedding 2: [-0.12319146  0.7632575  -2.75350285 -0.71378863  0.1317367 ]...\n",
      "Text 1:      7 [79] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Mena- pace, Aliaksandr Siarohin, Junli Cao, L Â´aszlÂ´o Jeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealis- tic 4d scene generation via video diffusion models.Advances in Neural Information Processing Systems, 37:45256â€“45280, 2024. 1 [80] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splat- ting.\n",
      "Text 2:      2 [65] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splat- ting. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 20051â€“20060, 2024. 2, 3 [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In International conference on machine learning , pages 8748â€“8763. PMLR, 2021. 2 [67] Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Re- constructing 3d human pose from 2d image landmarks. In ECCV, 2012. 3 [68] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Ro- man R Â¨adle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 2, 3, 6, 15 [69] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142 , 2023.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01528):\n",
      "Paper 1:     2503.20731v1.RecTable__Fast_Modeling_Tabular_Data_with_Rectified_Flow.pdf\n",
      "Paper 2:     2503.20756v1.ADS_Edit__A_Multimodal_Knowledge_Editing_Dataset_for_Autonomous_Driving_Systems.pdf\n",
      "Embedding 1: [-0.87691307  0.82541573 -3.02300024  0.44100133  0.6095866 ]...\n",
      "Embedding 2: [-0.872105    0.93293494 -2.94638729  0.91827071  0.52684981]...\n",
      "Text 1:      Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao.\n",
      "Text 2:      Yanze Li, Wenhua Zhang, Kai Chen, Yanxin Liu, Pengx- iang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, Dit-Yan Yeung, Huchuan Lu, and Xu Jia.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01639):\n",
      "Paper 1:     2503.20561v1.A_Theoretical_Framework_for_Prompt_Engineering__Approximating_Smooth_Functions_with_Transformer_Prompts.pdf\n",
      "Paper 2:     2503.20725v1.Continual_learning_via_probabilistic_exchangeable_sequence_modelling.pdf\n",
      "Embedding 1: [-0.24792844  0.32877767 -3.21716166  0.69676179  0.45494679]...\n",
      "Embedding 2: [-0.16741818 -0.23342744 -3.18590808  0.32262772  0.82616132]...\n",
      "Text 1:      Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., Jiang, L., Zhang, X., Zhang, S., Liu, J., et al.\n",
      "Text 2:      Wang, L., Zhang, M., Jia, Z., Li, Q., Bao, C., Ma, K., Zhu, J., and Zhong, Y .\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01649):\n",
      "Paper 1:     2503.20502v1.MLLM_Selector__Necessity_and_Diversity_driven_High_Value_Data_Selection_for_Enhanced_Visual_Instruction_Tuning.pdf\n",
      "Paper 2:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Embedding 1: [ 0.0754448   0.97465003 -2.55316663 -1.36470389  1.93024611]...\n",
      "Embedding 2: [-0.11574981  0.39994994 -2.49489856 -1.01540279  1.67212021]...\n",
      "Text 1:      IEEE Transactions on Image Processing 31, 4321â€“4335 (2022) [55] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa matter: Elevating the role of image understanding in visual question answering.\n",
      "Text 2:      1 [37] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In International Conference on Document Analysis and Recognition, pages 947â€“952. IEEE, 2019.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01649):\n",
      "Paper 1:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Paper 2:     2503.20502v1.MLLM_Selector__Necessity_and_Diversity_driven_High_Value_Data_Selection_for_Enhanced_Visual_Instruction_Tuning.pdf\n",
      "Embedding 1: [-0.11574981  0.39994994 -2.49489856 -1.01540279  1.67212021]...\n",
      "Embedding 2: [ 0.0754448   0.97465003 -2.55316663 -1.36470389  1.93024611]...\n",
      "Text 1:      1 [37] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In International Conference on Document Analysis and Recognition, pages 947â€“952. IEEE, 2019.\n",
      "Text 2:      IEEE Transactions on Image Processing 31, 4321â€“4335 (2022) [55] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa matter: Elevating the role of image understanding in visual question answering.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01651):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf\n",
      "Embedding 1: [ 0.45935434  1.21808982 -3.78729844 -0.49215904  0.34075722]...\n",
      "Embedding 2: [ 0.38228539  1.61753058 -4.37459612 -0.05794864  0.57724339]...\n",
      "Text 1:      Latent video diffusion models for high- fidelity video generation with arbitrary lengths. CoRR, abs/2211.13221, 2022. 2 [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance.\n",
      "Text 2:      Stable video diffusion: Scaling latent video diffusion models to large datasets.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01673):\n",
      "Paper 1:     2503.20719v1.Learning_Straight_Flows_by_Learning_Curved_Interpolants.pdf\n",
      "Paper 2:     2503.20731v1.RecTable__Fast_Modeling_Tabular_Data_with_Rectified_Flow.pdf\n",
      "Embedding 1: [ 0.07459769  0.86461723 -3.69143558 -0.49649954  1.53992522]...\n",
      "Embedding 2: [ 0.02693853  1.1655072  -3.93572497 -0.46123052  0.83004582]...\n",
      "Text 1:      Liu, X., Gong, C., et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022. 9Liu, X., Zhang, X., Ma, J., Peng, J., and Liu, Q. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation.\n",
      "Text 2:      Instaflow: One step is enough for high-quality diffusion-based text-to-image generation.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01673):\n",
      "Paper 1:     2503.20719v1.Learning_Straight_Flows_by_Learning_Curved_Interpolants.pdf\n",
      "Paper 2:     2503.20744v1.High_Quality_Diffusion_Distillation_on_a_Single_GPU_with_Relative_and_Absolute_Position_Matching.pdf\n",
      "Embedding 1: [ 0.07459769  0.86461723 -3.69143558 -0.49649954  1.53992522]...\n",
      "Embedding 2: [ 0.02693853  1.1655072  -3.93572497 -0.46123052  0.83004582]...\n",
      "Text 1:      Liu, X., Gong, C., et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022. 9Liu, X., Zhang, X., Ma, J., Peng, J., and Liu, Q. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation.\n",
      "Text 2:      Instaflow: One step is enough for high-quality diffusion-based text-to-image generation.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01692):\n",
      "Paper 1:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Paper 2:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Embedding 1: [-1.03622735  0.3329587  -3.50044727 -0.3811869  -0.32566828]...\n",
      "Embedding 2: [-0.84091824  0.00872125 -3.74858403  0.13222414  0.01993512]...\n",
      "Text 1:      5 [12] Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang.\n",
      "Text 2:      6 [3] Hanting Chen, Tianyu Guo, Chang Xu, Wenshuo Li, Chun- jing Xu, Chao Xu, and Yunhe Wang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01721):\n",
      "Paper 1:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Paper 2:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Embedding 1: [-0.13868922  1.02672982 -3.20760202 -0.62714058  0.52503669]...\n",
      "Embedding 2: [-0.40728459  1.40387821 -3.28111172 -0.61902153  0.00990929]...\n",
      "Text 1:      Our research adheres to this paradigm, but extends it by generating consistent multi-view videos (instead of images) and subsequently re- constructing the 4D object.\n",
      "Text 2:      To over- come this second challenge, we introduce an effective op- timization strategy designed to seamlessly integrate multi- view videos into a unified 4D representation.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01721):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Embedding 1: [-0.40728459  1.40387821 -3.28111172 -0.61902153  0.00990929]...\n",
      "Embedding 2: [-0.13868922  1.02672982 -3.20760202 -0.62714058  0.52503669]...\n",
      "Text 1:      To over- come this second challenge, we introduce an effective op- timization strategy designed to seamlessly integrate multi- view videos into a unified 4D representation.\n",
      "Text 2:      Our research adheres to this paradigm, but extends it by generating consistent multi-view videos (instead of images) and subsequently re- constructing the 4D object.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01761):\n",
      "Paper 1:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Paper 2:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Embedding 1: [-0.4671891   0.57906324 -3.61789083 -0.25250199  1.2139225 ]...\n",
      "Embedding 2: [-0.81383812  0.10616253 -3.81506896 -0.09195267  0.74755561]...\n",
      "Text 1:      In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296â€“26306, 2024. 1, 2, 5, 7, 8 [30] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216â€“233.\n",
      "Text 2:      Mmbench: Is your multi-modal model an all-around player?\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01761):\n",
      "Paper 1:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Paper 2:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Embedding 1: [-0.4671891   0.57906324 -3.61789083 -0.25250199  1.2139225 ]...\n",
      "Embedding 2: [-0.81383812  0.10616253 -3.81506896 -0.09195267  0.74755561]...\n",
      "Text 1:      In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296â€“26306, 2024. 1, 2, 5, 7, 8 [30] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216â€“233.\n",
      "Text 2:      Mmbench: Is your multi-modal model an all-around player?\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01826):\n",
      "Paper 1:     2503.20767v1.Reliable_algorithm_selection_for_machine_learning_guided_design.pdf\n",
      "Paper 2:     2503.20731v1.RecTable__Fast_Modeling_Tabular_Data_with_Rectified_Flow.pdf\n",
      "Embedding 1: [ 0.12719671  0.23233263 -2.55927014 -0.46543062  1.79091597]...\n",
      "Embedding 2: [ 0.21607521  0.43166924 -3.52156734 -0.97860932  1.22464716]...\n",
      "Text 1:      In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5897â€“5906.\n",
      "Text 2:      ), Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learn- ing Research, pp.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01828):\n",
      "Paper 1:     2503.20752v1.Reason_RFT__Reinforcement_Fine_Tuning_for_Visual_Reasoning.pdf\n",
      "Paper 2:     2503.20672v1.BizGen__Advancing_Article_level_Visual_Text_Rendering_for_Infographics_Generation.pdf\n",
      "Embedding 1: [-0.91925114 -0.48638731 -3.52071762  0.29216754  0.48963797]...\n",
      "Embedding 2: [-0.79493439  0.0376522  -3.29397726 -0.1538682   0.21832682]...\n",
      "Text 1:      2, 3 [8] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine, and Yi Ma.\n",
      "Text 2:      2, 3, 5, 6, 8 [23] Zeyu Liu, Weicong Liang, Yiming Zhao, Bohan Chen, Ji Li, and Yuhui Yuan.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01895):\n",
      "Paper 1:     2503.20744v1.High_Quality_Diffusion_Distillation_on_a_Single_GPU_with_Relative_and_Absolute_Position_Matching.pdf\n",
      "Paper 2:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Embedding 1: [ 0.10669913  0.71017408 -2.98035359 -0.87925655  1.44198322]...\n",
      "Embedding 2: [-0.19135045  0.66732043 -3.35470915 -0.94319832  1.51609731]...\n",
      "Text 1:      Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in neural information processing systems (NeurIPS), page 11895â€“11907, 2019. [36] Y .\n",
      "Text 2:      Generative modeling by estimating gradients of the data distribution.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01902):\n",
      "Paper 1:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Paper 2:     2503.20519v1.MAR_3D__Progressive_Masked_Auto_regressor_for_High_Resolution_3D_Generation.pdf\n",
      "Embedding 1: [-1.04672909 -0.1070838  -4.12132406  0.06564521  0.33723745]...\n",
      "Embedding 2: [-0.75777632 -0.32923463 -3.74743652 -0.38684285 -0.47049236]...\n",
      "Text 1:      6 [17] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n",
      "Text 2:      6 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01931):\n",
      "Paper 1:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Paper 2:     2503.20771v1.Disentangled_Source_Free_Personalization_for_Facial_Expression_Recognition_with_Neutral_Target_Data.pdf\n",
      "Embedding 1: [ 0.4210313   0.39043295 -3.63690758 -1.07537138  1.1915946 ]...\n",
      "Embedding 2: [ 0.69910091  0.27379218 -2.75452137 -0.56295127  1.15276301]...\n",
      "Text 1:      B. Denoising Diffusion Probabilistic Models Diffusion models are a generative deep learning technique that leverage an approach for data synthesis.\n",
      "Text 2:      Neural Computing and Applications, 33(15):9125â€“9136. [34] Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162â€“8171.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01931):\n",
      "Paper 1:     2503.20771v1.Disentangled_Source_Free_Personalization_for_Facial_Expression_Recognition_with_Neutral_Target_Data.pdf\n",
      "Paper 2:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Embedding 1: [ 0.69910091  0.27379218 -2.75452137 -0.56295127  1.15276301]...\n",
      "Embedding 2: [ 0.4210313   0.39043295 -3.63690758 -1.07537138  1.1915946 ]...\n",
      "Text 1:      Neural Computing and Applications, 33(15):9125â€“9136. [34] Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162â€“8171.\n",
      "Text 2:      B. Denoising Diffusion Probabilistic Models Diffusion models are a generative deep learning technique that leverage an approach for data synthesis.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01937):\n",
      "Paper 1:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Paper 2:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Embedding 1: [-1.08269179  0.23464607 -3.52810025  0.70787549  0.07131348]...\n",
      "Embedding 2: [-1.0922581   0.20596603 -3.42527032  0.25752017 -0.12405041]...\n",
      "Text 1:      3 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siaro- hin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al.\n",
      "Text 2:      3 [75] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Mena- pace, Aliaksandr Siarohin, Junli Cao, LÂ´aszlÂ´o A. Jeni, Sergey Tulyakov, and Hsin-Ying Lee.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01937):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Embedding 1: [-1.0922581   0.20596603 -3.42527032  0.25752017 -0.12405041]...\n",
      "Embedding 2: [-1.08269179  0.23464607 -3.52810025  0.70787549  0.07131348]...\n",
      "Text 1:      3 [75] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Mena- pace, Aliaksandr Siarohin, Junli Cao, LÂ´aszlÂ´o A. Jeni, Sergey Tulyakov, and Hsin-Ying Lee.\n",
      "Text 2:      3 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siaro- hin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01955):\n",
      "Paper 1:     2503.20757v1.MCTS_RAG__Enhancing_Retrieval_Augmented_Generation_with_Monte_Carlo_Tree_Search.pdf\n",
      "Paper 2:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Embedding 1: [-0.85240799  0.8207137  -2.94417214  0.61742401  0.24487422]...\n",
      "Embedding 2: [-0.78751808  0.61681396 -3.1917634   0.3644824   0.21174851]...\n",
      "Text 1:      Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.\n",
      "Text 2:      Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01955):\n",
      "Paper 1:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Paper 2:     2503.20757v1.MCTS_RAG__Enhancing_Retrieval_Augmented_Generation_with_Monte_Carlo_Tree_Search.pdf\n",
      "Embedding 1: [-0.78751808  0.61681396 -3.1917634   0.3644824   0.21174851]...\n",
      "Embedding 2: [-0.85240799  0.8207137  -2.94417214  0.61742401  0.24487422]...\n",
      "Text 1:      Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n",
      "Text 2:      Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02016):\n",
      "Paper 1:     2503.20783v1.Understanding_R1_Zero_Like_Training__A_Critical_Perspective.pdf\n",
      "Paper 2:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Embedding 1: [-0.92313284  0.23256113 -3.22854686  0.72958112  0.82365274]...\n",
      "Embedding 2: [-0.71550363  0.54225439 -2.87493801  0.4975338   0.48939863]...\n",
      "Text 1:      Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al.\n",
      "Text 2:      Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02023):\n",
      "Paper 1:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Paper 2:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Embedding 1: [-0.06129128  0.61484218 -3.0063715  -0.92595994  1.46256828]...\n",
      "Embedding 2: [-0.26617178  0.34801811 -3.75007343 -0.88167095  1.28414178]...\n",
      "Text 1:      arXiv preprint arXiv:2410.07073, 2024. 2 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men- sch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716â€“23736, 2022.\n",
      "Text 2:      Flamingo: a visual language model for few-shot learning.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02062):\n",
      "Paper 1:     2503.20576v1.Optimizing_Case_Based_Reasoning_System_for_Functional_Test_Script_Generation_with_Large_Language_Models.pdf\n",
      "Paper 2:     2503.20654v1.AccidentSim__Generating_Physically_Realistic_Vehicle_Collision_Videos_from_Real_World_Accident_Reports.pdf\n",
      "Embedding 1: [-0.63436979  0.48627776 -3.58833504  0.52618611  0.25054598]...\n",
      "Embedding 2: [-0.79905319  0.29141098 -3.23141956  0.32684678  0.69229925]...\n",
      "Text 1:      [7] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang.\n",
      "Text 2:      1, 3 [34] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02073):\n",
      "Paper 1:     2503.20537v1.TD_BFR__Truncated_Diffusion_Model_for_Efficient_Blind_Face_Restoration.pdf\n",
      "Paper 2:     2503.20771v1.Disentangled_Source_Free_Personalization_for_Facial_Expression_Recognition_with_Neutral_Target_Data.pdf\n",
      "Embedding 1: [ 0.94510585  0.02659261 -2.97154236 -0.16870785  1.09315658]...\n",
      "Embedding 2: [ 0.69910091  0.27379218 -2.75452137 -0.56295127  1.15276301]...\n",
      "Text 1:      [3] Jonathan Ho, Ajay Jain, and Pieter Abbeel, â€œDenoising diffusion probabilistic models,â€ Advances in Neural Information Processing Systems, vol. 33, pp. 6840â€“6851, 2020. [4] Zhixin Wang, Ziying Zhang, Xiaoyun Zhang, Huangjie Zheng, Mingyuan Zhou, Ya Zhang, and Yanfeng Wang, â€œDr2: Diffusion-based robust degradation remover for blind face restoration,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 1704â€“1713.\n",
      "Text 2:      Neural Computing and Applications, 33(15):9125â€“9136. [34] Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162â€“8171.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02073):\n",
      "Paper 1:     2503.20771v1.Disentangled_Source_Free_Personalization_for_Facial_Expression_Recognition_with_Neutral_Target_Data.pdf\n",
      "Paper 2:     2503.20537v1.TD_BFR__Truncated_Diffusion_Model_for_Efficient_Blind_Face_Restoration.pdf\n",
      "Embedding 1: [ 0.69910091  0.27379218 -2.75452137 -0.56295127  1.15276301]...\n",
      "Embedding 2: [ 0.94510585  0.02659261 -2.97154236 -0.16870785  1.09315658]...\n",
      "Text 1:      Neural Computing and Applications, 33(15):9125â€“9136. [34] Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162â€“8171.\n",
      "Text 2:      [3] Jonathan Ho, Ajay Jain, and Pieter Abbeel, â€œDenoising diffusion probabilistic models,â€ Advances in Neural Information Processing Systems, vol. 33, pp. 6840â€“6851, 2020. [4] Zhixin Wang, Ziying Zhang, Xiaoyun Zhang, Huangjie Zheng, Mingyuan Zhou, Ya Zhang, and Yanfeng Wang, â€œDr2: Diffusion-based robust degradation remover for blind face restoration,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 1704â€“1713.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02073):\n",
      "Paper 1:     2503.20756v1.ADS_Edit__A_Multimodal_Knowledge_Editing_Dataset_for_Autonomous_Driving_Systems.pdf\n",
      "Paper 2:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Embedding 1: [-0.46456611  0.99340516 -3.10964036 -1.17084336  0.95314306]...\n",
      "Embedding 2: [-0.87687224  0.80689895 -3.13455033 -1.24877059  0.87135404]...\n",
      "Text 1:      Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi- hao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language modelâ€™s per- ception of the world at any resolution. CoRR, abs/2409.12191. Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Hua- jun Chen.\n",
      "Text 2:      2 [46] P Wang, S Bai, S Tan, S Wang, Z Fan, J Bai, K Chen, X Liu, J Wang, W Ge, et al. Qwen2-vl: Enhancing vision- language modelâ€™s perception of the world at any resolution, 2024. URL https://arxiv. org/abs/2409.12191. 2, 4 [47] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained lan- guage models. Advances in Neural Information Processing Systems, 37:121475â€“121499, 2024. 2 [48] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition, pages 2575â€“2584, 2020. 4 [49] Ge Zhang Yao Fu Wenhao Huang Huan Sun Yu Su Wenhu Chen Xiang Yue, Xingwei Qu. Mammoth: Build- ing math generalist models through hybrid instruction tun- ing. arXiv preprint arXiv:2309.05653, 2023. 4 [50] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02126):\n",
      "Paper 1:     2503.20496v1.Enhancing_Depression_Detection_via_Question_wise_Modality_Fusion.pdf\n",
      "Paper 2:     2503.20701v1.UniEDU__A_Unified_Language_and_Vision_Assistant_for_Education_Applications.pdf\n",
      "Embedding 1: [-0.42096663  1.2046597  -3.21952677 -0.39780548  1.13039589]...\n",
      "Embedding 2: [-0.89637011  1.88280618 -3.63882923 -0.18470052  1.30690789]...\n",
      "Text 1:      IEEE. Stefan Scherer, Giota Stratou, Gale M. Lucas, Marwa Mahmoud, Jill Boberg, Jonathan Gratch, Albert A. Rizzo, and Louis-Philippe Morency.\n",
      "Text 2:      IEEE. Longwei Zheng, Fei Jiang, Xiaoqing Gu, Yuanyuan Li, Gong Wang, and Haomin Zhang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02166):\n",
      "Paper 1:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Paper 2:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Embedding 1: [-0.3047114  -0.85612321 -3.36086106  0.83282393 -0.33435637]...\n",
      "Embedding 2: [-0.49814945  0.29662806 -3.24633217  0.43851608 -0.99079072]...\n",
      "Text 1:      2 [10] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock- horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\n",
      "Text 2:      2 [34] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fi- dler, and Karsten Kreis.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02185):\n",
      "Paper 1:     2503.20633v1.Enhancing_Multi_modal_Models_with_Heterogeneous_MoE_Adapters_for_Fine_tuning.pdf\n",
      "Paper 2:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Embedding 1: [-0.82281154  0.82012892 -2.63438725 -0.83916718  0.90070647]...\n",
      "Embedding 2: [-0.89277411  1.50889516 -2.27555084 -0.905433    0.29677722]...\n",
      "Text 1:      38, pp. 1110â€“1119. [25] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan, â€œMoe-llava: Mixture of experts for large vision-language models,â€ arXiv preprint arXiv:2401.15947, 2024. [26] Jonas Pfeiffer, Aishwarya Kamath, Andreas RÂ¨ ucklÂ´e, Kyunghyun Cho, and Iryna Gurevych, â€œAdapterfusion: Non-destructive task composition for transfer learning,â€ arXiv preprint arXiv:2005.00247, 2020. [27] Brian Lester, Rami Al-Rfou, and Noah Constant, â€œThe power of scale for parameter-efficient prompt tuning,â€ in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7- 11 November, 2021 , Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, Eds.\n",
      "Text 2:      Beyond Intermediate States: Explaining Visual Redundancy through Language Dingchen Yang* Tongji University dingchen yang@tongji.edu.cn Bowen Cao CUHK Anran Zhang Tencent Hunyuan Team Weibo Gu Tencent Hunyuan Team Winston Hu Tencent Hunyuan Team Guang Chenâ€  Tongji University guangchen@tongji.edu.cn Abstract Multi-modal Large Langue Models (MLLMs) often process thousands of visual tokens, which consume a significant portion of the context window and impose a substantial computational burden. Prior work has empirically explored visual token pruning methods based on MLLMsâ€™ interme- diate states ( e.g., attention scores). However, they have limitations in precisely defining visual redundancy due to their inability to capture the influence of visual tokens on MLLMsâ€™ visual understanding (i.e., the predicted probabil- ities for textual token candidates). To address this issue, we manipulate the visual input and investigate variations in the textual output from both token-centric and context- centric perspectives, achieving intuitive and comprehen- sive analysis. Experimental results reveal that visual to- kens with low ViTâˆ’[cls] association and low text-to-image attention scores can contain recognizable visual cues and significantly contribute to imagesâ€™ overall information. To develop a more reliable method for identifying and prun- ing redundant visual tokens, we integrate these two per- spectives and introduce a context-independent condition to identify redundant prototypes from training images, which probes the redundancy of each visual token during infer- ence. Extensive experiments on single-image, multi-image and video comprehension tasks demonstrate the effective- ness of our method, notably achieving 90% to 110% of the performance while pruning 80% to 90% of visual tokens. Code will be available at https://github.com/ DingchenYang99/RedundancyCodebook.git. 1. Introduction Multi-modal Large Language Models (MLLMs) [19, 20, 26] have demonstrated remarkable performance across a *Work done during an internship at tencent hunyuan team â€ Corresponding author range of vision-language tasks, including high-resolution image and video comprehension, by integrating thousands of visual tokens. However, This approach introduces sev- eral challenges. First, visual tokens encroach upon the con- text window required for textual tokens, and may interfere with MLLMsâ€™ text processing capabilities [45]. Second, the quadratic complexity of the self-attention mechanism [37] significantly increases the computational burden. Conse- quently, reducing redundant visual tokens is crucial for en- hancing the overall performance and efficiency of MLLMs. To reduce the number of visual tokens while mitigating performance degradation, recent research has empirically explored leveraging MLLMsâ€™ intermediate states to guide inference-time visual token pruning. The two primary ap- proaches are: (1) utilizing the ViT âˆ’[cls] token [31], which encodes global image information, and (2) leveraging the scalar attention scores of textual tokens to visual tokens in the LLM [4], which capture cross-modal information flow. However, these intermediate-state-based methods struggle to explicitly characterize the influence of each visual token on MLLMsâ€™ visual understanding outcome, i.e., the final probability prediction, as attention value vectors also play a crucial role in the attention mechanism, and the represen- tation of one token progressively transforms into that of its next token in auto-regressive LLMs. This limitation hin- ders the interpretable definition of visual redundancy and risks pruning informative visual tokens in MLLMs. In this study, we aim to provide a more precise expla- nation of visual redundancy in MLLMs, which first re- quires identifying the direct impact of each visual token on MLLMsâ€™ visual understanding. Since humans understand images by attending to individual visual cues and assessing their contributions to the overall image representation, we analyze the influence of visual tokens from two perspec- tives: (1) Token-Centric perspective, which examines the inherent visual information encoded in each visual token, and (2) Context-Centric perspective, which evaluates how arXiv:2503.20540v1 [cs.CV] 26 Mar 2025Attention Scores of the First Decoded Token to Image Patches @ LLM layer # [2, 16, 30] top-1 candidateâ€™s probability Semantic interpretation of individual visual tokensfrom the MLLMâ€™s own language decoder ViT-[cls] sim @ ViTlayer #22 Patch # Greedy Decoded Token Top-1 Prob. Top-1 Logits ViT-[cls] Similariy LLM Layer #2 Attn Score LLM Layer #16 Attn Score LLM Layer #30 Attn Score 22 Food 0.3539 17.281 0.0117 7.27E-06 2.26E-04 6.46E-04 89 Carrot 0.7897 21.172 0.0275 1.33E-05 2.15E-04 4.30E-04 114 Carrot 0.8167 21.172 0.0366 5.29E-05 2.64E-04 2.19E-04 115 Pepper 0.4341 18.891 0.0005 4.31E-05 3.78E-04 2.75E-04 118 Bowl 0.6089 19.781 0.0074 9.78E-06 1.70E-04 4.62E-05 141 Spices 0.4127 17.625 0.0027 1.06E-05 1.65E-04 1.32E-04 160 Potato 0.4094 18.109 0.0510 1.20E-05 2.31E-04 1.20E-05 425 Spoon 0.8455 20.797 0.1009 9.42E-06 2.73E-05 3.14E-05 431 Cat 0.0644 14.758 0.2793 1.27E-03 1.49E-03 4.37E-03 501 Nothing 0.0945 14.914 0.4121 2.05E-03 1.69E-03 4.68E-04 523 Tree 0.0701 14.086 0.2439 7.14E-03 5.81E-03 5.11E-03 524 Picture 0.0771 14.789 0.3865 1.84E-02 1.33E-03 2.24E-04 548 Picture 0.0929 14.742 0.2749 4.61E-03 3.20E-03 7.23E-03 571 Picture 0.0907 14.297 0.3972 2.01E-03 4.60E-03 6.73E-04 lower valuehigher value Figure 1. We investigate the inherent information encoded in individual visual tokens by instructing LLaV A-Next to describe them and analyzing the corresponding decoding results, predicted probabilities, and confidence scores (logits). â€œPatch #â€ indicates the index in the flattened patch sequence. Some visual tokens with low ViT âˆ’[cls] similarity and low attention scores ( e.g., Patch #114, #160, and #425) contain valid visual information (e.g., Carrot, Potato, and Spoon) that the model recognizes with high confidence (40% to 80% probability). Conversely, despite having high ViTâˆ’[cls] similarity and high attention scores (highlighted in the red box), certain visual tokens yield text descriptions unrelated to the image patches (e.g., Cat and Tree), with model confidence lower than 10%. each visual token affects the broader visual context (i.e., im- ages or image regions). For the token-centric analysis, we devise a single-token-input experiment (Section 3.1.1), iso- lating each visual token and instructing the MLLM to inter- pret the information it contains. This experiment reveals that MLLM can recognize valid visual information from visual tokens with low ViT âˆ’[cls] similarity and low text- to-image attention scores. As shown in Figure 1, LLaV A- Next [19] predicts Carrot and Spoon with over 80% con- fidence from patches #114 and #425, which depict carrot and spoon, respectively. For the context-centric analysis, we design a leave-one-token-out experiment (Section 3.1.2) to examine how removing individual visual tokens affects the predicted probability distribution. Experimental results indicate that certain visual tokens with low ViTâˆ’[cls] simi- larity and low text-to-image attention scores can still signifi- cantly influence MLLMsâ€™ understanding of their associated image (Section 3.2). These findings warrant a reconsidera- tion of the definition of visual redundancy in MLLMs. Based on our token-centric and context-centric analyses, we propose that redundant visual tokens should be identi- fied according to two fundamental criteria: the visual to- ken (1) lacks recognizable visual information and (2) does not significantly impact the overall information of its as- sociated image. Building on the feature analysis of visual tokens that satisfy these criteria, we introduce a context- independent condition to identify redundant prototypes that are unlikely to influence visual information across differ- ent images, thus demonstrating potential for generaliza- tion. Leveraging these criteria, we propose an identify- then-probe strategy for inference-time visual token pruning. First, We use training images to identify redundant proto- types and store them in an extensibleredundancy codebook. During inference, visual tokens that exhibit higher similar- ity to these prototypes are deemed more likely to be redun- dant and are removed before sending to the LLM. We evaluate the effectiveness of our approach on five single-image Visual Question Answering (VQA) bench- marks [10, 18, 23, 28, 40] and two image captioning bench- marks [1, 49]. On average, our method preserves 90% of the performance of LLaV A-Next [19, 26] and LLaV A-1.5 [24, 25] while pruning 90% of visual tokens, outperforming rep- resentative methods [4, 31] that rely on MLLMsâ€™intermedi- ate states. Furthermore, our approach is adaptable to multi- image and video comprehension tasks [15, 21, 39], achiev- ing up to a 10% performance improvement for LLaV A- OneVision [20] while pruning 80% of visual tokens. These results validate the effectiveness of our approach2. Related Work Leading MLLMs [19, 20, 25, 26] process high-resolution images and multiple video frames by incorporating numer- ous visual tokens. For instance, LLaV A-Next and LLaV A- OneVision represent an image using a maximum of 2,880 and 7,290 visual tokens, respectively. These visual tokens occupy a large portion of the context window of their LLM1, leading to increased computational overhead and poten- tially impairing MLLMsâ€™ text processing capabilities [45]. 2.1. Identifying Redundant Visual Tokens To alleviate the computational burden associated with vi- sual tokens, pioneering studies explore MLLMsâ€™intermedi- ate states to estimate the redundancy of visual tokens. The methodologies can be broadly categorized into two types: 2.1.1. Vision-Centric Visual Redundancy Estimation This line of work presumes that visual tokens that do not align with the overall information of the image or exhibit duplicated features are redundant. The alignment between image patches and the imageâ€™s overall information is eval- uated by their association with the [cls] token in the Vision Transformer (ViT [8]) model [12, 27, 31, 45, 51], or by the attention scores between one image patch and all other patches [38, 42, 44, 50]. To identify duplicate visual to- kens, the feature similarity of patches within a local spa- tial region [27, 51] or a spatio-temporal region [32, 34] is assessed. These strategies typically distinguish foreground objects from background patches. However, given that vi- sual tokens are further processed by the LLM during the prefill stage2 for cross-modal feature interaction and text decoding, we advocate for explaining the information en- coded in visual tokens from the viewpoint of the LLM. 2.1.2. Instruction-Based Visual Redundancy Estimation This line of work focuses on the cross-modal information flow within LLMs, identifying visual tokens that are irrel- evant to the input question as redundant. This relevancy is typically estimated using the attention scores of textual to- kens to visual tokens (referred to as text-to-image attention scores) [4, 12, 27, 32, 33, 41, 52, 54], or the accumulative attention scores of visual tokens [13, 16, 36, 47]. These methods propose classifying visual tokens with lower at- tention scores as redundant, as they are minimally involved in the cross-modal feature interaction process. In summary, both vision-centric and instruction-based strategies extensively utilize MLLMsâ€™ intermediate states to estimate visual redundancy. However, the specific influ- ence of visual tokens with low ViT-[cls] association or low 1Length of 8,192 tokens for LLaMA3 [2] and 32,768 for Qwen2 [43]. 2The first forward computation process in the LLM that decode the first token utilizing all visual and textual token embeddings. text-to-image attention scores on MLLMsâ€™ output probabil- ity distribution remains unclear. This ambiguity can result in inaccurate identification of redundant visual tokens. 2.2. Reducing Visual Tokens in MLLMs Training-based Methods. Earlier works design additional networks modules [5, 6, 14, 22, 29, 35] or tunable embed- dings [46, 48] to compress image patch features into com- pact embeddings, resulting in substantial training cost. Training-free Methods. Recent work achieves training- free visual token pruning by leveraging MLLMsâ€™ interme- diate states , discarding visual tokens based on carefully crafted redundancy estimation metrics [4, 16, 32, 41, 47]. Furthermore, visual tokens can be aggregated into identi- fied anchor tokens that encapsulate condensed visual infor- mation [12, 31, 42, 45, 52], thereby mitigating information loss. However, inaccurate identification of redundant visual tokens can compromise the effectiveness of these methods. In this study, we propose to explain visual redundancy by examining the impact of visual tokens on MLLMsâ€™ predic- tions, instead of MLLMsâ€™ intermediate states, and design a training-free pruning strategy. 3. Visual Redundancy Analysis An interpretable definition of visual redundancy necessi- tates recognizing the direct impact of individual visual to- kens on the MLLMâ€™s visual understanding outcome, i.e., the final probability prediction. In this section, we devise novel experimental frameworks and metrics to explore this often-overlooked issue, thereby providing new insights into the identification of redundant visual tokens in MLLMs. 3.1. Background and Analysis Method Existing methods estimate visual redundancy by exten- sively utilizing the scalar attention scores derived from the query and key matrices, and infer that a lower attention score indicates a weaker correlation between the query and a key feature. However, these attention scores are insuf- ficient for elucidating the exact contribution of visual to- kens on MLLMsâ€™ final probability prediction, considering the numerous attention layers and heads, the impact of the attention value vector, and the feature transformation pro- cess in auto-regressive LLMs, where the feature of one to- ken progressively transform into that of its next token (more details in Appendix 1.1). Given these challenges, we shift our research focus to an input-output analytical approach, examining variations in model output upon manipulating input visual tokens. We anticipate that this approach will yield more intuitive and interpretable results. Additionally, to rigorously analyze MLLMsâ€™ compre- hension of visual tokens, we propose an approach inspired by human interpretation of visual media. As humans typ- ically achieve comprehensive image understanding by ob-Prompt: What does this image show? You must provide a single word or phrase that best describes it. + promptLLMsingle visual token inputViTMLP [y1]measure top-1 candidateâ€™sprobabilityregion level leave-one-out ViTMLP + prompt[y1] ViTMLP + prompt[!y1] measure changes in predicted probability distribution3x3region all visualtokens global level leave-one-out ViTMLP + prompt[y1] ViTMLP + prompt[!y1] â€¦ â€¦ Img decode P LLMdecode LLMdecode Img Img LLMdecode LLMdecode Img Img â€¦ â€¦ â€¦ â€¦ â€¦ â€¦PPPPPP PPP target visual tokenneighbor image region PLLMâ€™s Pad Token Embedding measure changes in predicted probability distribution Figure 2. Overview of our proposed visual redundancy analysis pipeline. In the single visual token inputexperiment, we provide a single visual token to the LLM and instruct it to describe the visual content. By analyzing the predicted probabilities, we assess the significance of the information encoded in each visual token. Next, we examine the influence of individual visual tokens on the broader visual context (image or image region) by measuring changes in the predicted probability distribution before and after ablating specific visual tokens. The region level leave-one-outexperiment evaluates the influence of a single visual token (highlighted in red) on its neighboring image region, while the global level leave-one-outexperiment assesses the impact of this region on the entire image. The results from these two experiments are combined to quantify the influence of individual visual tokens on the overall image representation. serving individual visual elements and assessing their im- pact on the overall semantic context of the image, we ad- dress the following two problems: 3.1.1. Addressing the Token-Centric Problem In this part, we investigate what information does indi- vidual visual token inherently possess. Note that we dis- cuss visual information from the viewpoint of the LLM, as it further aggregates visual information from visual tokens produced by the vision encoder and generates textual re- sponses. To explore this, we devise asingle visual token in- put experiment, as illustrated in Figure 2. We provide only one visual token to the LLM to eliminate the interference from other visual tokens and instruct the LLM to describe the visual content. Subsequently, we analyze the text de- coding results and the predicted probabilities to uncover the LLMâ€™s interpretation of the visual information. To evaluate whether individual visual tokens contain rec- ognizable information, we assess the magnitude of the prob- ability of the 1st ranked textual token candidate (denoted as top-1 probability). A higher top-1 probability indicates that the LLM has greater confidence in a strong association between the 1st ranked textual token and the input visual token. Conversely, if the top-1 probability is close to zero, we infer that the visual token does not contain significant visual information, as the LLM predicts close confidence scores (i.e., logits) for various candidates in the vocabulary, indicating high uncertainty. Details are in Appendix 1.2. 3.1.2. Addressing the Context-Centric Problem We further investigatehow individual visual tokens influ- ence the overall information of the broader visual con- text (image or image region) by conducting a leave-one- token-out experiment, evaluating the difference in the pre- dicted probability distribution before and after the ablation of input visual tokens. However, our preliminary experi- ments reveal that removing a single visual token from the image token sequence results in numerically insignificant changes in the predicted probabilities, which poses chal- lenges for subsequent analysis (details in Appendix 1.3). To address this, we devise a cascaded leave-one-out ex- periment, as illustrated in Figure 2. First, we conduct a region-level leave-one-out experiment within the 3Ã—3 spa- tial neighborhood of a target visual token. We compare the output variations before and after replacing the target visual token with the LLMâ€™s pad token embedding P. This exper- iment demonstrates the impact of a single visual token on the information of its neighboring region. To reveal the in- fluence of this region on the overall information of the im- age, we conduct a global-level leave-one-out experiment, inspecting the output variations before and after replacing nine visual tokens in this region with P. By cascading the results of these two experiments, we determine the influence of individual visual tokens on the overall information of the image. We employ Jensen-Shannon Divergence (JSD) to assess the difference between two probability distributions. The final results are obtained by a weighted sum of the JSD values from these two experiments.region-level JSD values global-level JSD values #510 #510#523 #523 Rank Candi. region input logits w/ pad input logits diff 1 Soup 17.3125 13.8281 3.4844 2 Food 16.0625 15.4688 0.5938 3 St 14.7813 12.2813 2.5000 4 Nothing 14.5781 14.8438 -0.2656 5 B 14.3516 12.8750 1.4766 6 Picture 14.3438 14.3594 -0.0156 7 Chicken 14.0234 11.9688 2.0547 8 Pot 13.9375 12.3125 1.6250 9 P 13.9141 13.8594 0.0547 10 L 13.8828 13.4063 0.4766 Rank Candi. global input logits w/ pad input logits diff 1 Soup 19.2969 19.0000 0.2969 2 B 18.4219 18.2813 0.1406 3 Food 18.4063 18.3750 0.0313 4 Ve 18.2031 18.1875 0.0156 5 St 18.0781 17.8594 0.2188 6 S 17.9688 18.0156 -0.0469 7 D 17.9375 17.9063 0.0313 8 Pot 17.6875 17.7656 -0.0781 9 Car 17.3594 17.4375 -0.0781 10 P 16.8594 16.9219 -0.0625 Rank Candi. region input logits w/ pad input logits diff 1 Soup 17.3125 13.8281 3.4844 2 Food 16.0625 15.4688 0.5938 3 St 14.7813 12.2813 2.5000 4 Nothing 14.5781 14.8438 -0.2656 5 B 14.3516 12.8750 1.4766 6 Picture 14.3438 14.3594 -0.0156 7 Chicken 14.0234 11.9688 2.0547 8 Pot 13.9375 12.3125 1.6250 9 P 13.9141 13.8594 0.0547 10 L 13.8828 13.4063 0.4766 Rank Candi. global input logits w/ pad input logits diff 1 Soup 19.2969 19.0000 0.2969 2 B 18.4219 18.2813 0.1406 3 Food 18.4063 18.3750 0.0313 4 Ve 18.2031 18.1875 0.0156 5 St 18.0781 17.8594 0.2188 6 S 17.9688 18.0156 -0.0469 7 D 17.9375 17.9063 0.0313 8 Pot 17.6875 17.7656 -0.0781 9 Car 17.3594 17.4375 -0.0781 10 P 16.8594 16.9219 -0.0625 rankcandi.3x3 logits3x3+Pad logitsdiff.rankcandi.576 xlogits576+Pad logitsdiff. Patch #Top-1 ProbabilityViT-[cls]-simAttn Score @ layer #[2, 16, 30]510523 0.150.070.030.24[6.4e-5, 2.6e-4, 4.6e-4][7.1e-3, 5.8e-3, 5.1e-3] Figure 3. Visual tokens with low ViT âˆ’[cls] similarity and text- to-image attention scores can more significantly impact LLaV A- Nextâ€™s understanding of the image, as patch #510 has higher JSD values than patch #523. candi. and diff. denote candidates and differences, respectively. Patch #510 primarily contributes the se- mantic information Soup to its neighboring region (+3.4844 con- fidence scores) and to the entire image (+0.2969 scores). 3.2. Discoveries We compare the top-1 probability and the JSD results with commonly addressed intermediate states in MLLMs, in- cluding the cosine similarity to the [cls] token in the penul- timate ViT layer and the attention scores of textual tokens to visual tokens in the LLM (i.e., the text-to-image attention scores). Our main findings are summarized as follows: Finding 1. Visual tokens with low ViT âˆ’[cls] similar- ity and low text-to-image attention scores may contain recognizable visual information. For instance, LLaV A- Next predicts the word Carrot with 80% confidence for the image patches depicting carrots in the pink box in Fig- ure 1. However, the ViT âˆ’[cls] similarities and attention scores of these patches are only around 0.03 and in the range of 1e-5 to 1e-4, respectively. Conversely, some vi- sual tokens with higher ViT âˆ’[cls] similarity and text-to- image attention scores do not contain recognizable visual information. For instance, LLaV A-Next predicts irrelevant textual responses (e.g., Cat and Tree) with low confidence (<10%) for six patches in the uninformative white region in the red box in Figure 1, which have high ViTâˆ’[cls] similar- ity around 0.4 and attention scores on the order of 1e-2. Finding 2. Visual tokens with low ViT âˆ’[cls] similar- ity and low text-to-image attention scores can substan- tially influence the information of their visual context . For example, patch #510 in Figure 3 significantly affects JSD ValuesTop-1 Candidateâ€™s ProbabilitiesSum of Attention Scores in LLM LayersViT-[cls] Cosine Similarities Figure 4. Quantitative results on 6,400 image patches sampled from the VQAv2 validation set. As the text-to-image attention score and the ViTâˆ’[cls] similarity decrease, the top-1 probability and the Jenson-Shannon Divergence do not show a declining trend; instead, they fluctuate around 0.24 and 4e-3, respectively. The results are averaged across 100 image samples. the information of its 3 Ã—3 neighboring region. The pre- dicted confidence scores (i.e., logits) for specific candidates (e.g., Soup and Chicken) show notable variation ( âˆ’2 to âˆ’3 scores) after patch #510 is ablated. This pattern results in a more significant difference in the probability distribu- tion and a larger JSD value. Additionally, the neighboring region of patch #510 also notably impacts the overall image information, achieving one of the highest JSD values across all image regions. However, patch #510â€™s text-to-image at- tention scores are only at the magnitude of 1e-4, and its ViTâˆ’[cls] similarity is merely 0.03. In contrast, patch #523 has attention scores and ViT âˆ’[cls] similarity an order of magnitude higher than those of patch #510, while ablating it or its neighboring region results in a more negligible dif- ference in the model prediction and a lower JSD value. Additional Evidences. To substantiate the two find- ings, we sample 6,400 image patches from the VQAv2 [11] validation set to conduct single-token-input and cascaded leave-one-out experiments. The results for these image patches are reordered based on the ViT âˆ’[cls] similarity or the text-to-image attention score to illustrate variation trends. As shown in Figure 4, when the text-to-image atten- tion score and the ViT âˆ’[cls] similarity decrease, the top-1 probability and JSD value do not show a corresponding de- cline but rather a fluctuating pattern. More details and dis- cussions are in Appendix 1.4. Therefore, directly pruning visual tokens with low ViTâˆ’[cls] similarity or low text-to- image attention scores may lead to the loss of visual infor- mation and changes in the overall information of the image.VisionEncoder (ViT) Connector (MLP) splitpatches patch features visual tokens[B, L, d] Language Decoder (LLM)retained visual tokens+ textual response test imagestrain imagessame MLLMvisual tokens &model outputsRedundancyEvaluation Is there a red double-decker bus in <image> and <image>?questiontokenizer âŒ Word Token Embed textualtokens[B, R, d] identify-then-pruneapproach Redundancy Codebook ðŸ“–Prototypes linearprobing Figure 5. An overview of our identify-then-probe approach. We identify redundant prototypes from training images using single- input and cascaded leave-one-out experiments, and store them in a extensible codebook. During inference, visual tokens with higher similarity to these prototypes are considered more likely to be re- dundant and are removed before the first layer of the LLM. L and R are the number of input and retained visual tokens, respectively. 4. Method Building on our analysis of the direct impact of individual visual tokens on MLLMsâ€™ visual understanding outcomes, we explore more reliable approach to identify redundant vi- sual tokens. Next, we propose an identify-then-probe strat- egy for efficient inference-time visual token pruning, recog- nizing that single-input and leave-one-out experiments en- tail significant computational overhead. An overview of our approach is depicted in Figure 5. Initially, we identify re- dundant prototypes from training images using these two experimental frameworks and store them in acodebook. We then utilize these prototypes to probe the redundancy of vi- sual tokens during inference. 4.1. Constructing the Redundancy Codebook Based on the impact of individual visual tokens on MLLMsâ€™ visual understanding from both token-centric and context- centric perspectives, we define a potentially redundant vi- sual token (referred to as redundant candidate) as one that meets two fundamental criteria: (1) it lacks recognizable visual information, and (2) it does not substantially af- fect the overall information of its associated image. Ad- ditionally, we observe that certain redundant candidates from different images exhibit high similarity, indicating that these clusters fail to contribute substantial informa- tion across various visual contexts, thus demonstrating po- tential generalization capability. Consequently, we intro- duce a context-independent condition to identify redundant candidates with this characteristic as redundant prototypes, which are stored in an extensible redundancy codebook to facilitate flexible and scalable applications. 4.1.1. Token-Centric Visual Redundancy Evaluation The token-centric criterion is designed to identify visual tokens that lack recognizable visual information. As dis- cussed in Section 3.2, a low top-1 probability (the probabil- ity of the 1st ranked textual token candidate, obtained from the single-token-input experiment) indicates the MLLMâ€™s inability to recognize valid information in individual visual tokens. Thus, we establish a probability threshold Ï„prob to filter out visual tokens with lower top-1 probability. To improve the accuracy in identifying visual tokens that lack recognizable visual information, we employ t-SNE to visualize the distribution of visual tokens of an image in the high-dimensional feature space. We observe that visual tokens with very low top-1 probability frequently manifest as discrete outliers (as illustrated in Appendix Figure 2). Therefore, we use the Density Peaks Clustering (DPC) al- gorithm to find visual tokens that belong to clusters with sizes below a specified outlier threshold Ï„out. 4.1.2. Context-Centric Visual Redundancy Evaluation The context-centric criterion is designed to identify visual tokens with minimal contribution to their visual context . Recall that a low Jensen Shannon Divergence (JSD) in the cascaded leave-one-out experiment indicates negligible in- fluence of individual visual tokens on MLLMsâ€™ understand- ing of their associated image (Section 3.2), we set a JSD threshold Ï„jsd to filter out visual tokens with lower JSD. We then identify redundant candidates by taking the inter- section of visual tokens filtered by Ï„prob, Ï„out, and Ï„jsd. Context-independent Condition. After identifying the redundant candidates from training images, we further in- vestigate their capability to generalize in evaluating the vi- sual redundancy of test images. We analyze the distribution of these redundant candidates utilizing t-SNE and observe that some redundant candidates from different images es- tablish several high-density clusters (as shown in Appendix Figure 3). This phenomenon suggests that, despite dif- ferences in the images, certain redundant candidates share common features. This characteristic indicates potential for generalization. Consequently, we apply the DPC algorithm again to filter out redundant candidates that belong to clus- ters with sizes exceeding a specified inlier threshold Ï„in, thereby gathering visual tokens that are unlikely to con- tribute substantial information regardless of the visual con- text in which they appear. Summary. We use the four thresholds to filter out N visual tokens {vi}N i=1 from training images X: {vi}N i=1 = CC(T C(X|Ï„prob, Ï„out)|Ï„jsd, Ï„in), (1) where vi âˆˆ Rd, d is the feature dimension, T C(Â·) andCC(Â·) are token-centric and context-centric redundancy evaluation methods, respectively. {vi}N i=1 are the redun- dant prototypes, We stack them together to build the re- dundancy codebook CNÃ—d. We sample images X from the Karpathy train split of the COCO Caption dataset [17]. 4.2. Pruning Visual Tokens using the Codebook In the preceding paragraphs, we have identified redundant prototypes from different images that exhibit analogous fea- tures. Based on this characteristic, we infer that visual to- kens with higher similarity to these prototypes are more likely to be redundant, and pruning them should have lower impact on MLLMâ€™s visual understanding outcome. There- fore, we utilize the redundancy codebook CNÃ—d to probe the redundancy of L input visual tokens T LÃ—d of the test images using the cosine similarity: SLÃ—N = norm(T LÃ—d) Â· (norm(CNÃ—d))T , (2) where the norm(Â·) function is the L2 normalization algo- rithm along the feature dimension. We define the redun- dancy score as the maximum cosine similarity among the N results. Finally, R visual tokens with the lowest redun- dancy scores are retained for the LLM (R<L, more details are in Appendix 2.2). Different from previous work that employs a huge codebook (e.g., 217 embeddings as in [30]) to augment the input visual embeddings, we find that a tiny codebook with fewer than 1,000 redundant prototypes gen- eralizes well to test images. Our method can be integrated into various MLLMs without additional training. 5. Experiments 5.1. Experimental Settings Benchmarks and Metrics. We evaluate the effective- ness of our approach on various vision-language tasks, including single-image Visual Question Answering (on POPE [23], MMBench [28], SEED-Image [18], MME [10], and RealWorld-QA [40] benchmarks), image caption- ing (NoCaps-val [1] and Flickr30k-test [49]), and multi- image and video comprehension (Mantis-test [15], Muir- Bench [39], and MVBench [21]). We adhere to the officially defined metrics (Exact Match Accuracy) for VQA, and uti- lize the SPICE [3] metric for image captioning, which em- phasizes semantic correspondence. Implementation Details. We implement our method on three MLLMs: LLaV A-1.5 [24, 25], LLaV A-Next [19, 26], and LLaV A-OneVision [20]. For each model, we construct a distinct codebook, as model predictions are necessary to evaluate the contribution of visual tokens. We set a thresh- old to remove visual tokens with the highest redundancy score. We employ the greedy decoding method for repro- ducible results. Detailed settings are in Appendix 3.1. 5.2. Experimental Results We compare the performance of our method with two representative approaches that leverage MLLMsâ€™ interme- diate states : the vision-centric method PruMerge [31], which prunes visual tokens with lower association with the ViT âˆ’[cls] token, and the instruction-based method FastV [4], which leverages the attention scores of the last textual token to visual tokens within the LLM. For a fair comparison, we maintain a training-free setting and adhere to the same visual token quantity budgets. 5.2.1. Single-Image Comprehension Results on single-image VQA and captioning tasks are pre- sented in Table 1. Notably, for the LLaV A-1.5 model, our method preserves 90% of peak performance ( i.e., with 576 input visual tokens) on average across five VQA bench- marks while retaining only 11% of visual tokens. In con- trast, both the vision-centric and instruction-based strate- gies achieve approximately 85%. When retaining 25% of visual tokens, our method maintains or slightly exceeds the performance ceiling on two image captioning benchmarks, significantly outperforming the vision-centric strategy (82% performance) and the instruction-based strategy (94% per- formance). Under both the sub-image splitting and non- splitting settings of the LLaV A-Next model, our method preserves 95% and 91% performance, respectively, while retaining only 11% of visual tokens. In contrast, the ran- dom pruning baseline achieves 87% and 84%. Additionally, our method maintains 90% performance for the LLaV A- Next model under a very low retention rate of visual tokens (5.5%). These results demonstrate that assessing visual re- dundancy based on MLLMsâ€™ predictions is superior to uti- lizing MLLMsâ€™ intermediate states. Qualitative results in Appendix Figures 5 to 9 show that our method allocates the limited visual token budget to critical visual cues in both natural photographs and text-rich images. 5.2.2. Multi-Image and Video Comprehension Results on multi-image and video comprehension tasks are presented in Table 2. On Mantis-test and MuirBench, the performance of LLaV A-OneVision improves by 5% after randomly removing 80% of visual tokens, while our method achieves a higher enhancement of 10%. This suggests that an excessive number of visual tokens may impede the modelâ€™s ability to comprehend image-text-interleaved con- texts. In the MVBench video understanding benchmark, our approach maintains 94% performance even with an extreme visual token removal rate of 92%, significantly surpassing the random baseline. These results demonstrate that our method can effectively transfer from single-image to multi- image and video comprehension tasks.Model Method POPE MMB en SEEDI RWQA MME P NoCaps Flickr30k LLaV A-1.57B w/o Split576Ã— 85.6 62.9 65.4 56.1 1458.9 16.5 20.0 Retain 144 visual tokens PruMerge [31] 75.2 57.7 55.7 46.8 1280.8 14.0 15.8 FastV [4] 79.5 62.2 61.2 51.2 1388.2 15.5 18.8 Ours 84.7 61.6 62.6 52.7 1369.1 16.4 20.2 Retain 64 visual tokens PruMerge [31] 73.5 54.6 53.2 48.4 1228.6 12.9 14.8 FastV [4] 69.3 59.9 54.6 47.6 1150.6 13.4 15.3 Ours 79.9 57.1 57.3 48.5 1290.5 15.1 18.8 LLaV A-Next8B w/o Split576Ã— 83.9 72.2 71.4 56.2 1504.2 16.1 18.8 w Split2880Ã— 87.8 72.1 72.7 59.5 1555.8 16.6 19.3 w/o Split, Retain 64 visual tokens Random 76.7 Â±0.2 59.2Â±0.7 62.0Â±0.2 46.7Â±0.9 1188.2Â±10.6 13.5Â±0.02 15.1Â±0.02 Ours 80.8 66.6 63.7 54.6 1224.4 15.1 17.8 w Split, Retain 64 visual tokens per sub-image Random 81.7 Â±0.3 63.3Â±0.4 65.7Â±0.1 47.9Â±1.1 1339.0Â±14.3 14.6Â±0.1 16.6Â±0.01 Ours 85.2 69.6 68.3 57.5 1343.8 16.1 18.8 w Split, Retain 32 visual tokens per sub-image Random 77.9 Â±0.2 58.4Â±0.4 62.1Â±0.1 45.5Â±0.2 1209.4Â±19.3 13.5Â±0.04 15.0Â±0.02 Ours 82.7 66.2 64.4 55.2 1254.1 15.2 17.7 Table 1. Results on single-image VQA and image captioning benchmarks. The officially defined accuracy metric is reported for POPE, MMB-en, SEED-Image, RealWorldQA (RWQA) and MME-Perception. For the image captioning benchmarks NoCaps and Flickr30k, we report the SPICE metric. Our method outperforms representative methods that utilize MLLMsâ€™ intermediate states. For the random baseline, we report the average results and the standard deviations from three separate runs. Method Mantis-test MUIRBench MVBench 729 per image 196 / img w/o Split 59.0 (1814Ã—) 42.7(3158Ã—) 58.7(3136Ã—) Retain 144 per image 16 / img Random 61.4 Â±0.6(358Ã—) 45.2Â±0.1(624Ã—) 53.2Â±0.3(256Ã—) Ours 63.6 (351Ã—) 48.1(626Ã—) 55.0(256Ã—) Table 2. LLaV A-OneVision-7B results on multi-image and video comprehension benchmarks. Our proposed method maintains over 90% of peak performance and achieves a 10% performance gain by pruning 80% to 90% of input visual tokens. 5.3. Efficiency Analysis During inference, the primary computational overhead in- troduced by our method is the calculation of the similarity matrix SLÃ—N , which incurs a marginal cost of L Ã— N Ã— (2d âˆ’ 1) floating-point operations (FLOPs). The codebook requires approximately 0.5 GB of GPU memory. 5.4. Ablation Study We assess the effectiveness of each component (Ï„prob, Ï„jsd, Ï„out, and Ï„in) in our proposed method by individually ab- lating them and evaluating the average performance on five single-image VQA benchmarks. Table 3 demonstrates that each component contributes positively to the overall per- formance. Notably, the removal of Ï„prob leads to a signif- icant performance drop for LLaV A-Next (decreasing from Ï„prob Ï„jsd Ï„out Ï„in # Img. N Avg. Perf. âœ“ âœ“ âœ“ âœ“ 100% 969 91.3% - âœ“ âœ“ âœ“ 100% 5,086 84.9% âœ“ - âœ“ âœ“ 100% 1,474 90.6% âœ“ âœ“ - âœ“ 100% 2,884 91.2% âœ“ âœ“ âœ“ - 100% 1,151 90.1% âœ“ âœ“ âœ“ âœ“ 20% 185 88.0% random baseline 84.5% Table 3. Ablation studies on five single-image VQA benchmarks of LLaV A-Next. Each component in our proposed method con- tributes positively to the average performance (Avg. Perf.). â€œ# Img.â€ denotes the percentage of sampled images used to identify redundant prototypes. N is the number of redundant prototypes. 91.3% peak performance to 84.9%, approaching the random baseline). In contrast, the performance degradation caused by the removal of other components is relatively moderate. Additionally, reducing the number of sampled training im- ages decreases the number of redundant prototypes from 969 to 185, accompanied by a 3.3% performance decline. Consequently, we opt to use the 969 identified redundant prototypes for LLaV A-Next. 6. Conclusion We explore interpretable definition of visual redundancy in MLLMs, focusing on the influence of individual visual tokens on MLLMsâ€™ visual understanding outcome, whichis a often-overlooked issue. To intuitively and comprehen- sively investigate this issue, we develop input-to-output analytical approaches from both token-centric and context- centric perspectives. We reveal that visual tokens with low ViTâˆ’[cls] similarity and low text-to-image attention scores can contain recognizable visual information and substantially influence their visual context . Building on these findings, we propose a novel method to identify redundant visual tokens by combining thetoken-centric and context-centric criteria, along with a context-independent condition. Utilizing this redundancy evaluation method, we design an efficient and scalable identify-then-probe approach for training-free visual token pruning. On single- image, multi-image and video comprehension benchmarks, our method achieves 90% to 110% performance while pruning 80% to 90% of visual tokens, surpassing exist- ing methods that rely on MLLMsâ€™ intermediate states . References [1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Ste- fan Lee, and Peter Anderson. Nocaps: Novel object caption- ing at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8948â€“8957, 2019. 2, 7 [2] AI@Meta. Llama 3 model card. 2024. 3, 1 [3] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image cap- tion evaluation. In Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, Octo- ber 11-14, 2016, Proceedings, Part V 14 , pages 382â€“398. Springer, 2016. 7 [4] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 19â€“35. Springer, 2025. 1, 2, 3, 7, 8 [5] Shimin Chen, Yitian Yuan, Shaoxiang Chen, Zequn Jie, and Lin Ma. Fewer tokens and fewer videos: Extending video understanding abilities in large vision-language mod- els. arXiv preprint arXiv:2406.08024, 2024. 3 [6] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty- seventh Conference on Neural Information Processing Sys- tems, 2023. 3 [7] Timoth Â´ee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In The Twelfth International Conference on Learning Representa- tions, 2024. 3 [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 [9] Mingjing Du, Shifei Ding, and Hongjie Jia. Study on den- sity peaks clustering based on k-nearest neighbors and prin- cipal component analysis. Knowledge-Based Systems, 99: 135â€“145, 2016. 4 [10] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A compre- hensive evaluation benchmark for multimodal large language models, 2024. 2, 7 [11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba- tra, and Devi Parikh.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02278):\n",
      "Paper 1:     2503.20672v1.BizGen__Advancing_Article_level_Visual_Text_Rendering_for_Infographics_Generation.pdf\n",
      "Paper 2:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Embedding 1: [-0.41494468  0.21788508 -3.14588547  0.49967784  0.26698312]...\n",
      "Embedding 2: [-0.88073677 -0.39981037 -3.37348676 -0.16465193  0.22870453]...\n",
      "Text 1:      1 [9] Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, and Shiyu Chang.\n",
      "Text 2:      1 [18] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wan- jun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02282):\n",
      "Paper 1:     2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "Paper 2:     2503.20676v1.Inductive_Link_Prediction_on_N_ary_Relational_Facts_via_Semantic_Hypergraph_Reasoning.pdf\n",
      "Embedding 1: [-0.26407242  0.12544024 -3.36957955  0.41642034 -0.26200402]...\n",
      "Embedding 2: [-0.85915941  0.05299517 -3.26583338 -0.20076534 -0.40341896]...\n",
      "Text 1:      [42] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.\n",
      "Text 2:      [42] Liang Yang, Wenmiao Zhou, Weihang Peng, Bingxin Niu, Junhua Gu, Chuan Wang, Xiaochun Cao, and Dongxiao He.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02282):\n",
      "Paper 1:     2503.20676v1.Inductive_Link_Prediction_on_N_ary_Relational_Facts_via_Semantic_Hypergraph_Reasoning.pdf\n",
      "Paper 2:     2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "Embedding 1: [-0.85915941  0.05299517 -3.26583338 -0.20076534 -0.40341896]...\n",
      "Embedding 2: [-0.26407242  0.12544024 -3.36957955  0.41642034 -0.26200402]...\n",
      "Text 1:      [42] Liang Yang, Wenmiao Zhou, Weihang Peng, Bingxin Niu, Junhua Gu, Chuan Wang, Xiaochun Cao, and Dongxiao He.\n",
      "Text 2:      [42] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02298):\n",
      "Paper 1:     2503.20731v1.RecTable__Fast_Modeling_Tabular_Data_with_Rectified_Flow.pdf\n",
      "Paper 2:     2503.20767v1.Reliable_algorithm_selection_for_machine_learning_guided_design.pdf\n",
      "Embedding 1: [ 0.62496722  1.09928942 -2.37330818 -0.67665511  1.00318086]...\n",
      "Embedding 2: [ 0.56001431  0.96091229 -2.53640223 -0.50456661  0.55357599]...\n",
      "Text 1:      In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds. ), Advances in Neural Information Processing Systems , volume 31. Cur- ran Associates, Inc., 2018. URLhttps://proceedings.neurips.cc/paper_files/ paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf.\n",
      "Text 2:      In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, ed- itors, Advances in Neural Information Processing Systems , volume 36, pages 12489â€“12517.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02308):\n",
      "Paper 1:     2503.20756v1.ADS_Edit__A_Multimodal_Knowledge_Editing_Dataset_for_Autonomous_Driving_Systems.pdf\n",
      "Paper 2:     2503.20630v1._Î²__GNN__A_Robust_Ensemble_Approach_Against_Graph_Structure_Perturbation.pdf\n",
      "Embedding 1: [-0.48799059  1.40491891 -3.25624418 -1.37792373  0.51993191]...\n",
      "Embedding 2: [-0.17985758  0.99286252 -2.64047956 -1.22088754  1.07165313]...\n",
      "Text 1:      In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelli- gence, IJCAI 2024, Jeju, South Korea, August 3-9, 2024, pages 6633â€“6641.\n",
      "Text 2:      In Proceedings of the Twenty- Eighth International Joint Conference on Artificial Intelligence, IJCAI-19 .\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02346):\n",
      "Paper 1:     2503.20642v1.Representation_Improvement_in_Latent_Space_for_Search_Based_Testing_of_Autonomous_Robotic_Systems.pdf\n",
      "Paper 2:     2503.20781v1.BASKET__A_Large_Scale_Video_Dataset_for_Fine_Grained_Skill_Estimation.pdf\n",
      "Embedding 1: [-0.90545171  0.226154   -3.61019754 -0.49003294 -0.14423786]...\n",
      "Embedding 2: [-0.42345944  0.17552027 -3.62448096  0.26781988  0.15535305]...\n",
      "Text 1:      [38] Yuqi Huai, Yuntianyi Chen, Sumaya Almanee, Tuan Ngo, Xiang Liao, Ziwen Wan, Qi Alfred Chen, and Joshua Garcia.\n",
      "Text 2:      [38] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02346):\n",
      "Paper 1:     2503.20781v1.BASKET__A_Large_Scale_Video_Dataset_for_Fine_Grained_Skill_Estimation.pdf\n",
      "Paper 2:     2503.20642v1.Representation_Improvement_in_Latent_Space_for_Search_Based_Testing_of_Autonomous_Robotic_Systems.pdf\n",
      "Embedding 1: [-0.42345944  0.17552027 -3.62448096  0.26781988  0.15535305]...\n",
      "Embedding 2: [-0.90545171  0.226154   -3.61019754 -0.49003294 -0.14423786]...\n",
      "Text 1:      [38] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.\n",
      "Text 2:      [38] Yuqi Huai, Yuntianyi Chen, Sumaya Almanee, Tuan Ngo, Xiang Liao, Ziwen Wan, Qi Alfred Chen, and Joshua Garcia.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02504):\n",
      "Paper 1:     2503.20527v1.StableToolBench_MirrorAPI__Modeling_Tool_Environments_as_Mirrors_of_7_000__Real_World_APIs.pdf\n",
      "Paper 2:     2503.20749v1.Beyond_Believability__Accurate_Human_Behavior_Simulation_with_Fine_Tuned_LLMs.pdf\n",
      "Embedding 1: [-0.69282591  0.89182723 -2.8461957   0.66086125  0.88627607]...\n",
      "Embedding 2: [-1.0486058   1.01827645 -3.09781432  0.27269888  0.49867502]...\n",
      "Text 1:      Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruim- ing Tang, and Enhong Chen.\n",
      "Text 2:      Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02512):\n",
      "Paper 1:     2503.20750v1.Optimal_Scaling_Laws_for_Efficiency_Gains_in_a_Theoretical_Transformer_Augmented_Sectional_MoE_Framework.pdf\n",
      "Paper 2:     2503.20648v1.TN_Eval__Rubric_and_Evaluation_Protocols_for_Measuring_the_Quality_of_Behavioral_Therapy_Notes.pdf\n",
      "Embedding 1: [-0.44037038  1.36343622 -2.90856743 -1.92816138  0.82714778]...\n",
      "Embedding 2: [-0.14802466  0.71208978 -3.21480894 -1.70350909  0.59590185]...\n",
      "Text 1:      [22] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi` ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models.arXiv preprint arXiv:2302.13971. Retrieved from https://arxiv.org/abs/2302.13971. [23] Zhu, T., Qu, X., Dong, D., Ruan, J., Tong, J., He, C., & Cheng, Y.\n",
      "Text 2:      2023. Llama: Open and efficient foundation language models.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02517):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20519v1.MAR_3D__Progressive_Masked_Auto_regressor_for_High_Resolution_3D_Generation.pdf\n",
      "Embedding 1: [-0.95077783  0.05121423 -3.08989048  0.61656666 -0.19967434]...\n",
      "Embedding 2: [-0.50589395  0.33906686 -3.4342196   0.28063199  0.39581114]...\n",
      "Text 1:      1, 3 [50] Jiawei Ren, Cheng Xie, Ashkan Mirzaei, Karsten Kreis, Zi- wei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, Huan Ling, et al.\n",
      "Text 2:      2, 6 [47] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Ji- ahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02531):\n",
      "Paper 1:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Paper 2:     2503.20698v1.MMMORRF__Multimodal_Multilingual_Modularized_Reciprocal_Rank_Fusion.pdf\n",
      "Embedding 1: [-0.68471253  0.18979128 -3.49268532  0.28081715 -0.13896376]...\n",
      "Embedding 2: [-0.71091413  0.57295609 -3.24542737 -0.07515862 -0.16373605]...\n",
      "Text 1:      6 [32] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al.\n",
      "Text 2:      [32] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al .\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02546):\n",
      "Paper 1:     2503.20568v1.Low_resource_Information_Extraction_with_the_European_Clinical_Case_Corpus.pdf\n",
      "Paper 2:     2503.20523v1.GAIA_2__A_Controllable_Multi_View_Generative_World_Model_for_Autonomous_Driving.pdf\n",
      "Embedding 1: [-0.09755131  0.70422459 -3.11101866 -1.21790528  0.87377483]...\n",
      "Embedding 2: [-0.13959827  0.70750856 -2.78272986 -0.78360164  0.74000525]...\n",
      "Text 1:      arXiv preprint arXiv:1904.05342 (2020) arXiv:1904.05342 [cs.CL] [22] Cai, P.-X., Fan, Y.-C., Leu, F.-Y.\n",
      "Text 2:      arXiv preprint arXiv:2412.04842, 2024. [49] J. Ni, Y . Guo, Y . Liu, R. Chen, L. Lu, and Z. Wu.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02561):\n",
      "Paper 1:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Paper 2:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Embedding 1: [-0.71021843 -0.23540547 -3.47188973  1.04081607  0.56257308]...\n",
      "Embedding 2: [-1.00698924  0.21361995 -3.48185635 -0.034922    0.17919201]...\n",
      "Text 1:      1, 2 [39] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.\n",
      "Text 2:      1 [57] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02614):\n",
      "Paper 1:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Paper 2:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Embedding 1: [-0.34025851  0.19393927 -2.99514008 -0.29300782  0.93354928]...\n",
      "Embedding 2: [-0.51019365  0.99092054 -2.64675593  0.25091931  0.94801921]...\n",
      "Text 1:      In ICCV, 2023. 2, 3, 4, 5, 6, 7, 8 [19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, and Dani Lischinski. Prompt-to-prompt image editing with cross attention control. In ICLR, 2023. 2, 3 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. In NeurIPS, 2020. 1 [21] Jonathan Ho, Chitwan Saharia, William Chan, Tim Sali- mans, David J. Fleet, and Mohammad Norouzi. Imagen video: High definition video generation with diffusion mod- els. arXiv Preprint, 2022. 1, 2 [22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv Preprint, 2022. 4 [23] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffu- sion models.\n",
      "Text 2:      Understanding object dynamics for inter- active image-to-video synthesis. In CVPR, 2021. 3 [7] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and BjÃ¶rn Ommer. ipoke: Poking a still image for controlled stochastic video synthesis. In ICCV, 2021. 3 [8] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bjorn Ommer. Understanding object dynamics for inter- active image-to-video synthesis. In CVPR, 2021. 3 [9] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram V oleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [10] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock- horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 2 [11] Richard Strong Bowen, Richard Tucker, Ramin Zabih, and Noah Snavely. Dimensions of motion: Monocular prediction through flow subspaces. In 3DV, 2022. 3 [12] Xu Cao, Hiroaki Santo, Boxin Shi, Fumio Okura, and Ya- suyuki Matsushita. Bilateral normal integration. In Eu- ropean Conference on Computer Vision , pages 552â€“567. Springer, 2022. 4 [13] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840, 2023. 2 [14] Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, and Hengshuang Zhao. Livephoto: Real image animation with text-guided motion control. arXiv preprint arXiv:2312.02928, 2023. 3 [15] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffu- sion model for generative transition and prediction. arXiv preprint arXiv:2310.20700, 2023. 2 [16] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, and Raquel Urtasun. Geosim: Realistic video sim- ulation via geometry-aware composition for self-driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7230â€“7240, 2021. 5 [17] Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian Curless, David H Salesin, and Richard Szeliski. Animating pictures with stochastic motion textures. ACM TOG, 2005. 2, 3 [18] Abe Davis, Justin G Chen, and FrÃ©do Durand. Image-space modal bases for plausible manipulation of objects in video. ACM TOG, 2015. 2, 3 [19] Yuki Endo, Yoshihiro Kanamori, and Shigeru Kuriyama. Animating landscape: self-supervised learning of decoupled motion and appearance for single-image video synthesis. arXiv preprint arXiv:1910.07192, 2019. 3 [20] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography.Communications of the ACM, 24(6):381â€“395, 1981. 4 [21] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming- Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. In ICCV, 2023. 2 [22] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. In ICLR, 2024. 3 [23] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du- val, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factoriz- ing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 2 [24] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to- image diffusion models without specific tuning, 2023. 3 [25] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and JosÃ© Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. 2 [26] Jingwen He, Tianfan Xue, Dongyang Liu, Xinqi Lin, Peng Gao, Dahua Lin, Yu Qiao, Wanli Ouyang, and Ziwei Liu. Venhancer: Generative space-time enhancement for video generation. arXiv preprint arXiv:2407.07667, 2024. 5 [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. 2020. 2 [28] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02687):\n",
      "Paper 1:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Paper 2:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Embedding 1: [-0.61934227  0.79810488 -3.1166575  -0.56308389  0.51380807]...\n",
      "Embedding 2: [-0.31968707  0.81807876 -3.47001028 -0.60473037  0.48605889]...\n",
      "Text 1:      2, 3, 5 [56] Alex Nichol and Prafulla Dhariwal. Improved denoising dif- fusion probabilistic models. In ICML, 2021. 1 [57] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image genera- tion and editing with text-guided diffusion models. arXiv Preprint, 2021. 1 [58] Santiago Pascual, Chunghsin Yeh, Ioannis Tsiamas, and Joan Serr`a. Masked generative video-to-audio transformers with enhanced synchronicity. In ECCV, 2024. 2 [59] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih- Yao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of media foundation models. arXiv Preprint, 2024. 1 [60] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden- hall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 2, 3, 4 [61] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen.\n",
      "Text 2:      3 [36] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie- niu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. CoRR, abs/2303.08320, 2023. 2 [37] Micha Â¨el Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Confer- ence Track Proceedings, 2016. 2 [38] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7038â€“ 7048, 2024. 2 [39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: representing scenes as neural radiance fields for view synthe- sis. Commun. ACM, 65(1):99â€“106, 2022. 2, 3 [40] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real im- ages using guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6038â€“6047, 2023. 4 [41] Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng, Junjie Yan, and Xiaogang Wang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02771):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Embedding 1: [-1.60130954  0.20404598 -3.72366714 -0.59034938 -0.1536561 ]...\n",
      "Embedding 2: [-1.46522975 -0.43966484 -3.3423183   0.44929761  0.2858898 ]...\n",
      "Text 1:      14 [53] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\n",
      "Text 2:      2 [71] Uri Singer, Adam Polyak, Ethan Fetaya, Jonathan Berant, Yaniv Hoshen, and Ronen Shalev.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02773):\n",
      "Paper 1:     2503.20673v1.Mitigating_Low_Level_Visual_Hallucinations_Requires_Self_Awareness__Database__Model_and_Training_Strategy.pdf\n",
      "Paper 2:     2503.20682v1.GLRD__Global_Local_Collaborative_Reason_and_Debate_with_PSL_for_3D_Open_Vocabulary_Detection.pdf\n",
      "Embedding 1: [-0.65872777  0.88298249 -3.24284148 -0.8498255   0.8874144 ]...\n",
      "Embedding 2: [-0.24608923  0.79968572 -2.9219451  -1.46086228  0.67355716]...\n",
      "Text 1:      To better align with diverse usage scenarios, researchers have inte- grated pretrained vision encoders with LLMs and subsequently finetuned these combinations, leading to the emergence of Multimodal Large Language Models (MLLMs).\n",
      "Text 2:      To equip LLMs with the ability to process other modals, e.g., pictures, sounds, etc., Multi-Modal Large Language Models (MLLMs) are de- vised.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02773):\n",
      "Paper 1:     2503.20682v1.GLRD__Global_Local_Collaborative_Reason_and_Debate_with_PSL_for_3D_Open_Vocabulary_Detection.pdf\n",
      "Paper 2:     2503.20673v1.Mitigating_Low_Level_Visual_Hallucinations_Requires_Self_Awareness__Database__Model_and_Training_Strategy.pdf\n",
      "Embedding 1: [-0.24608923  0.79968572 -2.9219451  -1.46086228  0.67355716]...\n",
      "Embedding 2: [-0.65872777  0.88298249 -3.24284148 -0.8498255   0.8874144 ]...\n",
      "Text 1:      To equip LLMs with the ability to process other modals, e.g., pictures, sounds, etc., Multi-Modal Large Language Models (MLLMs) are de- vised.\n",
      "Text 2:      To better align with diverse usage scenarios, researchers have inte- grated pretrained vision encoders with LLMs and subsequently finetuned these combinations, leading to the emergence of Multimodal Large Language Models (MLLMs).\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02801):\n",
      "Paper 1:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Paper 2:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Embedding 1: [-0.30927658  0.24905992 -3.59528899 -0.08730818 -0.0933055 ]...\n",
      "Embedding 2: [-0.91568482  0.05795411 -3.25849867 -0.05789727 -0.04194427]...\n",
      "Text 1:      2 [94] Albert J Zhai, Yuan Shen, Emily Y Chen, Gloria X Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, and Shenlong Wang.\n",
      "Text 2:      2 [80] Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Store matched pairs\n",
    "matches = []\n",
    "\n",
    "# Loop through each embedding and find best inter-paper matches\n",
    "for i, title in enumerate(all_titles):\n",
    "    for dist, j in zip(distances[i], indices[i]):\n",
    "        if paper_indices[i] != paper_indices[j]:  # Ensure it's not the same paper\n",
    "            matches.append({\n",
    "                'distance': dist,\n",
    "                'embedding_i': i,\n",
    "                'embedding_j': j,\n",
    "                'paper_i': all_titles[i],\n",
    "                'paper_j': all_titles[j],\n",
    "                'text_i': all_texts[i],\n",
    "                'text_j': all_texts[j]\n",
    "            })\n",
    "\n",
    "# Sort matches by distance (ascending)\n",
    "matches = sorted(matches, key=lambda x: x['distance'])\n",
    "\n",
    "# Print the top matches\n",
    "print_count = 0\n",
    "for match in matches:\n",
    "    if print_count >= 100:\n",
    "        break\n",
    "\n",
    "    if match['distance'] < 10 or len(match['text_i']) > 1000 or len(match['text_i']) < 100:\n",
    "        continue\n",
    "\n",
    "    print_count += 1\n",
    "    print(f\"Match (Distance: {match['distance']:.5f}):\")\n",
    "    print(f\"Paper 1:     {match['paper_i']}\")\n",
    "    print(f\"Paper 2:     {match['paper_j']}\")\n",
    "    print(f\"Embedding 1: {all_embeddings[match['embedding_i']][:5]}...\")\n",
    "    print(f\"Embedding 2: {all_embeddings[match['embedding_j']][:5]}...\")\n",
    "    print(f\"Text 1:      {match['text_i']}\")  # Limit text to first 100 chars\n",
    "    print(f\"Text 2:      {match['text_j']}\")  # Limit text to first 100 chars\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
