{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = './arxiv_downloads_processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pkl',\n",
       "  '2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pkl',\n",
       "  '2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pkl'],\n",
       " 100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir(preprocessed_data)\n",
    "files[:3], len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rolling.paper import Paper, load_paper, print_paper\n",
    "\n",
    "load_n = 100\n",
    "\n",
    "papers:list[Paper] = []\n",
    "for i in range(load_n):\n",
    "    papers.append(load_paper(os.path.join(preprocessed_data, files[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: 2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf\n",
      "> [ 1.09270895  1.30160093 -2.66840649] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA. where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq. (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp. 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab. (LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n",
      "  > [ 1.51249397  1.30708337 -2.13811707] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA. where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq.\n",
      "    > [ 0.5607357   0.48944172 -2.37554383] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq.\n",
      "      > [ 0.59995002  1.83745313 -3.12929296] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters.\n",
      "        > [ 1.47802949  1.3865068  -3.02602625] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs.\n",
      "          > [ 1.41769803  1.27047706 -3.15513825] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a.\n",
      "          > [ 1.99488759  0.7405777  -4.12540531] local cluster) surrounding vs in time roughly linear with the size of Cs.\n",
      "          > [ 0.35239524  1.89398146 -3.05035019] This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs.\n",
      "          > [ 0.21401146  1.60931432 -3.01792622] However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs.\n",
      "        > [ 0.98459125  1.14680135 -3.5964818 ] To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation.\n",
      "          > [ 0.83557868  1.45382118 -3.3706789 ] To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality.\n",
      "          > [ 0.44359392  1.02308512 -3.88586831] To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes.\n",
      "          > [ 0.93769324  1.4336102  -3.63604832] Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality.\n",
      "          > [ 0.89692289  0.72259998 -2.98592877] The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation.\n",
      "        > [ 1.55274737  1.97804773 -3.58837652] Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more.\n",
      "          > [ 1.38456714  1.33846092 -3.55438757] Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster.\n",
      "          > [ 0.89499038  1.33556437 -3.81551647] Index Terms—local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a.\n",
      "          > [ 1.32031989  1.06898713 -3.78155184] local cluster) pertinent to a given seed node by exploring a small region around it in the input graph.\n",
      "          > [ 0.56314409  2.55118036 -3.55792594] Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more.\n",
      "        > [ 0.58675414  1.97431254 -3.59304786] In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters.\n",
      "          > [ 0.09591432  1.73751116 -3.17208862] In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13].\n",
      "          > [ 0.72806567  1.34387183 -4.01423788] Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges.\n",
      "          > [ 0.44286263  1.86974394 -3.11720133] This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20].\n",
      "          > [ 0.94925308  1.91508937 -3.42663479] To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters.\n",
      "      > [ 0.14417896  0.36159837 -3.1875999 ] Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions.\n",
      "        > [ 0.66280317  1.15901268 -3.28384137] Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V).\n",
      "          > [ 0.06269412  1.66945899 -3.33678007] Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency.\n",
      "          > [ 0.35274819  1.10541832 -3.42873716] Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23].\n",
      "          > [ 1.26517344  1.02646637 -4.0949297 ] Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp.\n",
      "          > [ 1.30154848  1.05918956 -3.75689507] Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V).\n",
      "        > [ 0.02052707  1.2205559  -3.05985165] As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs.\n",
      "          > [-0.3230086   1.20907462 -3.33162689] As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering.\n",
      "          > [ 0.91186255  0.50403953 -3.63398719] However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges.\n",
      "          > [ 0.24938962  1.20348501 -2.65910745] Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly.\n",
      "          > [ 0.86817539  1.66749334 -3.57692099] In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs.\n",
      "        > [ 1.24319875  1.23686421 -3.44839525] These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints.\n",
      "          > [ 1.83776855  1.11541748 -3.94506836] These are termed attributed graphs.\n",
      "          > [ 0.08765912  1.30436993 -3.6067338 ] Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks.\n",
      "          > [ 0.12829979  1.2452774  -3.75549793] Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality.\n",
      "          > [ 1.46819329  0.95442349 -3.53446627] Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints.\n",
      "        > [ 0.91526288  1.11322737 -3.36430073] However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions.\n",
      "          > [ 0.55834186  0.51185775 -3.93968272] However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections.\n",
      "          > [ 0.82721591  1.72266269 -2.94459558] To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency.\n",
      "          > [ 1.06409144  1.44789267 -3.8814373 ] Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs.\n",
      "          > [ 0.08955733  0.15701222 -3.61845088] The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions.\n",
      "      > [ 0.23884243  1.0666219  -2.75948238] Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges.\n",
      "        > [ 0.75951689  1.51906788 -3.41158533] Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph.\n",
      "          > [ 0.83603394  0.99682504 -3.55803394] Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t.\n",
      "          > [ 0.4821851   1.97027266 -3.53461862] vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes.\n",
      "          > [ 1.1812439   1.63779879 -3.5802002 ] This is non-local and infeasible for large graphs.\n",
      "          > [ 0.98563766  1.25804305 -3.0569458 ] In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph.\n",
      "        > [ 1.20175004  0.74455404 -3.457618  ] First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance.\n",
      "          > [ 0.8048352   0.79389262 -3.28426647] First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35].\n",
      "          > [ 0.98802674  0.9571743  -3.27971625] This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs.\n",
      "          > [ 1.18743169  0.49821597 -3.67599559] On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion.\n",
      "          > [ 1.37247968  0.93544096 -3.58053041] It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance.\n",
      "        > [ 1.25763011  1.54354763 -2.97842169] Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor.\n",
      "          > [ 0.80721867  1.16819608 -2.88103461] Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD.\n",
      "          > [ 1.32515669  1.95499682 -2.81065512] We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38].\n",
      "          > [ 1.36913323  1.17630935 -3.14730835] Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost.\n",
      "          > [ 1.37972367  0.69885409 -3.60182858] In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor.\n",
      "        > [ 0.3144837   0.73731202 -2.90282869] II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges.\n",
      "          > [ 0.76526749 -0.11198903 -3.13219213] II. P ROBLEM FORMULATION A.\n",
      "          > [ 0.53023309  0.20357326 -3.49417782] Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a.\n",
      "          > [-0.05841827 -0.07971954 -3.83825684] network), where V = {v1, v2, .\n",
      "          > [ 0.61812657  0.63529885 -3.14277816] . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges.\n",
      "      > [ 0.29290217  0.41245431 -2.66963315] For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq.\n",
      "        > [ 0.03341744  0.12760648 -2.89785337] For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations.\n",
      "          > [ 0.80250031 -0.33329964 -3.23660421] For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other.\n",
      "          > [ 0.46119595 -0.34275627 -3.38879395] We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree.\n",
      "          > [-0.32677069  0.36428669 -2.37970877] Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0.\n",
      "          > [-0.27734748  0.28928205 -4.12075424] The TABLE I: Frequently used notations.\n",
      "        > [-0.04447474  0.36848432 -3.13175464] Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively.\n",
      "          > [ 0.24672806  0.17788059 -3.26032972] Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively.\n",
      "          > [ 0.11167698  0.95086646 -3.33404922] n, m, d The numbers of nodes, edges, and distinct attributes, respectively.\n",
      "          > [ 0.25268415 -0.23056969 -3.53345776] N(vi), d(vi) The set of neighbors and degree of node vi, respectively.\n",
      "          > [-0.38645291  0.64805168 -3.38963699] A, D, P The adjacency, degree, and transition matrices of the graph G, respectively.\n",
      "        > [ 0.74762011  0.82183641 -3.51766348] vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq.\n",
      "          > [ 1.09465754  1.02506113 -3.79631972] vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi).\n",
      "          > [ 0.74755859  0.98074406 -2.76336288] supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}.\n",
      "          > [ 0.16045247 -0.49128771 -3.8243475 ] α The restart factor in RWR, α ∈ (0, 1).\n",
      "          > [ 0.6443367   0.86215413 -3.15854168] s(vi, vj) The SNAS defined by Eq.\n",
      "        > [ 0.65165007 -0.53651792 -3.57360649] (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq.\n",
      "          > [-0.01172906 -0.56641972 -3.54607677] (1). π(vx, vy) The RWR score of vy w.r.t.\n",
      "          > [ 1.13128507  0.27703586 -3.07102442] vx (See Eq.\n",
      "          > [ 0.47350025 -0.52021885 -4.21026611] (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t.\n",
      "          > [ 1.19686139  0.01140662 -3.6999681 ] vs respectively (See definition in Eq.\n",
      "    > [ 1.73766041  0.58441114 -3.00418472] (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA.\n",
      "      > [ 0.55003071 -0.1340666  -3.36296916] (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors.\n",
      "        > [-0.02340592  0.8871963  -3.43438983] (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0.\n",
      "          > [ 0.28276572 -0.31028086 -3.55085707] (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq.\n",
      "          > [ 0.68674636  1.58101046 -3.76734161] (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo.\n",
      "          > [-0.10637227  0.35976177 -2.60294294] 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi).\n",
      "          > [-0.49848139  0.46334773 -2.8799839 ] The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0.\n",
      "        > [ 0.42188513  1.03491139 -3.10797811] Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i).\n",
      "          > [ 0.22770482  0.4803558  -3.09930182] Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj.\n",
      "          > [ 0.79961795  0.81011707 -3.26888967] A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1.\n",
      "          > [-0.60732114 -0.45512205 -3.06730509] Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively.\n",
      "          > [ 0.60090542  0.65007561 -2.58050156] The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i).\n",
      "        > [ 0.93527031  0.99508464 -3.33765602] The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t.\n",
      "          > [ 1.21679425  1.03055906 -3.39389467] The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi).\n",
      "          > [ 1.09098935  1.10492969 -3.14768839] Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi).\n",
      "          > [ 0.43509385  0.33414552 -3.76289129] Table I lists notations frequently used throughout this paper.\n",
      "          > [ 0.5184617   0.21172693 -3.67309403] A local cluster Cs w.r.t.\n",
      "        > [ 1.23321843  0.96841085 -3.34796429] a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors.\n",
      "          > [ 0.59763211  0.98897254 -3.39422703] a seed node vs in graph G is defined as a subset of V containing vs.\n",
      "          > [ 0.75782895  0.99267364 -3.18419552] Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity.\n",
      "          > [ 1.11412776  1.60323763 -3.09550953] Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs).\n",
      "          > [ 0.6870023   0.10982413 -3.64090323] B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors.\n",
      "      > [ 1.15430987  0.56024688 -2.4537704 ] The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq.\n",
      "        > [ 0.87034488  0.55731124 -3.05895257] The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj.\n",
      "          > [ 0.71110129  0.6572544  -3.41104388] The denominator in Eq.\n",
      "          > [ 0.5914405  -1.11535442 -3.60754657] (1) is to ensure the s(vi, vj) values w.r.t.\n",
      "          > [ 0.76701611  0.47285283 -3.19977689] any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section.\n",
      "          > [ 0.68674266  0.30092597 -3.31122613] Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj.\n",
      "        > [ 1.16966665  0.24963592 -4.0424037 ] We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣!\n",
      "          > [ 0.98433125 -0.26325461 -3.39598036] We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39].\n",
      "          > [ 0.73317331  0.80569589 -3.71658659] When f(·, ·) 2𝑣!\n",
      "          > [ 0.39453185  0.49879792 -3.75992298] 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣!\n",
      "          > [ 0.64557242  1.15075839 -4.2001729 ] 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣!\n",
      "        > [ 0.56822306  1.9586823  -3.84752584] 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD.\n",
      "          > [ 0.60402536  0.77252018 -3.5744257 ] 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣!\n",
      "          > [-0.04179356  1.41722262 -3.88531637] 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣!\n",
      "          > [ 0.683801    0.99197447 -3.74423242] 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig.\n",
      "          > [ 0.05110189  1.09293771 -3.37217045] 1: Figurative Illustration of BDD.\n",
      "        > [ 1.30419338  0.09255464 -3.51538897] is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq.\n",
      "          > [ 1.14921641 -0.38499841 -3.43740058] is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1.\n",
      "          > [ 0.85628819  0.57066834 -3.13706875] Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) .\n",
      "          > [ 1.21105039 -0.03403091 -3.68773031] (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor.\n",
      "          > [ 0.67663187  0.40588567 -2.45592999] Essentially, Eq.\n",
      "      > [ 0.82290375  0.64571691 -2.88998938] (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA.\n",
      "        > [ 0.26019332  0.56637049 -3.53243828] (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t.\n",
      "          > [ 0.55859864  0.98862213 -3.63084936] (4) can be deemed as a variant of the softmax function.\n",
      "          > [ 0.24313423  0.53070772 -3.66999292] C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster.\n",
      "          > [ 0.46428284  0.68174952 -3.28625321] In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target.\n",
      "          > [-0.05700332  0.25088677 -3.3092804 ] More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t.\n",
      "        > [ 0.01800476  0.18423207 -3.46553016] node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp.\n",
      "          > [ 0.59735912  0.27518982 -2.96123743] node vx.\n",
      "          > [-0.77241814  0.28888896 -3.50773525] At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α.\n",
      "          > [ 1.03210962  0.15339705 -2.61265182] In essence, π(vs, vi) (resp.\n",
      "          > [ 0.51119143 -0.22021636 -3.47441196] π(vt, vj)) is the probability that an RWR originating from vs (resp.\n",
      "        > [ 0.65261447  0.25781167 -3.61584663] vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively.\n",
      "          > [-0.05163765 -0.31461716 -3.75415039] vt) termi- nates at node vi (resp.\n",
      "          > [ 0.86829019 -0.10746651 -2.39857364] vj). Put differently, π(vs, vi) ∀vi ∈ V (resp.\n",
      "          > [ 1.22055578 -0.07979427 -3.20298576] π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp.\n",
      "          > [-0.98373413  0.69329071 -3.92333984] vt) to all the nodes in G via random walks, respectively.\n",
      "        > [ 1.2302264   0.96766317 -3.53858089] Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA.\n",
      "          > [ 0.85594118  0.77942127 -3.30767727] Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt.\n",
      "          > [ 0.83625633 -0.14738619 -3.31586671] The BDD − →ρ t of (vs, vt) in Eq.\n",
      "          > [ 0.67049152  0.50456041 -4.23610783] (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig.\n",
      "          > [ 0.81557465  0.29410535 -3.74405551] 2: Basic Idea of LACA.\n",
      "      > [ 1.84927881  0.48515752 -2.82868505] the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA.\n",
      "        > [ 0.58151263  1.09326875 -3.16802073] the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs.\n",
      "          > [ 0.02905995  0.98761547 -3.20822883] the overall SNAS of all such ending pairs, as illustrated in Fig.\n",
      "          > [ 0.09005471  0.92831391 -3.31260252] 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links.\n",
      "          > [ 0.93562537  0.66758251 -3.75476551] Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph.\n",
      "          > [ 0.73550671  1.21885097 -3.14642739] D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs.\n",
      "        > [ 0.71199942  1.23981225 -3.11676478] In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t.\n",
      "          > [ 1.01082027  0.73655438 -2.95534205] In turn, our major focus of the LGC problem lies in the computation of − →ρ ′.\n",
      "          > [ 0.87581772  1.5183717  -3.12418532] More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality.\n",
      "          > [ 1.16308951  0.0869081  -3.0289433 ] Notice that the direct computation of BDD − →ρ in Eq.\n",
      "          > [-0.28447318 -0.76185858 -3.69948816] (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t.\n",
      "        > [ 0.72083396  0.01921706 -3.27665353] the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows.\n",
      "          > [ 0.2241246  -0.1296168  -3.24641299] the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t.\n",
      "          > [ 0.26225907  0.09360355 -3.35873413] all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging.\n",
      "          > [ 1.2877059  -0.8871963  -2.77863669] Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′.\n",
      "          > [ 0.99438691  0.83678526 -3.28752589] III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows.\n",
      "        > [ 1.13343763  0.59449768 -3.3015089 ] A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA.\n",
      "          > [ 1.25502729  1.11962199 -2.9997921 ] A. Problem Transformation Firstly, by the definition of BDD in Eq.\n",
      "          > [ 0.2395691  -0.3651495  -3.71914387] (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t.\n",
      "          > [ 0.95109534  0.57574004 -3.65571737] the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig.\n",
      "          > [ 1.01638091  0.76257563 -3.66085815] 3: Overview of LACA.\n",
      "    > [ 1.35306954  1.01135981 -2.46613955] where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f .\n",
      "      > [ 0.12600404  1.17455184 -2.78734565] where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq.\n",
      "        > [ 0.7448054   0.87141961 -3.81685233] where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq.\n",
      "          > [ 1.43318266e-01 -1.81001425e-03 -3.81508422e+00] where − →ϕ is called the RWR-SNAS vector w.r.t.\n",
      "          > [ 0.48748916 -0.04274962 -3.24471951] vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi).\n",
      "          > [ 0.3230651   0.53321737 -3.12940216] (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq.\n",
      "          > [ 0.7351737   0.84278053 -3.89028931] (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq.\n",
      "        > [ 0.58824986  0.73963124 -3.34604383] (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq.\n",
      "          > [ 1.15787232  1.24798894 -4.04457903] (7)) of − →ϕ over the graph G with a diffusion threshold.\n",
      "          > [ 0.23003221  0.08968966 -3.01243138] However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq.\n",
      "          > [ 0.53454411  0.69965929 -2.36991739] (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM).\n",
      "          > [ 0.14407711  0.51116008 -3.5285368 ] In doing so, the RWR-SNAS vector − →ϕ in Eq.\n",
      "        > [ 0.54383248 -0.00716944 -2.90171123] (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i.\n",
      "          > [ 0.12270992 -0.28603318 -3.33391595] (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t.\n",
      "          > [ 1.28470445 -0.04374065 -3.15493536] seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq.\n",
      "          > [ 0.36045659  0.7662769  -2.82454872] (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}.\n",
      "          > [ 1.32350886  0.46457708 -2.83199072] (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i.\n",
      "        > [ 0.13461693  1.50377047 -3.51422739] If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq.\n",
      "          > [ 0.39130193  1.82626355 -3.57414794] If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ).\n",
      "          > [ 0.71756577  1.66802144 -4.21825552] As illustrated in Fig.\n",
      "          > [-0.17042515  0.16523299 -3.2461555 ] 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig.\n",
      "          > [-0.05084448  0.36916161 -3.39444065] 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq.\n",
      "      > [ 0.68076247  0.71272761 -3.30518103] (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq.\n",
      "        > [ 1.1409719   0.80555946 -3.87593627] (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′.\n",
      "          > [ 0.55764961  0.59446049 -3.94973922] (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′.\n",
      "          > [ 0.64668292 -0.4072935  -4.05012465] 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig.\n",
      "          > [ 1.13798738  0.8636862  -3.81484938] 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1.\n",
      "          > [ 0.80668384  0.71909124 -3.45332026] B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′.\n",
      "        > [ 0.28144002 -0.09420399 -3.30023074] Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion.\n",
      "          > [ 1.24772239 -0.38271055 -2.9647646 ] Since − →ϕ′ can be properly computed using Eq.\n",
      "          > [ 0.10806362  0.1269078  -3.2352829 ] (13), our main tasks are to construct Z, − →π ′, and − →ρ ′.\n",
      "          > [-0.37409228 -0.49932617 -2.81197596] Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else.\n",
      "          > [ 0.49061868  0.30897495 -3.77686667] The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion.\n",
      "        > [ 0.64345145  0.81054652 -3.42548847] This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo.\n",
      "          > [ 0.78122908  1.26131237 -3.70296907] This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency.\n",
      "          > [ 0.16210741  0.15387894 -2.93013549] As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig.\n",
      "          > [ 0.96252334  0.71865726 -2.80095387] 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq.\n",
      "          > [ 0.34565988  1.43140733 -3.34636998] (11) (Algo.\n",
      "        > [ 0.46525237  0.71208346 -3.86179352] 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq.\n",
      "          > [ 0.42640954  0.38730773 -3.67102766] 3), and a three-step scheme for the online approximation of BDD vector.\n",
      "          > [ 0.57119113  0.87905902 -3.82133245] Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo.\n",
      "          > [ 0.0035902  -0.19281167 -2.90946507] 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq.\n",
      "          > [-0.33921349  0.76851416 -3.58760381] (12)) to build the RWR-SNAS vector − →ϕ′ (Eq.\n",
      "      > [ 1.01895988  1.05444849 -3.05875778] (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7).\n",
      "        > [ 0.96994025  0.58634877 -3.5244801 ] (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo.\n",
      "          > [ 1.04638815  0.99325907 -3.56450987] (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo.\n",
      "          > [ 0.12885788  1.34282136 -3.70933342] 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo.\n",
      "          > [ 4.69546497e-01  3.19588184e-03 -3.31094217e+00] 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo.\n",
      "          > [ 0.99487078  0.5800367  -3.06656504] 3 for constructing TNAM Z and the complete LACA algorithm (Algo.\n",
      "        > [ 0.75511098  0.58279777 -3.01114082] 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively.\n",
      "          > [ 0.70755368  0.28793025 -3.42364597] 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime.\n",
      "          > [ 0.68587643  0.43945751 -3.3478632 ] IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq.\n",
      "          > [ 0.36181307 -0.48308784 -2.82759356] (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq.\n",
      "          > [ 0.56560075 -0.16106971 -2.911165  ] (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively.\n",
      "        > [ 0.84576422  1.25655854 -3.2236557 ] Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f .\n",
      "          > [ 0.64384168  1.8836776  -3.3536377 ] Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency.\n",
      "          > [ 1.10326385  1.43025494 -3.25036621] Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme.\n",
      "          > [ 1.36988401  1.56927872 -3.9699564 ] A. The GreedyDiffuse Approach Algo.\n",
      "          > [ 0.63406694  0.5409801  -3.35493326] 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f .\n",
      "        > [ 0.75565398  1.59134173 -3.73165059] Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7).\n",
      "          > [ 1.11503887  2.31315446 -3.48644018] Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee.\n",
      "          > [ 0.20519452  1.24525261 -3.39409304] Given the transition matrix P of the input graph G and the diffusion threshold, Algo.\n",
      "          > [ 0.84504306 -1.22221494 -3.74405384] 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1.\n",
      "          > [ 1.10619211 -0.76233643 -3.73017359] Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7).\n",
      "      > [ 1.16991436 -0.39488143 -2.86808348] Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f .\n",
      "        > [ 1.14999521 -0.41287237 -3.96730781] Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors.\n",
      "          > [ 1.02234244 -0.96272641 -3.99752164] Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion.\n",
      "          > [ 0.51552796 -0.08309352 -3.0414083 ] More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise.\n",
      "          > [ 0.58835143 -0.7189157  -3.7111783 ] (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6).\n",
      "          > [ 1.57289839 -0.41110176 -3.49412632] ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors.\n",
      "        > [ 0.91047233  0.49732202 -3.58924532] That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7).\n",
      "          > [ 0.80860585  0.25589383 -3.63966894] That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig.\n",
      "          > [ 0.84638768  1.15093517 -4.07330132] 5: Greedy v.s.\n",
      "          > [ 1.33700812  2.2626636  -3.79492903] Non-greedy.\n",
      "          > [ 0.90934622 -0.51558024 -3.41917133] These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7).\n",
      "        > [ 0.95871735  0.86253494 -3.78720975] Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time.\n",
      "          > [ 1.01045632  0.12330114 -3.89100885] Theorem IV .1.\n",
      "          > [ 0.60024929  0.12372153 -3.74005938] Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo.\n",
      "          > [ 0.88642085 -0.30235234 -3.32753563] 1 outputs a diffused vector − →q satisfying Eq.\n",
      "          > [ 0.23245955  1.48987198 -3.35464478] (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time.\n",
      "        > [ 1.62704039  0.08741125 -3.23457146] Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f .\n",
      "          > [ 1.93568361  0.99315476 -3.25438762] Proof. All missing proofs can be found in Appendix A. Algo.\n",
      "          > [ 1.23841882 -0.71612287 -2.98950434] 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq.\n",
      "          > [ 0.65372944  0.66914988 -3.48164272] (15). Eventually, Algo.\n",
      "          > [ 1.32822907 -0.93317723 -3.66086388] 1 returns − →q as the diffused vector of− →f .\n",
      "    > [ 1.53783441  1.28753912 -2.14712596] Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq.\n",
      "      > [ 1.64209533  0.78471893 -1.72256958] Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq.\n",
      "        > [ 1.32768321  1.62496114 -3.8099432 ] Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig.\n",
      "          > [ 1.42142594  0.96766907 -3.5054698 ] Theorem IV .1 establishes the approximation accuracy guarantees of Algo.\n",
      "          > [ 1.3193624   1.25084519 -4.0019989 ] 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively.\n",
      "          > [ 0.58772725  0.75796968 -4.09123421] A Running Example.\n",
      "          > [ 0.70380032  1.17039979 -2.56342983] Consider the example G in Fig.\n",
      "        > [ 0.92501491  0.44991007 -3.74562764] 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly.\n",
      "          > [ 0.34860802  0.16349149 -3.16616821] 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6.\n",
      "          > [ 0.85862809  1.22815132 -3.97523785] We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1.\n",
      "          > [ 1.10983562 -1.07138646 -3.85217071] Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig.\n",
      "          > [ 1.01895583  0.53190631 -3.96449113] 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly.\n",
      "        > [ 0.96581835 -0.10243485 -4.25369883] Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively.\n",
      "          > [ 1.08261883 -1.01129854 -4.18765354] Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual.\n",
      "          > [ 0.89770293 -0.6540606  -3.69351625] Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ.\n",
      "          > [ 1.06386781  0.24205969 -3.65438104] Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4.\n",
      "          > [ 0.62125796 -0.7110315  -4.22048044] A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively.\n",
      "        > [ 1.49292231  0.42686206 -3.02779746] The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq.\n",
      "          > [ 1.12689078 -1.4137876  -4.02288914] The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1.\n",
      "          > [ 1.76238704  1.07025123 -3.40028453] GreedyDiffuse then terminates and returns the reserve values as the result.\n",
      "          > [ 1.31485355  1.03318226 -3.01492405] B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo.\n",
      "          > [ 0.10074004 -0.10792398 -3.19378185] 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq.\n",
      "      > [ 1.4149816   1.31204093 -2.99769044] (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations.\n",
      "        > [ 1.26404381  1.46172965 -3.17216349] (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq.\n",
      "          > [ 0.52575916  0.40408432 -3.63866949] (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo.\n",
      "          > [ 0.83539528  1.51654124 -3.73502827] 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq.\n",
      "          > [ 1.15345454  0.4116942  -3.31303668] (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo.\n",
      "          > [ 0.92074537  1.16198421 -3.40716267] 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq.\n",
      "        > [ 0.98518831  0.20681043 -3.63734794] (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration.\n",
      "          > [-0.02037021  0.40002352 -3.96426296] (17)) on PubMed and ArXiv datasets (Table III).\n",
      "          > [ 1.29225647  0.52870697 -3.15882349] − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq.\n",
      "          > [ 0.20866118  1.72717381 -3.58844995] (16), non-greedy opera- tions in Eq.\n",
      "          > [ 1.07276797 -0.71740413 -4.16845894] (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration.\n",
      "        > [ 1.27452087  1.60444295 -3.27404237] From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq.\n",
      "          > [ 1.63100123  0.65710336 -3.95896029] From Fig.\n",
      "          > [ 1.07463861  0.65251875 -3.41494632] 5, we can observe that Algo.\n",
      "          > [ 0.95521414  1.21989441 -3.51160622] 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency.\n",
      "          > [ 1.28220248  1.59799719 -3.61505961] The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq.\n",
      "        > [ 1.43091083  0.56932807 -3.71613812] (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations.\n",
      "          > [ 1.35328317 -0.14397123 -4.00717354] (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched.\n",
      "          > [ 1.22974288  0.51212329 -3.16625595] Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q .\n",
      "          > [ 1.13802266  0.70572364 -3.52668762] As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy.\n",
      "          > [ 1.399526    1.26087618 -3.21226549] In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations.\n",
      "      > [ 1.411955    1.04128909 -3.07405853] Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11.\n",
      "        > [ 1.25685453  0.94122171 -3.22331238] Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7).\n",
      "          > [ 1.48630476 -0.36388153 -3.54514003] Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination.\n",
      "          > [ 1.18481517  2.23721385 -2.96677542] However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs.\n",
      "          > [ 0.89696681  1.02796054 -3.26626587] C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality.\n",
      "          > [ 0.49289     0.30876487 -3.99906182] TABLE II: Average node degrees of local clusters (ϵ = 10−7).\n",
      "        > [ 1.18519557  1.26231825 -3.37418675] Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo.\n",
      "          > [ 0.78724194  1.72363281 -4.203125  ] Dataset Global avg.\n",
      "          > [ 0.76176363  1.21003342 -3.51139784] degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo.\n",
      "          > [ 0.63686377  0.44772315 -3.28571272] 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations.\n",
      "          > [ 0.37998962  1.86474609 -3.26879883] In contrast to Algo.\n",
      "        > [ 1.23912597  0.83361107 -2.97817945] 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees).\n",
      "          > [ 0.67929506  0.01918228 -2.70213437] 1, in each iteration, after calculating − →γ by Eq.\n",
      "          > [-0.37358633  0.06006427 -3.71051979] (15) at Line 3, Algo.\n",
      "          > [ 0.20412064  0.59950638 -3.57000732] 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions.\n",
      "          > [ 2.16417599  0.94660473 -3.43744421] The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees).\n",
      "        > [ 1.31426823  1.15326762 -3.29599738] To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11.\n",
      "          > [ 1.10616469  1.00673044 -3.6214838 ] To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq.\n",
      "          > [ 0.92489547  1.58710551 -3.03800941] (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ .\n",
      "          > [ 1.05792058  1.98313403 -3.16175556] Notice that the smaller σ is, the more non-greedy operations will be conducted.\n",
      "          > [ 0.84596753 -0.36335561 -3.30156231] When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11.\n",
      "      > [ 1.17643046  1.38499463 -2.82851672] Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq.\n",
      "        > [ 0.94866973  1.75132573 -3.23909521] Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo.\n",
      "          > [ 0.9439708   1.53378284 -3.20972395] Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo.\n",
      "          > [ 0.21142861 -0.60724521 -3.05223751] 2 outputs a vector − →q such that Eq.\n",
      "          > [ 0.17816664  1.23716283 -3.41620588] (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time.\n",
      "          > [ 0.97225916  1.62970448 -2.47199535] In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo.\n",
      "        > [ 0.12802574  1.02628946 -2.1384244 ] 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1.\n",
      "          > [ 0.3356328   0.8310703  -2.22705722] 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo.\n",
      "          > [-0.14125936 -0.21533005 -3.97949386] 2, respectively.\n",
      "          > [ 0.41533798  1.04350746 -2.76868439] Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2.\n",
      "          > [ 0.45327029  0.59082651 -3.23613548] In particular, when σ ≥ 1, β = 1.\n",
      "        > [ 1.21804571  0.36308527 -3.35720682] V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo.\n",
      "          > [ 1.41447055  0.09601451 -3.46905851] V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC.\n",
      "          > [ 0.54214859  2.42163086 -3.26916504] We first elucidate the algorithmic details of Algo.\n",
      "          > [-0.07641271  0.51953053 -3.36273646] 3 for constructing TNAM Z.\n",
      "          > [ 1.60610318  1.45094752 -2.95674801] The complete algorithmic details of LACA (Algo.\n",
      "        > [ 0.63657337  0.33423132 -2.67020202] 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq.\n",
      "          > [ 1.08923578  0.10804992 -2.92850447] 4) and related analyses are provided in Section V-B.\n",
      "          > [ 0.9513604   1.19147301 -2.72421622] Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38].\n",
      "          > [ 0.38428521  0.01605849 -3.26173592] A. Construction of TNAM Z Basic Idea.\n",
      "          > [ 0.71342868 -0.05881568 -2.83287907] To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq.\n",
      "  > [ 0.95979726  0.77378416 -2.64053869] (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395.\n",
      "    > [ 1.55392766  1.4662869  -2.33687663] (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix.\n",
      "      > [ 0.38303214  0.94094282 -2.02572656] (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1).\n",
      "        > [ 0.34478119  0.05132717 -2.34861231] (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq.\n",
      "          > [ 0.29555064  0.21730159 -2.93789721] (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j).\n",
      "          > [ 0.8552624   0.23373452 -3.03256583] Accordingly, Eq.\n",
      "          > [ 0.08471966 -0.65994108 -2.57651997] (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, .\n",
      "          > [ 1.04559326  0.43471241 -2.72338867] . . , k}; 9 Compute Y ; ▷ Eq.\n",
      "        > [-0.15861832  0.22853272 -3.05466676] (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively.\n",
      "          > [ 0.46120763  0.41896629 -3.29919434] (19) 10 Compute − →y ∗ ; ▷ Eq.\n",
      "          > [ 0.55760014 -0.3257041  -3.06268311] (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq.\n",
      "          > [ 0.28300661  0.20772909 -3.48326159] (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ .\n",
      "          > [-0.26901776  0.33361182 -3.39431763] (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively.\n",
      "        > [ 0.38615653  1.00403249 -2.80405426] Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details.\n",
      "          > [ 0.56351495  0.5378834  -3.0465014 ] Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y .\n",
      "          > [ 0.29427382  0.92181116 -2.72157311] Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X.\n",
      "          > [-0.17402405  1.13678694 -2.53710032] Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2.\n",
      "          > [ 0.99462891  0.62744141 -4.47265625] Details.\n",
      "        > [ 0.25118959  1.10177469 -2.67371368] In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1).\n",
      "          > [ 0.49260378  1.85832906 -3.49087501] In Algo.\n",
      "          > [ 0.14811626  0.74957585 -2.81323433] 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X.\n",
      "          > [ 0.7041626   1.68969727 -2.37194824] That is, Algo.\n",
      "          > [ 0.16811375 -0.05905931 -2.97332883] 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1).\n",
      "      > [ 0.91166401 -0.32214254 -2.22600198] UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11).\n",
      "        > [ 0.39450151 -0.39816201 -2.88224363] UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo.\n",
      "          > [ 0.82857096 -0.29339683 -2.63996339] UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4).\n",
      "          > [ 0.99655652 -0.24435723 -2.96049452] However, when f(·, ·) is the exponential cosine similarity function (Eq.\n",
      "          > [ 0.43124557  0.57833523 -3.05748987] (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs.\n",
      "          > [ 0.03026884 -0.47628888 -3.08508134] As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo.\n",
      "        > [ 0.09067842 -0.06248374 -2.91206312] 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq.\n",
      "          > [ 0.3545008  -0.30984727 -3.41948414] 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7.\n",
      "          > [-0.40043122 -0.78866053 -3.30928469] This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44].\n",
      "          > [ 0.26941219  0.61809963 -2.98603082] Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq.\n",
      "          > [ 0.10552977  0.48727292 -2.59025764] (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq.\n",
      "        > [ 0.85373509 -0.24153823 -2.95617461] (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2.\n",
      "          > [ 0.56001335  0.13399142 -3.33801389] (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d.\n",
      "          > [ 0.93169594  0.60531473 -3.36157227] from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed.\n",
      "          > [ 0.66898513 -0.7249403  -2.64912772] Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices.\n",
      "          > [ 0.77013636 -0.44326475 -3.63479018] Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2.\n",
      "        > [ 0.29215294  0.05072393 -2.84134269] E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11).\n",
      "          > [ 0.50824207 -0.38012719 -3.08675313] E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq.\n",
      "          > [ 0.12618928 -0.16343167 -3.37440252] (3). After computing vector − →y (i) for each node vi ∈ V, Algo.\n",
      "          > [ 0.08793569 -0.22154811 -2.83146024] 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq.\n",
      "          > [-0.05098623 -0.7249766  -4.02928925] (18) (Lines 10- 11).\n",
      "      > [ 1.39363003  1.63787329 -2.77810073] The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7.\n",
      "        > [ 1.14701569  1.75392592 -2.61041546] The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd).\n",
      "          > [ 1.33926654  2.22703409 -2.96344376] The total processing cost entailed by Algo.\n",
      "          > [ 0.9887104   0.80456758 -2.48550987] 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3.\n",
      "          > [ 0.39549595  2.47647119 -3.47220325] The runtime cost of Algo.\n",
      "          > [ 0.30256212  0.41894448 -3.33765864] 3 is O(nd).\n",
      "        > [ 1.27616513  1.18869543 -3.31860518] B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo.\n",
      "          > [ 0.97280526  1.53579843 -3.08274317] B. Complete Algorithm and Analysis In Algo.\n",
      "          > [ 1.18236446  0.76142621 -3.2759223 ] 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ.\n",
      "          > [ 1.04323113  1.04391026 -3.36067247] In the first place, Algo.\n",
      "          > [ 0.59951591  0.26325336 -3.78059626] 4 invokes AdaptiveDiffuse (Algo.\n",
      "        > [ 0.25151554  0.90135449 -2.65173244] 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq.\n",
      "          > [-0.22518668 -0.71521294 -3.11226606] 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2).\n",
      "          > [ 0.70408219  1.61162949 -2.73771167] By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 .\n",
      "          > [ 0.62212998 -0.2554014  -3.25326443] Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4.\n",
      "          > [ 0.27176216  0.07962849 -2.80135012] In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq.\n",
      "        > [ 1.71907735  0.81904674 -3.461308  ] (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7.\n",
      "          > [ 1.22084379  1.51967752 -3.11765409] (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 .\n",
      "          > [ 1.55605388  0.40229601 -3.49398375] After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5).\n",
      "          > [ 1.02678204 -0.42759323 -3.42272949] Let − →ρ ′ be the output of the above diffusion process.\n",
      "          > [ 1.62055385  0.1054054  -3.14995217] LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7.\n",
      "      > [-0.75986737 -0.3746933  -1.8354739 ] On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix.\n",
      "        > [ 1.34717333  0.3529028  -2.95732141] On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ.\n",
      "          > [ 2.14167166  0.05404542 -3.53338671] On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4.\n",
      "          > [ 0.69741774  0.17936331 -2.88026643] When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq.\n",
      "          > [ 0.3733142   0.74385405 -3.15170479] (10), − →ρ ′ output by Algo.\n",
      "          > [ 0.12848814  0.10769761 -3.11472511] 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ.\n",
      "        > [ 1.58727837  1.78928578 -3.06211376] Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA.\n",
      "          > [ 1.02468073  1.40382159 -3.12591863] Volume and Complexity Analysis.\n",
      "          > [ 1.55881977 -0.05540812 -3.69581556] Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1.\n",
      "          > [ 0.77806926  1.88381243 -2.52289605] Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3.\n",
      "          > [ 1.61225474  1.79772305 -3.54886413] Next, we analyze the time complexity of LACA.\n",
      "        > [ 0.39360416  1.89196122 -3.00474691] First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 .\n",
      "          > [-0.12760861  0.0311572  -3.67667389] First, Line 2 invokes Algo.\n",
      "          > [ 0.17939368  0.27652287 -3.70337176] 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2.\n",
      "          > [ 0.82213765  1.46965647 -2.97207069] The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3.\n",
      "          > [ 0.65351468  1.86456823 -3.1390903 ] Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 .\n",
      "        > [ 1.30521381  1.07517481 -3.00804806] Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix.\n",
      "          > [ 1.33906782  1.34072256 -3.02442002] Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′.\n",
      "          > [ 0.8507179   0.80849838 -3.39396548] C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5.\n",
      "          > [ 1.67136049  0.06031044 -3.85280538] Definition V .5 (Graph Signal Denoising [45]) .\n",
      "          > [ 0.26591611 -0.25510716 -2.49254537] Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix.\n",
      "    > [ 1.11146104  1.19094777 -2.66541481] The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets.\n",
      "      > [ 0.79504514  0.66657698 -2.43512201] The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}.\n",
      "        > [ 1.25779963  0.04060837 -3.26979184] The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization.\n",
      "          > [ 0.7834084  -0.02507786 -3.41814828] The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm.\n",
      "          > [ 1.25763714  0.51610076 -3.16730499] The fitting term ∥H − H◦∥2 F in Eq.\n",
      "          > [ 0.6477204   0.05223788 -3.37273145] (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar.\n",
      "          > [ 2.22008967  0.4395276  -3.40004826] The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization.\n",
      "        > [ 0.61170483  0.41912839 -2.50469971] Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq.\n",
      "          > [ 0.55937958 -0.09149551 -3.38525391] Lemma V .6.\n",
      "          > [ 0.77111602  0.60923547 -2.5474782 ] The closed-form solution to Eq.\n",
      "          > [ 0.1831356   0.17094781 -2.70771646] (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 .\n",
      "          > [ 1.17457891  1.12114549 -3.38507843] By applying the gradient descent to solve Eq.\n",
      "        > [ 0.28361592  0.89393866 -3.18540883] (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq.\n",
      "          > [ 0.47324467  0.33635101 -3.24542236] (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets.\n",
      "          > [-0.51338363  1.00523055 -3.40758204] Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47].\n",
      "          > [ 0.06831074  0.10972121 -3.03491211] If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a.\n",
      "          > [ 0.72121477  0.86718225 -2.90118313] embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq.\n",
      "        > [ 0.56047255  0.59171444 -2.85300541] (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}.\n",
      "          > [ 0.67887044  0.66999048 -3.6895442 ] (10) holds, combining Eq.\n",
      "          > [ 0.64062768  0.4360773  -3.76222301] (5) and Eq.\n",
      "          > [ 1.19877577 -0.02180272 -3.18638396] (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤.\n",
      "          > [ 0.04421113  0.88718957 -2.99366093] In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}.\n",
      "      > [ 1.40914142  1.79476285 -2.7854526 ] Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods.\n",
      "        > [ 1.52148402  2.05003023 -2.64105034] Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B.\n",
      "          > [ 1.56753993  1.62435663 -2.63319659] Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI.\n",
      "          > [ 0.80796051  1.24656403 -3.2474947 ] E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency.\n",
      "          > [ 0.35870835  2.9598999  -3.46956015] All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory.\n",
      "          > [ 1.99896824  0.93420041 -2.89054155] Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B.\n",
      "        > [ 0.91083151  0.85542792 -3.59140062] For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments.\n",
      "          > [ 0.82970589  0.85667503 -3.04130507] For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github.\n",
      "          > [ 1.06538606  0.08583467 -2.70402575] com/HaoranZ99/laca.\n",
      "          > [-0.2968663   0.83157456 -4.53616142] A. Experimental Setup Datasets.\n",
      "          > [ 0.14747913  1.19835579 -3.66044545] Table III lists the statistics of the datasets used in the experiments.\n",
      "        > [ 0.53618199  0.84329247 -3.34852242] The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication.\n",
      "          > [ 0.42691395  0.58830446 -3.28902721] The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively.\n",
      "          > [ 0.73991334  0.81925356 -3.8625598 ] |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph.\n",
      "          > [ 0.1310645   0.81253082 -3.47897339] Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively.\n",
      "          > [-0.02924892  0.23463111 -3.15723586] The attributes of each node are bag-of-words embed- dings of the corresponding publication.\n",
      "        > [ 0.29998979  0.48431766 -3.101758  ] The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods.\n",
      "          > [ 0.72896451  0.14063013 -3.79014587] The ground-truth local cluster Ys of each publication contains the publications in its same subject areas.\n",
      "          > [ 0.09225523  0.8965438  -3.56378627] BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively.\n",
      "          > [-0.45713833  0.55838299 -2.95411253] Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups.\n",
      "          > [ 0.18273306 -0.4305293  -2.93256569] Yelp and Reddit 8TABLE IV: Evaluated methods.\n",
      "      > [ 0.94758701  0.99083483 -2.85063338] Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes.\n",
      "        > [ 0.5807128   1.29696453 -3.50836754] Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters.\n",
      "          > [ 0.39974338  1.31624615 -3.50447226] Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ !\n",
      "          > [ 0.37337908  0.54376626 -3.29584408] WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52].\n",
      "          > [-0.4151414   0.58205783 -3.89033651] Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters.\n",
      "          > [ 0.43637109  0.80371523 -3.04061365] Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters.\n",
      "        > [ 0.68323559  0.72530127 -3.50160217] Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively.\n",
      "          > [-0.33635941  1.09818709 -3.71931148] Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together.\n",
      "          > [ 0.19018139  1.00756621 -3.75769043] The ground-truth local clusters are generated based on the categories of products.\n",
      "          > [-0.01364136  0.71240234 -4.81445312] Competitors.\n",
      "          > [ 1.85211384 -0.14903159 -3.26124787] We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively.\n",
      "        > [ 1.16295695  1.10759652 -3.43473077] We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods.\n",
      "          > [ 0.70767301  0.93325406 -3.06818724] We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62].\n",
      "          > [ 1.1893512   1.06678092 -3.89208055] Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms.\n",
      "          > [ 1.17651784  0.57647341 -3.35802317] APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33].\n",
      "          > [ 0.17590249  0.8021996  -3.91538787] Groups 2)-4) comprise all global methods.\n",
      "        > [ 0.75483263  0.64328235 -3.05364275] Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes.\n",
      "          > [ 0.53045154  0.15983963 -3.29467773] Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively.\n",
      "          > [ 0.56597823  0.71788293 -3.61630988] The local clusters are then generated by sorting all nodes according to the similarity scores.\n",
      "          > [ 0.82134724  0.54156411 -3.18291593] The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors.\n",
      "          > [ 0.36294544  1.14903116 -2.74726605] Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes.\n",
      "      > [ 0.77413625  1.37514949 -3.091501  ] Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets.\n",
      "        > [ 0.59526914  1.1852603  -3.34436321] Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank.\n",
      "          > [ 0.50907767  1.72399116 -3.66699171] Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters.\n",
      "          > [ 0.20054244  0.45569551 -3.40953827] Implementations and Parameter Settings.\n",
      "          > [ 0.84539962 -0.2809839  -3.04525208] For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63].\n",
      "          > [ 0.19100121  0.8697468  -2.96837425] We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank.\n",
      "        > [ 0.94447583  1.22004938 -3.07492065] As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers.\n",
      "          > [ 0.20322987  0.25039545 -3.0059917 ] As for other competitors, we obtain their source codes from the respective authors.\n",
      "          > [ 0.02236881  1.03204954 -3.28683233] All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia.\n",
      "          > [ 1.46908212  0.68806881 -3.13612103] For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E).\n",
      "          > [ 1.0112915   1.16726685 -3.19506836] The parameters in global methods are set as suggested in their respective papers.\n",
      "        > [ 1.00240541  0.57555926 -3.00787067] On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|.\n",
      "          > [ 0.11902261  1.39268398 -3.51116729] On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks.\n",
      "          > [ 0.2976166   0.20354182 -3.48998737] All evaluation results reported in the experiments are averaged over seed nodes in S. B.\n",
      "          > [ 0.85621238  0.32681721 -3.65487671] Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters.\n",
      "          > [ 1.25752711 -0.04127071 -2.7266717 ] More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|.\n",
      "        > [ 1.12619817  0.28606328 -3.61476398] Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets.\n",
      "          > [ 0.59798968 -0.3471806  -3.51622057] Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets.\n",
      "          > [ 1.24750757  1.04998565 -3.3489542 ] The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined.\n",
      "          > [ 1.11616504  1.32499385 -3.40782452] We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours.\n",
      "          > [ 0.9022125  -0.15920572 -3.83962679] Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets.\n",
      "    > [ 0.63471013 -0.31325439 -4.15703678] Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16].\n",
      "      > [ 1.93630588  1.09508657 -2.86771536] Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA.\n",
      "        > [ 0.87150764  1.14433253 -4.06922865] Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision.\n",
      "          > [ 0.62977213  0.85146636 -3.76132035] Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin.\n",
      "          > [ 1.15523076  0.5953635  -4.02560902] For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively.\n",
      "          > [ 0.98142391  1.38101661 -3.43777394] Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively.\n",
      "          > [ 1.52202928  0.68057638 -3.76598763] Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision.\n",
      "        > [ 0.4834398   0.53414488 -3.2271018 ] Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%.\n",
      "          > [ 1.0618279   0.01509938 -3.41527629] Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth.\n",
      "          > [ 1.11532819  0.35866499 -3.71865559] Best is bolded and best baseline underlined .\n",
      "          > [ 0.91830283  0.5687831  -2.37660336] Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig.\n",
      "          > [ 0.49538153  0.52290988 -4.25693607] 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%.\n",
      "        > [ 1.29330301  1.22715783 -3.47191381] The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets.\n",
      "          > [ 0.5209229   1.36214614 -3.57316923] The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures.\n",
      "          > [ 1.44407392  1.33275557 -3.24788165] LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information.\n",
      "          > [ 1.62068295  1.21863174 -3.0847497 ] This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs.\n",
      "          > [ 1.19863892  0.52332973 -3.54553223] In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets.\n",
      "        > [ 1.5407598   1.01722527 -3.17641401] The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA.\n",
      "          > [ 1.21901894  0.36110628 -3.27562928] The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C).\n",
      "          > [ 1.04243779  0.96548694 -3.84920454] 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied.\n",
      "          > [ 1.50662172  1.1174314  -3.50138521] For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ).\n",
      "          > [ 1.89072406  0.15840046 -2.18046713] We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA.\n",
      "      > [ 1.42019403  1.72675073 -2.95141077] For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets.\n",
      "        > [ 0.91389722  0.66350651 -3.48316479] For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets.\n",
      "          > [ 0.89641368  1.20229518 -2.96565437] For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8.\n",
      "          > [ 0.60160679 -0.04469127 -3.75139308] For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|.\n",
      "          > [ 1.27623832  1.44155335 -3.78890657] Intuitively, the smaller ϵ is, the higher the recall should be.\n",
      "          > [ 0.26015085  0.48470923 -4.03511286] Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets.\n",
      "        > [ 1.3298043   0.36433002 -3.54531503] The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ.\n",
      "          > [ 0.66817701  0.46405628 -3.66781116] The x-axis and y-axis represent ϵ and the average recall, respectively.\n",
      "          > [ 1.63100123  0.65710336 -3.95896029] From Fig.\n",
      "          > [ 0.16460949  0.14146733 -3.84017897] 6, we can make the following observations.\n",
      "          > [ 1.47040558  0.30199766 -3.47888184] In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ.\n",
      "        > [ 1.0697422   1.18035781 -3.58107305] In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively.\n",
      "          > [ 1.21176279  1.35619462 -3.17161369] In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored).\n",
      "          > [ 0.63641822  0.51678932 -4.18603039] When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig.\n",
      "          > [ 0.39403856  1.30928433 -4.53845549] 7: Running times.\n",
      "          > [ 1.08116817  0.35258088 -3.43129849] Best method and competitor (in terms of precision) are bolded and underlined , respectively.\n",
      "        > [ 1.01608396  1.48100698 -3.10776567] except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets.\n",
      "          > [ 0.22182055  0.59898424 -3.13875389] except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably.\n",
      "          > [ 0.62570417  0.89499277 -3.60487795] Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5.\n",
      "          > [ 1.76776123  1.7825706  -3.55942822] The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV).\n",
      "          > [ 1.45504022  0.8166436  -2.61793208] Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets.\n",
      "      > [ 1.51590133  1.44843674 -3.09981441] This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV.\n",
      "        > [ 1.03751397  1.17149079 -3.40654826] This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V).\n",
      "          > [ 1.12773073  1.51302373 -3.56310534] This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed.\n",
      "          > [ 0.04143925  0.57734632 -3.64467335] Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a.\n",
      "          > [ 0.38314319  0.00640694 -3.71966076] WCSS) of nodes in local clusters output by all methods.\n",
      "          > [ 1.61952436  0.65249699 -3.22718   ] We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V).\n",
      "        > [ 0.92391789  1.32994044 -3.51270819] In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average.\n",
      "          > [ 0.47783059  1.09571791 -3.76094556] In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach.\n",
      "          > [ 0.80361491  1.13537073 -3.70774031] The y-axis represents the running time in seconds on a log scale.\n",
      "          > [ 1.39057159  0.6464982  -3.71922469] The first observation we can make from Fig.\n",
      "          > [ 0.81147826  1.00697732 -3.65442681] 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average.\n",
      "        > [ 1.28045833  1.60520649 -3.61948943] However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task.\n",
      "          > [ 0.86527997  1.19745767 -3.61223531] However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds.\n",
      "          > [ 1.38973773  1.88544703 -3.87167907] On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions.\n",
      "          > [ 0.58478767  1.31233406 -4.29185343] On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average.\n",
      "          > [ 1.09098577  1.4067651  -3.31332588] Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task.\n",
      "        > [ 1.64101553  1.09182501 -3.50471044] For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV.\n",
      "          > [ 1.63771975  1.25036037 -3.50008559] For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo.\n",
      "          > [-4.46378112e-01 -3.39169800e-03 -4.21918440e+00] 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage).\n",
      "          > [ 1.38486648  0.70670646 -4.30817413] In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503.\n",
      "          > [ 1.5379647   0.74669123 -3.43857837] The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV.\n",
      "      > [ 1.09764493  1.14630711 -3.1875391 ] On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16].\n",
      "        > [ 1.0651412   1.2092514  -3.60996389] On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network.\n",
      "          > [ 0.84756178 -0.02957096 -3.66622972] On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time.\n",
      "          > [ 1.79932499  1.63977051 -3.27449799] In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality.\n",
      "          > [ 0.19175833  1.02323771 -3.76088572] D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig.\n",
      "          > [-0.13348523  0.95704782 -3.89729691] 8: A real-world scenario on an academic network.\n",
      "        > [ 0.61796451  1.06264329 -3.06645274] detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)).\n",
      "          > [ 0.57156515  0.66806626 -4.05576944] detection) [17], [66], [67], and academic collaboration net- works [68].\n",
      "          > [ 0.53198588  0.9855091  -3.26624823] A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests.\n",
      "          > [ 0.51730096  1.38370514 -3.23949718] Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig.\n",
      "          > [-0.41203901  0.19699118 -3.52778578] 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)).\n",
      "        > [ 1.19404721  0.89468437 -3.15611911] In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph.\n",
      "          > [ 0.38107985  0.12131334 -3.21341276] In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig.\n",
      "          > [ 0.828637    0.87515014 -2.93162417] 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist.\n",
      "          > [-0.19406354  1.50893426 -4.04473877] VII. R ELATED WORK A.\n",
      "          > [ 1.32583404  1.74654424 -3.90436602] Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph.\n",
      "        > [-0.01955049  1.02909553 -3.8888495 ] The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16].\n",
      "          > [-0.01473215  0.41756624 -3.62305641] The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster.\n",
      "          > [ 0.47191793  1.45705128 -4.19873047] Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey).\n",
      "          > [-0.32140493  0.64317471 -4.02174473] Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank.\n",
      "          > [ 0.60939276  1.03641105 -3.49279404] Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16].\n",
      "    > [ 1.04971802  0.80967033 -3.25799775] The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395.\n",
      "      > [ 1.36579418  1.44972849 -3.17380571] The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss.\n",
      "        > [ 1.64180815  0.79838693 -3.88270283] The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality.\n",
      "          > [ 0.82652318  0.12749165 -3.93971157] The flow-based methods include [20]–[22], [33], [72].\n",
      "          > [ 2.06506109  0.90188414 -3.87464666] CRD [20] converts capacity diffusion into a maximum-flow problem.\n",
      "          > [ 0.71059835  0.58092666 -3.53179502] p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values.\n",
      "          > [ 1.44922447  0.8441847  -3.76423621] These methods usually provide theoretical guarantees on running time and approximation quality.\n",
      "        > [ 1.11697066  0.98563707 -3.3697381 ] However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization.\n",
      "          > [-1.11977148  1.55141628 -2.94206738] However, these guar- antees may not be useful in practical efficacy.\n",
      "          > [ 1.21944034  1.05617607 -3.69865918] Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach.\n",
      "          > [ 1.45926929  0.29986954 -3.87320709] [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs.\n",
      "          > [ 1.26316428  1.08474517 -3.00232387] Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization.\n",
      "        > [ 0.51824421  1.82549059 -3.6999402 ] B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed.\n",
      "          > [ 0.3764745   1.62828827 -3.52313232] B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74].\n",
      "          > [ 1.31624758  1.81638992 -3.49621677] Similar to local clustering, community search is query-dependent and does not need to compute the graph globally.\n",
      "          > [-0.34070331  1.82315719 -3.6673429 ] Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80].\n",
      "          > [ 0.71903402  1.25955665 -3.94464087] To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed.\n",
      "        > [ 0.06317718  1.2747879  -3.132653  ] These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss.\n",
      "          > [ 0.16653232  0.52809197 -3.16773653] These methods usually consider keywords as the attributes of the nodes.\n",
      "          > [ 0.19931781  1.58614922 -3.38839769] Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community.\n",
      "          > [ 0.48458165  0.73410022 -3.13762474] Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity.\n",
      "          > [ 0.53009504  0.78777421 -2.95502162] Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss.\n",
      "      > [ 1.25050545  1.45967615 -2.92900515] Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y .\n",
      "        > [ 0.72412479  1.58007205 -3.11390114] Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search.\n",
      "          > [ 0.81113482  1.57143474 -3.4379189 ] Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes.\n",
      "          > [ 0.54203933  1.30199444 -3.41071129] Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32].\n",
      "          > [ 0.58034831  0.84902632 -2.95479298] In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices.\n",
      "          > [ 1.00141239  1.54107225 -3.15155959] Therefore, the problem setting of our work is orthogonal to that of community search.\n",
      "        > [ 1.62580943  1.02303541 -3.42708325] VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs.\n",
      "          > [ 1.78835511  0.73156297 -3.34585118] VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs.\n",
      "          > [ 1.72913468  1.15741575 -3.57034278] LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation.\n",
      "          > [ 1.00316894  0.87231928 -3.40979767] The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency.\n",
      "          > [ 0.74175805  1.27818716 -3.41971135] Regarding future work, we plan to study the local clustering on heterophilic graphs.\n",
      "        > [ 0.09526316  1.35494566 -3.166991  ] 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596.\n",
      "          > [ 0.00942802  1.11195612 -3.34488535] 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) .\n",
      "          > [-0.64905107  1.10464585 -3.22269702] IEEE, 2022, pp.\n",
      "          > [ 0.29253361  0.99326706 -2.90137863] 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp.\n",
      "          > [-0.88124871 -0.20482805 -3.25056744] 5586–5596.\n",
      "        > [-0.21072885  0.37617397 -2.91459846] [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y .\n",
      "          > [-0.30728945  0.60632718 -3.07229686] [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining .\n",
      "          > [-0.20433998  1.43713379 -3.36254883] IEEE, 2009, pp.\n",
      "          > [-0.54809701  1.2727803  -3.15782118] 237–242.\n",
      "          > [-0.28322807 -0.11488842 -3.280164  ] [4] K. V oevodski, S.-H. Teng, and Y .\n",
      "      > [ 0.58250415  1.31282759 -3.51007795] Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099.\n",
      "        > [ 0.74929678  0.78308851 -3.35883427] Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009.\n",
      "          > [ 1.09884894  1.11517775 -3.30289602] Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol.\n",
      "          > [ 0.21987885  0.65022171 -3.17427373] 10, pp. 1–14, 2009.\n",
      "          > [ 0.66422749  0.45289439 -3.21613312] [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol.\n",
      "          > [-0.56403732  0.52500343 -2.46011353] 25, pp. i253–i258, 2009.\n",
      "        > [ 0.88250095  1.28953815 -3.3658247 ] [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527.\n",
      "          > [ 0.15480855 -0.54861295 -3.882411  ] [6] J. Li, J.\n",
      "          > [-0.37512469 -0.86878908 -2.8619597 ] He, and Y .\n",
      "          > [ 0.81321925  1.41502714 -3.60751081] Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp.\n",
      "          > [ 0.23254013 -0.40048027 -3.53198242] 519–527.\n",
      "        > [ 0.03280616  0.56344408 -3.67080688] [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724.\n",
      "          > [-0.35637283 -0.88664865 -3.97781229] [7] Y . Zhu, J. Li, J.\n",
      "          > [ 0.44055966 -0.57706439 -2.99761105] He, B. L. Quanz, and A.\n",
      "          > [ 0.301357    0.63086373 -3.48614979] A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp.\n",
      "          > [-0.87548828 -0.03449631 -3.68286133] 3718–3724.\n",
      "        > [-0.01703776  0.69164068 -3.76306677] [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099.\n",
      "          > [-0.31734869 -0.23787555 -3.88965416] [8] H. Yang, Y .\n",
      "          > [-0.52182263 -0.88331193 -3.10950017] Zhu, and J.\n",
      "          > [ 0.07935552  1.00775707 -3.68267941] He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp.\n",
      "          > [-0.00517845  0.1027832  -4.04248047] 2091–2099.\n",
      "      > [ 0.74402535  0.61740345 -2.96722412] [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395.\n",
      "        > [ 0.3067039   0.2036801  -2.87884021] [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368.\n",
      "          > [ 0.66106975  0.62518549 -2.88952756] [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol.\n",
      "          > [-0.56267953 -0.0756043  -2.96193242] 15, no. 13, pp. 3883–3896, 2022.\n",
      "          > [ 0.53407449  0.26123855 -3.55394101] [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp.\n",
      "          > [-0.1266861  -0.29516602 -3.51318359] 359–368.\n",
      "        > [ 0.88777977  0.55465376 -2.83841085] [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol.\n",
      "          > [ 0.06417687  0.85237759 -3.0808332 ] [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp.\n",
      "          > [-0.20318604 -0.29872894 -3.47973633] 2120–2129.\n",
      "          > [ 1.13446379  0.74233514 -2.85036993] [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011.\n",
      "          > [ 0.60718787 -0.14733431 -3.23833084] [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol.\n",
      "        > [ 0.76673579  0.78670126 -3.61616039] 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) .\n",
      "          > [-0.22587606  0.26329491 -3.36292577] 13, no. 77, pp. 2339–2365, 2012.\n",
      "          > [ 1.05906403  0.64560252 -3.55764961] [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol.\n",
      "          > [-0.83875656 -0.2650032  -3.79992676] 42, no. 1, pp. 1–26, 2013.\n",
      "          > [ 0.51235539  0.53155804 -3.6053772 ] [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) .\n",
      "        > [ 0.84335375  0.93298846 -3.79853439] IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395.\n",
      "          > [-0.02382278  0.98406982 -3.65454102] IEEE, 2006, pp.\n",
      "          > [-0.42588702  0.11862507 -3.65011287] 475–486.\n",
      "          > [ 0.98214775  0.66450971 -3.65154362] [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp.\n",
      "          > [ 0.33772278  0.01596832 -3.92651367] 1386–1395.\n",
      "  > [ 1.031376    2.10364819 -2.06967926] [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp. 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab. (LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0.\n",
      "    > [ 0.21087125  0.99555886 -2.71285987] [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp.\n",
      "      > [ 0.65233958  1.12580693 -3.54339099] [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020.\n",
      "        > [ 0.29170796  0.90681034 -3.83151031] [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp.\n",
      "          > [ 0.3785533   0.77961224 -3.60281467] [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp.\n",
      "          > [ 0.31501007 -0.18812847 -3.51489258] 1339–1356.\n",
      "          > [ 0.01451126  0.83261681 -3.88307524] [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol.\n",
      "          > [ 0.84590632  0.43009275 -2.7141304 ] 716, pp.\n",
      "        > [ 0.215747    1.04052031 -3.23243332] 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning .\n",
      "          > [-0.499897   -0.1489315  -4.39257812] 1–58, 2017.\n",
      "          > [ 0.40421122  0.59647655 -3.19885015] [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol.\n",
      "          > [ 0.30140659 -0.45504299 -3.60604358] 91, 2015.\n",
      "          > [ 0.14665815  1.22656775 -3.19071269] [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning .\n",
      "        > [ 0.710549    1.191679   -3.36923242] PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp.\n",
      "          > [-0.04663843 -0.57790577 -3.13523936] PMLR, 2017, pp.\n",
      "          > [-0.35317913 -0.32157078 -3.27221537] 3598–3607.\n",
      "          > [ 0.63905847  1.19285965 -3.57376528] [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning .\n",
      "          > [-0.09196955 -0.61187911 -2.6610415 ] PMLR, 2020, pp.\n",
      "        > [ 0.90099651  0.91312462 -3.65077925] 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020.\n",
      "          > [ 0.23356628  0.06836891 -3.2800293 ] 3222–3232.\n",
      "          > [-0.30234459 -1.0609206  -3.89595699] [22] A. Jung and Y .\n",
      "          > [ 1.02294469  0.99033272 -3.53092146] SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol.\n",
      "          > [-0.61035514 -0.29367015 -2.8947196 ] 28, pp. 106–110, 2020.\n",
      "      > [ 1.08512151  1.31383049 -3.16215777] [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol.\n",
      "        > [ 0.67858899  1.02634537 -3.60408068] [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564.\n",
      "          > [ 0.45553398  1.01517868 -3.44616699] [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol.\n",
      "          > [ 0.31847143 -0.46393967 -3.81970215] 2, no. 1-46, p. 4, 1993.\n",
      "          > [ 0.80705845  1.10621262 -3.55044556] [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp.\n",
      "          > [-0.22099091 -0.31133023 -3.20778871] 555–564.\n",
      "        > [ 0.77413243  1.53400755 -3.2378943 ] [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) .\n",
      "          > [-1.0430603   0.88225937 -3.69744873] [25] D. Fu, D. Zhou, and J.\n",
      "          > [ 0.3021974   1.33484221 -3.31604505] He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp.\n",
      "          > [ 0.67415076  0.03737507 -4.23264551] 390–400.\n",
      "          > [ 0.73106462  1.31937945 -3.11058807] [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) .\n",
      "        > [ 0.83347499  1.17740607 -3.09998488] SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y .\n",
      "          > [-0.11240862  0.75696093 -3.48836637] SIAM, 2023, pp.\n",
      "          > [ 0.80224001  1.25693822 -3.44105744] 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol.\n",
      "          > [-0.08675348 -0.48936582 -3.35003281] 18, no. 3, p. 183404, 2024.\n",
      "          > [-0.32506543 -0.66186213 -3.61771441] [28] C. Huang, H. Li, Y .\n",
      "        > [ 1.03331828  1.01027131 -3.18544793] Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol.\n",
      "          > [ 1.16889572  0.89738667 -3.12867355] Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp.\n",
      "          > [-0.51086426 -0.0477953  -3.48168945] 803–814.\n",
      "          > [ 0.70957911  0.72725976 -3.6879015 ] [29] L. Liao, X.\n",
      "          > [ 0.12964229  0.99313009 -3.15968323] He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol.\n",
      "      > [ 1.0154202   1.26623142 -3.39298964] 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J.\n",
      "        > [ 0.84854692  1.22417331 -3.91103196] 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015. [31] S. Freitas, N. Cao, Y .\n",
      "          > [-0.05858854  0.36168551 -3.55252028] 30, no. 12, pp. 2257–2270, 2018.\n",
      "          > [ 1.04601097  1.14050436 -4.02922344] [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol.\n",
      "          > [-0.83441395 -0.24955855 -2.95311618] 3, no. 3, pp. 408–444, 2015.\n",
      "          > [ 0.09227675 -0.54903394 -3.57171106] [31] S. Freitas, N. Cao, Y .\n",
      "        > [ 0.61976826  1.20483565 -3.83132744] Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y .\n",
      "          > [ 0.52538061  1.18104708 -3.81156921] Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data).\n",
      "          > [-0.62781906  0.99234772 -3.53320312] IEEE, 2018, pp.\n",
      "          > [ 0.11593437  0.28095245 -3.99926758] 1001–1008.\n",
      "          > [ 0.2430788   0.35547367 -3.70091915] [32] Y .\n",
      "        > [ 0.53606379  1.17402995 -3.05993485] Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817.\n",
      "          > [-0.14446013  0.09471326 -2.81845927] Niu, Y .\n",
      "          > [ 0.52181047  1.21904325 -3.11164761] Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) .\n",
      "          > [-0.64905107  1.10464585 -3.22269702] IEEE, 2022, pp.\n",
      "          > [-0.29428101  0.27716637 -3.26586914] 2805–2817.\n",
      "        > [ 0.96258593  1.11332476 -3.33623838] [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J.\n",
      "          > [ 1.05403137  1.10873067 -3.31420755] [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol.\n",
      "          > [ 0.37922683  0.20853752 -2.38860059] 202, 2023, pp.\n",
      "          > [ 0.03757477  0.27275848 -3.57739258] 39 252–39 276.\n",
      "          > [ 9.42245051e-02 -2.36130692e-03 -3.27459335e+00] [34] N. Halko, P.-G. Martinsson, and J.\n",
      "      > [-0.01777313  0.86837953 -3.23414993] A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp.\n",
      "        > [ 0.08819249  0.45794398 -3.10998869] A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol. 29, 2016.\n",
      "          > [ 0.49316403  0.43825203 -3.27513361] A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol.\n",
      "          > [-0.51695251 -0.64958191 -3.15704346] 53, no. 2, pp. 217–288, 2011.\n",
      "          > [-0.22828132  0.45701021 -3.17533565] [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol.\n",
      "          > [ 0.25403419 -0.15308884 -3.91349435] 29, 2016.\n",
      "        > [-0.04012275  0.95252347 -3.16278243] [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp.\n",
      "          > [ 0.03354191  0.06738422 -3.27845573] [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y .\n",
      "          > [-0.15834357  0.96680647 -3.61774015] Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp.\n",
      "          > [ 0.07930541 -0.53768116 -3.63393903] 505–514.\n",
      "          > [ 0.4234263   1.36419749 -3.26857471] [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp.\n",
      "        > [ 0.32123625  1.10508442 -3.75702333] 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034.\n",
      "          > [ 0.64876765 -0.44767898 -3.54583478] 308–318.\n",
      "          > [ 0.46753228  0.95556754 -3.88604355] [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol.\n",
      "          > [ 0.879776    0.30902863 -3.28076172] 30, 2017, pp.\n",
      "          > [ 0.38951111 -0.26905823 -3.76245117] 1024–1034.\n",
      "        > [ 0.13695791  0.66218466 -3.72894788] [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp.\n",
      "          > [ 0.16555667  0.68151617 -3.26451492] [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022.\n",
      "          > [-0.20439328 -0.18509746 -3.3215239 ] [40] H. Tong, C. Faloutsos, and J.-Y .\n",
      "          > [ 0.14624281  0.72590381 -4.47727299] Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06).\n",
      "          > [-0.02382278  0.98406982 -3.65454102] IEEE, 2006, pp.\n",
      "    > [ 0.53387612  1.79757917 -3.19454789] 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab. (LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol.\n",
      "      > [-0.00944829  0.97411305 -2.77201629] 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226.\n",
      "        > [ 0.09912057  1.2653476  -3.12121296] 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp.\n",
      "          > [-0.20471573  0.0320282  -3.45690918] 613–622.\n",
      "          > [ 0.04813793  1.35476482 -3.1036067 ] [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp.\n",
      "          > [-0.05824189 -0.18987888 -3.62937093] 271– 279.\n",
      "          > [ 0.61468828  0.65607822 -3.49472451] [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp.\n",
      "        > [-0.2281158   0.83968979 -3.46043444] 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176.\n",
      "          > [-0.08283234  0.08981991 -3.76330566] 1392– 1402.\n",
      "          > [-0.02576863  0.6846686  -3.48036718] [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 .\n",
      "          > [-0.43566895  0.15791702 -3.40258789] Springer, 2015, pp.\n",
      "          > [-0.07416721  0.38260478 -3.98535204] 164–176.\n",
      "        > [ 0.72251058 -0.38011912 -3.61737061] [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y .\n",
      "          > [ 0.85324007 -0.25434768 -3.84788847] [44] R. J. Muirhead, Aspects of multivariate statistical theory .\n",
      "          > [-0.29891381  0.32695904 -3.31364894] John Wiley & Sons, 2009.\n",
      "          > [ 0.47715542 -0.10065159 -4.05197525] [45] Y .\n",
      "          > [ 0.15646626 -0.26889935 -3.24112344] Ma, X. Liu, T. Zhao, Y .\n",
      "        > [ 0.3868725   1.04564774 -2.60850859] Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226.\n",
      "          > [ 0.67818451  0.74929309 -2.82509518] Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp.\n",
      "          > [ 0.3529892   0.79602051 -3.41113281] 1202–1211.\n",
      "          > [-0.18113115  1.52943897 -2.44897723] 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp.\n",
      "          > [ 0.6340332   0.24146461 -3.35742188] 1215–1226.\n",
      "      > [ 0.19284745  1.33676386 -2.6753509 ] [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp.\n",
      "        > [ 0.33868852  1.40608752 -3.11124301] [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008.\n",
      "          > [ 0.42105338  1.62364292 -3.18042731] [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp.\n",
      "          > [-0.47509766 -0.02072144 -3.45996094] 2464–2473.\n",
      "          > [ 0.06451739  0.75687468 -3.10440922] [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol.\n",
      "          > [ 0.19174004 -0.14296627 -2.97338867] 29, no. 3, pp. 93–93, 2008.\n",
      "        > [ 0.29387248  0.82570922 -3.03846002] [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739.\n",
      "          > [ 0.68604523  0.44518715 -3.11612177] [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp.\n",
      "          > [-0.45772552  0.14134979 -3.421875  ] 817–826.\n",
      "          > [ 0.17872912  1.25829864 -2.9351759 ] [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp.\n",
      "          > [ 0.60068321 -0.55285645 -3.72338867] 731–739.\n",
      "        > [ 0.33640435  1.20935678 -3.12359023] [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V .\n",
      "          > [-1.21990848 -0.59434313 -3.53237367] [51] W. Hu, M. Fey, M. Zitnik, Y .\n",
      "          > [ 0.474594    0.96438837 -3.23215294] Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol.\n",
      "          > [-0.07642215 -0.12218636 -2.87986827] 33, pp. 22 118–22 133, 2020.\n",
      "          > [-0.19326988  0.29382896 -3.18437004] [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V .\n",
      "        > [ 0.16818975  1.19333017 -2.79329586] K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp.\n",
      "          > [ 0.33356899  0.90108806 -3.00344896] K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023.\n",
      "          > [ 0.24835177  0.00721778 -3.00664258] [53] W.-L. Chiang, X. Liu, S. Si, Y .\n",
      "          > [ 0.04568337  0.29665256 -2.44298172] Li, S. Bengio, and C.-J.\n",
      "          > [ 0.01527928  1.45644653 -3.28346968] Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp.\n",
      "      > [-0.12365283  1.19631886 -3.17378402] 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol.\n",
      "        > [-0.16893667  0.27662003 -3.37465954] 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp.\n",
      "          > [ 0.55664158  0.7081598  -3.87609053] 257–266.\n",
      "          > [-0.33309081  0.38024646 -3.23409438] [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp.\n",
      "          > [-0.46895504 -0.17524385 -3.38903809] 556–559.\n",
      "          > [ 0.53501213  0.18925497 -3.54369974] [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp.\n",
      "        > [-0.77028865  0.9707129  -3.36159468] 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1.\n",
      "          > [-0.36174011 -0.66043091 -3.56994629] 538– 543.\n",
      "          > [-0.880669    1.24699843 -3.65754461] [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010.\n",
      "          > [ 0.45615071  0.1766877  -2.92007971] [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol.\n",
      "          > [-0.24651277 -0.28217244 -3.25200844] 4, no. 1, 2012, p. 1.\n",
      "        > [ 0.49078226  1.34924769 -2.84371591] [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y .\n",
      "          > [ 0.21228598 -0.1727457  -3.69412422] [58] C.-C. Hsu, Y .-A.\n",
      "          > [ 0.4964388   1.1379323  -3.26327252] Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017.\n",
      "          > [ 0.27653149  1.22203171 -3.11857891] [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016.\n",
      "          > [-0.23513104  0.10009164 -3.3169713 ] [60] R. Yang, J. Shi, X. Xiao, Y .\n",
      "        > [ 0.85990584  2.39996338 -3.5583055 ] Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol.\n",
      "          > [ 1.29348958  2.65600014 -3.51772881] Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol.\n",
      "          > [-0.6532802  -0.13573647 -3.29376221] 14, no. 1, pp. 37–49, 2020.\n",
      "          > [-0.22873405  0.28988045 -3.39781761] [61] R. Yang, J. Shi, X. Xiao, Y .\n",
      "          > [ 0.30006897  1.45296168 -3.45902967] Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol.\n",
      "      > [ 0.72318339  1.68967235 -3.30099344] 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab. (LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol.\n",
      "        > [ 0.64173239  1.34182549 -3.25570083] 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018.\n",
      "          > [ 0.12200884  0.63838339 -2.99780035] 32, pp. 1237–1262, 2023.\n",
      "          > [-0.29292744  0.24863674 -3.65135193] [62] G. Pan, Y .\n",
      "          > [ 0.34932357  1.45216393 -3.35869741] Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021.\n",
      "          > [ 0.48638749  0.49217737 -3.70286751] [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018.\n",
      "        > [ 0.35524696  1.1163801  -3.36980677] [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab. (LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol.\n",
      "          > [ 0.00811289  1.43765509 -3.39664173] [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab.\n",
      "          > [ 0.79001796  0.75723928 -3.72958589] (LANL), Los Alamos, NM (United States), Tech.\n",
      "          > [ 0.29361501  0.34136909 -3.27236843] Rep., 2008.\n",
      "          > [ 1.07881546  0.64106369 -3.6930542 ] [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol.\n",
      "        > [ 1.08213329  1.57144582 -3.64936805] 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol.\n",
      "          > [ 0.54940999 -0.07968718 -2.91709471] 104, no.\n",
      "          > [ 0.78576601 -0.29992518 -2.82167816] 50, pp. 19 735–19 740, 2007.\n",
      "          > [ 0.16149274  0.32171741 -4.10592031] [66] Y .\n",
      "          > [ 0.95375878  1.65800405 -3.72314072] Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol.\n",
      "        > [ 0.5119738   1.678092   -3.42067933] 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol.\n",
      "          > [ 0.54730982  0.84374374 -3.35440445] 9. Association for Computing Machinery (ACM), 2016, pp.\n",
      "          > [ 0.17454529  0.35948181 -3.84277344] 1233–1244.\n",
      "          > [ 0.30666927  0.08891109 -4.11654854] [67] Y .\n",
      "          > [ 0.47470373  1.92758727 -3.51830101] Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol.\n",
      "    > [ 0.75226337  1.41805172 -2.57206583] 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      "      > [ 0.34962225  1.29501653 -3.24991989] 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V .\n",
      "        > [ 0.09774137  1.43625021 -3.51017785] 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol.\n",
      "          > [-0.41961712  0.07066396 -3.44709373] 31, no. 11, pp. 2093–2107, 2018.\n",
      "          > [ 0.18409555  0.06020011 -3.94798088] [68] Y .\n",
      "          > [ 1.23191071  0.37649205 -3.85348773] Fang, Y .\n",
      "          > [ 0.25177246  1.7558254  -3.53291202] Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol.\n",
      "        > [ 0.66311342  0.98115551 -3.45683813] 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp.\n",
      "          > [ 0.68704718  0.73115742 -3.52272677] 13, no. 6. VLDB Endowment, 2020, pp.\n",
      "          > [ 0.29721832  0.47351837 -3.5592041 ] 854–867.\n",
      "          > [ 0.38777217 -1.10024488 -3.59576106] [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y .\n",
      "          > [ 0.1640002   1.3971591  -3.39301085] Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp.\n",
      "        > [ 0.47042045  0.47858348 -3.73951387] 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y .\n",
      "          > [ 0.39620972 -0.6230011  -3.69335938] 1445–1454.\n",
      "          > [ 0.48740086  0.40379554 -3.53803396] [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol.\n",
      "          > [-0.23284747  0.36926654 -2.63298059] 10, pp. 110 701–110 726, 2022.\n",
      "          > [-0.13674513 -0.24639551 -3.34894896] [71] R. Andersen, S. O. Gharan, Y .\n",
      "        > [ 0.066083    1.10685194 -3.17932129] Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V .\n",
      "          > [ 0.31390134  1.49271822 -3.14981151] Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol.\n",
      "          > [-0.45283508 -0.08031178 -3.24621582] 63, no. 2, pp. 1–31, 2016.\n",
      "          > [ 0.20170468  0.7174114  -3.17114019] [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023.\n",
      "          > [ 0.65410972 -0.02719082 -4.05499458] [73] X. Huang, L. V .\n",
      "      > [ 0.30587927  2.20627999 -3.16847587] Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y .\n",
      "        > [ 0.30186495  1.97586524 -3.19191694] Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454.\n",
      "          > [ 0.36928213  2.05331707 -2.95873547] Lakshmanan, and J. Xu, Community Search over Big Graphs.\n",
      "          > [ 0.13824469 -0.38714069 -3.9348805 ] Morgan & Claypool Publishers, 2019.\n",
      "          > [ 0.22529955  1.93478227 -3.37100315] [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp.\n",
      "          > [ 0.25158501 -0.72988892 -3.69262695] 1451–1454.\n",
      "        > [ 0.3398644   2.12641382 -3.01080823] [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol.\n",
      "          > [-0.26396263  1.64226699 -3.00827146] [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010.\n",
      "          > [ 0.44102132 -0.29273957 -3.85566664] [76] W. Cui, Y .\n",
      "          > [ 0.30311868  1.74460399 -3.42403841] Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014.\n",
      "          > [ 0.68202889  1.53134847 -3.33327746] [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol.\n",
      "        > [ 0.13091762  1.22519553 -3.46615171] 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol.\n",
      "          > [-0.43376672 -0.09749819 -2.76139545] 29, pp. 1406–1433, 2015.\n",
      "          > [ 0.18285553  1.74080443 -3.74891067] [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014.\n",
      "          > [ 0.56249988  0.20313583 -3.95854568] [79] X. Huang, L. V .\n",
      "          > [-0.04621084  0.69376558 -3.42363024] S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol.\n",
      "        > [ 0.13865034  0.57086635 -2.86998606] 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y .\n",
      "          > [ 0.28120154  0.76493979 -3.12023473] 9. Association for Computing Machinery (ACM), 2015, pp.\n",
      "          > [-0.29375359  0.66305864 -3.54116154] 276–287.\n",
      "          > [-0.05504864  0.7187264  -3.55452871] [80] Q. Liu, Y .\n",
      "          > [-0.34022152 -0.51225036 -3.28065252] Zhu, M. Zhao, X. Huang, J. Xu, and Y .\n",
      "      > [ 0.68407631  1.3850404  -3.27230024] Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786.\n",
      "        > [ 0.86155683  1.46614814 -3.84346247] Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol.\n",
      "          > [ 0.37518686  1.47749925 -3.94033718] Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020.\n",
      "          > [ 0.07261854  0.55221057 -3.75244331] [81] Y .\n",
      "          > [ 0.96258914 -0.2268572  -3.32938671] Fang, R. Cheng, Y .\n",
      "          > [ 0.76503748  0.71249622 -3.53755546] Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol.\n",
      "        > [ 0.71216112  1.12295711 -3.17344236] 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp.\n",
      "          > [-0.63493276  0.05278307 -3.49433231] 26, pp. 803–828, 2017.\n",
      "          > [ 0.28534353 -0.22402608 -4.12258291] [82] X. Huang and L. V .\n",
      "          > [ 0.79640651  0.82320613 -3.22748566] S. Lakshmanan, “Attribute-driven community search,” vol.\n",
      "          > [ 0.33720803  0.74779743 -3.63678789] 10. Association for Computing Machinery (ACM), 2017, pp.\n",
      "        > [ 0.28437775  0.81361389 -3.00088739] 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996.\n",
      "          > [ 0.27715701  0.31857982 -4.16564894] 949–960.\n",
      "          > [ 0.30454275  0.21258906 -3.71767211] [83] Y .\n",
      "          > [ 0.00380794  0.80094624 -3.06076431] Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020.\n",
      "          > [ 0.72129291  0.38385111 -3.42399263] [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996.\n",
      "        > [ 0.53502047  0.19230726 -3.12066436] [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786.\n",
      "          > [ 1.00492513  0.29714212 -3.20596385] [85] G. Strang, Introduction to linear algebra .\n",
      "          > [-0.6623174   0.21148479 -3.56864142] SIAM, 2022.\n",
      "          > [ 0.34225377  0.48153615 -3.20968843] [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp.\n",
      "          > [-0.99359131 -0.2655735  -3.22167969] 2778–2786.\n",
      "      > [ 0.60264534  1.00644159 -3.25741625] [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      "        > [ 1.39548922 -0.40999791 -3.44000483] [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297.\n",
      "          > [ 0.77503538  0.58371657 -3.97545958] [87] J. MacQueen et al.\n",
      "          > [ 1.2371943  -0.49116063 -3.58205128] , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol.\n",
      "          > [ 1.00824988  1.08575416 -3.99647427] 1, no. 14. Oakland, CA, USA, 1967, pp.\n",
      "          > [ 0.47818652  0.30509588 -3.74711204] 281–297.\n",
      "        > [ 0.11848589  1.17026031 -3.40755486] [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp.\n",
      "          > [ 0.57514191  0.97147751 -3.68041992] [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012.\n",
      "          > [ 0.00899615 -0.11822467 -3.37601423] [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y .\n",
      "          > [-0.13072282  1.32642353 -3.6455245 ] Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence .\n",
      "          > [-0.72211576  1.93572092 -2.64110804] International Joint Conferences on Artificial Intelligence Organization, 2015, pp.\n",
      "        > [ 0.46252495  1.3796072  -3.24372506] 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp.\n",
      "          > [-0.36370897  0.61727905 -3.67578125] 2111–2117.\n",
      "          > [ 0.66210049  1.16033685 -3.70462728] [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol.\n",
      "          > [-0.42225012  0.73686594 -3.68352532] Not available.\n",
      "          > [ 0.40741357  0.61903125 -3.6915679 ] Society for Industrial and Applied Mathematics, 2017, pp.\n",
      "        > [ 0.33397266  1.45576608 -3.22466969] 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      "          > [ 0.329422    0.69476318 -3.65380859] 633–641.\n",
      "          > [ 0.44029069  1.13461781 -3.26409721] [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019.\n",
      "          > [ 0.28641173  1.18676579 -3.02675962] [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      "          > [ 0.15796554  1.61540616 -3.54741955] [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      "    > [ 1.35109174  2.29778504 -2.13137174] [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0.\n",
      "      > [ 0.64134699  0.7009483  -2.69080901] [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi).\n",
      "        > [-0.07255742  0.49757093 -3.28148532] [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119.\n",
      "          > [-0.43363106  0.03376308 -3.45837212] [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J.\n",
      "          > [ 0.2307055   0.18838003 -3.5014236 ] Dean, “Distributed representations of words and phrases and their composi- tionality,” vol.\n",
      "          > [ 0.13253784  0.35259247 -2.83630371] 26, 2013, pp.\n",
      "          > [-0.37836456 -0.0859642  -3.72875977] 3111–3119.\n",
      "        > [ 0.91962636 -0.23063408 -2.69385815] 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ.\n",
      "          > [ 0.22172928  0.22666931 -3.09960938] 14APPENDIX A.\n",
      "          > [ 1.34915161  0.70385742 -3.6875    ] Theoretical Proofs 1) Proof of Theorem IV .1: Proof.\n",
      "          > [ 1.07445478 -0.87175149 -2.95042419] We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration.\n",
      "          > [ 0.5064677   0.08618581 -2.63732338] Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ.\n",
      "        > [-0.04627847  0.87274134 -2.60772038] (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· .\n",
      "          > [-0.38810086  0.07859719 -3.14276624] (21) Next, we consider {− →γ 1, − →γ 2, .\n",
      "          > [ 0.10201441  0.72820657 -2.84401846] . . ,− →γ ℓ, − →γ ℓ+1, .\n",
      "          > [ 1.28117156 -0.1087402  -2.66457915] . . ,− →γ ∞}.\n",
      "          > [ 0.41792011  0.87533158 -2.97667813] By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· .\n",
      "        > [ 0.23323537  0.57787728 -1.99708915] Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi).\n",
      "          > [ 1.35127139  0.1327365  -2.50097585] Then, Eq.\n",
      "          > [ 0.614384    0.37190717 -2.41522288] (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ.\n",
      "          > [ 0.49144098  0.52612507 -2.86269665] (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5).\n",
      "          > [-0.03578183  0.6234141  -2.83435082] Let − →b be a length- n vector where each i- th entry is ϵ · d(vi).\n",
      "      > [ 0.72604275  0.32058072 -2.35678363] Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1.\n",
      "        > [ 0.0048353   0.57274497 -2.34328938] Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq.\n",
      "          > [ 0.45645106  0.33195654 -3.77733088] Together with Eq.\n",
      "          > [ 0.89585364 -0.14563784 -2.56059718] (22) and the matrix definition of π(vi, vj) defined in Eq.\n",
      "          > [-0.27710852  0.72523648 -3.0757637 ] (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt).\n",
      "          > [-0.16603044  1.00898612 -2.82301641] By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq.\n",
      "        > [ 1.04441357  2.25214624 -2.92848897] (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ.\n",
      "          > [ 0.59746766  2.95010138 -3.85605478] (14). In what follows, we analyze the time complexity of Algo.\n",
      "          > [ 0.6469906   0.94136161 -3.51359272] (1). First, suppose that Lines 3-7 in Algo.\n",
      "          > [ 0.68840766 -0.86460751 -3.02397656] 1 are executed for L iterations.\n",
      "          > [ 0.86790532  0.26624593 -2.57242155] Now, we consider any iteration ℓ.\n",
      "        > [ 0.30370745  0.1655094  -3.1515975 ] For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction.\n",
      "          > [ 0.7974053  -0.56874955 -3.25100183] For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively.\n",
      "          > [-0.67233795  0.79029506 -3.10377693] Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ.\n",
      "          > [ 9.33843195e-01  2.06824392e-03 -2.98551726e+00] First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1.\n",
      "          > [-0.08112752  1.45935619 -3.94573545] (23) We prove this by induction.\n",
      "        > [ 0.69644099  0.3361088  -2.8804822 ] For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1.\n",
      "          > [ 1.34085393  0.34132603 -2.71082473] For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq.\n",
      "          > [ 0.13730213  0.61058861 -4.15443897] (23) holds.\n",
      "          > [ 1.39426279  0.18342291 -3.1353972 ] Next, we assume Eq.\n",
      "          > [ 0.55361235  0.07500093 -3.36541367] (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1.\n",
      "      > [ 1.23550534  1.62751389 -2.17189646] According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1.\n",
      "        > [ 0.52100992  0.81196404 -2.33454633] According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq.\n",
      "          > [ 0.55977976  0.75962293 -2.72763562] According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP.\n",
      "          > [ 0.61193722  0.66191339 -3.14451361] As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1.\n",
      "          > [ 0.40950945  0.59943604 -2.88310814] (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq.\n",
      "          > [ 1.02606082  0.41711336 -3.1940794 ] (24) equals 0, namely, Eq.\n",
      "        > [ 0.75653493  0.55339652 -2.77341652] (23) still holds. As per Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq.\n",
      "          > [ 0.94515991  0.29795456 -4.18286133] (23) still holds.\n",
      "          > [ 0.52238625  0.0564643  -3.06966925] As per Line 6, in each ℓ-th iteration, Algo.\n",
      "          > [ 0.61275917  0.12108608 -2.86251664] 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ.\n",
      "          > [ 1.41183782 -0.17711437 -2.48396373] After L iterations, we obtain − →q L. Recall that by Eq.\n",
      "        > [ 0.37638521  1.19438279 -2.25198245] (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned.\n",
      "          > [ 0.31706047  0.83178711 -3.78991699] (23), ∥− →q L∥1 ≤ ∥− →f ∥1.\n",
      "          > [-0.01077236  1.03209579 -2.51851749] We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ .\n",
      "          > [ 0.03332167  0.56553495 -2.65238476] Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP.\n",
      "          > [ 1.32783604  1.48643231 -2.43904543] Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned.\n",
      "        > [ 1.00031519  1.47690785 -2.70633602] In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1.\n",
      "          > [ 0.35460752  0.47515243 -2.58640337] In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ.\n",
      "          > [ 1.65539145  1.42722201 -2.28767419] Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well.\n",
      "          > [ 0.52182621  1.26947522 -3.025177  ] As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) .\n",
      "          > [ 1.13576365  0.47719458 -2.73530412] In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1.\n",
      "      > [ 1.21686935  2.65872002 -2.729568  ] Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0.\n",
      "        > [ 0.75879729  2.80988669 -3.15631056] Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof.\n",
      "          > [ 0.52708024  2.84500837 -3.56569815] Hence, the overall time complexity of Algo.\n",
      "          > [-0.03373285  1.33613396 -3.00260472] 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 .\n",
      "          > [ 2.09450674  1.58798134 -3.80266261] The theorem is proved.\n",
      "          > [ 1.14259279  0.24493892 -3.12182426] 152) Proof of Theorem IV .2: Proof.\n",
      "        > [ 1.60872233 -0.21629137 -2.57282686] Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6.\n",
      "          > [ 1.50748444 -0.12192726 -2.56594849] Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo.\n",
      "          > [ 0.18591213 -0.63814163 -3.62634277] 2. By Lines 6 and Lines 8-11, Eq.\n",
      "          > [ 1.20977783 -0.3196106  -4.22460938] (22) still holds.\n",
      "          > [ 1.34344625 -0.22638984 -2.63040209] Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6.\n",
      "        > [ 0.69157666  2.05948138 -2.87065816] As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo.\n",
      "          > [ 0.44739056  0.45227814 -2.55996704] As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq.\n",
      "          > [ 1.33164215  0.06759644 -3.09375   ] (14) follows as in the proof of Theorem IV .1.\n",
      "          > [ 0.61732948  2.8415761  -3.51119304] Next, we analyze the time complexity of Algo.\n",
      "          > [ 0.79255062  0.35190538 -3.36121368] 2. Notice that according to Line 4 in Algo.\n",
      "        > [ 1.06493866  1.76440847 -2.87859178] (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0.\n",
      "          > [ 0.83894336  1.56364131 -3.12245393] (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 .\n",
      "          > [ 0.34085816  1.41836524 -3.30760884] As for greedy operations (Lines 8-11), they will be conducted as in Algo.\n",
      "          > [ 0.86016482  0.78125989 -3.00096083] (1). Note that Algo.\n",
      "          > [ 1.41821241 -0.50606132 -2.37108445] 2 also terminates when − →γ is 0.\n",
      "  > [ 1.09270895  1.30160093 -2.66840649] Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n",
      "    > [ 0.72970587  0.71107984 -1.56995893] Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq.\n",
      "      > [ 0.99800676  1.97315073 -2.86735892] Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ.\n",
      "        > [ 1.02753007  2.29556036 -3.00187731] Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof.\n",
      "          > [ 1.16125202  2.02263403 -3.48317552] Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1).\n",
      "          > [ 0.5467847   2.83882499 -3.16068649] In sum, the overall complexity of Algo.\n",
      "          > [ 0.15271211  1.18968594 -3.0404892 ] 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 .\n",
      "          > [ 1.12632179  0.64270568 -2.97219086] 3) Proof of Lemma IV .3: Proof.\n",
      "        > [ 0.5110755   0.00906449 -2.85649157] We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, .\n",
      "          > [ 1.55226576  0.62605387 -2.97598124] We assume Algo.\n",
      "          > [-0.16586125 -0.61748087 -3.49983215] 2 conducts Lines 3-11 for L it- erations.\n",
      "          > [ 1.07049704 -0.03462601 -3.26499033] and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1.\n",
      "          > [-0.31168321  0.01381164 -3.15557575] Further, we assume that in ℓ1-th, ℓ2-th, .\n",
      "        > [ 0.5172801   0.29540607 -2.90850329] . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, .\n",
      "          > [ 0.87626362 -0.16863917 -3.2826457 ] . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11.\n",
      "          > [-0.25520518  0.36238778 -3.38165212] Based on Inequal- ity (25) and Eq.\n",
      "          > [-0.3200244   0.7030983  -3.19066978] (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ .\n",
      "          > [ 0.49092817 -0.35382888 -2.63897085] Let L = {1, 2, .\n",
      "        > [-0.07562731  0.22870998 -2.73660088] . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ.\n",
      "          > [-0.12659726 -0.13588747 -3.13111806] . . , L} and T = {ℓ1, ℓ2, .\n",
      "          > [ 0.68817049  0.42337543 -2.91242838] . . , ℓT }. As for the L \\ Titerations, Algo.\n",
      "          > [-0.3375563  -0.69242162 -4.24808979] (2) conducts Lines 5-6.\n",
      "          > [ 0.04889941  0.31701601 -2.80298114] Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ.\n",
      "      > [ 0.56912935  0.6916855  -1.84872377] According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof.\n",
      "        > [ 0.31665629  0.30356866 -2.95139265] According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof.\n",
      "          > [ 0.34142691 -0.09624711 -2.84726334] According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1.\n",
      "          > [ 0.25975275  0.64843017 -3.07108283] Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ.\n",
      "          > [ 0.51986927 -0.23509961 -3.18650532] When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set.\n",
      "          > [ 0.47734708  1.12203288 -2.56502008] The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof.\n",
      "        > [ 0.73783392  0.66337395 -3.68348169] 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X.\n",
      "          > [ 0.95587164  0.68380857 -3.32250595] 4) Proof of Lemma V .1: Proof.\n",
      "          > [ 0.65024424  1.44409621 -3.42291546] We first need the following theorem.\n",
      "          > [ 0.51696241  0.53970087 -3.88153553] Theorem A.1 (Eckart–Young Theorem [84]).\n",
      "          > [ 0.80156845  0.59375226 -3.26122618] Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X.\n",
      "        > [ 0.36861116  0.9693529  -2.34631705] Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative.\n",
      "          > [ 0.29432854  0.47730035 -2.76787066] Let bU bΛ bV ⊤ be the k-SVD of XX⊤.\n",
      "          > [ 0.1407977   1.05198109 -2.31329966] Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤.\n",
      "          > [ 0.1187899   0.30667013 -2.78979206] According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤.\n",
      "          > [ 0.90710104  0.31995255 -2.53899956] Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative.\n",
      "        > [ 0.12654503  0.63635409 -2.45401144] Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof.\n",
      "          > [-0.10047781  0.81743526 -2.38058019] Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1.\n",
      "          > [ 0.02946187  0.43079329 -2.69428325] Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤.\n",
      "          > [ 1.02690482  0.71442544 -2.66323328] Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done.\n",
      "          > [ 1.87538266  0.66936982 -3.6308198 ] 5) Proof of Theorem V .2: Proof.\n",
      "      > [ 1.21565962  0.82270169 -2.85656738] Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo.\n",
      "        > [ 0.80587971 -0.39564258 -2.64824557] Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ !\n",
      "          > [ 1.17827344 -0.25070339 -2.72414351] Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1.\n",
      "          > [-0.05632605 -0.63348514 -2.62215519] Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4].\n",
      "          > [ 0.83326948 -0.23769081 -3.32303357] On its basis, f(vi, vj) in Eq.\n",
      "          > [ 0.53721255  0.34545699 -3.45236778] (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ !\n",
      "        > [ 1.35799527  0.01917798 -2.67715859] = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq.\n",
      "          > [ 1.30190516  0.34699446 -3.09500146] = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 .\n",
      "          > [ 0.93982458 -0.01668661 -2.60886192] (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)).\n",
      "          > [ 0.90998572  0.16957954 -3.12666273] Plugging the definitions of f(vi, vj) in Eq.\n",
      "          > [ 0.13282827 -0.07224442 -3.82330012] (26) and Y in Eq.\n",
      "        > [ 1.41796076  1.49023151 -3.19844532] (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq.\n",
      "          > [ 1.61850071  1.06869042 -3.67086744] (19) into the above equation proves the theorem.\n",
      "          > [ 1.22476172  1.09563279 -2.9769094 ] 166) Proof of Lemma V .3: Proof.\n",
      "          > [ 0.69893324  0.74761832 -3.76424813] According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time.\n",
      "          > [ 1.7512908   1.39222407 -2.71938181] Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq.\n",
      "        > [ 0.86247492  0.37884119 -3.35761619] (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo.\n",
      "          > [-0.10964227 -0.1264789  -3.58256721] (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time.\n",
      "          > [ 1.02817297 -0.25867403 -3.34313416] Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time.\n",
      "          > [ 1.48976409 -0.09270306 -3.31535292] In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively.\n",
      "          > [ 0.52213997  2.70393682 -3.61336136] In turn, the overall time complexity of Algo.\n",
      "      > [ 0.78492844  1.01824975 -1.84450126] 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq.\n",
      "        > [ 1.19523048  0.77044511 -2.48142242] 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo.\n",
      "          > [ 1.02817237  0.71658456 -2.53152299] 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant.\n",
      "          > [ 1.81566501  1.0373112  -3.68985271] 7) Proof of Theorem V .4: Proof.\n",
      "          > [ 0.80987138 -0.238584   -3.01957798] Let − →ρ ◦ be the vector returned by Algo.\n",
      "          > [ 0.22676198  0.14938064 -3.43616676] 2 invoked at Line 5 in Algo.\n",
      "        > [ 0.22874178  0.29738668 -2.45403886] 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice that − →π obtained at Line 2 in Algo.\n",
      "          > [ 0.53505123  0.11516577 -2.74012852] 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ.\n",
      "          > [-0.00509164  0.20868139 -2.46440887] By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ.\n",
      "          > [-0.17097552  0.4339616  -2.62776089] Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ.\n",
      "          > [ 1.1177063   0.39913294 -2.97451091] Notice that − →π obtained at Line 2 in Algo.\n",
      "        > [ 0.56850767  0.50494486 -2.73581123] 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem.\n",
      "          > [ 0.83760977  0.15100512 -2.99487019] 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2.\n",
      "          > [ 0.1346145   0.38343358 -2.75279522] We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj).\n",
      "          > [ 0.92063344  0.66361177 -2.63755345] Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi).\n",
      "          > [ 0.02129052  0.44631279 -3.12651634] Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem.\n",
      "        > [ 0.87422603  0.54785502 -2.73845172] 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq.\n",
      "          > [ 0.97990382  0.66762376 -3.09006882] 8) Proof of Lemma V .6: Proof.\n",
      "          > [ 0.9645915   0.41222441 -3.07287431] By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦.\n",
      "          > [ 0.4508611   0.28146803 -2.65840816] (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ .\n",
      "          > [ 0.68947512  1.16319025 -3.5562458 ] Plugging it into Eq.\n",
      "    > [ 1.75447631  0.72275937 -2.6103301 ] (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88].\n",
      "      > [ 1.77390981  0.0798886  -3.04203796] (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters.\n",
      "        > [ 0.91527557  0.71616614 -3.36128569] (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs.\n",
      "          > [ 1.21348703  1.02482295 -3.74014902] (27) completes the proof.\n",
      "          > [ 0.84741569  0.29437777 -3.28766346] B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors.\n",
      "          > [ 0.6605919   0.45982721 -2.95897388] For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed.\n",
      "          > [ 0.67150879  0.82582092 -3.42102051] Varying α. Figs.\n",
      "        > [ 1.54269028  0.44956747 -3.16224599] 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8.\n",
      "          > [ 1.40468121  0.27530533 -3.33887339] 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1.\n",
      "          > [ 1.10529149 -0.34538236 -3.42677331] It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α.\n",
      "          > [ 1.60283899  0.47673827 -3.43886757] That is, the precision scores increase conspicuously with α increasing.\n",
      "          > [ 0.90536588  1.03042066 -2.97502613] The only exception is on BlogCL, where the best result is attained when α = 0.8.\n",
      "        > [ 0.99516106  1.45359004 -3.15280867] Recall that in Algo. 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse.\n",
      "          > [ 0.84606934  1.2666626  -3.85253906] Recall that in Algo.\n",
      "          > [ 0.89096516  0.68743736 -3.61303759] 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality.\n",
      "          > [ 0.88098508  0.20495838 -3.25670385] Varying σ.\n",
      "          > [ 0.8539362   1.10521829 -3.40749931] Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse.\n",
      "        > [ 1.94214737  0.78944361 -3.53105235] Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters.\n",
      "          > [ 1.3533783   1.58163357 -3.29522705] Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1.\n",
      "          > [ 1.80133748  0.23830324 -3.281106  ] Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets.\n",
      "          > [ 1.51199639 -0.16628547 -3.58913326] We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig.\n",
      "          > [ 1.31067514  0.27315095 -3.7323904 ] 9: Precision when varying parameters.\n",
      "      > [ 1.6738615   0.2491456  -2.87142086] sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes.\n",
      "        > [ 1.4364624   0.80447012 -3.47327471] sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}.\n",
      "          > [ 1.05722141  0.56022751 -3.52702332] sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward.\n",
      "          > [ 1.21637011  1.36255944 -3.45759749] LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A.\n",
      "          > [ 0.63178253  1.02435303 -3.73706055] Varying k. Figs.\n",
      "          > [ 1.49888754 -0.03576803 -3.36497617] 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}.\n",
      "        > [ 1.25224352  0.95349509 -3.52586222] On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components.\n",
      "          > [ 1.23385644  0.36397964 -3.51028466] On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8.\n",
      "          > [ 0.51302797  1.44799995 -3.42768741] In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively.\n",
      "          > [ 1.02196264  1.77535987 -3.70944595] The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information.\n",
      "          > [ 1.04631555  0.29830191 -2.7824893 ] Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components.\n",
      "        > [ 1.18293464  0.49066135 -3.03382945] The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component.\n",
      "          > [ 0.06729899 -0.02775292 -3.22023344] The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32.\n",
      "          > [ 0.9668901   0.54100859 -2.67197633] 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq.\n",
      "          > [ 1.6191808   0.40757829 -3.85065603] (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively.\n",
      "          > [ 1.35265481  0.64896554 -3.20120072] Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component.\n",
      "        > [ 1.84263325  0.21195787 -3.17415142] Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes.\n",
      "          > [ 1.16352654  0.24691914 -3.72999763] Table VI reports the best precision scores attained by each method on 8 datasets.\n",
      "          > [ 1.94527149  0.84762388 -3.03525591] We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS.\n",
      "          > [ 1.55515075  0.75256252 -3.01654267] The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon.\n",
      "          > [ 0.71236652  0.01162583 -4.06177473] This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes.\n",
      "      > [ 1.07206559  1.20125639 -3.08660126] From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS.\n",
      "        > [ 1.6296097   0.97540742 -3.09335089] From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability.\n",
      "          > [ 1.29880393  1.11589575 -3.57637286] From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C.\n",
      "          > [ 1.3623178   0.53688949 -3.99414587] Consistent with the observations from Figs.\n",
      "          > [ 0.56258863  1.23643422 -3.68917036] 9(e) and 9(f), the k-SVD in Algo.\n",
      "          > [ 1.53028047 -0.01092611 -3.30554223] 3 improves the performance of LACA due to its denoising ability.\n",
      "        > [ 1.54671705  0.15965903 -2.80848122] TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs.\n",
      "          > [ 1.93490052  0.53631097 -3.14281559] TABLE VI: Ablation study.\n",
      "          > [ 0.56588745  0.02231598 -3.29418945] Darker shades indicate better results.\n",
      "          > [ 0.59818548  0.73049873 -3.58779645] Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors.\n",
      "          > [ 0.71966136 -0.11958014 -3.86580133] Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs.\n",
      "        > [ 0.73913503  0.49373645 -4.12441206] Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets.\n",
      "          > [ 0.58239359  0.54354537 -4.46923637] Intuitively, lower conductance and WCSS indicate a higher clustering quality.\n",
      "          > [ 0.55428797  0.06142577 -4.00235939] The conductance and WCSS values of all methods on the eight datasets are reported in Table VII.\n",
      "          > [ 0.82992542  0.17969641 -3.06287813] We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality.\n",
      "          > [ 0.66912222  0.94057363 -3.34232092] From Table VII, we can see that none of the evaluated methods perform best on all datasets.\n",
      "        > [ 0.93053329  1.064026   -3.64940381] But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS.\n",
      "          > [ 0.75103533  0.72040391 -3.58689451] But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity.\n",
      "          > [ 0.74256158  0.20421572 -4.21235037] Another observation is that compared to WCSS, conductance values by different methods vary markedly.\n",
      "          > [ 1.20870662  1.06614232 -3.90873575] This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance.\n",
      "          > [ 0.56737566  1.04157162 -3.78892684] 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS.\n",
      "      > [ 1.76236701  0.72181517 -2.60439086] Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88].\n",
      "        > [ 1.68086886  1.34219289 -2.52579618] Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B.\n",
      "          > [ 1.57093859  0.51716167 -2.46325922] Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig.\n",
      "          > [ 0.21155098  1.99300528 -3.72319078] 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig.\n",
      "          > [ 0.63130546  0.18251252 -3.81777692] 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively.\n",
      "          > [ 1.69061494  1.36665368 -3.12911725] Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B.\n",
      "        > [ 1.54781353  1.88151824 -3.7006135 ] Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo.\n",
      "          > [ 1.77760208  0.95791209 -3.89745021] Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes.\n",
      "          > [ 1.248191    1.61954725 -3.56776714] Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns.\n",
      "          > [ 1.14931524  2.42532682 -3.29533744] For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4.\n",
      "          > [ 0.46333939  1.50718153 -3.29803181] The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo.\n",
      "        > [ 0.74961799  1.50073266 -3.31384897] 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes.\n",
      "          > [-0.68334085  0.89102697 -3.57197475] 2 inefficient.\n",
      "          > [-0.07265732  1.29867756 -3.01055622] As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency.\n",
      "          > [ 1.16555476  1.55691266 -3.57822967] In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large.\n",
      "          > [ 0.4814584   0.53725982 -3.27538323] TABLE VIII: Statistics of Datasets without Attributes.\n",
      "        > [ 1.33208227  0.99436796 -3.23767352] Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88].\n",
      "          > [ 0.69879055  0.99490887 -3.92046261] Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth.\n",
      "          > [ 1.11532819  0.35866499 -3.71865559] Best is bolded and best baseline underlined .\n",
      "          > [ 1.26721275  0.96570641 -3.43587184] Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B.\n",
      "          > [ 0.72716063  0.8913064  -3.15282941] Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88].\n",
      "    > [ 1.03636241  1.29825556 -2.76014853] In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n",
      "      > [-0.15700844  0.47941008 -3.03450942] In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps.\n",
      "        > [ 0.70332259  1.59768248 -3.6234324 ] In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories.\n",
      "          > [ 0.77344215  1.09356952 -3.23961115] In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors.\n",
      "          > [ 0.20577691  0.71343434 -3.86753559] The ground- truth clusters are formed based on the publication venue.\n",
      "          > [ 0.09007151  2.08723903 -3.46653986] com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items.\n",
      "          > [ 0.50159204  1.20516491 -3.80021358] 19The ground-truth clusters are determined by product cate- gories.\n",
      "        > [ 0.33866808  0.62957364 -3.77324748] com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets.\n",
      "          > [-0.34213114  0.53670055 -3.70687509] com-Orkut is a social network where users establish friendships and join groups.\n",
      "          > [-0.11968482  1.34287786 -3.57625484] The groups created by users are considered the ground-truth clusters.\n",
      "          > [ 1.23381698  0.65223497 -3.66404152] The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX.\n",
      "          > [ 1.28406405  0.5754987  -3.59975123] It can be observed that LACA consistently delivers the best performance across all three datasets.\n",
      "        > [ 1.21587622  0.22355573 -2.6210444 ] Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures.\n",
      "          > [ 1.09210885  1.27211237 -3.51652384] Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively.\n",
      "          > [ 1.40071702  1.19849932 -3.44816232] On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%.\n",
      "          > [ 0.68200415  0.62000167 -3.34345746] The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations.\n",
      "          > [ 0.92417616  0.61704397 -2.27360916] Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures.\n",
      "        > [ 0.91631061  0.78115124 -3.36760688] Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps.\n",
      "          > [ 0.97379082  0.8445974  -3.49289751] Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested.\n",
      "          > [ 0.13032192  0.49862936 -3.34112525] Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps.\n",
      "          > [ 0.23216926  0.36029059 -3.28788376] For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj).\n",
      "          > [-0.17608783  0.54697031 -3.93199778] 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps.\n",
      "      > [ 1.1246742   1.1698451  -3.00808144] For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors.\n",
      "        > [ 0.12014078  0.572155   -3.4822135 ] For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD.\n",
      "          > [ 0.25977093  0.36296093 -3.28371119] For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj).\n",
      "          > [-0.03851635  0.50807989 -4.00798321] 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps.\n",
      "          > [ 0.29279923  0.36267018 -3.25446367] For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj).\n",
      "          > [ 0.58236206  0.12607548 -3.70994997] and have compared them against our original BDD.\n",
      "        > [ 1.13069797  0.94752228 -3.6797328 ] Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks.\n",
      "          > [ 0.99888897  0.54058057 -3.71176529] Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions.\n",
      "          > [ 0.64293468  0.42978689 -3.39394045] It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets.\n",
      "          > [ 1.40090287  0.92980456 -3.67650151] For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively.\n",
      "          > [ 0.4902513   1.43125927 -3.5596118 ] The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks.\n",
      "        > [ 1.34279692  0.21672994 -3.10973215] In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%.\n",
      "          > [ 1.52219427  0.21905389 -3.0295527 ] In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets.\n",
      "          > [ 0.8627184   0.5002538  -3.05335712] Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs.\n",
      "          > [ 0.67080963  0.68855023 -3.69090176] As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains.\n",
      "          > [ 1.90863252  0.20923078 -3.16227293] For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%.\n",
      "        > [ 0.71371126  1.40267205 -3.40364528] D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors.\n",
      "          > [ 0.88157707  1.41582882 -3.40385199] D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information.\n",
      "          > [ 0.21845046  1.94828379 -3.15094399] The obtained embeddings can be used in many downstream tasks, including graph clustering.\n",
      "          > [ 0.36799133  0.82562572 -3.65832114] ANE methods can be categorized into two types: factorization- based and learning-based.\n",
      "          > [ 0.80396235  0.30382177 -3.13118649] Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors.\n",
      "      > [ 0.19851846  0.78798032 -2.75470901] TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n",
      "        > [-0.08323112  0.75795418 -2.73817635] TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss.\n",
      "          > [-0.31157374  0.789361   -2.96389771] TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique.\n",
      "          > [ 0.69174004  1.53045034 -2.80860567] However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix.\n",
      "          > [ 0.36781853 -0.17182258 -3.36832809] Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories.\n",
      "          > [ 0.35397649  0.72722316 -3.13569045] Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss.\n",
      "        > [ 0.66051447  0.65522194 -2.91625929] Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n",
      "          > [ 0.32838699  1.05331981 -2.98008251] Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94].\n",
      "          > [ 0.58858091  0.20116812 -3.1958046 ] CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model.\n",
      "          > [ 0.78130078  0.79800248 -3.3121388 ] In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph.\n"
     ]
    }
   ],
   "source": [
    "print_paper(papers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_color():\n",
    "    return (random.random(), random.random(), random.random())  # RGB tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64706"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_embeddings = np.stack([emb for paper in papers for emb in paper.embeddings])\n",
    "len(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Collect all embeddings and corresponding PDF labels\n",
    "all_embeddings = np.stack([emb for paper in papers for emb in paper.embeddings])\n",
    "labels = [paper.title for paper in papers for _ in paper.embeddings]\n",
    "\n",
    "# Reduce to 3D using PCA\n",
    "reducer = PCA(n_components=3)\n",
    "low_dim_embeddings = reducer.fit_transform(all_embeddings)\n",
    "\n",
    "color_map = {title: random_color() for title in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\n",
    "import mplcursors\n",
    "\n",
    "# Your 3D plotting code\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "start = 0\n",
    "scatter_plots = []\n",
    "for paper in papers:\n",
    "    num_points = len(paper.embeddings)\n",
    "    subset = low_dim_embeddings[start:start+num_points]\n",
    "    \n",
    "    # Scatter plot for each PDF in 3D\n",
    "    scatter = ax.scatter(subset[:, 0], subset[:, 1], subset[:, 2], \n",
    "                         color=color_map[paper.title], label=paper.title, alpha=0.6)\n",
    "    \n",
    "    # Add the scatter plot and corresponding text parts for hover\n",
    "    scatter_plots.append((scatter, paper.texts))  # Store the scatter and the corresponding text parts\n",
    "    \n",
    "    start += num_points\n",
    "\n",
    "# Add the legend and labels\n",
    "ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.1))\n",
    "ax.set_title(\"3D Embeddings Visualization\")\n",
    "ax.set_xlabel(\"UMAP Dim 1\")\n",
    "ax.set_ylabel(\"UMAP Dim 2\")\n",
    "ax.set_zlabel(\"UMAP Dim 3\")\n",
    "\n",
    "# Use mplcursors to display text on hover\n",
    "cursor = mplcursors.cursor(hover=True)\n",
    "\n",
    "# Define the callback to show and hide annotations\n",
    "@cursor.connect(\"add\")\n",
    "def on_add(sel):\n",
    "    # Get the index of the hovered point\n",
    "    index = sel.index\n",
    "    # Find the corresponding part of the text\n",
    "    part_text = paper.texts[index]\n",
    "    # Set the annotation text to the text part\n",
    "    sel.annotation.set_text(part_text)\n",
    "\n",
    "@cursor.connect(\"remove\")\n",
    "def on_remove(sel):\n",
    "    # Hide the annotation when the mouse moves off\n",
    "    sel.annotation.set_text(\"\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Prepare flat lists\n",
    "all_embeddings = []\n",
    "all_texts = []\n",
    "all_titles = []\n",
    "paper_indices = []  # map each embedding to its corpus index\n",
    "\n",
    "for paper_idx, paper in enumerate(papers):\n",
    "    all_embeddings.extend(paper.embeddings)\n",
    "    all_texts.extend(paper.texts)\n",
    "    all_titles.extend([paper.title] * len(paper.embeddings))\n",
    "    paper_indices.extend([paper_idx] * len(paper.embeddings))\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "# Fit NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=10, algorithm='auto').fit(all_embeddings)\n",
    "distances, indices = nbrs.kneighbors(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match (Distance: 10.00047):\n",
      "Paper 1:     2503.20698v1.MMMORRF__Multimodal_Multilingual_Modularized_Reciprocal_Rank_Fusion.pdf\n",
      "Paper 2:     2503.20630v1._β__GNN__A_Robust_Ensemble_Approach_Against_Graph_Structure_Perturbation.pdf\n",
      "Embedding 1: [-0.31322026  0.32337621 -2.7252872  -0.08644752  0.83922893]...\n",
      "Embedding 2: [ 0.25969315  0.25360304 -2.9244976  -0.30857641  1.14607215]...\n",
      "Text 1:      arXiv:1611.09268 [cs.CL] https://arxiv.org/abs/1611.09268 [4] Meng Cao, Haoran Tang, Jinfa Huang, Peng Jin, Can Zhang, Ruyang Liu, Long Chen, Xiaodan Liang, Li Yuan, and Ge Li.\n",
      "Text 2:      arXiv:1609.02907 http://arxiv.org/abs/1609.02907 [13] Y. Li, W. Jin, H. Xu, and J. Tang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00090):\n",
      "Paper 1:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Paper 2:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Embedding 1: [-0.76816636 -0.40030932 -3.75645161  0.00444262 -1.09072435]...\n",
      "Embedding 2: [-0.85856473 -0.05290981 -3.44286585  0.33531082 -0.46727812]...\n",
      "Text 1:      3 [40] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, and Zhaoyang Lv.\n",
      "Text 2:      3 [45] Jiawei Ren, Cheng Xie, Ashkan Mirzaei, Karsten Kreis, Zi- wei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, Huan Ling, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00105):\n",
      "Paper 1:     2503.20672v1.BizGen__Advancing_Article_level_Visual_Text_Rendering_for_Infographics_Generation.pdf\n",
      "Paper 2:     2503.20701v1.UniEDU__A_Unified_Language_and_Vision_Assistant_for_Education_Applications.pdf\n",
      "Embedding 1: [-0.84272152  0.41310707 -3.43686366 -0.77808887  0.68887198]...\n",
      "Embedding 2: [-0.26615274  0.27263296 -3.9682312  -0.59843922  0.52997398]...\n",
      "Text 1:      arXiv preprint arXiv:2310.15144, 2023. 1 [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 4 [20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\n",
      "Text 2:      Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruc- tion tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00105):\n",
      "Paper 1:     2503.20701v1.UniEDU__A_Unified_Language_and_Vision_Assistant_for_Education_Applications.pdf\n",
      "Paper 2:     2503.20672v1.BizGen__Advancing_Article_level_Visual_Text_Rendering_for_Infographics_Generation.pdf\n",
      "Embedding 1: [-0.26615274  0.27263296 -3.9682312  -0.59843922  0.52997398]...\n",
      "Embedding 2: [-0.84272152  0.41310707 -3.43686366 -0.77808887  0.68887198]...\n",
      "Text 1:      Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruc- tion tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning.\n",
      "Text 2:      arXiv preprint arXiv:2310.15144, 2023. 1 [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 4 [20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00124):\n",
      "Paper 1:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Paper 2:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Embedding 1: [-0.76385856  0.46243095 -3.22211933 -0.39503106  1.02693009]...\n",
      "Embedding 2: [-0.79716486 -0.06419816 -3.16164184  0.7342869   0.28415883]...\n",
      "Text 1:      1, 2, 3, 10 [66] Ludan Ruan, Y Ma, Huan Yang, Huiguo He, Bei Liu, Jian- long Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo.\n",
      "Text 2:      2, 3, 5, 8 [50] Xinyu Liu, Jinlong Li, Jin Ma, Huiming Sun, Zhigang Xu, Tianyun Zhang, and Hongkai Yu.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00238):\n",
      "Paper 1:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Paper 2:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Embedding 1: [-0.90331805 -0.12819886 -3.36623383 -0.2121639  -0.34727886]...\n",
      "Embedding 2: [-0.88630038 -0.00944226 -3.424438   -0.28795075  0.22402428]...\n",
      "Text 1:      5 [31] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.\n",
      "Text 2:      5 [39] Levon Khachatryan, Andranik Movsisyan, Vahram Tade- vosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00238):\n",
      "Paper 1:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Paper 2:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Embedding 1: [-0.88630038 -0.00944226 -3.424438   -0.28795075  0.22402428]...\n",
      "Embedding 2: [-0.90331805 -0.12819886 -3.36623383 -0.2121639  -0.34727886]...\n",
      "Text 1:      5 [39] Levon Khachatryan, Andranik Movsisyan, Vahram Tade- vosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi.\n",
      "Text 2:      5 [31] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00256):\n",
      "Paper 1:     2503.20630v1._β__GNN__A_Robust_Ensemble_Approach_Against_Graph_Structure_Perturbation.pdf\n",
      "Paper 2:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Embedding 1: [-0.17985758  0.99286252 -2.64047956 -1.22088754  1.07165313]...\n",
      "Embedding 2: [-1.03473425  1.17336202 -3.00379848 -0.63641232  1.29367614]...\n",
      "Text 1:      In Proceedings of the Twenty- Eighth International Joint Conference on Artificial Intelligence, IJCAI-19 .\n",
      "Text 2:      In Proceed- ings of the Thirtieth International Joint Conference on Ar- tificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021 , pages 4839–4843.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00256):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20630v1._β__GNN__A_Robust_Ensemble_Approach_Against_Graph_Structure_Perturbation.pdf\n",
      "Embedding 1: [-1.03473425  1.17336202 -3.00379848 -0.63641232  1.29367614]...\n",
      "Embedding 2: [-0.17985758  0.99286252 -2.64047956 -1.22088754  1.07165313]...\n",
      "Text 1:      In Proceed- ings of the Thirtieth International Joint Conference on Ar- tificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021 , pages 4839–4843.\n",
      "Text 2:      In Proceedings of the Twenty- Eighth International Joint Conference on Artificial Intelligence, IJCAI-19 .\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00273):\n",
      "Paper 1:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Paper 2:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Embedding 1: [-0.55702281  0.28228849 -3.400774    0.63421404  0.89778799]...\n",
      "Embedding 2: [-0.97764313  0.01223509 -3.63167191  0.31481618  0.31851038]...\n",
      "Text 1:      2, 3, 4, 5, 6, 15, 18 [103] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, and Achuta Kadambi.\n",
      "Text 2:      3 [14] Chaoya Jiang, Jia Hongrui, Haiyang Xu, Wei Ye, Mengfan Dong, Ming Yan, Ji Zhang, Fei Huang, and Shikun Zhang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00287):\n",
      "Paper 1:     2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "Paper 2:     2503.20676v1.Inductive_Link_Prediction_on_N_ary_Relational_Facts_via_Semantic_Hypergraph_Reasoning.pdf\n",
      "Embedding 1: [-0.26407242  0.12544024 -3.36957955  0.41642034 -0.26200402]...\n",
      "Embedding 2: [-0.6453259   0.02283143 -3.30595708 -0.03973304  0.19991767]...\n",
      "Text 1:      [42] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.\n",
      "Text 2:      [44] Gongzhu Yin, Xing Wang, Hongli Zhang, Chao Meng, Yuchen Yang, Kun Lu, and Yi Luo.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00302):\n",
      "Paper 1:     2503.20676v1.Inductive_Link_Prediction_on_N_ary_Relational_Facts_via_Semantic_Hypergraph_Reasoning.pdf\n",
      "Paper 2:     2503.20697v1.Semi_supervised_Node_Importance_Estimation_with_Informative_Distribution_Modeling_for_Uncertainty_Regularization.pdf\n",
      "Embedding 1: [-1.16733134 -0.08720005 -3.36913157 -0.94676387  0.04051014]...\n",
      "Embedding 2: [-1.24897766  0.33469677 -3.51879883 -0.2708025  -0.04416466]...\n",
      "Text 1:      [25] Haoran Luo, Haihong E, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, and Wei Lin.\n",
      "Text 2:      [25] Dongyuan Li, Zhen Wang, Yankai Chen, Renhe Jiang, Weiping Ding, and Manabu Okumura.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00357):\n",
      "Paper 1:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Paper 2:     2503.20719v1.Learning_Straight_Flows_by_Learning_Curved_Interpolants.pdf\n",
      "Embedding 1: [ 0.4210313   0.39043295 -3.63690758 -1.07537138  1.1915946 ]...\n",
      "Embedding 2: [ 0.5242964   0.15371916 -2.84144139  0.00913331  0.86660677]...\n",
      "Text 1:      B. Denoising Diffusion Probabilistic Models Diffusion models are a generative deep learning technique that leverage an approach for data synthesis.\n",
      "Text 2:      Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00360):\n",
      "Paper 1:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Paper 2:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Embedding 1: [-1.03622735  0.3329587  -3.50044727 -0.3811869  -0.32566828]...\n",
      "Embedding 2: [-0.9097625   0.36923003 -3.25577497  0.09444313  0.13176672]...\n",
      "Text 1:      5 [12] Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang.\n",
      "Text 2:      2 [12] Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wen- zheng Chen, and Baoquan Chen.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00402):\n",
      "Paper 1:     2503.20576v1.Optimizing_Case_Based_Reasoning_System_for_Functional_Test_Script_Generation_with_Large_Language_Models.pdf\n",
      "Paper 2:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Embedding 1: [-0.92541611  0.24384409 -3.78731155  0.06930272 -0.10919204]...\n",
      "Embedding 2: [-0.68471253  0.18979128 -3.49268532  0.28081715 -0.13896376]...\n",
      "Text 1:      [32] Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Haotian Hui, Weichuan Liu, Zhiyuan Liu, et al.\n",
      "Text 2:      6 [32] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00402):\n",
      "Paper 1:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Paper 2:     2503.20576v1.Optimizing_Case_Based_Reasoning_System_for_Functional_Test_Script_Generation_with_Large_Language_Models.pdf\n",
      "Embedding 1: [-0.68471253  0.18979128 -3.49268532  0.28081715 -0.13896376]...\n",
      "Embedding 2: [-0.92541611  0.24384409 -3.78731155  0.06930272 -0.10919204]...\n",
      "Text 1:      6 [32] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al.\n",
      "Text 2:      [32] Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Haotian Hui, Weichuan Liu, Zhiyuan Liu, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00417):\n",
      "Paper 1:     2503.20749v1.Beyond_Believability__Accurate_Human_Behavior_Simulation_with_Fine_Tuned_LLMs.pdf\n",
      "Paper 2:     2503.20527v1.StableToolBench_MirrorAPI__Modeling_Tool_Environments_as_Mirrors_of_7_000__Real_World_APIs.pdf\n",
      "Embedding 1: [-0.11837462  0.46750614 -2.93045712  0.49106118  0.63530815]...\n",
      "Embedding 2: [-1.14279819  0.75479728 -2.79952335  0.48071304  0.4732708 ]...\n",
      "Text 1:      K. Li, Y . Q. Wang, Y . X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yu- jia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y . X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.\n",
      "Text 2:      Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00423):\n",
      "Paper 1:     2503.20744v1.High_Quality_Diffusion_Distillation_on_a_Single_GPU_with_Relative_and_Absolute_Position_Matching.pdf\n",
      "Paper 2:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Embedding 1: [ 0.40733823  0.42384604 -3.30691218 -0.49437711  0.41100261]...\n",
      "Embedding 2: [ 0.74098778 -0.22127867 -3.01928711 -0.19178009  1.03062487]...\n",
      "Text 1:      Improved denoising diffusion probabilistic models. arXiv preprint arXiv:2102.09672, 2021. [25] A. Nichol, P. Dharwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. GLIDE: Towards Photorealistic image generation and editing with text-guided diffusion models.\n",
      "Text 2:      [21] A. Nichol and P. Dhariwal, “Improved Denoising Diffusion Probabilistic Models,” 2 2021.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00441):\n",
      "Paper 1:     2503.20588v1.Synthetic_Data_Augmentation_for_Cross_domain_Implicit_Discourse_Relation_Recognition.pdf\n",
      "Paper 2:     2503.20715v1.From_Annotation_to_Adaptation__Metrics__Synthetic_Data__and_Aspect_Extraction_for_Aspect_Based_Sentiment_Analysis_with_Large_Language_Models.pdf\n",
      "Embedding 1: [ 0.23947439  0.51162755 -2.86092997 -1.52896059  1.20393717]...\n",
      "Embedding 2: [-0.1626699   0.75690097 -2.78527975 -1.29855883  0.33066133]...\n",
      "Text 1:      In Proceedings of the 18th Confer- ence of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 179–192, St. Julian’s, Malta.\n",
      "Text 2:      In Proceedings of the 52nd annual meeting of the association for computational linguistics (vol- ume 2: Short papers), pages 49–54.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00495):\n",
      "Paper 1:     2503.20682v1.GLRD__Global_Local_Collaborative_Reason_and_Debate_with_PSL_for_3D_Open_Vocabulary_Detection.pdf\n",
      "Paper 2:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Embedding 1: [ 0.02204378  1.44320953 -3.71918035 -1.39806962  1.42111814]...\n",
      "Embedding 2: [ 0.30231211  1.152089   -3.97101116 -0.2117447   0.98284841]...\n",
      "Text 1:      : Grounding dino: Marrying dino with grounded pre- training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023) [36] Liu, X., Ji, K., Fu, Y ., Tam, W., Du, Z., Yang, Z., Tang, J.: P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (V olume 2: Short Papers) (2022) [37] Liu, X., Ji, K., Fu, Y ., Tam, W.L., Du, Z., Yang, Z., Tang, J.: P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602 (2021) [38] Liu, Z., Zhang, Z., Cao, Y ., Hu, H., Tong, X.: Group-free 3d object de- tection via transformers.\n",
      "Text 2:      Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. 5 [50] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00495):\n",
      "Paper 1:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Paper 2:     2503.20682v1.GLRD__Global_Local_Collaborative_Reason_and_Debate_with_PSL_for_3D_Open_Vocabulary_Detection.pdf\n",
      "Embedding 1: [ 0.30231211  1.152089   -3.97101116 -0.2117447   0.98284841]...\n",
      "Embedding 2: [ 0.02204378  1.44320953 -3.71918035 -1.39806962  1.42111814]...\n",
      "Text 1:      Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. 5 [50] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control.\n",
      "Text 2:      : Grounding dino: Marrying dino with grounded pre- training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023) [36] Liu, X., Ji, K., Fu, Y ., Tam, W., Du, Z., Yang, Z., Tang, J.: P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (V olume 2: Short Papers) (2022) [37] Liu, X., Ji, K., Fu, Y ., Tam, W.L., Du, Z., Yang, Z., Tang, J.: P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602 (2021) [38] Liu, Z., Zhang, Z., Cao, Y ., Hu, H., Tong, X.: Group-free 3d object de- tection via transformers.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00535):\n",
      "Paper 1:     2503.20719v1.Learning_Straight_Flows_by_Learning_Curved_Interpolants.pdf\n",
      "Paper 2:     2503.20725v1.Continual_learning_via_probabilistic_exchangeable_sequence_modelling.pdf\n",
      "Embedding 1: [ 1.0509783   1.23758459 -3.17813993 -0.65102887  1.27719021]...\n",
      "Embedding 2: [ 1.51086283  1.10165417 -2.72437143 -1.02369642  0.82346833]...\n",
      "Text 1:      Chen, R. T., Rubanova, Y ., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations.\n",
      "Text 2:      Neural ordinary differential equations.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00542):\n",
      "Paper 1:     2503.20519v1.MAR_3D__Progressive_Masked_Auto_regressor_for_High_Resolution_3D_Generation.pdf\n",
      "Paper 2:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Embedding 1: [ 0.50999892  0.84907144 -3.55169654 -1.22992289 -0.13008453]...\n",
      "Embedding 2: [ 1.06979918  0.93050182 -3.43796563 -0.80914533 -0.00811747]...\n",
      "Text 1:      3d gaussian splatting for real-time radiance field rendering. ACM TOG, 2023. 2 [18] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model.\n",
      "Text 2:      3d gaussian splatting for real-time radiance field rendering.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00542):\n",
      "Paper 1:     2503.20519v1.MAR_3D__Progressive_Masked_Auto_regressor_for_High_Resolution_3D_Generation.pdf\n",
      "Paper 2:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Embedding 1: [ 0.50999892  0.84907144 -3.55169654 -1.22992289 -0.13008453]...\n",
      "Embedding 2: [ 1.06979918  0.93050182 -3.43796563 -0.80914533 -0.00811747]...\n",
      "Text 1:      3d gaussian splatting for real-time radiance field rendering. ACM TOG, 2023. 2 [18] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model.\n",
      "Text 2:      3d gaussian splatting for real-time radiance field rendering.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00542):\n",
      "Paper 1:     2503.20519v1.MAR_3D__Progressive_Masked_Auto_regressor_for_High_Resolution_3D_Generation.pdf\n",
      "Paper 2:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Embedding 1: [ 0.50999892  0.84907144 -3.55169654 -1.22992289 -0.13008453]...\n",
      "Embedding 2: [ 1.06979918  0.93050182 -3.43796563 -0.80914533 -0.00811747]...\n",
      "Text 1:      3d gaussian splatting for real-time radiance field rendering. ACM TOG, 2023. 2 [18] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model.\n",
      "Text 2:      3d gaussian splatting for real-time radiance field rendering.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00596):\n",
      "Paper 1:     2503.20496v1.Enhancing_Depression_Detection_via_Question_wise_Modality_Fusion.pdf\n",
      "Paper 2:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Embedding 1: [-1.11847425  0.79319972 -3.17771268 -1.32906795  0.96865976]...\n",
      "Embedding 2: [-1.26320982  0.73467922 -3.15986633 -0.63470989  0.91755342]...\n",
      "Text 1:      Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n",
      "Text 2:      Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00596):\n",
      "Paper 1:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Paper 2:     2503.20496v1.Enhancing_Depression_Detection_via_Question_wise_Modality_Fusion.pdf\n",
      "Embedding 1: [-1.26320982  0.73467922 -3.15986633 -0.63470989  0.91755342]...\n",
      "Embedding 2: [-1.11847425  0.79319972 -3.17771268 -1.32906795  0.96865976]...\n",
      "Text 1:      Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018. John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization.\n",
      "Text 2:      Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00684):\n",
      "Paper 1:     2503.20698v1.MMMORRF__Multimodal_Multilingual_Modularized_Reciprocal_Rank_Fusion.pdf\n",
      "Paper 2:     2503.20781v1.BASKET__A_Large_Scale_Video_Dataset_for_Fine_Grained_Skill_Estimation.pdf\n",
      "Embedding 1: [ 0.62616569  0.676925   -3.02507925 -0.61383307  0.08364763]...\n",
      "Embedding 2: [ 0.28230226  1.18540442 -3.01204681 -0.97885954  0.176009  ]...\n",
      "Text 1:      To bridge the modality gap between textual queries and video frames, we use SigLIP [40], a variant of CLIP that replaces the standard contrastive learning softmax normalization with a sigmoid loss function.\n",
      "Text 2:      X-CLIP [24] extends CLIP to video-text retrieval with multi-grained contrastive learning, aligning coarse- grained and fine-grained visual features. SigLIP [45] uses a pairwise sigmoid loss to learn visual representations from large-scale image-language data. To extend SigLIP to video, we apply temporal pooling on individually extracted frame-level features. LLaVA-OneVision [16] introduces a large vision-language model that achieves impressive cross- scenario generalization transfer across many image and video tasks.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00684):\n",
      "Paper 1:     2503.20781v1.BASKET__A_Large_Scale_Video_Dataset_for_Fine_Grained_Skill_Estimation.pdf\n",
      "Paper 2:     2503.20698v1.MMMORRF__Multimodal_Multilingual_Modularized_Reciprocal_Rank_Fusion.pdf\n",
      "Embedding 1: [ 0.28230226  1.18540442 -3.01204681 -0.97885954  0.176009  ]...\n",
      "Embedding 2: [ 0.62616569  0.676925   -3.02507925 -0.61383307  0.08364763]...\n",
      "Text 1:      X-CLIP [24] extends CLIP to video-text retrieval with multi-grained contrastive learning, aligning coarse- grained and fine-grained visual features. SigLIP [45] uses a pairwise sigmoid loss to learn visual representations from large-scale image-language data. To extend SigLIP to video, we apply temporal pooling on individually extracted frame-level features. LLaVA-OneVision [16] introduces a large vision-language model that achieves impressive cross- scenario generalization transfer across many image and video tasks.\n",
      "Text 2:      To bridge the modality gap between textual queries and video frames, we use SigLIP [40], a variant of CLIP that replaces the standard contrastive learning softmax normalization with a sigmoid loss function.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00733):\n",
      "Paper 1:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Paper 2:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Embedding 1: [-0.38535309 -0.65451211 -3.06295228 -0.0250945   0.65944332]...\n",
      "Embedding 2: [-0.3077507  -0.44322371 -3.12826538 -0.25081432  0.5201478 ]...\n",
      "Text 1:      2, 3 [8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\n",
      "Text 2:      3 [7] Timoth ´ee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00735):\n",
      "Paper 1:     2503.20756v1.ADS_Edit__A_Multimodal_Knowledge_Editing_Dataset_for_Autonomous_Driving_Systems.pdf\n",
      "Paper 2:     2503.20641v1.Unlocking_Efficient_Long_to_Short_LLM_Reasoning_with_Model_Merging.pdf\n",
      "Embedding 1: [-0.49965549  0.9159717  -3.16695118  0.55632675  0.49091563]...\n",
      "Embedding 2: [-0.68888688  0.64382964 -2.86730289  0.19127829  0.54357785]...\n",
      "Text 1:      Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang.\n",
      "Text 2:      Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, and Cheng-Lin Liu.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00737):\n",
      "Paper 1:     2503.20508v1.Explainable_ICD_Coding_via_Entity_Linking.pdf\n",
      "Paper 2:     2503.20757v1.MCTS_RAG__Enhancing_Retrieval_Augmented_Generation_with_Monte_Carlo_Tree_Search.pdf\n",
      "Embedding 1: [-0.8406927   1.27333045 -2.8696456   0.27122545  0.09989339]...\n",
      "Embedding 2: [-0.86551964  0.62670678 -2.76883602  0.17671356  0.34486702]...\n",
      "Text 1:      Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhi- fang Sui.\n",
      "Text 2:      Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00737):\n",
      "Paper 1:     2503.20757v1.MCTS_RAG__Enhancing_Retrieval_Augmented_Generation_with_Monte_Carlo_Tree_Search.pdf\n",
      "Paper 2:     2503.20508v1.Explainable_ICD_Coding_via_Entity_Linking.pdf\n",
      "Embedding 1: [-0.86551964  0.62670678 -2.76883602  0.17671356  0.34486702]...\n",
      "Embedding 2: [-0.8406927   1.27333045 -2.8696456   0.27122545  0.09989339]...\n",
      "Text 1:      Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou.\n",
      "Text 2:      Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhi- fang Sui.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00855):\n",
      "Paper 1:     2503.20744v1.High_Quality_Diffusion_Distillation_on_a_Single_GPU_with_Relative_and_Absolute_Position_Matching.pdf\n",
      "Paper 2:     2503.20502v1.MLLM_Selector__Necessity_and_Diversity_driven_High_Value_Data_Selection_for_Enhanced_Visual_Instruction_Tuning.pdf\n",
      "Embedding 1: [-0.08327165  0.89244163 -2.3162775  -1.16949737  0.6078527 ]...\n",
      "Embedding 2: [-0.68145365  0.52476317 -2.64683604 -1.45847499  0.85154068]...\n",
      "Text 1:      arXiv preprint arXiv:2211.17091 [cs.CV], 2022. [16] D. Kim, C.-H. Lai, W.-H. Liao, N. Murata, Y . Takida, T. Uesaka, Y . He, Y . Mitsufuji, and S. Ermon.\n",
      "Text 2:      arXiv preprint arXiv:2308.10792 (2023) [45] Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.W., Tay, Y., Zhou, D., Le, Q.V., Zoph, B., Wei, J., et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00925):\n",
      "Paper 1:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Paper 2:     2503.20673v1.Mitigating_Low_Level_Visual_Hallucinations_Requires_Self_Awareness__Database__Model_and_Training_Strategy.pdf\n",
      "Embedding 1: [ 0.79204375  0.29869705 -2.94697809 -0.64255208  1.23725033]...\n",
      "Embedding 2: [ 0.54960817  0.43789077 -2.76246023 -0.66140711  0.78718615]...\n",
      "Text 1:      [35] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: From error visibility to structural similarity,” IEEE Transactions on Image Processing , vol.\n",
      "Text 2:      [6] Y . Gao, X. Min, Y . Cao, X. Liu, and G. Zhai, “No-reference image quality assessment: Obtain mos from image quality score distribution,” IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2024. [7] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli, “Image quality assessment: Unifying structure and texture similarity,”IEEE transactions on pattern analysis and machine intelligence (TPAMI) , vol. 44, no. 5, pp. 2567–2581, 2020.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00925):\n",
      "Paper 1:     2503.20673v1.Mitigating_Low_Level_Visual_Hallucinations_Requires_Self_Awareness__Database__Model_and_Training_Strategy.pdf\n",
      "Paper 2:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Embedding 1: [ 0.54960817  0.43789077 -2.76246023 -0.66140711  0.78718615]...\n",
      "Embedding 2: [ 0.79204375  0.29869705 -2.94697809 -0.64255208  1.23725033]...\n",
      "Text 1:      [6] Y . Gao, X. Min, Y . Cao, X. Liu, and G. Zhai, “No-reference image quality assessment: Obtain mos from image quality score distribution,” IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2024. [7] K. Ding, K. Ma, S. Wang, and E. P. Simoncelli, “Image quality assessment: Unifying structure and texture similarity,”IEEE transactions on pattern analysis and machine intelligence (TPAMI) , vol. 44, no. 5, pp. 2567–2581, 2020.\n",
      "Text 2:      [35] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: From error visibility to structural similarity,” IEEE Transactions on Image Processing , vol.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.00926):\n",
      "Paper 1:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Paper 2:     2503.20783v1.Understanding_R1_Zero_Like_Training__A_Critical_Perspective.pdf\n",
      "Embedding 1: [-0.79976487  0.42520118 -3.18665361  0.84199464  0.4681257 ]...\n",
      "Embedding 2: [-0.75691414  0.63283849 -3.01701355  1.21545601  1.26791859]...\n",
      "Text 1:      Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Diederik P Kingma, Yinyu Ye, Zhi-Quan Luo, and Ruoyu Sun.\n",
      "Text 2:      Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01017):\n",
      "Paper 1:     2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "Paper 2:     2503.20505v1.Riemannian_Optimization_on_Relaxed_Indicator_Matrix_Manifold.pdf\n",
      "Embedding 1: [-1.15853477  0.20527029 -3.65785313 -0.16562131 -0.54943335]...\n",
      "Embedding 2: [-0.53020424 -0.2337538  -3.67873907  0.61905956 -0.07440683]...\n",
      "Text 1:      [32] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang.\n",
      "Text 2:      [69] Shuai Li, Yingjie Zhang, Hongtu Zhu, Christina Wang, Hai Shu, Ziqi Chen, Zhuoran Sun, and Yanfeng Yang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01017):\n",
      "Paper 1:     2503.20505v1.Riemannian_Optimization_on_Relaxed_Indicator_Matrix_Manifold.pdf\n",
      "Paper 2:     2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "Embedding 1: [-0.53020424 -0.2337538  -3.67873907  0.61905956 -0.07440683]...\n",
      "Embedding 2: [-1.15853477  0.20527029 -3.65785313 -0.16562131 -0.54943335]...\n",
      "Text 1:      [69] Shuai Li, Yingjie Zhang, Hongtu Zhu, Christina Wang, Hai Shu, Ziqi Chen, Zhuoran Sun, and Yanfeng Yang.\n",
      "Text 2:      [32] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01057):\n",
      "Paper 1:     2503.20641v1.Unlocking_Efficient_Long_to_Short_LLM_Reasoning_with_Model_Merging.pdf\n",
      "Paper 2:     2503.20527v1.StableToolBench_MirrorAPI__Modeling_Tool_Environments_as_Mirrors_of_7_000__Real_World_APIs.pdf\n",
      "Embedding 1: [ 0.15030776  1.09910607 -3.10773706 -1.54364491  1.35493636]...\n",
      "Embedding 2: [ 0.50989199  1.10934448 -3.01998901 -1.57685089  1.24127197]...\n",
      "Text 1:      17167– 17186. Association for Computational Linguistics, 2024. URL https://aclanthology. org/2024.findings-emnlp.1000.\n",
      "Text 2:      In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 7346–7356, Mi- ami, Florida, USA.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01070):\n",
      "Paper 1:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Paper 2:     2503.20748v1.UniSTD__Towards_Unified_Spatio_Temporal_Learning_across_Diverse_Disciplines.pdf\n",
      "Embedding 1: [-0.67765909 -0.21583897 -3.59526134 -0.63158739 -0.29429278]...\n",
      "Embedding 2: [-0.89311856  0.13556072 -3.85847807 -0.20742305 -0.27961078]...\n",
      "Text 1:      2, 6, 7 [42] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon- tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al.\n",
      "Text 2:      3 [24] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,Vaishaal Shankar, Hongseok Namkoong, John Miller, Han- naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01070):\n",
      "Paper 1:     2503.20748v1.UniSTD__Towards_Unified_Spatio_Temporal_Learning_across_Diverse_Disciplines.pdf\n",
      "Paper 2:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Embedding 1: [-0.89311856  0.13556072 -3.85847807 -0.20742305 -0.27961078]...\n",
      "Embedding 2: [-0.67765909 -0.21583897 -3.59526134 -0.63158739 -0.29429278]...\n",
      "Text 1:      3 [24] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,Vaishaal Shankar, Hongseok Namkoong, John Miller, Han- naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.\n",
      "Text 2:      2, 6, 7 [42] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gon- tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01125):\n",
      "Paper 1:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Paper 2:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Embedding 1: [-5.78380585e-01 -6.04595989e-02 -3.78781724e+00 -8.91968608e-04\n",
      "  3.27735364e-01]...\n",
      "Embedding 2: [-0.70480245 -0.04608679 -3.89190197  0.23959728 -0.40968567]...\n",
      "Text 1:      2 [18] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.\n",
      "Text 2:      2 [24] Wenyi Hong, Ming Ding, Wendi Chen, Woonhyuk Baek Zhang, Zhuoyi Yang, Xiaojie Xu, Junyuan Wang, Chang Zhou, Hongxia Yang, and Jie Tang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01235):\n",
      "Paper 1:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Paper 2:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Embedding 1: [-1.08269179  0.23464607 -3.52810025  0.70787549  0.07131348]...\n",
      "Embedding 2: [-1.28039467 -0.26093975 -3.35694408  0.38049185  0.52905715]...\n",
      "Text 1:      3 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siaro- hin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al.\n",
      "Text 2:      2, 3 [41] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01245):\n",
      "Paper 1:     2503.20641v1.Unlocking_Efficient_Long_to_Short_LLM_Reasoning_with_Model_Merging.pdf\n",
      "Paper 2:     2503.20724v1.Dynamic_Motion_Blending_for_Versatile_Motion_Editing.pdf\n",
      "Embedding 1: [ 0.09441435  1.04933691 -3.75085449 -1.06126404  1.25140381]...\n",
      "Embedding 2: [-0.50995237  0.5631178  -3.22738552 -1.25661457  1.46463251]...\n",
      "Text 1:      In The Twelfth International Conference on Learning Rep- resentations, ICLR 2024, Vienna, Austria, May 7-11, 2024 .\n",
      "Text 2:      In International Conference on Learning Repre- sentations (ICLR), 2022.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01314):\n",
      "Paper 1:     2503.20502v1.MLLM_Selector__Necessity_and_Diversity_driven_High_Value_Data_Selection_for_Enhanced_Visual_Instruction_Tuning.pdf\n",
      "Paper 2:     2503.20662v1.AutoRad_Lung__A_Radiomic_Guided_Prompting_Autoregressive_Vision_Language_Model_for_Lung_Nodule_Malignancy_Prediction.pdf\n",
      "Embedding 1: [-0.41678432  1.06291139 -3.10052824 -1.25204873  0.49744952]...\n",
      "Embedding 2: [-0.17182703  1.18555188 -3.40343547 -0.90960801  0.91050529]...\n",
      "Text 1:      To further extend these capa- bilities into the realm of visual understanding, a series of pioneering studies [14–17] have inte- grated visual encoders into LLMs, leading to the development of multimodal large language models (MLLMs).\n",
      "Text 2:      A further breakthrough in the field has been driven by the emergence of Vision- Language Models (VLMs) [20], inspired by advancements in Large Language Models (LLMs) [7].\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01314):\n",
      "Paper 1:     2503.20662v1.AutoRad_Lung__A_Radiomic_Guided_Prompting_Autoregressive_Vision_Language_Model_for_Lung_Nodule_Malignancy_Prediction.pdf\n",
      "Paper 2:     2503.20502v1.MLLM_Selector__Necessity_and_Diversity_driven_High_Value_Data_Selection_for_Enhanced_Visual_Instruction_Tuning.pdf\n",
      "Embedding 1: [-0.17182703  1.18555188 -3.40343547 -0.90960801  0.91050529]...\n",
      "Embedding 2: [-0.41678432  1.06291139 -3.10052824 -1.25204873  0.49744952]...\n",
      "Text 1:      A further breakthrough in the field has been driven by the emergence of Vision- Language Models (VLMs) [20], inspired by advancements in Large Language Models (LLMs) [7].\n",
      "Text 2:      To further extend these capa- bilities into the realm of visual understanding, a series of pioneering studies [14–17] have inte- grated visual encoders into LLMs, leading to the development of multimodal large language models (MLLMs).\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01361):\n",
      "Paper 1:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Paper 2:     2503.20527v1.StableToolBench_MirrorAPI__Modeling_Tool_Environments_as_Mirrors_of_7_000__Real_World_APIs.pdf\n",
      "Embedding 1: [-0.71550363  0.54225439 -2.87493801  0.4975338   0.48939863]...\n",
      "Embedding 2: [-0.90097702  0.61948252 -3.12543511  0.47973961  0.75292367]...\n",
      "Text 1:      Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al.\n",
      "Text 2:      Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, and Feng Zhao.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01395):\n",
      "Paper 1:     2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf\n",
      "Paper 2:     2503.20768v1.An_Empirical_Study_of_the_Impact_of_Federated_Learning_on_Machine_Learning_Model_Accuracy.pdf\n",
      "Embedding 1: [ 0.89737856  0.70978546 -3.56553483 -0.84487885 -0.30352181]...\n",
      "Embedding 2: [ 0.43660757  0.8859306  -3.54330182 -0.51569206  0.2755861 ]...\n",
      "Text 1:      Hierarchical text-conditional image gener- ation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 3 [21] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ¨orn Ommer. High-resolution image synthesis with latent diffusion models.\n",
      "Text 2:      CoRR, abs/2003.00295, 2020. [56] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. InIEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 10674–10685.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01399):\n",
      "Paper 1:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Paper 2:     2503.20776v1.Feature4X__Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_Versatile_Gaussian_Feature_Fields.pdf\n",
      "Embedding 1: [ 0.42793173  0.74717212 -3.25557232 -0.15300289 -0.06007813]...\n",
      "Embedding 2: [-0.12319146  0.7632575  -2.75350285 -0.71378863  0.1317367 ]...\n",
      "Text 1:      7 [79] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Mena- pace, Aliaksandr Siarohin, Junli Cao, L ´aszl´o Jeni, Sergey Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealis- tic 4d scene generation via video diffusion models.Advances in Neural Information Processing Systems, 37:45256–45280, 2024. 1 [80] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splat- ting.\n",
      "Text 2:      2 [65] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splat- ting. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 20051–20060, 2024. 2, 3 [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In International conference on machine learning , pages 8748–8763. PMLR, 2021. 2 [67] Varun Ramakrishna, Takeo Kanade, and Yaser Sheikh. Re- constructing 3d human pose from 2d image landmarks. In ECCV, 2012. 3 [68] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Ro- man R ¨adle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 2, 3, 6, 15 [69] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao, Gang Zeng, and Ziwei Liu. Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142 , 2023.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01528):\n",
      "Paper 1:     2503.20731v1.RecTable__Fast_Modeling_Tabular_Data_with_Rectified_Flow.pdf\n",
      "Paper 2:     2503.20756v1.ADS_Edit__A_Multimodal_Knowledge_Editing_Dataset_for_Autonomous_Driving_Systems.pdf\n",
      "Embedding 1: [-0.87691307  0.82541573 -3.02300024  0.44100133  0.6095866 ]...\n",
      "Embedding 2: [-0.872105    0.93293494 -2.94638729  0.91827071  0.52684981]...\n",
      "Text 1:      Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao.\n",
      "Text 2:      Yanze Li, Wenhua Zhang, Kai Chen, Yanxin Liu, Pengx- iang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, Dit-Yan Yeung, Huchuan Lu, and Xu Jia.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01639):\n",
      "Paper 1:     2503.20561v1.A_Theoretical_Framework_for_Prompt_Engineering__Approximating_Smooth_Functions_with_Transformer_Prompts.pdf\n",
      "Paper 2:     2503.20725v1.Continual_learning_via_probabilistic_exchangeable_sequence_modelling.pdf\n",
      "Embedding 1: [-0.24792844  0.32877767 -3.21716166  0.69676179  0.45494679]...\n",
      "Embedding 2: [-0.16741818 -0.23342744 -3.18590808  0.32262772  0.82616132]...\n",
      "Text 1:      Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., Jiang, L., Zhang, X., Zhang, S., Liu, J., et al.\n",
      "Text 2:      Wang, L., Zhang, M., Jia, Z., Li, Q., Bao, C., Ma, K., Zhu, J., and Zhong, Y .\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01649):\n",
      "Paper 1:     2503.20502v1.MLLM_Selector__Necessity_and_Diversity_driven_High_Value_Data_Selection_for_Enhanced_Visual_Instruction_Tuning.pdf\n",
      "Paper 2:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Embedding 1: [ 0.0754448   0.97465003 -2.55316663 -1.36470389  1.93024611]...\n",
      "Embedding 2: [-0.11574981  0.39994994 -2.49489856 -1.01540279  1.67212021]...\n",
      "Text 1:      IEEE Transactions on Image Processing 31, 4321–4335 (2022) [55] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa matter: Elevating the role of image understanding in visual question answering.\n",
      "Text 2:      1 [37] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In International Conference on Document Analysis and Recognition, pages 947–952. IEEE, 2019.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01649):\n",
      "Paper 1:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Paper 2:     2503.20502v1.MLLM_Selector__Necessity_and_Diversity_driven_High_Value_Data_Selection_for_Enhanced_Visual_Instruction_Tuning.pdf\n",
      "Embedding 1: [-0.11574981  0.39994994 -2.49489856 -1.01540279  1.67212021]...\n",
      "Embedding 2: [ 0.0754448   0.97465003 -2.55316663 -1.36470389  1.93024611]...\n",
      "Text 1:      1 [37] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In International Conference on Document Analysis and Recognition, pages 947–952. IEEE, 2019.\n",
      "Text 2:      IEEE Transactions on Image Processing 31, 4321–4335 (2022) [55] Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the v in vqa matter: Elevating the role of image understanding in visual question answering.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01651):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf\n",
      "Embedding 1: [ 0.45935434  1.21808982 -3.78729844 -0.49215904  0.34075722]...\n",
      "Embedding 2: [ 0.38228539  1.61753058 -4.37459612 -0.05794864  0.57724339]...\n",
      "Text 1:      Latent video diffusion models for high- fidelity video generation with arbitrary lengths. CoRR, abs/2211.13221, 2022. 2 [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance.\n",
      "Text 2:      Stable video diffusion: Scaling latent video diffusion models to large datasets.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01673):\n",
      "Paper 1:     2503.20719v1.Learning_Straight_Flows_by_Learning_Curved_Interpolants.pdf\n",
      "Paper 2:     2503.20731v1.RecTable__Fast_Modeling_Tabular_Data_with_Rectified_Flow.pdf\n",
      "Embedding 1: [ 0.07459769  0.86461723 -3.69143558 -0.49649954  1.53992522]...\n",
      "Embedding 2: [ 0.02693853  1.1655072  -3.93572497 -0.46123052  0.83004582]...\n",
      "Text 1:      Liu, X., Gong, C., et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022. 9Liu, X., Zhang, X., Ma, J., Peng, J., and Liu, Q. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation.\n",
      "Text 2:      Instaflow: One step is enough for high-quality diffusion-based text-to-image generation.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01673):\n",
      "Paper 1:     2503.20719v1.Learning_Straight_Flows_by_Learning_Curved_Interpolants.pdf\n",
      "Paper 2:     2503.20744v1.High_Quality_Diffusion_Distillation_on_a_Single_GPU_with_Relative_and_Absolute_Position_Matching.pdf\n",
      "Embedding 1: [ 0.07459769  0.86461723 -3.69143558 -0.49649954  1.53992522]...\n",
      "Embedding 2: [ 0.02693853  1.1655072  -3.93572497 -0.46123052  0.83004582]...\n",
      "Text 1:      Liu, X., Gong, C., et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022. 9Liu, X., Zhang, X., Ma, J., Peng, J., and Liu, Q. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation.\n",
      "Text 2:      Instaflow: One step is enough for high-quality diffusion-based text-to-image generation.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01692):\n",
      "Paper 1:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Paper 2:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Embedding 1: [-1.03622735  0.3329587  -3.50044727 -0.3811869  -0.32566828]...\n",
      "Embedding 2: [-0.84091824  0.00872125 -3.74858403  0.13222414  0.01993512]...\n",
      "Text 1:      5 [12] Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang.\n",
      "Text 2:      6 [3] Hanting Chen, Tianyu Guo, Chang Xu, Wenshuo Li, Chun- jing Xu, Chao Xu, and Yunhe Wang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01721):\n",
      "Paper 1:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Paper 2:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Embedding 1: [-0.13868922  1.02672982 -3.20760202 -0.62714058  0.52503669]...\n",
      "Embedding 2: [-0.40728459  1.40387821 -3.28111172 -0.61902153  0.00990929]...\n",
      "Text 1:      Our research adheres to this paradigm, but extends it by generating consistent multi-view videos (instead of images) and subsequently re- constructing the 4D object.\n",
      "Text 2:      To over- come this second challenge, we introduce an effective op- timization strategy designed to seamlessly integrate multi- view videos into a unified 4D representation.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01721):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Embedding 1: [-0.40728459  1.40387821 -3.28111172 -0.61902153  0.00990929]...\n",
      "Embedding 2: [-0.13868922  1.02672982 -3.20760202 -0.62714058  0.52503669]...\n",
      "Text 1:      To over- come this second challenge, we introduce an effective op- timization strategy designed to seamlessly integrate multi- view videos into a unified 4D representation.\n",
      "Text 2:      Our research adheres to this paradigm, but extends it by generating consistent multi-view videos (instead of images) and subsequently re- constructing the 4D object.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01761):\n",
      "Paper 1:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Paper 2:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Embedding 1: [-0.4671891   0.57906324 -3.61789083 -0.25250199  1.2139225 ]...\n",
      "Embedding 2: [-0.81383812  0.10616253 -3.81506896 -0.09195267  0.74755561]...\n",
      "Text 1:      In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296–26306, 2024. 1, 2, 5, 7, 8 [30] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216–233.\n",
      "Text 2:      Mmbench: Is your multi-modal model an all-around player?\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01761):\n",
      "Paper 1:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Paper 2:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Embedding 1: [-0.4671891   0.57906324 -3.61789083 -0.25250199  1.2139225 ]...\n",
      "Embedding 2: [-0.81383812  0.10616253 -3.81506896 -0.09195267  0.74755561]...\n",
      "Text 1:      In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26296–26306, 2024. 1, 2, 5, 7, 8 [30] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216–233.\n",
      "Text 2:      Mmbench: Is your multi-modal model an all-around player?\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01826):\n",
      "Paper 1:     2503.20767v1.Reliable_algorithm_selection_for_machine_learning_guided_design.pdf\n",
      "Paper 2:     2503.20731v1.RecTable__Fast_Modeling_Tabular_Data_with_Rectified_Flow.pdf\n",
      "Embedding 1: [ 0.12719671  0.23233263 -2.55927014 -0.46543062  1.79091597]...\n",
      "Embedding 2: [ 0.21607521  0.43166924 -3.52156734 -0.97860932  1.22464716]...\n",
      "Text 1:      In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5897–5906.\n",
      "Text 2:      ), Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learn- ing Research, pp.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01828):\n",
      "Paper 1:     2503.20752v1.Reason_RFT__Reinforcement_Fine_Tuning_for_Visual_Reasoning.pdf\n",
      "Paper 2:     2503.20672v1.BizGen__Advancing_Article_level_Visual_Text_Rendering_for_Infographics_Generation.pdf\n",
      "Embedding 1: [-0.91925114 -0.48638731 -3.52071762  0.29216754  0.48963797]...\n",
      "Embedding 2: [-0.79493439  0.0376522  -3.29397726 -0.1538682   0.21832682]...\n",
      "Text 1:      2, 3 [8] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine, and Yi Ma.\n",
      "Text 2:      2, 3, 5, 6, 8 [23] Zeyu Liu, Weicong Liang, Yiming Zhao, Bohan Chen, Ji Li, and Yuhui Yuan.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01895):\n",
      "Paper 1:     2503.20744v1.High_Quality_Diffusion_Distillation_on_a_Single_GPU_with_Relative_and_Absolute_Position_Matching.pdf\n",
      "Paper 2:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Embedding 1: [ 0.10669913  0.71017408 -2.98035359 -0.87925655  1.44198322]...\n",
      "Embedding 2: [-0.19135045  0.66732043 -3.35470915 -0.94319832  1.51609731]...\n",
      "Text 1:      Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in neural information processing systems (NeurIPS), page 11895–11907, 2019. [36] Y .\n",
      "Text 2:      Generative modeling by estimating gradients of the data distribution.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01902):\n",
      "Paper 1:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Paper 2:     2503.20519v1.MAR_3D__Progressive_Masked_Auto_regressor_for_High_Resolution_3D_Generation.pdf\n",
      "Embedding 1: [-1.04672909 -0.1070838  -4.12132406  0.06564521  0.33723745]...\n",
      "Embedding 2: [-0.75777632 -0.32923463 -3.74743652 -0.38684285 -0.47049236]...\n",
      "Text 1:      6 [17] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n",
      "Text 2:      6 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01931):\n",
      "Paper 1:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Paper 2:     2503.20771v1.Disentangled_Source_Free_Personalization_for_Facial_Expression_Recognition_with_Neutral_Target_Data.pdf\n",
      "Embedding 1: [ 0.4210313   0.39043295 -3.63690758 -1.07537138  1.1915946 ]...\n",
      "Embedding 2: [ 0.69910091  0.27379218 -2.75452137 -0.56295127  1.15276301]...\n",
      "Text 1:      B. Denoising Diffusion Probabilistic Models Diffusion models are a generative deep learning technique that leverage an approach for data synthesis.\n",
      "Text 2:      Neural Computing and Applications, 33(15):9125–9136. [34] Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162–8171.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01931):\n",
      "Paper 1:     2503.20771v1.Disentangled_Source_Free_Personalization_for_Facial_Expression_Recognition_with_Neutral_Target_Data.pdf\n",
      "Paper 2:     2503.20571v1.Exploring_Robustness_of_Cortical_Morphometry_in_the_presence_of_white_matter_lesions__using_Diffusion_Models_for_Lesion_Filling.pdf\n",
      "Embedding 1: [ 0.69910091  0.27379218 -2.75452137 -0.56295127  1.15276301]...\n",
      "Embedding 2: [ 0.4210313   0.39043295 -3.63690758 -1.07537138  1.1915946 ]...\n",
      "Text 1:      Neural Computing and Applications, 33(15):9125–9136. [34] Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162–8171.\n",
      "Text 2:      B. Denoising Diffusion Probabilistic Models Diffusion models are a generative deep learning technique that leverage an approach for data synthesis.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01937):\n",
      "Paper 1:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Paper 2:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Embedding 1: [-1.08269179  0.23464607 -3.52810025  0.70787549  0.07131348]...\n",
      "Embedding 2: [-1.0922581   0.20596603 -3.42527032  0.25752017 -0.12405041]...\n",
      "Text 1:      3 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siaro- hin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al.\n",
      "Text 2:      3 [75] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Mena- pace, Aliaksandr Siarohin, Junli Cao, L´aszl´o A. Jeni, Sergey Tulyakov, and Hsin-Ying Lee.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01937):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Embedding 1: [-1.0922581   0.20596603 -3.42527032  0.25752017 -0.12405041]...\n",
      "Embedding 2: [-1.08269179  0.23464607 -3.52810025  0.70787549  0.07131348]...\n",
      "Text 1:      3 [75] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Mena- pace, Aliaksandr Siarohin, Junli Cao, L´aszl´o A. Jeni, Sergey Tulyakov, and Hsin-Ying Lee.\n",
      "Text 2:      3 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siaro- hin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01955):\n",
      "Paper 1:     2503.20757v1.MCTS_RAG__Enhancing_Retrieval_Augmented_Generation_with_Monte_Carlo_Tree_Search.pdf\n",
      "Paper 2:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Embedding 1: [-0.85240799  0.8207137  -2.94417214  0.61742401  0.24487422]...\n",
      "Embedding 2: [-0.78751808  0.61681396 -3.1917634   0.3644824   0.21174851]...\n",
      "Text 1:      Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.\n",
      "Text 2:      Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.01955):\n",
      "Paper 1:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Paper 2:     2503.20757v1.MCTS_RAG__Enhancing_Retrieval_Augmented_Generation_with_Monte_Carlo_Tree_Search.pdf\n",
      "Embedding 1: [-0.78751808  0.61681396 -3.1917634   0.3644824   0.21174851]...\n",
      "Embedding 2: [-0.85240799  0.8207137  -2.94417214  0.61742401  0.24487422]...\n",
      "Text 1:      Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.\n",
      "Text 2:      Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02016):\n",
      "Paper 1:     2503.20783v1.Understanding_R1_Zero_Like_Training__A_Critical_Perspective.pdf\n",
      "Paper 2:     2503.20762v1.ASGO__Adaptive_Structured_Gradient_Optimization.pdf\n",
      "Embedding 1: [-0.92313284  0.23256113 -3.22854686  0.72958112  0.82365274]...\n",
      "Embedding 2: [-0.71550363  0.54225439 -2.87493801  0.4975338   0.48939863]...\n",
      "Text 1:      Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al.\n",
      "Text 2:      Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02023):\n",
      "Paper 1:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Paper 2:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Embedding 1: [-0.06129128  0.61484218 -3.0063715  -0.92595994  1.46256828]...\n",
      "Embedding 2: [-0.26617178  0.34801811 -3.75007343 -0.88167095  1.28414178]...\n",
      "Text 1:      arXiv preprint arXiv:2410.07073, 2024. 2 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men- sch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems, 35:23716–23736, 2022.\n",
      "Text 2:      Flamingo: a visual language model for few-shot learning.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02062):\n",
      "Paper 1:     2503.20576v1.Optimizing_Case_Based_Reasoning_System_for_Functional_Test_Script_Generation_with_Large_Language_Models.pdf\n",
      "Paper 2:     2503.20654v1.AccidentSim__Generating_Physically_Realistic_Vehicle_Collision_Videos_from_Real_World_Accident_Reports.pdf\n",
      "Embedding 1: [-0.63436979  0.48627776 -3.58833504  0.52618611  0.25054598]...\n",
      "Embedding 2: [-0.79905319  0.29141098 -3.23141956  0.32684678  0.69229925]...\n",
      "Text 1:      [7] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang.\n",
      "Text 2:      1, 3 [34] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02073):\n",
      "Paper 1:     2503.20537v1.TD_BFR__Truncated_Diffusion_Model_for_Efficient_Blind_Face_Restoration.pdf\n",
      "Paper 2:     2503.20771v1.Disentangled_Source_Free_Personalization_for_Facial_Expression_Recognition_with_Neutral_Target_Data.pdf\n",
      "Embedding 1: [ 0.94510585  0.02659261 -2.97154236 -0.16870785  1.09315658]...\n",
      "Embedding 2: [ 0.69910091  0.27379218 -2.75452137 -0.56295127  1.15276301]...\n",
      "Text 1:      [3] Jonathan Ho, Ajay Jain, and Pieter Abbeel, “Denoising diffusion probabilistic models,” Advances in Neural Information Processing Systems, vol. 33, pp. 6840–6851, 2020. [4] Zhixin Wang, Ziying Zhang, Xiaoyun Zhang, Huangjie Zheng, Mingyuan Zhou, Ya Zhang, and Yanfeng Wang, “Dr2: Diffusion-based robust degradation remover for blind face restoration,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 1704–1713.\n",
      "Text 2:      Neural Computing and Applications, 33(15):9125–9136. [34] Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162–8171.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02073):\n",
      "Paper 1:     2503.20771v1.Disentangled_Source_Free_Personalization_for_Facial_Expression_Recognition_with_Neutral_Target_Data.pdf\n",
      "Paper 2:     2503.20537v1.TD_BFR__Truncated_Diffusion_Model_for_Efficient_Blind_Face_Restoration.pdf\n",
      "Embedding 1: [ 0.69910091  0.27379218 -2.75452137 -0.56295127  1.15276301]...\n",
      "Embedding 2: [ 0.94510585  0.02659261 -2.97154236 -0.16870785  1.09315658]...\n",
      "Text 1:      Neural Computing and Applications, 33(15):9125–9136. [34] Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 8162–8171.\n",
      "Text 2:      [3] Jonathan Ho, Ajay Jain, and Pieter Abbeel, “Denoising diffusion probabilistic models,” Advances in Neural Information Processing Systems, vol. 33, pp. 6840–6851, 2020. [4] Zhixin Wang, Ziying Zhang, Xiaoyun Zhang, Huangjie Zheng, Mingyuan Zhou, Ya Zhang, and Yanfeng Wang, “Dr2: Diffusion-based robust degradation remover for blind face restoration,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 1704–1713.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02073):\n",
      "Paper 1:     2503.20756v1.ADS_Edit__A_Multimodal_Knowledge_Editing_Dataset_for_Autonomous_Driving_Systems.pdf\n",
      "Paper 2:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Embedding 1: [-0.46456611  0.99340516 -3.10964036 -1.17084336  0.95314306]...\n",
      "Embedding 2: [-0.87687224  0.80689895 -3.13455033 -1.24877059  0.87135404]...\n",
      "Text 1:      Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi- hao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language model’s per- ception of the world at any resolution. CoRR, abs/2409.12191. Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Hua- jun Chen.\n",
      "Text 2:      2 [46] P Wang, S Bai, S Tan, S Wang, Z Fan, J Bai, K Chen, X Liu, J Wang, W Ge, et al. Qwen2-vl: Enhancing vision- language model’s perception of the world at any resolution, 2024. URL https://arxiv. org/abs/2409.12191. 2, 4 [47] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained lan- guage models. Advances in Neural Information Processing Systems, 37:121475–121499, 2024. 2 [48] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition, pages 2575–2584, 2020. 4 [49] Ge Zhang Yao Fu Wenhao Huang Huan Sun Yu Su Wenhu Chen Xiang Yue, Xingwei Qu. Mammoth: Build- ing math generalist models through hybrid instruction tun- ing. arXiv preprint arXiv:2309.05653, 2023. 4 [50] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02126):\n",
      "Paper 1:     2503.20496v1.Enhancing_Depression_Detection_via_Question_wise_Modality_Fusion.pdf\n",
      "Paper 2:     2503.20701v1.UniEDU__A_Unified_Language_and_Vision_Assistant_for_Education_Applications.pdf\n",
      "Embedding 1: [-0.42096663  1.2046597  -3.21952677 -0.39780548  1.13039589]...\n",
      "Embedding 2: [-0.89637011  1.88280618 -3.63882923 -0.18470052  1.30690789]...\n",
      "Text 1:      IEEE. Stefan Scherer, Giota Stratou, Gale M. Lucas, Marwa Mahmoud, Jill Boberg, Jonathan Gratch, Albert A. Rizzo, and Louis-Philippe Morency.\n",
      "Text 2:      IEEE. Longwei Zheng, Fei Jiang, Xiaoqing Gu, Yuanyuan Li, Gong Wang, and Haomin Zhang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02166):\n",
      "Paper 1:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Paper 2:     2503.20784v1.FB_4D__Spatial_Temporal_Coherent_Dynamic_3D_Content_Generation_with_Feature_Banks.pdf\n",
      "Embedding 1: [-0.3047114  -0.85612321 -3.36086106  0.83282393 -0.33435637]...\n",
      "Embedding 2: [-0.49814945  0.29662806 -3.24633217  0.43851608 -0.99079072]...\n",
      "Text 1:      2 [10] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock- horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\n",
      "Text 2:      2 [34] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fi- dler, and Karsten Kreis.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02185):\n",
      "Paper 1:     2503.20633v1.Enhancing_Multi_modal_Models_with_Heterogeneous_MoE_Adapters_for_Fine_tuning.pdf\n",
      "Paper 2:     2503.20540v1.Beyond_Intermediate_States__Explaining_Visual_Redundancy_through_Language.pdf\n",
      "Embedding 1: [-0.82281154  0.82012892 -2.63438725 -0.83916718  0.90070647]...\n",
      "Embedding 2: [-0.89277411  1.50889516 -2.27555084 -0.905433    0.29677722]...\n",
      "Text 1:      38, pp. 1110–1119. [25] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan, “Moe-llava: Mixture of experts for large vision-language models,” arXiv preprint arXiv:2401.15947, 2024. [26] Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨ uckl´e, Kyunghyun Cho, and Iryna Gurevych, “Adapterfusion: Non-destructive task composition for transfer learning,” arXiv preprint arXiv:2005.00247, 2020. [27] Brian Lester, Rami Al-Rfou, and Noah Constant, “The power of scale for parameter-efficient prompt tuning,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7- 11 November, 2021 , Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, Eds.\n",
      "Text 2:      Beyond Intermediate States: Explaining Visual Redundancy through Language Dingchen Yang* Tongji University dingchen yang@tongji.edu.cn Bowen Cao CUHK Anran Zhang Tencent Hunyuan Team Weibo Gu Tencent Hunyuan Team Winston Hu Tencent Hunyuan Team Guang Chen† Tongji University guangchen@tongji.edu.cn Abstract Multi-modal Large Langue Models (MLLMs) often process thousands of visual tokens, which consume a significant portion of the context window and impose a substantial computational burden. Prior work has empirically explored visual token pruning methods based on MLLMs’ interme- diate states ( e.g., attention scores). However, they have limitations in precisely defining visual redundancy due to their inability to capture the influence of visual tokens on MLLMs’ visual understanding (i.e., the predicted probabil- ities for textual token candidates). To address this issue, we manipulate the visual input and investigate variations in the textual output from both token-centric and context- centric perspectives, achieving intuitive and comprehen- sive analysis. Experimental results reveal that visual to- kens with low ViT−[cls] association and low text-to-image attention scores can contain recognizable visual cues and significantly contribute to images’ overall information. To develop a more reliable method for identifying and prun- ing redundant visual tokens, we integrate these two per- spectives and introduce a context-independent condition to identify redundant prototypes from training images, which probes the redundancy of each visual token during infer- ence. Extensive experiments on single-image, multi-image and video comprehension tasks demonstrate the effective- ness of our method, notably achieving 90% to 110% of the performance while pruning 80% to 90% of visual tokens. Code will be available at https://github.com/ DingchenYang99/RedundancyCodebook.git. 1. Introduction Multi-modal Large Language Models (MLLMs) [19, 20, 26] have demonstrated remarkable performance across a *Work done during an internship at tencent hunyuan team †Corresponding author range of vision-language tasks, including high-resolution image and video comprehension, by integrating thousands of visual tokens. However, This approach introduces sev- eral challenges. First, visual tokens encroach upon the con- text window required for textual tokens, and may interfere with MLLMs’ text processing capabilities [45]. Second, the quadratic complexity of the self-attention mechanism [37] significantly increases the computational burden. Conse- quently, reducing redundant visual tokens is crucial for en- hancing the overall performance and efficiency of MLLMs. To reduce the number of visual tokens while mitigating performance degradation, recent research has empirically explored leveraging MLLMs’ intermediate states to guide inference-time visual token pruning. The two primary ap- proaches are: (1) utilizing the ViT −[cls] token [31], which encodes global image information, and (2) leveraging the scalar attention scores of textual tokens to visual tokens in the LLM [4], which capture cross-modal information flow. However, these intermediate-state-based methods struggle to explicitly characterize the influence of each visual token on MLLMs’ visual understanding outcome, i.e., the final probability prediction, as attention value vectors also play a crucial role in the attention mechanism, and the represen- tation of one token progressively transforms into that of its next token in auto-regressive LLMs. This limitation hin- ders the interpretable definition of visual redundancy and risks pruning informative visual tokens in MLLMs. In this study, we aim to provide a more precise expla- nation of visual redundancy in MLLMs, which first re- quires identifying the direct impact of each visual token on MLLMs’ visual understanding. Since humans understand images by attending to individual visual cues and assessing their contributions to the overall image representation, we analyze the influence of visual tokens from two perspec- tives: (1) Token-Centric perspective, which examines the inherent visual information encoded in each visual token, and (2) Context-Centric perspective, which evaluates how arXiv:2503.20540v1 [cs.CV] 26 Mar 2025Attention Scores of the First Decoded Token to Image Patches @ LLM layer # [2, 16, 30] top-1 candidate’s probability Semantic interpretation of individual visual tokensfrom the MLLM’s own language decoder ViT-[cls] sim @ ViTlayer #22 Patch # Greedy Decoded Token Top-1 Prob. Top-1 Logits ViT-[cls] Similariy LLM Layer #2 Attn Score LLM Layer #16 Attn Score LLM Layer #30 Attn Score 22 Food 0.3539 17.281 0.0117 7.27E-06 2.26E-04 6.46E-04 89 Carrot 0.7897 21.172 0.0275 1.33E-05 2.15E-04 4.30E-04 114 Carrot 0.8167 21.172 0.0366 5.29E-05 2.64E-04 2.19E-04 115 Pepper 0.4341 18.891 0.0005 4.31E-05 3.78E-04 2.75E-04 118 Bowl 0.6089 19.781 0.0074 9.78E-06 1.70E-04 4.62E-05 141 Spices 0.4127 17.625 0.0027 1.06E-05 1.65E-04 1.32E-04 160 Potato 0.4094 18.109 0.0510 1.20E-05 2.31E-04 1.20E-05 425 Spoon 0.8455 20.797 0.1009 9.42E-06 2.73E-05 3.14E-05 431 Cat 0.0644 14.758 0.2793 1.27E-03 1.49E-03 4.37E-03 501 Nothing 0.0945 14.914 0.4121 2.05E-03 1.69E-03 4.68E-04 523 Tree 0.0701 14.086 0.2439 7.14E-03 5.81E-03 5.11E-03 524 Picture 0.0771 14.789 0.3865 1.84E-02 1.33E-03 2.24E-04 548 Picture 0.0929 14.742 0.2749 4.61E-03 3.20E-03 7.23E-03 571 Picture 0.0907 14.297 0.3972 2.01E-03 4.60E-03 6.73E-04 lower valuehigher value Figure 1. We investigate the inherent information encoded in individual visual tokens by instructing LLaV A-Next to describe them and analyzing the corresponding decoding results, predicted probabilities, and confidence scores (logits). “Patch #” indicates the index in the flattened patch sequence. Some visual tokens with low ViT −[cls] similarity and low attention scores ( e.g., Patch #114, #160, and #425) contain valid visual information (e.g., Carrot, Potato, and Spoon) that the model recognizes with high confidence (40% to 80% probability). Conversely, despite having high ViT−[cls] similarity and high attention scores (highlighted in the red box), certain visual tokens yield text descriptions unrelated to the image patches (e.g., Cat and Tree), with model confidence lower than 10%. each visual token affects the broader visual context (i.e., im- ages or image regions). For the token-centric analysis, we devise a single-token-input experiment (Section 3.1.1), iso- lating each visual token and instructing the MLLM to inter- pret the information it contains. This experiment reveals that MLLM can recognize valid visual information from visual tokens with low ViT −[cls] similarity and low text- to-image attention scores. As shown in Figure 1, LLaV A- Next [19] predicts Carrot and Spoon with over 80% con- fidence from patches #114 and #425, which depict carrot and spoon, respectively. For the context-centric analysis, we design a leave-one-token-out experiment (Section 3.1.2) to examine how removing individual visual tokens affects the predicted probability distribution. Experimental results indicate that certain visual tokens with low ViT−[cls] simi- larity and low text-to-image attention scores can still signifi- cantly influence MLLMs’ understanding of their associated image (Section 3.2). These findings warrant a reconsidera- tion of the definition of visual redundancy in MLLMs. Based on our token-centric and context-centric analyses, we propose that redundant visual tokens should be identi- fied according to two fundamental criteria: the visual to- ken (1) lacks recognizable visual information and (2) does not significantly impact the overall information of its as- sociated image. Building on the feature analysis of visual tokens that satisfy these criteria, we introduce a context- independent condition to identify redundant prototypes that are unlikely to influence visual information across differ- ent images, thus demonstrating potential for generaliza- tion. Leveraging these criteria, we propose an identify- then-probe strategy for inference-time visual token pruning. First, We use training images to identify redundant proto- types and store them in an extensibleredundancy codebook. During inference, visual tokens that exhibit higher similar- ity to these prototypes are deemed more likely to be redun- dant and are removed before sending to the LLM. We evaluate the effectiveness of our approach on five single-image Visual Question Answering (VQA) bench- marks [10, 18, 23, 28, 40] and two image captioning bench- marks [1, 49]. On average, our method preserves 90% of the performance of LLaV A-Next [19, 26] and LLaV A-1.5 [24, 25] while pruning 90% of visual tokens, outperforming rep- resentative methods [4, 31] that rely on MLLMs’intermedi- ate states. Furthermore, our approach is adaptable to multi- image and video comprehension tasks [15, 21, 39], achiev- ing up to a 10% performance improvement for LLaV A- OneVision [20] while pruning 80% of visual tokens. These results validate the effectiveness of our approach2. Related Work Leading MLLMs [19, 20, 25, 26] process high-resolution images and multiple video frames by incorporating numer- ous visual tokens. For instance, LLaV A-Next and LLaV A- OneVision represent an image using a maximum of 2,880 and 7,290 visual tokens, respectively. These visual tokens occupy a large portion of the context window of their LLM1, leading to increased computational overhead and poten- tially impairing MLLMs’ text processing capabilities [45]. 2.1. Identifying Redundant Visual Tokens To alleviate the computational burden associated with vi- sual tokens, pioneering studies explore MLLMs’intermedi- ate states to estimate the redundancy of visual tokens. The methodologies can be broadly categorized into two types: 2.1.1. Vision-Centric Visual Redundancy Estimation This line of work presumes that visual tokens that do not align with the overall information of the image or exhibit duplicated features are redundant. The alignment between image patches and the image’s overall information is eval- uated by their association with the [cls] token in the Vision Transformer (ViT [8]) model [12, 27, 31, 45, 51], or by the attention scores between one image patch and all other patches [38, 42, 44, 50]. To identify duplicate visual to- kens, the feature similarity of patches within a local spa- tial region [27, 51] or a spatio-temporal region [32, 34] is assessed. These strategies typically distinguish foreground objects from background patches. However, given that vi- sual tokens are further processed by the LLM during the prefill stage2 for cross-modal feature interaction and text decoding, we advocate for explaining the information en- coded in visual tokens from the viewpoint of the LLM. 2.1.2. Instruction-Based Visual Redundancy Estimation This line of work focuses on the cross-modal information flow within LLMs, identifying visual tokens that are irrel- evant to the input question as redundant. This relevancy is typically estimated using the attention scores of textual to- kens to visual tokens (referred to as text-to-image attention scores) [4, 12, 27, 32, 33, 41, 52, 54], or the accumulative attention scores of visual tokens [13, 16, 36, 47]. These methods propose classifying visual tokens with lower at- tention scores as redundant, as they are minimally involved in the cross-modal feature interaction process. In summary, both vision-centric and instruction-based strategies extensively utilize MLLMs’ intermediate states to estimate visual redundancy. However, the specific influ- ence of visual tokens with low ViT-[cls] association or low 1Length of 8,192 tokens for LLaMA3 [2] and 32,768 for Qwen2 [43]. 2The first forward computation process in the LLM that decode the first token utilizing all visual and textual token embeddings. text-to-image attention scores on MLLMs’ output probabil- ity distribution remains unclear. This ambiguity can result in inaccurate identification of redundant visual tokens. 2.2. Reducing Visual Tokens in MLLMs Training-based Methods. Earlier works design additional networks modules [5, 6, 14, 22, 29, 35] or tunable embed- dings [46, 48] to compress image patch features into com- pact embeddings, resulting in substantial training cost. Training-free Methods. Recent work achieves training- free visual token pruning by leveraging MLLMs’ interme- diate states , discarding visual tokens based on carefully crafted redundancy estimation metrics [4, 16, 32, 41, 47]. Furthermore, visual tokens can be aggregated into identi- fied anchor tokens that encapsulate condensed visual infor- mation [12, 31, 42, 45, 52], thereby mitigating information loss. However, inaccurate identification of redundant visual tokens can compromise the effectiveness of these methods. In this study, we propose to explain visual redundancy by examining the impact of visual tokens on MLLMs’ predic- tions, instead of MLLMs’ intermediate states, and design a training-free pruning strategy. 3. Visual Redundancy Analysis An interpretable definition of visual redundancy necessi- tates recognizing the direct impact of individual visual to- kens on the MLLM’s visual understanding outcome, i.e., the final probability prediction. In this section, we devise novel experimental frameworks and metrics to explore this often-overlooked issue, thereby providing new insights into the identification of redundant visual tokens in MLLMs. 3.1. Background and Analysis Method Existing methods estimate visual redundancy by exten- sively utilizing the scalar attention scores derived from the query and key matrices, and infer that a lower attention score indicates a weaker correlation between the query and a key feature. However, these attention scores are insuf- ficient for elucidating the exact contribution of visual to- kens on MLLMs’ final probability prediction, considering the numerous attention layers and heads, the impact of the attention value vector, and the feature transformation pro- cess in auto-regressive LLMs, where the feature of one to- ken progressively transform into that of its next token (more details in Appendix 1.1). Given these challenges, we shift our research focus to an input-output analytical approach, examining variations in model output upon manipulating input visual tokens. We anticipate that this approach will yield more intuitive and interpretable results. Additionally, to rigorously analyze MLLMs’ compre- hension of visual tokens, we propose an approach inspired by human interpretation of visual media. As humans typ- ically achieve comprehensive image understanding by ob-Prompt: What does this image show? You must provide a single word or phrase that best describes it. + promptLLMsingle visual token inputViTMLP [y1]measure top-1 candidate’sprobabilityregion level leave-one-out ViTMLP + prompt[y1] ViTMLP + prompt[!y1] measure changes in predicted probability distribution3x3region all visualtokens global level leave-one-out ViTMLP + prompt[y1] ViTMLP + prompt[!y1] … … Img decode P LLMdecode LLMdecode Img Img LLMdecode LLMdecode Img Img … … … … … …PPPPPP PPP target visual tokenneighbor image region PLLM’s Pad Token Embedding measure changes in predicted probability distribution Figure 2. Overview of our proposed visual redundancy analysis pipeline. In the single visual token inputexperiment, we provide a single visual token to the LLM and instruct it to describe the visual content. By analyzing the predicted probabilities, we assess the significance of the information encoded in each visual token. Next, we examine the influence of individual visual tokens on the broader visual context (image or image region) by measuring changes in the predicted probability distribution before and after ablating specific visual tokens. The region level leave-one-outexperiment evaluates the influence of a single visual token (highlighted in red) on its neighboring image region, while the global level leave-one-outexperiment assesses the impact of this region on the entire image. The results from these two experiments are combined to quantify the influence of individual visual tokens on the overall image representation. serving individual visual elements and assessing their im- pact on the overall semantic context of the image, we ad- dress the following two problems: 3.1.1. Addressing the Token-Centric Problem In this part, we investigate what information does indi- vidual visual token inherently possess. Note that we dis- cuss visual information from the viewpoint of the LLM, as it further aggregates visual information from visual tokens produced by the vision encoder and generates textual re- sponses. To explore this, we devise asingle visual token in- put experiment, as illustrated in Figure 2. We provide only one visual token to the LLM to eliminate the interference from other visual tokens and instruct the LLM to describe the visual content. Subsequently, we analyze the text de- coding results and the predicted probabilities to uncover the LLM’s interpretation of the visual information. To evaluate whether individual visual tokens contain rec- ognizable information, we assess the magnitude of the prob- ability of the 1st ranked textual token candidate (denoted as top-1 probability). A higher top-1 probability indicates that the LLM has greater confidence in a strong association between the 1st ranked textual token and the input visual token. Conversely, if the top-1 probability is close to zero, we infer that the visual token does not contain significant visual information, as the LLM predicts close confidence scores (i.e., logits) for various candidates in the vocabulary, indicating high uncertainty. Details are in Appendix 1.2. 3.1.2. Addressing the Context-Centric Problem We further investigatehow individual visual tokens influ- ence the overall information of the broader visual con- text (image or image region) by conducting a leave-one- token-out experiment, evaluating the difference in the pre- dicted probability distribution before and after the ablation of input visual tokens. However, our preliminary experi- ments reveal that removing a single visual token from the image token sequence results in numerically insignificant changes in the predicted probabilities, which poses chal- lenges for subsequent analysis (details in Appendix 1.3). To address this, we devise a cascaded leave-one-out ex- periment, as illustrated in Figure 2. First, we conduct a region-level leave-one-out experiment within the 3×3 spa- tial neighborhood of a target visual token. We compare the output variations before and after replacing the target visual token with the LLM’s pad token embedding P. This exper- iment demonstrates the impact of a single visual token on the information of its neighboring region. To reveal the in- fluence of this region on the overall information of the im- age, we conduct a global-level leave-one-out experiment, inspecting the output variations before and after replacing nine visual tokens in this region with P. By cascading the results of these two experiments, we determine the influence of individual visual tokens on the overall information of the image. We employ Jensen-Shannon Divergence (JSD) to assess the difference between two probability distributions. The final results are obtained by a weighted sum of the JSD values from these two experiments.region-level JSD values global-level JSD values #510 #510#523 #523 Rank Candi. region input logits w/ pad input logits diff 1 Soup 17.3125 13.8281 3.4844 2 Food 16.0625 15.4688 0.5938 3 St 14.7813 12.2813 2.5000 4 Nothing 14.5781 14.8438 -0.2656 5 B 14.3516 12.8750 1.4766 6 Picture 14.3438 14.3594 -0.0156 7 Chicken 14.0234 11.9688 2.0547 8 Pot 13.9375 12.3125 1.6250 9 P 13.9141 13.8594 0.0547 10 L 13.8828 13.4063 0.4766 Rank Candi. global input logits w/ pad input logits diff 1 Soup 19.2969 19.0000 0.2969 2 B 18.4219 18.2813 0.1406 3 Food 18.4063 18.3750 0.0313 4 Ve 18.2031 18.1875 0.0156 5 St 18.0781 17.8594 0.2188 6 S 17.9688 18.0156 -0.0469 7 D 17.9375 17.9063 0.0313 8 Pot 17.6875 17.7656 -0.0781 9 Car 17.3594 17.4375 -0.0781 10 P 16.8594 16.9219 -0.0625 Rank Candi. region input logits w/ pad input logits diff 1 Soup 17.3125 13.8281 3.4844 2 Food 16.0625 15.4688 0.5938 3 St 14.7813 12.2813 2.5000 4 Nothing 14.5781 14.8438 -0.2656 5 B 14.3516 12.8750 1.4766 6 Picture 14.3438 14.3594 -0.0156 7 Chicken 14.0234 11.9688 2.0547 8 Pot 13.9375 12.3125 1.6250 9 P 13.9141 13.8594 0.0547 10 L 13.8828 13.4063 0.4766 Rank Candi. global input logits w/ pad input logits diff 1 Soup 19.2969 19.0000 0.2969 2 B 18.4219 18.2813 0.1406 3 Food 18.4063 18.3750 0.0313 4 Ve 18.2031 18.1875 0.0156 5 St 18.0781 17.8594 0.2188 6 S 17.9688 18.0156 -0.0469 7 D 17.9375 17.9063 0.0313 8 Pot 17.6875 17.7656 -0.0781 9 Car 17.3594 17.4375 -0.0781 10 P 16.8594 16.9219 -0.0625 rankcandi.3x3 logits3x3+Pad logitsdiff.rankcandi.576 xlogits576+Pad logitsdiff. Patch #Top-1 ProbabilityViT-[cls]-simAttn Score @ layer #[2, 16, 30]510523 0.150.070.030.24[6.4e-5, 2.6e-4, 4.6e-4][7.1e-3, 5.8e-3, 5.1e-3] Figure 3. Visual tokens with low ViT −[cls] similarity and text- to-image attention scores can more significantly impact LLaV A- Next’s understanding of the image, as patch #510 has higher JSD values than patch #523. candi. and diff. denote candidates and differences, respectively. Patch #510 primarily contributes the se- mantic information Soup to its neighboring region (+3.4844 con- fidence scores) and to the entire image (+0.2969 scores). 3.2. Discoveries We compare the top-1 probability and the JSD results with commonly addressed intermediate states in MLLMs, in- cluding the cosine similarity to the [cls] token in the penul- timate ViT layer and the attention scores of textual tokens to visual tokens in the LLM (i.e., the text-to-image attention scores). Our main findings are summarized as follows: Finding 1. Visual tokens with low ViT −[cls] similar- ity and low text-to-image attention scores may contain recognizable visual information. For instance, LLaV A- Next predicts the word Carrot with 80% confidence for the image patches depicting carrots in the pink box in Fig- ure 1. However, the ViT −[cls] similarities and attention scores of these patches are only around 0.03 and in the range of 1e-5 to 1e-4, respectively. Conversely, some vi- sual tokens with higher ViT −[cls] similarity and text-to- image attention scores do not contain recognizable visual information. For instance, LLaV A-Next predicts irrelevant textual responses (e.g., Cat and Tree) with low confidence (<10%) for six patches in the uninformative white region in the red box in Figure 1, which have high ViT−[cls] similar- ity around 0.4 and attention scores on the order of 1e-2. Finding 2. Visual tokens with low ViT −[cls] similar- ity and low text-to-image attention scores can substan- tially influence the information of their visual context . For example, patch #510 in Figure 3 significantly affects JSD ValuesTop-1 Candidate’s ProbabilitiesSum of Attention Scores in LLM LayersViT-[cls] Cosine Similarities Figure 4. Quantitative results on 6,400 image patches sampled from the VQAv2 validation set. As the text-to-image attention score and the ViT−[cls] similarity decrease, the top-1 probability and the Jenson-Shannon Divergence do not show a declining trend; instead, they fluctuate around 0.24 and 4e-3, respectively. The results are averaged across 100 image samples. the information of its 3 ×3 neighboring region. The pre- dicted confidence scores (i.e., logits) for specific candidates (e.g., Soup and Chicken) show notable variation ( −2 to −3 scores) after patch #510 is ablated. This pattern results in a more significant difference in the probability distribu- tion and a larger JSD value. Additionally, the neighboring region of patch #510 also notably impacts the overall image information, achieving one of the highest JSD values across all image regions. However, patch #510’s text-to-image at- tention scores are only at the magnitude of 1e-4, and its ViT−[cls] similarity is merely 0.03. In contrast, patch #523 has attention scores and ViT −[cls] similarity an order of magnitude higher than those of patch #510, while ablating it or its neighboring region results in a more negligible dif- ference in the model prediction and a lower JSD value. Additional Evidences. To substantiate the two find- ings, we sample 6,400 image patches from the VQAv2 [11] validation set to conduct single-token-input and cascaded leave-one-out experiments. The results for these image patches are reordered based on the ViT −[cls] similarity or the text-to-image attention score to illustrate variation trends. As shown in Figure 4, when the text-to-image atten- tion score and the ViT −[cls] similarity decrease, the top-1 probability and JSD value do not show a corresponding de- cline but rather a fluctuating pattern. More details and dis- cussions are in Appendix 1.4. Therefore, directly pruning visual tokens with low ViT−[cls] similarity or low text-to- image attention scores may lead to the loss of visual infor- mation and changes in the overall information of the image.VisionEncoder (ViT) Connector (MLP) splitpatches patch features visual tokens[B, L, d] Language Decoder (LLM)retained visual tokens+ textual response test imagestrain imagessame MLLMvisual tokens &model outputsRedundancyEvaluation Is there a red double-decker bus in <image> and <image>?questiontokenizer ❌ Word Token Embed textualtokens[B, R, d] identify-then-pruneapproach Redundancy Codebook 📖Prototypes linearprobing Figure 5. An overview of our identify-then-probe approach. We identify redundant prototypes from training images using single- input and cascaded leave-one-out experiments, and store them in a extensible codebook. During inference, visual tokens with higher similarity to these prototypes are considered more likely to be re- dundant and are removed before the first layer of the LLM. L and R are the number of input and retained visual tokens, respectively. 4. Method Building on our analysis of the direct impact of individual visual tokens on MLLMs’ visual understanding outcomes, we explore more reliable approach to identify redundant vi- sual tokens. Next, we propose an identify-then-probe strat- egy for efficient inference-time visual token pruning, recog- nizing that single-input and leave-one-out experiments en- tail significant computational overhead. An overview of our approach is depicted in Figure 5. Initially, we identify re- dundant prototypes from training images using these two experimental frameworks and store them in acodebook. We then utilize these prototypes to probe the redundancy of vi- sual tokens during inference. 4.1. Constructing the Redundancy Codebook Based on the impact of individual visual tokens on MLLMs’ visual understanding from both token-centric and context- centric perspectives, we define a potentially redundant vi- sual token (referred to as redundant candidate) as one that meets two fundamental criteria: (1) it lacks recognizable visual information, and (2) it does not substantially af- fect the overall information of its associated image. Ad- ditionally, we observe that certain redundant candidates from different images exhibit high similarity, indicating that these clusters fail to contribute substantial informa- tion across various visual contexts, thus demonstrating po- tential generalization capability. Consequently, we intro- duce a context-independent condition to identify redundant candidates with this characteristic as redundant prototypes, which are stored in an extensible redundancy codebook to facilitate flexible and scalable applications. 4.1.1. Token-Centric Visual Redundancy Evaluation The token-centric criterion is designed to identify visual tokens that lack recognizable visual information. As dis- cussed in Section 3.2, a low top-1 probability (the probabil- ity of the 1st ranked textual token candidate, obtained from the single-token-input experiment) indicates the MLLM’s inability to recognize valid information in individual visual tokens. Thus, we establish a probability threshold τprob to filter out visual tokens with lower top-1 probability. To improve the accuracy in identifying visual tokens that lack recognizable visual information, we employ t-SNE to visualize the distribution of visual tokens of an image in the high-dimensional feature space. We observe that visual tokens with very low top-1 probability frequently manifest as discrete outliers (as illustrated in Appendix Figure 2). Therefore, we use the Density Peaks Clustering (DPC) al- gorithm to find visual tokens that belong to clusters with sizes below a specified outlier threshold τout. 4.1.2. Context-Centric Visual Redundancy Evaluation The context-centric criterion is designed to identify visual tokens with minimal contribution to their visual context . Recall that a low Jensen Shannon Divergence (JSD) in the cascaded leave-one-out experiment indicates negligible in- fluence of individual visual tokens on MLLMs’ understand- ing of their associated image (Section 3.2), we set a JSD threshold τjsd to filter out visual tokens with lower JSD. We then identify redundant candidates by taking the inter- section of visual tokens filtered by τprob, τout, and τjsd. Context-independent Condition. After identifying the redundant candidates from training images, we further in- vestigate their capability to generalize in evaluating the vi- sual redundancy of test images. We analyze the distribution of these redundant candidates utilizing t-SNE and observe that some redundant candidates from different images es- tablish several high-density clusters (as shown in Appendix Figure 3). This phenomenon suggests that, despite dif- ferences in the images, certain redundant candidates share common features. This characteristic indicates potential for generalization. Consequently, we apply the DPC algorithm again to filter out redundant candidates that belong to clus- ters with sizes exceeding a specified inlier threshold τin, thereby gathering visual tokens that are unlikely to con- tribute substantial information regardless of the visual con- text in which they appear. Summary. We use the four thresholds to filter out N visual tokens {vi}N i=1 from training images X: {vi}N i=1 = CC(T C(X|τprob, τout)|τjsd, τin), (1) where vi ∈ Rd, d is the feature dimension, T C(·) andCC(·) are token-centric and context-centric redundancy evaluation methods, respectively. {vi}N i=1 are the redun- dant prototypes, We stack them together to build the re- dundancy codebook CN×d. We sample images X from the Karpathy train split of the COCO Caption dataset [17]. 4.2. Pruning Visual Tokens using the Codebook In the preceding paragraphs, we have identified redundant prototypes from different images that exhibit analogous fea- tures. Based on this characteristic, we infer that visual to- kens with higher similarity to these prototypes are more likely to be redundant, and pruning them should have lower impact on MLLM’s visual understanding outcome. There- fore, we utilize the redundancy codebook CN×d to probe the redundancy of L input visual tokens T L×d of the test images using the cosine similarity: SL×N = norm(T L×d) · (norm(CN×d))T , (2) where the norm(·) function is the L2 normalization algo- rithm along the feature dimension. We define the redun- dancy score as the maximum cosine similarity among the N results. Finally, R visual tokens with the lowest redun- dancy scores are retained for the LLM (R<L, more details are in Appendix 2.2). Different from previous work that employs a huge codebook (e.g., 217 embeddings as in [30]) to augment the input visual embeddings, we find that a tiny codebook with fewer than 1,000 redundant prototypes gen- eralizes well to test images. Our method can be integrated into various MLLMs without additional training. 5. Experiments 5.1. Experimental Settings Benchmarks and Metrics. We evaluate the effective- ness of our approach on various vision-language tasks, including single-image Visual Question Answering (on POPE [23], MMBench [28], SEED-Image [18], MME [10], and RealWorld-QA [40] benchmarks), image caption- ing (NoCaps-val [1] and Flickr30k-test [49]), and multi- image and video comprehension (Mantis-test [15], Muir- Bench [39], and MVBench [21]). We adhere to the officially defined metrics (Exact Match Accuracy) for VQA, and uti- lize the SPICE [3] metric for image captioning, which em- phasizes semantic correspondence. Implementation Details. We implement our method on three MLLMs: LLaV A-1.5 [24, 25], LLaV A-Next [19, 26], and LLaV A-OneVision [20]. For each model, we construct a distinct codebook, as model predictions are necessary to evaluate the contribution of visual tokens. We set a thresh- old to remove visual tokens with the highest redundancy score. We employ the greedy decoding method for repro- ducible results. Detailed settings are in Appendix 3.1. 5.2. Experimental Results We compare the performance of our method with two representative approaches that leverage MLLMs’ interme- diate states : the vision-centric method PruMerge [31], which prunes visual tokens with lower association with the ViT −[cls] token, and the instruction-based method FastV [4], which leverages the attention scores of the last textual token to visual tokens within the LLM. For a fair comparison, we maintain a training-free setting and adhere to the same visual token quantity budgets. 5.2.1. Single-Image Comprehension Results on single-image VQA and captioning tasks are pre- sented in Table 1. Notably, for the LLaV A-1.5 model, our method preserves 90% of peak performance ( i.e., with 576 input visual tokens) on average across five VQA bench- marks while retaining only 11% of visual tokens. In con- trast, both the vision-centric and instruction-based strate- gies achieve approximately 85%. When retaining 25% of visual tokens, our method maintains or slightly exceeds the performance ceiling on two image captioning benchmarks, significantly outperforming the vision-centric strategy (82% performance) and the instruction-based strategy (94% per- formance). Under both the sub-image splitting and non- splitting settings of the LLaV A-Next model, our method preserves 95% and 91% performance, respectively, while retaining only 11% of visual tokens. In contrast, the ran- dom pruning baseline achieves 87% and 84%. Additionally, our method maintains 90% performance for the LLaV A- Next model under a very low retention rate of visual tokens (5.5%). These results demonstrate that assessing visual re- dundancy based on MLLMs’ predictions is superior to uti- lizing MLLMs’ intermediate states. Qualitative results in Appendix Figures 5 to 9 show that our method allocates the limited visual token budget to critical visual cues in both natural photographs and text-rich images. 5.2.2. Multi-Image and Video Comprehension Results on multi-image and video comprehension tasks are presented in Table 2. On Mantis-test and MuirBench, the performance of LLaV A-OneVision improves by 5% after randomly removing 80% of visual tokens, while our method achieves a higher enhancement of 10%. This suggests that an excessive number of visual tokens may impede the model’s ability to comprehend image-text-interleaved con- texts. In the MVBench video understanding benchmark, our approach maintains 94% performance even with an extreme visual token removal rate of 92%, significantly surpassing the random baseline. These results demonstrate that our method can effectively transfer from single-image to multi- image and video comprehension tasks.Model Method POPE MMB en SEEDI RWQA MME P NoCaps Flickr30k LLaV A-1.57B w/o Split576× 85.6 62.9 65.4 56.1 1458.9 16.5 20.0 Retain 144 visual tokens PruMerge [31] 75.2 57.7 55.7 46.8 1280.8 14.0 15.8 FastV [4] 79.5 62.2 61.2 51.2 1388.2 15.5 18.8 Ours 84.7 61.6 62.6 52.7 1369.1 16.4 20.2 Retain 64 visual tokens PruMerge [31] 73.5 54.6 53.2 48.4 1228.6 12.9 14.8 FastV [4] 69.3 59.9 54.6 47.6 1150.6 13.4 15.3 Ours 79.9 57.1 57.3 48.5 1290.5 15.1 18.8 LLaV A-Next8B w/o Split576× 83.9 72.2 71.4 56.2 1504.2 16.1 18.8 w Split2880× 87.8 72.1 72.7 59.5 1555.8 16.6 19.3 w/o Split, Retain 64 visual tokens Random 76.7 ±0.2 59.2±0.7 62.0±0.2 46.7±0.9 1188.2±10.6 13.5±0.02 15.1±0.02 Ours 80.8 66.6 63.7 54.6 1224.4 15.1 17.8 w Split, Retain 64 visual tokens per sub-image Random 81.7 ±0.3 63.3±0.4 65.7±0.1 47.9±1.1 1339.0±14.3 14.6±0.1 16.6±0.01 Ours 85.2 69.6 68.3 57.5 1343.8 16.1 18.8 w Split, Retain 32 visual tokens per sub-image Random 77.9 ±0.2 58.4±0.4 62.1±0.1 45.5±0.2 1209.4±19.3 13.5±0.04 15.0±0.02 Ours 82.7 66.2 64.4 55.2 1254.1 15.2 17.7 Table 1. Results on single-image VQA and image captioning benchmarks. The officially defined accuracy metric is reported for POPE, MMB-en, SEED-Image, RealWorldQA (RWQA) and MME-Perception. For the image captioning benchmarks NoCaps and Flickr30k, we report the SPICE metric. Our method outperforms representative methods that utilize MLLMs’ intermediate states. For the random baseline, we report the average results and the standard deviations from three separate runs. Method Mantis-test MUIRBench MVBench 729 per image 196 / img w/o Split 59.0 (1814×) 42.7(3158×) 58.7(3136×) Retain 144 per image 16 / img Random 61.4 ±0.6(358×) 45.2±0.1(624×) 53.2±0.3(256×) Ours 63.6 (351×) 48.1(626×) 55.0(256×) Table 2. LLaV A-OneVision-7B results on multi-image and video comprehension benchmarks. Our proposed method maintains over 90% of peak performance and achieves a 10% performance gain by pruning 80% to 90% of input visual tokens. 5.3. Efficiency Analysis During inference, the primary computational overhead in- troduced by our method is the calculation of the similarity matrix SL×N , which incurs a marginal cost of L × N × (2d − 1) floating-point operations (FLOPs). The codebook requires approximately 0.5 GB of GPU memory. 5.4. Ablation Study We assess the effectiveness of each component (τprob, τjsd, τout, and τin) in our proposed method by individually ab- lating them and evaluating the average performance on five single-image VQA benchmarks. Table 3 demonstrates that each component contributes positively to the overall per- formance. Notably, the removal of τprob leads to a signif- icant performance drop for LLaV A-Next (decreasing from τprob τjsd τout τin # Img. N Avg. Perf. ✓ ✓ ✓ ✓ 100% 969 91.3% - ✓ ✓ ✓ 100% 5,086 84.9% ✓ - ✓ ✓ 100% 1,474 90.6% ✓ ✓ - ✓ 100% 2,884 91.2% ✓ ✓ ✓ - 100% 1,151 90.1% ✓ ✓ ✓ ✓ 20% 185 88.0% random baseline 84.5% Table 3. Ablation studies on five single-image VQA benchmarks of LLaV A-Next. Each component in our proposed method con- tributes positively to the average performance (Avg. Perf.). “# Img.” denotes the percentage of sampled images used to identify redundant prototypes. N is the number of redundant prototypes. 91.3% peak performance to 84.9%, approaching the random baseline). In contrast, the performance degradation caused by the removal of other components is relatively moderate. Additionally, reducing the number of sampled training im- ages decreases the number of redundant prototypes from 969 to 185, accompanied by a 3.3% performance decline. Consequently, we opt to use the 969 identified redundant prototypes for LLaV A-Next. 6. Conclusion We explore interpretable definition of visual redundancy in MLLMs, focusing on the influence of individual visual tokens on MLLMs’ visual understanding outcome, whichis a often-overlooked issue. To intuitively and comprehen- sively investigate this issue, we develop input-to-output analytical approaches from both token-centric and context- centric perspectives. We reveal that visual tokens with low ViT−[cls] similarity and low text-to-image attention scores can contain recognizable visual information and substantially influence their visual context . Building on these findings, we propose a novel method to identify redundant visual tokens by combining thetoken-centric and context-centric criteria, along with a context-independent condition. Utilizing this redundancy evaluation method, we design an efficient and scalable identify-then-probe approach for training-free visual token pruning. On single- image, multi-image and video comprehension benchmarks, our method achieves 90% to 110% performance while pruning 80% to 90% of visual tokens, surpassing exist- ing methods that rely on MLLMs’ intermediate states . References [1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Ste- fan Lee, and Peter Anderson. Nocaps: Novel object caption- ing at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8948–8957, 2019. 2, 7 [2] AI@Meta. Llama 3 model card. 2024. 3, 1 [3] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image cap- tion evaluation. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, Octo- ber 11-14, 2016, Proceedings, Part V 14 , pages 382–398. Springer, 2016. 7 [4] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 19–35. Springer, 2025. 1, 2, 3, 7, 8 [5] Shimin Chen, Yitian Yuan, Shaoxiang Chen, Zequn Jie, and Lin Ma. Fewer tokens and fewer videos: Extending video understanding abilities in large vision-language mod- els. arXiv preprint arXiv:2406.08024, 2024. 3 [6] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty- seventh Conference on Neural Information Processing Sys- tems, 2023. 3 [7] Timoth ´ee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In The Twelfth International Conference on Learning Representa- tions, 2024. 3 [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 [9] Mingjing Du, Shifei Ding, and Hongjie Jia. Study on den- sity peaks clustering based on k-nearest neighbors and prin- cipal component analysis. Knowledge-Based Systems, 99: 135–145, 2016. 4 [10] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A compre- hensive evaluation benchmark for multimodal large language models, 2024. 2, 7 [11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba- tra, and Devi Parikh.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02278):\n",
      "Paper 1:     2503.20672v1.BizGen__Advancing_Article_level_Visual_Text_Rendering_for_Infographics_Generation.pdf\n",
      "Paper 2:     2503.20745v1.MATHGLANCE__Multimodal_Large_Language_Models_Do_Not_Know_Where_to_Look_in_Mathematical_Diagrams.pdf\n",
      "Embedding 1: [-0.41494468  0.21788508 -3.14588547  0.49967784  0.26698312]...\n",
      "Embedding 2: [-0.88073677 -0.39981037 -3.37348676 -0.16465193  0.22870453]...\n",
      "Text 1:      1 [9] Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, and Shiyu Chang.\n",
      "Text 2:      1 [18] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wan- jun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02282):\n",
      "Paper 1:     2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "Paper 2:     2503.20676v1.Inductive_Link_Prediction_on_N_ary_Relational_Facts_via_Semantic_Hypergraph_Reasoning.pdf\n",
      "Embedding 1: [-0.26407242  0.12544024 -3.36957955  0.41642034 -0.26200402]...\n",
      "Embedding 2: [-0.85915941  0.05299517 -3.26583338 -0.20076534 -0.40341896]...\n",
      "Text 1:      [42] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.\n",
      "Text 2:      [42] Liang Yang, Wenmiao Zhou, Weihang Peng, Bingxin Niu, Junhua Gu, Chuan Wang, Xiaochun Cao, and Dongxiao He.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02282):\n",
      "Paper 1:     2503.20676v1.Inductive_Link_Prediction_on_N_ary_Relational_Facts_via_Semantic_Hypergraph_Reasoning.pdf\n",
      "Paper 2:     2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "Embedding 1: [-0.85915941  0.05299517 -3.26583338 -0.20076534 -0.40341896]...\n",
      "Embedding 2: [-0.26407242  0.12544024 -3.36957955  0.41642034 -0.26200402]...\n",
      "Text 1:      [42] Liang Yang, Wenmiao Zhou, Weihang Peng, Bingxin Niu, Junhua Gu, Chuan Wang, Xiaochun Cao, and Dongxiao He.\n",
      "Text 2:      [42] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02298):\n",
      "Paper 1:     2503.20731v1.RecTable__Fast_Modeling_Tabular_Data_with_Rectified_Flow.pdf\n",
      "Paper 2:     2503.20767v1.Reliable_algorithm_selection_for_machine_learning_guided_design.pdf\n",
      "Embedding 1: [ 0.62496722  1.09928942 -2.37330818 -0.67665511  1.00318086]...\n",
      "Embedding 2: [ 0.56001431  0.96091229 -2.53640223 -0.50456661  0.55357599]...\n",
      "Text 1:      In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds. ), Advances in Neural Information Processing Systems , volume 31. Cur- ran Associates, Inc., 2018. URLhttps://proceedings.neurips.cc/paper_files/ paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf.\n",
      "Text 2:      In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, ed- itors, Advances in Neural Information Processing Systems , volume 36, pages 12489–12517.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02308):\n",
      "Paper 1:     2503.20756v1.ADS_Edit__A_Multimodal_Knowledge_Editing_Dataset_for_Autonomous_Driving_Systems.pdf\n",
      "Paper 2:     2503.20630v1._β__GNN__A_Robust_Ensemble_Approach_Against_Graph_Structure_Perturbation.pdf\n",
      "Embedding 1: [-0.48799059  1.40491891 -3.25624418 -1.37792373  0.51993191]...\n",
      "Embedding 2: [-0.17985758  0.99286252 -2.64047956 -1.22088754  1.07165313]...\n",
      "Text 1:      In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelli- gence, IJCAI 2024, Jeju, South Korea, August 3-9, 2024, pages 6633–6641.\n",
      "Text 2:      In Proceedings of the Twenty- Eighth International Joint Conference on Artificial Intelligence, IJCAI-19 .\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02346):\n",
      "Paper 1:     2503.20642v1.Representation_Improvement_in_Latent_Space_for_Search_Based_Testing_of_Autonomous_Robotic_Systems.pdf\n",
      "Paper 2:     2503.20781v1.BASKET__A_Large_Scale_Video_Dataset_for_Fine_Grained_Skill_Estimation.pdf\n",
      "Embedding 1: [-0.90545171  0.226154   -3.61019754 -0.49003294 -0.14423786]...\n",
      "Embedding 2: [-0.42345944  0.17552027 -3.62448096  0.26781988  0.15535305]...\n",
      "Text 1:      [38] Yuqi Huai, Yuntianyi Chen, Sumaya Almanee, Tuan Ngo, Xiang Liao, Ziwen Wan, Qi Alfred Chen, and Joshua Garcia.\n",
      "Text 2:      [38] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02346):\n",
      "Paper 1:     2503.20781v1.BASKET__A_Large_Scale_Video_Dataset_for_Fine_Grained_Skill_Estimation.pdf\n",
      "Paper 2:     2503.20642v1.Representation_Improvement_in_Latent_Space_for_Search_Based_Testing_of_Autonomous_Robotic_Systems.pdf\n",
      "Embedding 1: [-0.42345944  0.17552027 -3.62448096  0.26781988  0.15535305]...\n",
      "Embedding 2: [-0.90545171  0.226154   -3.61019754 -0.49003294 -0.14423786]...\n",
      "Text 1:      [38] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.\n",
      "Text 2:      [38] Yuqi Huai, Yuntianyi Chen, Sumaya Almanee, Tuan Ngo, Xiang Liao, Ziwen Wan, Qi Alfred Chen, and Joshua Garcia.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02504):\n",
      "Paper 1:     2503.20527v1.StableToolBench_MirrorAPI__Modeling_Tool_Environments_as_Mirrors_of_7_000__Real_World_APIs.pdf\n",
      "Paper 2:     2503.20749v1.Beyond_Believability__Accurate_Human_Behavior_Simulation_with_Fine_Tuned_LLMs.pdf\n",
      "Embedding 1: [-0.69282591  0.89182723 -2.8461957   0.66086125  0.88627607]...\n",
      "Embedding 2: [-1.0486058   1.01827645 -3.09781432  0.27269888  0.49867502]...\n",
      "Text 1:      Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruim- ing Tang, and Enhong Chen.\n",
      "Text 2:      Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02512):\n",
      "Paper 1:     2503.20750v1.Optimal_Scaling_Laws_for_Efficiency_Gains_in_a_Theoretical_Transformer_Augmented_Sectional_MoE_Framework.pdf\n",
      "Paper 2:     2503.20648v1.TN_Eval__Rubric_and_Evaluation_Protocols_for_Measuring_the_Quality_of_Behavioral_Therapy_Notes.pdf\n",
      "Embedding 1: [-0.44037038  1.36343622 -2.90856743 -1.92816138  0.82714778]...\n",
      "Embedding 2: [-0.14802466  0.71208978 -3.21480894 -1.70350909  0.59590185]...\n",
      "Text 1:      [22] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi` ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., & Lample, G. (2023). LLaMA: Open and Efficient Foundation Language Models.arXiv preprint arXiv:2302.13971. Retrieved from https://arxiv.org/abs/2302.13971. [23] Zhu, T., Qu, X., Dong, D., Ruan, J., Tong, J., He, C., & Cheng, Y.\n",
      "Text 2:      2023. Llama: Open and efficient foundation language models.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02517):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20519v1.MAR_3D__Progressive_Masked_Auto_regressor_for_High_Resolution_3D_Generation.pdf\n",
      "Embedding 1: [-0.95077783  0.05121423 -3.08989048  0.61656666 -0.19967434]...\n",
      "Embedding 2: [-0.50589395  0.33906686 -3.4342196   0.28063199  0.39581114]...\n",
      "Text 1:      1, 3 [50] Jiawei Ren, Cheng Xie, Ashkan Mirzaei, Karsten Kreis, Zi- wei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, Huan Ling, et al.\n",
      "Text 2:      2, 6 [47] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Ji- ahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, and Kai Zhang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02531):\n",
      "Paper 1:     2503.20612v1.IAP__Improving_Continual_Learning_of_Vision_Language_Models_via_Instance_Aware_Prompting.pdf\n",
      "Paper 2:     2503.20698v1.MMMORRF__Multimodal_Multilingual_Modularized_Reciprocal_Rank_Fusion.pdf\n",
      "Embedding 1: [-0.68471253  0.18979128 -3.49268532  0.28081715 -0.13896376]...\n",
      "Embedding 2: [-0.71091413  0.57295609 -3.24542737 -0.07515862 -0.16373605]...\n",
      "Text 1:      6 [32] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu, Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al.\n",
      "Text 2:      [32] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al .\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02546):\n",
      "Paper 1:     2503.20568v1.Low_resource_Information_Extraction_with_the_European_Clinical_Case_Corpus.pdf\n",
      "Paper 2:     2503.20523v1.GAIA_2__A_Controllable_Multi_View_Generative_World_Model_for_Autonomous_Driving.pdf\n",
      "Embedding 1: [-0.09755131  0.70422459 -3.11101866 -1.21790528  0.87377483]...\n",
      "Embedding 2: [-0.13959827  0.70750856 -2.78272986 -0.78360164  0.74000525]...\n",
      "Text 1:      arXiv preprint arXiv:1904.05342 (2020) arXiv:1904.05342 [cs.CL] [22] Cai, P.-X., Fan, Y.-C., Leu, F.-Y.\n",
      "Text 2:      arXiv preprint arXiv:2412.04842, 2024. [49] J. Ni, Y . Guo, Y . Liu, R. Chen, L. Lu, and Z. Wu.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02561):\n",
      "Paper 1:     2503.20680v1.Vision_as_LoRA.pdf\n",
      "Paper 2:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Embedding 1: [-0.71021843 -0.23540547 -3.47188973  1.04081607  0.56257308]...\n",
      "Embedding 2: [-1.00698924  0.21361995 -3.48185635 -0.034922    0.17919201]...\n",
      "Text 1:      1, 2 [39] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.\n",
      "Text 2:      1 [57] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02614):\n",
      "Paper 1:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Paper 2:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Embedding 1: [-0.34025851  0.19393927 -2.99514008 -0.29300782  0.93354928]...\n",
      "Embedding 2: [-0.51019365  0.99092054 -2.64675593  0.25091931  0.94801921]...\n",
      "Text 1:      In ICCV, 2023. 2, 3, 4, 5, 6, 7, 8 [19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, and Dani Lischinski. Prompt-to-prompt image editing with cross attention control. In ICLR, 2023. 2, 3 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. In NeurIPS, 2020. 1 [21] Jonathan Ho, Chitwan Saharia, William Chan, Tim Sali- mans, David J. Fleet, and Mohammad Norouzi. Imagen video: High definition video generation with diffusion mod- els. arXiv Preprint, 2022. 1, 2 [22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv Preprint, 2022. 4 [23] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffu- sion models.\n",
      "Text 2:      Understanding object dynamics for inter- active image-to-video synthesis. In CVPR, 2021. 3 [7] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Björn Ommer. ipoke: Poking a still image for controlled stochastic video synthesis. In ICCV, 2021. 3 [8] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bjorn Ommer. Understanding object dynamics for inter- active image-to-video synthesis. In CVPR, 2021. 3 [9] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram V oleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [10] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock- horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 2 [11] Richard Strong Bowen, Richard Tucker, Ramin Zabih, and Noah Snavely. Dimensions of motion: Monocular prediction through flow subspaces. In 3DV, 2022. 3 [12] Xu Cao, Hiroaki Santo, Boxin Shi, Fumio Okura, and Ya- suyuki Matsushita. Bilateral normal integration. In Eu- ropean Conference on Computer Vision , pages 552–567. Springer, 2022. 4 [13] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840, 2023. 2 [14] Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, and Hengshuang Zhao. Livephoto: Real image animation with text-guided motion control. arXiv preprint arXiv:2312.02928, 2023. 3 [15] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffu- sion model for generative transition and prediction. arXiv preprint arXiv:2310.20700, 2023. 2 [16] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, and Raquel Urtasun. Geosim: Realistic video sim- ulation via geometry-aware composition for self-driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7230–7240, 2021. 5 [17] Yung-Yu Chuang, Dan B Goldman, Ke Colin Zheng, Brian Curless, David H Salesin, and Richard Szeliski. Animating pictures with stochastic motion textures. ACM TOG, 2005. 2, 3 [18] Abe Davis, Justin G Chen, and Frédo Durand. Image-space modal bases for plausible manipulation of objects in video. ACM TOG, 2015. 2, 3 [19] Yuki Endo, Yoshihiro Kanamori, and Shigeru Kuriyama. Animating landscape: self-supervised learning of decoupled motion and appearance for single-image video synthesis. arXiv preprint arXiv:1910.07192, 2019. 3 [20] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography.Communications of the ACM, 24(6):381–395, 1981. 4 [21] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming- Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. In ICCV, 2023. 2 [22] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. In ICLR, 2024. 3 [23] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du- val, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factoriz- ing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 2 [24] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to- image diffusion models without specific tuning, 2023. 3 [25] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. 2 [26] Jingwen He, Tianfan Xue, Dongyang Liu, Xinqi Lin, Peng Gao, Dahua Lin, Yu Qiao, Wanli Ouyang, and Ziwei Liu. Venhancer: Generative space-time enhancement for video generation. arXiv preprint arXiv:2407.07667, 2024. 5 [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. 2020. 2 [28] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02687):\n",
      "Paper 1:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Paper 2:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Embedding 1: [-0.61934227  0.79810488 -3.1166575  -0.56308389  0.51380807]...\n",
      "Embedding 2: [-0.31968707  0.81807876 -3.47001028 -0.60473037  0.48605889]...\n",
      "Text 1:      2, 3, 5 [56] Alex Nichol and Prafulla Dhariwal. Improved denoising dif- fusion probabilistic models. In ICML, 2021. 1 [57] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image genera- tion and editing with text-guided diffusion models. arXiv Preprint, 2021. 1 [58] Santiago Pascual, Chunghsin Yeh, Ioannis Tsiamas, and Joan Serr`a. Masked generative video-to-audio transformers with enhanced synchronicity. In ECCV, 2024. 2 [59] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih- Yao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of media foundation models. arXiv Preprint, 2024. 1 [60] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden- hall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 2, 3, 4 [61] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen.\n",
      "Text 2:      3 [36] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie- niu Tan. Videofusion: Decomposed diffusion models for high-quality video generation. CoRR, abs/2303.08320, 2023. 2 [37] Micha ¨el Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Confer- ence Track Proceedings, 2016. 2 [38] Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, Jian Ren, et al. Snap video: Scaled spatiotemporal transformers for text-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7038– 7048, 2024. 2 [39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: representing scenes as neural radiance fields for view synthe- sis. Commun. ACM, 65(1):99–106, 2022. 2, 3 [40] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real im- ages using guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6038–6047, 2023. 4 [41] Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng, Junjie Yan, and Xiaogang Wang.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02771):\n",
      "Paper 1:     2503.20785v1.Free4D__Tuning_free_4D_Scene_Generation_with_Spatial_Temporal_Consistency.pdf\n",
      "Paper 2:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Embedding 1: [-1.60130954  0.20404598 -3.72366714 -0.59034938 -0.1536561 ]...\n",
      "Embedding 2: [-1.46522975 -0.43966484 -3.3423183   0.44929761  0.2858898 ]...\n",
      "Text 1:      14 [53] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\n",
      "Text 2:      2 [71] Uri Singer, Adam Polyak, Ethan Fetaya, Jonathan Berant, Yaniv Hoshen, and Ronen Shalev.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02773):\n",
      "Paper 1:     2503.20673v1.Mitigating_Low_Level_Visual_Hallucinations_Requires_Self_Awareness__Database__Model_and_Training_Strategy.pdf\n",
      "Paper 2:     2503.20682v1.GLRD__Global_Local_Collaborative_Reason_and_Debate_with_PSL_for_3D_Open_Vocabulary_Detection.pdf\n",
      "Embedding 1: [-0.65872777  0.88298249 -3.24284148 -0.8498255   0.8874144 ]...\n",
      "Embedding 2: [-0.24608923  0.79968572 -2.9219451  -1.46086228  0.67355716]...\n",
      "Text 1:      To better align with diverse usage scenarios, researchers have inte- grated pretrained vision encoders with LLMs and subsequently finetuned these combinations, leading to the emergence of Multimodal Large Language Models (MLLMs).\n",
      "Text 2:      To equip LLMs with the ability to process other modals, e.g., pictures, sounds, etc., Multi-Modal Large Language Models (MLLMs) are de- vised.\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02773):\n",
      "Paper 1:     2503.20682v1.GLRD__Global_Local_Collaborative_Reason_and_Debate_with_PSL_for_3D_Open_Vocabulary_Detection.pdf\n",
      "Paper 2:     2503.20673v1.Mitigating_Low_Level_Visual_Hallucinations_Requires_Self_Awareness__Database__Model_and_Training_Strategy.pdf\n",
      "Embedding 1: [-0.24608923  0.79968572 -2.9219451  -1.46086228  0.67355716]...\n",
      "Embedding 2: [-0.65872777  0.88298249 -3.24284148 -0.8498255   0.8874144 ]...\n",
      "Text 1:      To equip LLMs with the ability to process other modals, e.g., pictures, sounds, etc., Multi-Modal Large Language Models (MLLMs) are de- vised.\n",
      "Text 2:      To better align with diverse usage scenarios, researchers have inte- grated pretrained vision encoders with LLMs and subsequently finetuned these combinations, leading to the emergence of Multimodal Large Language Models (MLLMs).\n",
      "--------------------------------------------------------------------------------\n",
      "Match (Distance: 10.02801):\n",
      "Paper 1:     2503.20746v1.PhysGen3D__Crafting_a_Miniature_Interactive_World_from_a_Single_Image.pdf\n",
      "Paper 2:     2503.20782v1.Zero_Shot_Audio_Visual_Editing_via_Cross_Modal_Delta_Denoising.pdf\n",
      "Embedding 1: [-0.30927658  0.24905992 -3.59528899 -0.08730818 -0.0933055 ]...\n",
      "Embedding 2: [-0.91568482  0.05795411 -3.25849867 -0.05789727 -0.04194427]...\n",
      "Text 1:      2 [94] Albert J Zhai, Yuan Shen, Emily Y Chen, Gloria X Wang, Xinlei Wang, Sheng Wang, Kaiyu Guan, and Shenlong Wang.\n",
      "Text 2:      2 [80] Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Store matched pairs\n",
    "matches = []\n",
    "\n",
    "# Loop through each embedding and find best inter-paper matches\n",
    "for i, title in enumerate(all_titles):\n",
    "    for dist, j in zip(distances[i], indices[i]):\n",
    "        if paper_indices[i] != paper_indices[j]:  # Ensure it's not the same paper\n",
    "            matches.append({\n",
    "                'distance': dist,\n",
    "                'embedding_i': i,\n",
    "                'embedding_j': j,\n",
    "                'paper_i': all_titles[i],\n",
    "                'paper_j': all_titles[j],\n",
    "                'text_i': all_texts[i],\n",
    "                'text_j': all_texts[j]\n",
    "            })\n",
    "\n",
    "# Sort matches by distance (ascending)\n",
    "matches = sorted(matches, key=lambda x: x['distance'])\n",
    "\n",
    "# Print the top matches\n",
    "print_count = 0\n",
    "for match in matches:\n",
    "    if print_count >= 100:\n",
    "        break\n",
    "\n",
    "    if match['distance'] < 10 or len(match['text_i']) > 1000 or len(match['text_i']) < 100:\n",
    "        continue\n",
    "\n",
    "    print_count += 1\n",
    "    print(f\"Match (Distance: {match['distance']:.5f}):\")\n",
    "    print(f\"Paper 1:     {match['paper_i']}\")\n",
    "    print(f\"Paper 2:     {match['paper_j']}\")\n",
    "    print(f\"Embedding 1: {all_embeddings[match['embedding_i']][:5]}...\")\n",
    "    print(f\"Embedding 2: {all_embeddings[match['embedding_j']][:5]}...\")\n",
    "    print(f\"Text 1:      {match['text_i']}\")  # Limit text to first 100 chars\n",
    "    print(f\"Text 2:      {match['text_j']}\")  # Limit text to first 100 chars\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
