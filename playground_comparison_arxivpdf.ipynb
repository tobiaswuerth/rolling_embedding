{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = './arxiv_downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf',\n",
       " '2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf',\n",
       " '2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "files = os.listdir(download_dir)\n",
    "files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21,\n",
       " 'Adaptive Local Clustering over Attributed Graphs\\nTechnical Report\\nHaoran Zheng\\nHong Kong Baptist University\\nHong Kong SAR, China\\ncshrzheng@comp.hkbu.edu.hk\\nRenchi Yang\\nHong Kong Baptist University\\nHong Kong SAR, China\\nrenchi@hkbu.edu.hk\\nJianliang Xu\\nHong Kong Baptist University\\nHong Kong SAR, China\\nxujl@hkbu.edu.hk\\nAbstract—Given a graph G and a seed node vs, the objective\\nof local graph clustering (LGC) is to identify a subgraph Cs ∈ G\\n(a.k.a. local cluster) surrounding vs in time roughly linear with\\nthe size of Cs. This approach yields personalized clusters without\\nneeding to access the entire graph, which makes it highly suitable\\nfor numerous applications involving large graphs. However, most\\nexisting solutions merely rely on the topological connectivity\\nbetween nodes in G, rendering them vulnerable to missing or\\nnoisy links that are commonly present in real-world graphs.\\nTo address this issue, this paper resorts to leveraging the\\ncomplementary nature of graph topology and node attr')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "reader = PdfReader(os.path.join(download_dir, files[0]))\n",
    "len(reader.pages), reader.pages[0].extract_text()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:22<00:00,  7.52s/it]\n"
     ]
    }
   ],
   "source": [
    "from rolling.embedding import Corpus\n",
    "import tqdm\n",
    "corpora = []\n",
    "for file in tqdm.tqdm(files[:3]):\n",
    "    reader = PdfReader(os.path.join(download_dir, file))\n",
    "    text = ''\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    corpora.append(Corpus(file, text, n=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_color():\n",
    "    return (random.random(), random.random(), random.random())  # RGB tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Collect all embeddings and corresponding PDF labels\n",
    "all_embeddings = np.vstack([emb for corpus in corpora for emb in corpus.embeddings])\n",
    "labels = [corpus.title for corpus in corpora for _ in corpus.embeddings]\n",
    "\n",
    "# Reduce to 3D using PCA\n",
    "reducer = PCA(n_components=3)\n",
    "low_dim_embeddings = reducer.fit_transform(all_embeddings)\n",
    "\n",
    "color_map = {title: random_color() for title in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\rolling_embedding\\.venv\\Lib\\site-packages\\mplcursors\\_pick_info.py:503: UserWarning: 3d coordinates not supported yet\n",
      "  warnings.warn(\"3d coordinates not supported yet\")\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\matplotlib\\cbook.py\", line 361, in process\n",
      "    func(*args, **kwargs)\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\mplcursors\\_mplcursors.py\", line 580, in _on_hover_motion_notify\n",
      "    self._on_select_event(event)\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\mplcursors\\_mplcursors.py\", line 628, in _on_select_event\n",
      "    self.add_selection(pi)\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\mplcursors\\_mplcursors.py\", line 411, in add_selection\n",
      "    cb(sel)\n",
      "  File \"C:\\Users\\Tobias\\AppData\\Local\\Temp\\ipykernel_18664\\177993679.py\", line 40, in on_add\n",
      "    part_text = corpus.parts[index]\n",
      "                ~~~~~~~~~~~~^^^^^^^\n",
      "IndexError: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\matplotlib\\cbook.py\", line 361, in process\n",
      "    func(*args, **kwargs)\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\mplcursors\\_mplcursors.py\", line 580, in _on_hover_motion_notify\n",
      "    self._on_select_event(event)\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\mplcursors\\_mplcursors.py\", line 628, in _on_select_event\n",
      "    self.add_selection(pi)\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\mplcursors\\_mplcursors.py\", line 411, in add_selection\n",
      "    cb(sel)\n",
      "  File \"C:\\Users\\Tobias\\AppData\\Local\\Temp\\ipykernel_18664\\177993679.py\", line 40, in on_add\n",
      "    part_text = corpus.parts[index]\n",
      "                ~~~~~~~~~~~~^^^^^^^\n",
      "IndexError: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\matplotlib\\cbook.py\", line 361, in process\n",
      "    func(*args, **kwargs)\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\mplcursors\\_mplcursors.py\", line 580, in _on_hover_motion_notify\n",
      "    self._on_select_event(event)\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\mplcursors\\_mplcursors.py\", line 628, in _on_select_event\n",
      "    self.add_selection(pi)\n",
      "  File \"d:\\rolling_embedding\\.venv\\Lib\\site-packages\\mplcursors\\_mplcursors.py\", line 411, in add_selection\n",
      "    cb(sel)\n",
      "  File \"C:\\Users\\Tobias\\AppData\\Local\\Temp\\ipykernel_18664\\177993679.py\", line 40, in on_add\n",
      "    part_text = corpus.parts[index]\n",
      "                ~~~~~~~~~~~~^^^^^^^\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\n",
    "import mplcursors\n",
    "\n",
    "# Your 3D plotting code\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "start = 0\n",
    "scatter_plots = []\n",
    "for corpus in corpora:\n",
    "    num_points = len(corpus.embeddings)\n",
    "    subset = low_dim_embeddings[start:start+num_points]\n",
    "    \n",
    "    # Scatter plot for each PDF in 3D\n",
    "    scatter = ax.scatter(subset[:, 0], subset[:, 1], subset[:, 2], \n",
    "                         color=color_map[corpus.title], label=corpus.title, alpha=0.6)\n",
    "    \n",
    "    # Add the scatter plot and corresponding text parts for hover\n",
    "    scatter_plots.append((scatter, corpus.parts))  # Store the scatter and the corresponding text parts\n",
    "    \n",
    "    start += num_points\n",
    "\n",
    "# Add the legend and labels\n",
    "ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.1))\n",
    "ax.set_title(\"3D Embeddings Visualization\")\n",
    "ax.set_xlabel(\"UMAP Dim 1\")\n",
    "ax.set_ylabel(\"UMAP Dim 2\")\n",
    "ax.set_zlabel(\"UMAP Dim 3\")\n",
    "\n",
    "# Use mplcursors to display text on hover\n",
    "cursor = mplcursors.cursor(hover=True)\n",
    "\n",
    "# Define the callback to show and hide annotations\n",
    "@cursor.connect(\"add\")\n",
    "def on_add(sel):\n",
    "    # Get the index of the hovered point\n",
    "    index = sel.index\n",
    "    # Find the corresponding part of the text\n",
    "    part_text = corpus.parts[index]\n",
    "    # Set the annotation text to the text part\n",
    "    sel.annotation.set_text(part_text)\n",
    "\n",
    "@cursor.connect(\"remove\")\n",
    "def on_remove(sel):\n",
    "    # Hide the annotation when the mouse moves off\n",
    "    sel.annotation.set_text(\"\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Paper: 2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf\n",
      "--------------------------------------------------\n",
      "58: 13.1734\n",
      "  - A:  0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: B\n",
      "  - B: 50.76 76.37 72.16 82.26 Ours 74.38 280.36 132.07 77.38 77.64 77.67 72.39 350.57 148.32 76.21 73.46 82.66 74 75 76 77 78 1 2 4 8 16 72 74 76 78 80 1 2 4 8 16 CoOp LoCoOp Ours 47 49 51 53 55 1 2 4 8 16 AUROC ↑FPR95 ↓ Accuracy ↑ 70 74 78 82 86 1 2 4 8 16 CoOp\n",
      "  - P1: 2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf\n",
      "  - P2: 2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "--------------------------------------------------\n",
      "50: 13.3279\n",
      "  - A: →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribu\n",
      "  - B: sifier h is the network parameterized with θ to predict the softmax probability of given sample for each category,i.e., hθ(x), and allocate the input to the class exhibiting the highest probability, i.e., ˆy = argmaxy∈Yhy θ(x). 3.2 Problem Formulation The \n",
      "  - P1: 2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf\n",
      "  - P2: 2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "--------------------------------------------------\n",
      "47: 13.8946\n",
      "  - A: When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SN\n",
      "  - B: 50.76 76.37 72.16 82.26 Ours 74.38 280.36 132.07 77.38 77.64 77.67 72.39 350.57 148.32 76.21 73.46 82.66 74 75 76 77 78 1 2 4 8 16 72 74 76 78 80 1 2 4 8 16 CoOp LoCoOp Ours 47 49 51 53 55 1 2 4 8 16 AUROC ↑FPR95 ↓ Accuracy ↑ 70 74 78 82 86 1 2 4 8 16 CoOp\n",
      "  - P1: 2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf\n",
      "  - P2: 2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "\n",
      "################################################################################\n",
      "Paper: 2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf\n",
      "--------------------------------------------------\n",
      "444: 12.5721\n",
      "  - A: kable progress in text-to-video tasks. These models are typically trained on text-video pairs with highly detailed and care- fully crafted descriptions, while real-world user inputs dur- ing inference are often concise, vague, or poorly structured. This ga\n",
      "  - B: g for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787–6800, 2021. [19] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and \n",
      "  - P1: 2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf\n",
      "  - P2: 2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "--------------------------------------------------\n",
      "443: 12.6375\n",
      "  - A: The Conversational Artificial Intelligence (CoAI) Group, Tsinghua University 2Zhipu AI 3The Knowledge Engineering Group (KEG), Tsinghua University chengjl23@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn Abstract Video generation models have achieved remar\n",
      "  - B: g for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6787–6800, 2021. [19] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and \n",
      "  - P1: 2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf\n",
      "  - P2: 2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "--------------------------------------------------\n",
      "448: 12.7508\n",
      "  - A: imization approach. First, we construct and refine a supervised fine-tuning (SFT) dataset based on principles of safety and alignment. Second, we introduce both text-level and video-level feedback to fur- ther optimize the SFT model with preference learnin\n",
      "  - B: and computationally efficient enough to be extended to large-scale datasets? The arrival of vision language model [16] has promoted the research and performance improvement of many down- stream tasks [ 17, 18]. Some parameter-efficient tuning strategies gr\n",
      "  - P1: 2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf\n",
      "  - P2: 2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "\n",
      "################################################################################\n",
      "Paper: 2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "--------------------------------------------------\n",
      "672: 13.7357\n",
      "  - A: iments with prompt learning methods and validate the generalization ability across various datasets with domain shift. Significant and consistent improvement demonstrates the effectiveness, efficiency and generalizability of our approach. 1 Introduction Ne\n",
      "  - B: computer vision and pattern recognition, pages 10684–10695, 2022. 3 [22] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowl- edge from language models with automatically generated prompts. In Proce\n",
      "  - P1: 2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "  - P2: 2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf\n",
      "--------------------------------------------------\n",
      "673: 13.8696\n",
      "  - A: ural networks exhibit astonishing performance in diverse tasks and have been deployed in various scenarios. However, casualty is especially crucial for industries that require a high degree of safety, in which wrong predictions are likely to pose security \n",
      "  - B:  This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To \n",
      "  - P1: 2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "  - P2: 2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf\n",
      "--------------------------------------------------\n",
      "676: 14.5328\n",
      "  - A: t predictions with low confidence [8, 9]. In other words, it is for networks to be aware of potential risk in security-sensitive applications and make safe decisions accordingly. The greatest issue lies in the blind confidence in erroneous outcomes [6]. As\n",
      "  - B: pproximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on att\n",
      "  - P1: 2503.20492v1.Towards_Efficient_and_General_Purpose_Few_Shot_Misclassification_Detection_for_Vision_Language_Models.pdf\n",
      "  - P2: 2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Prepare flat lists\n",
    "all_embeddings = []\n",
    "all_texts = []\n",
    "all_titles = []\n",
    "paper_indices = []  # map each embedding to its corpus index\n",
    "\n",
    "for paper_idx, corpus in enumerate(corpora):\n",
    "    all_embeddings.extend(corpus.embeddings)\n",
    "    all_texts.extend(corpus.parts)\n",
    "    all_titles.extend([corpus.title] * len(corpus.embeddings))\n",
    "    paper_indices.extend([paper_idx] * len(corpus.embeddings))\n",
    "\n",
    "all_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "# Fit NearestNeighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=20, algorithm='auto').fit(all_embeddings)\n",
    "distances, indices = nbrs.kneighbors(all_embeddings)\n",
    "\n",
    "# For each paper, find 3 closest inter-paper matches\n",
    "seen = set()\n",
    "for paper_idx, corpus in enumerate(corpora):\n",
    "    print(\"#\"*80)\n",
    "    print(f\"Paper: {corpus.title}\")\n",
    "    matches = []\n",
    "\n",
    "    for i, title in enumerate(all_titles):\n",
    "        if paper_indices[i] != paper_idx:\n",
    "            continue  # only check embeddings from this paper\n",
    "\n",
    "        for dist, j in zip(distances[i], indices[i]):\n",
    "            if paper_indices[j] == paper_idx:\n",
    "                continue  # skip same paper\n",
    "            pair_key = (i, j)\n",
    "            if pair_key in seen:\n",
    "                continue\n",
    "            seen.add(pair_key)\n",
    "            matches.append((dist, i, j))\n",
    "            break  # only take the closest match for this embedding\n",
    "\n",
    "        if len(matches) >= 3:\n",
    "            break\n",
    "\n",
    "    for dist, i, j in sorted(matches, key=lambda x: x[0]):\n",
    "        print(\"-\"*50)\n",
    "        print(f\"{i}: {dist:.4f}\")\n",
    "        print(f\"  - A: {all_texts[i].replace('\\n', ' ')}\")\n",
    "        print(f\"  - B: {all_texts[j].replace('\\n', ' ')}\")\n",
    "        print(f\"  - P1: {all_titles[i]}\")\n",
    "        print(f\"  - P2: {all_titles[j]}\")\n",
    "    print()\n",
    "\n",
    "    if paper_idx >= 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adaptive Local Clustering over Attributed Graphs\\nTechnical Report\\nHaoran Zheng\\nHong Kong Baptist University\\nHong Kong SAR, China\\ncshrzheng@comp.hkbu.edu.hk\\nRenchi Yang\\nHong Kong Baptist University\\nHong Kong SAR, China\\nrenchi@hkbu.edu.hk\\nJianliang Xu\\nHong K',\n",
       " 'ang, “Local graph clustering\\nwith noisy labels,” in The Twelfth International Conference on Learning\\nRepresentations, 2023.\\n[73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big\\nGraphs. Morgan & Claypool Publishers, 2019.\\n[74] ——, “Communi',\n",
       " 'and\\nexploring data graphs locally,” Journal of Machine Learning Research ,\\nvol. 13, no. 77, pp. 2339–2365, 2012.\\n[14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas-\\nsive graphs and its application to nearly linear time graph partitio',\n",
       " 'ning for\\nnetworks,” Proceedings of the 22nd ACM SIGKDD International Con-\\nference on Knowledge Discovery and Data Mining , 2016.\\n[60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling\\nattributed network embedding to massive graphs,',\n",
       " 'f experiments evaluates the performance of LACA\\n(i.e., LACA (w/o SNAS)) against 4 strong LGC base-\\nlines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on\\ngraphs without node attributes in terms of local clustering\\nquality, using the same evaluation protocol in']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
