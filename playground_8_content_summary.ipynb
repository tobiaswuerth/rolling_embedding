{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize large papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./arxiv_downloads_processed\\2412.19437_deepseek-v3_technical_report\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Content(type='document', level=-1, data={}, children=[Content(type='chapter', level=0, data={'type': 'text', 'text': 'DeepSeek-V3 Technical Report ', 'text_level': 1, 'page_idx': 0}, children=[Content(type='text', level=1, data={'type': 'text', 'text': 'DeepSeek-AI ', 'page_idx': 0}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'research@deepseek.com ', 'page_idx': 0}, children=[])]), Content(type='chapter', level=0, data={'type': 'text', 'text': 'Abstract ', 'text_level': 1, 'page_idx': 0}, children=[Content(type='text', level=1, data={'type': 'text', 'text': 'We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3. ', 'page_idx': 0}, children=[]), Content(type='image', level=1, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/700c3b3526a18f86057af07c28c75896ba00dd4edc85c8f924fc81e859e84d80.jpg', 'img_caption': ['Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts. '], 'img_footnote': [], 'page_idx': 0}, children=[])]), Content(type='chapter', level=0, data={'type': 'text', 'text': 'Contents ', 'text_level': 1, 'page_idx': 1}, children=[]), Content(type='chapter', level=0, data={'type': 'text', 'text': '1 Introduction 4 ', 'text_level': 1, 'page_idx': 1}, children=[]), Content(type='chapter', level=0, data={'type': 'text', 'text': '2 Architecture 6 ', 'text_level': 1, 'page_idx': 1}, children=[Content(type='text', level=1, data={'type': 'text', 'text': '2.1 Basic Architecture 6   \\n2.1.1 Multi-Head Latent Attention 7   \\n2.1.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing 8   \\n2.2 Multi-Token Prediction . 10 ', 'page_idx': 1}, children=[])]), Content(type='chapter', level=0, data={'type': 'text', 'text': '3 Infrastructures 11 ', 'text_level': 1, 'page_idx': 1}, children=[Content(type='text', level=1, data={'type': 'text', 'text': '3.1 Compute Clusters . 11   \\n3.2 Training Framework 12   \\n3.2.1 DualPipe and Computation-Communication Overlap . 12   \\n3.2.2 Efficient Implementation of Cross-Node All-to-All Communication . . 13   \\n3.2.3 Extremely Memory Saving with Minimal Overhead . 14   \\n3.3 FP8 Training 14   \\n3.3.1 Mixed Precision Framework 15   \\n3.3.2 Improved Precision from Quantization and Multiplication 16   \\n3.3.3 Low-Precision Storage and Communication 18   \\n3.4 Inference and Deployment . 18   \\n3.4.1 Prefilling . 19   \\n3.4.2 Decoding 19   \\n3.5 Suggestions on Hardware Design 20   \\n3.5.1 Communication Hardware 20   \\n3.5.2 Compute Hardware 20 ', 'page_idx': 1}, children=[])]), Content(type='chapter', level=0, data={'type': 'text', 'text': '4 Pre-Training 21 ', 'text_level': 1, 'page_idx': 1}, children=[Content(type='chapter', level=1, data={'type': 'text', 'text': '4.1 Data Construction . 21 ', 'text_level': 1, 'page_idx': 1}, children=[Content(type='text', level=2, data={'type': 'text', 'text': '4.2 Hyper-Parameters . 22   \\n4.3 Long Context Extension 23   \\n4.4 Evaluations 24   \\n4.4.1 Evaluation Benchmarks 24   \\n4.4.2 Evaluation Results 24   \\n4.5 Discussion 26   \\n4.5.1 Ablation Studies for Multi-Token Prediction 26   \\n4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy 26 ', 'page_idx': 1}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': '4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance . 27 ', 'page_idx': 2}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': '5 Post-Training 28   \\n5.1 Supervised Fine-Tuning 28   \\n5.2 Reinforcement Learning . 29   \\n5.2.1 Reward Model 29   \\n5.2.2 Group Relative Policy Optimization 30   \\n5.3 Evaluations 30   \\n5.3.1 Evaluation Settings 30   \\n5.3.2 Standard Evaluation 31   \\n5.3.3 Open-Ended Evaluation 33   \\n5.3.4 DeepSeek-V3 as a Generative Reward Model 33   \\n5.4 Discussion 34   \\n5.4.1 Distillation from DeepSeek-R1 34   \\n5.4.2 Self-Rewarding 34   \\n5.4.3 Multi-Token Prediction Evaluation 35 ', 'page_idx': 2}, children=[])])]), Content(type='chapter', level=0, data={'type': 'text', 'text': '6 Conclusion, Limitations, and Future Directions 35 ', 'text_level': 1, 'page_idx': 2}, children=[Content(type='text', level=1, data={'type': 'text', 'text': 'A Contributions and Acknowledgments 45   \\nB Ablation Studies for Low-Precision Training 47   \\nB.1 FP8 v.s. BF16 Training 47   \\nB.2 Discussion About Block-Wise Quantization 47 ', 'page_idx': 2}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'C Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-Loss-Free Models 48 ', 'page_idx': 2}, children=[])]), Content(type='chapter', level=0, data={'type': 'text', 'text': '1. Introduction ', 'text_level': 1, 'page_idx': 3}, children=[Content(type='text', level=1, data={'type': 'text', 'text': 'In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Beyond closed-source models, open-source models, including DeepSeek series (DeepSeek-AI, 2024a,b,c; Guo et al., 2024), LLaMA series (AI@Meta, $2024\\\\mathsf{a},\\\\mathsf{b},$ ; Touvron et al., 2023a,b), Qwen series (Qwen, 2023, 2024a,b), and Mistral series (Jiang et al., 2023; Mistral, 2024), are also making significant strides, endeavoring to close the gap with their closed-source counterparts. To further push the boundaries of open-source model capabilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token. ', 'page_idx': 3}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'With a forward-looking perspective, we consistently strive for strong model performance and economical costs. Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for cost-effective training. These two architectures have been validated in DeepSeekV2 (DeepSeek-AI, 2024c), demonstrating their capability to maintain robust model performance while achieving efficient training and inference. Beyond the basic architecture, we implement two additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy (Wang et al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. Secondly, DeepSeek-V3 employs a multi-token prediction training objective, which we have observed to enhance the overall performance on evaluation benchmarks. ', 'page_idx': 3}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'In order to achieve efficient training, we support the FP8 mixed precision training and implement comprehensive optimizations for the training framework. Low-precision training has emerged as a promising solution for efficient training (Dettmers et al., 2022; Kalamkar et al., 2019; Narang et al., 2017; Peng et al., 2023b), its evolution being closely tied to advancements in hardware capabilities (Luo et al., 2024; Micikevicius et al., 2022; Rouhani et al., 2023a). In this work, we introduce an FP8 mixed precision training framework and, for the first time, validate its effectiveness on an extremely large-scale model. Through the support for FP8 computation and storage, we achieve both accelerated training and reduced GPU memory usage. As for the training framework, we design the DualPipe algorithm for efficient pipeline parallelism, which has fewer pipeline bubbles and hides most of the communication during training through computation-communication overlap. This overlap ensures that, as the model further scales up, as long as we maintain a constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead. In addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize InfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory footprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism. Combining these efforts, we achieve high training efficiency. ', 'page_idx': 3}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The pre-training process is remarkably stable. Throughout the entire training process, we did not encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage context length extension for DeepSeek-V3. In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K. Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential. During the post-training stage, we distill the reasoning capability from the DeepSeekR1 series of models, and meanwhile carefully maintain the balance between model accuracy ', 'page_idx': 3}, children=[]), Content(type='table', level=1, data={'type': 'table', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/11b8e875de1519088afb45aab53bb45a5c61507dc80cebf8f2ce4ab6e24b70bc.jpg', 'table_caption': [], 'table_footnote': [], 'table_body': '\\n\\n<html><body><table><tr><td>Training Costs</td><td>Pre-Training</td><td>Context Extension</td><td>Post-Training</td><td>Total</td></tr><tr><td> in H800 GPU Hours</td><td>2664K</td><td>119K</td><td>5K</td><td>2788K</td></tr><tr><td>in USD</td><td>$5.328M</td><td>$0.238M</td><td>$0.01M</td><td>$5.576M</td></tr></table></body></html>\\n\\n', 'page_idx': 4}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'Table 1 | Training costs of DeepSeek-V3, assuming the rental price of H800 is $\\\\$2$ per GPU hour. ', 'page_idx': 4}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'and generation length. ', 'page_idx': 4}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'We evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the strongest open-source base model currently available, especially in code and math. Its chat version also outperforms other open-source models and achieves performance comparable to leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard and open-ended benchmarks. ', 'page_idx': 4}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware. During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pretraining stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post-training, DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the $\\\\mathrm{{H800GPU}}$ is $\\\\$2$ per GPU hour, our total training costs amount to only $\\\\$5760$ . Note that the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data. ', 'page_idx': 4}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'Our main contribution includes: ', 'page_idx': 4}, children=[]), Content(type='chapter', level=1, data={'type': 'text', 'text': 'Architecture: Innovative Load Balancing Strategy and Training Objective ', 'text_level': 1, 'page_idx': 4}, children=[Content(type='text', level=2, data={'type': 'text', 'text': '‚Ä¢ On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing. ‚Ä¢ We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration. ', 'page_idx': 4}, children=[])]), Content(type='chapter', level=1, data={'type': 'text', 'text': 'Pre-Training: Towards Ultimate Training Efficiency ', 'text_level': 1, 'page_idx': 4}, children=[Content(type='text', level=2, data={'type': 'text', 'text': '‚Ä¢ We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model. ‚Ä¢ Through the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computationcommunication overlap. This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead. ‚Ä¢ At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours. ', 'page_idx': 4}, children=[])]), Content(type='chapter', level=1, data={'type': 'text', 'text': 'Post-Training: Knowledge Distillation from DeepSeek-R1 ', 'text_level': 1, 'page_idx': 4}, children=[Content(type='text', level=2, data={'type': 'text', 'text': '‚Ä¢ We introduce an innovative methodology to distill reasoning capabilities from the longChain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the ', 'page_idx': 4}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain control over the output style and length of DeepSeek-V3. ', 'page_idx': 5}, children=[])]), Content(type='chapter', level=1, data={'type': 'text', 'text': 'Summary of Core Evaluation Results ', 'text_level': 1, 'page_idx': 5}, children=[Content(type='text', level=2, data={'type': 'text', 'text': '‚Ä¢ Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA, DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9 on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3 demonstrates superior performance among open-source models on both SimpleQA and Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese SimpleQA), highlighting its strength in Chinese factual knowledge. ‚Ä¢ Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on math-related benchmarks among all non-long-CoT open-source and closed-source models. Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500, demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks, DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks, such as LiveCodeBench, solidifying its position as the leading model in this domain. For engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5, it still outpaces all other models by a significant margin, demonstrating its competitiveness across diverse technical benchmarks. ', 'page_idx': 5}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3 model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing our compute clusters, the training framework, the support for FP8 training, the inference deployment strategy, and our suggestions on future hardware design. Next, we describe our pre-training process, including the construction of training data, hyper-parameter settings, longcontext extension techniques, the associated evaluations, as well as some discussions (Section 4). Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly, we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential directions for future research (Section 6). ', 'page_idx': 5}, children=[])])]), Content(type='chapter', level=0, data={'type': 'text', 'text': '2. Architecture ', 'text_level': 1, 'page_idx': 5}, children=[Content(type='text', level=1, data={'type': 'text', 'text': 'We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for economical training. Then, we present a Multi-Token Prediction (MTP) training objective, which we have observed to enhance the overall performance on evaluation benchmarks. For other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeekV2 (DeepSeek-AI, 2024c). ', 'page_idx': 5}, children=[]), Content(type='chapter', level=1, data={'type': 'text', 'text': '2.1. Basic Architecture ', 'text_level': 1, 'page_idx': 5}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017) framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) for DeepSeekMoE to mitigate the performance degradation induced by the effort to ensure load balance. Figure 2 illustrates the basic architecture of DeepSeek-V3, and we will briefly review the details of MLA and DeepSeekMoE in this section. ', 'page_idx': 5}, children=[]), Content(type='image', level=2, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0a839f8095c46c85a9c566ea17c4b8aad3d84635b3051071b2cd9fed03bb13d7.jpg', 'img_caption': ['Figure 2 | Illustration of the basic architecture of DeepSeek-V3. Following DeepSeek-V2, we adopt MLA and DeepSeekMoE for efficient inference and economical training. '], 'img_footnote': [], 'page_idx': 6}, children=[]), Content(type='chapter', level=2, data={'type': 'text', 'text': '2.1.1. Multi-Head Latent Attention ', 'text_level': 1, 'page_idx': 6}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'For attention, DeepSeek-V3 adopts the MLA architecture. Let $d$ denote the embedding dimension, $n_{h}$ denote the number of attention heads, $d_{h}$ denote the dimension per head, and $\\\\mathbf h_{t}\\\\in\\\\mathbb R^{d}$ denote the attention input for the $t$ -th token at a given attention layer. The core of MLA is the low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during inference: ', 'page_idx': 6}, children=[]), Content(type='equation', level=3, data={'type': 'equation', 'text': '$$\\n\\\\begin{array}{c}{{{\\\\displaystyle\\\\left[{\\\\bf{\\\\overline{{c}}}}_{t}^{K V}}\\\\right]=W^{D K V}{\\\\bf h}_{t},}}\\\\\\\\ {{{\\\\displaystyle[{\\\\bf{k}}_{t,1}^{C};{\\\\bf{k}}_{t,2}^{C};...;{\\\\bf{k}}_{t,n_{h}}^{C}]={\\\\bf{k}}_{t}^{C}=W^{U K}{\\\\bf c}_{t}^{K V}},}}\\\\\\\\ {{{\\\\displaystyle\\\\left[{\\\\bf{\\\\overline{{k}}}}_{t}^{R}\\\\right]={\\\\mathrm{RoPE}}(W^{K R}{\\\\bf h}_{t})},}}\\\\\\\\ {{{\\\\displaystyle{\\\\bf{k}}_{t,i}=[{\\\\bf k}_{t,i}^{C};{\\\\bf k}_{t}^{R}]},}}\\\\\\\\ {{{\\\\displaystyle[{\\\\bf v}_{t,1}^{C};{\\\\bf v}_{t,2}^{C};...;{\\\\bf v}_{t,n_{h}}^{C}]={\\\\bf v}_{t}^{C}=W^{U V}{\\\\bf c}_{t}^{K V}},}}\\\\end{array}\\n$$', 'text_format': 'latex', 'page_idx': 6}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'where $\\\\mathbf{c}_{t}^{K V}\\\\in\\\\mathbb{R}^{d_{c}}$ is the compressed latent vector for keys and values; $d_{c}(\\\\ll d_{h}n_{h})$ indicates the KV compression dimension; $\\\\dot{W^{D K V}}\\\\in\\\\mathbb{R}^{d_{c}\\\\times d}$ denotes the down-projection matrix; $W^{U K}$ , $W^{U V}\\\\in\\\\mathbb{R}^{d_{h}n_{h}\\\\times d_{c}}$ are the up-projection matrices for keys and values, respectively; $W^{K R}\\\\in\\\\mathbb{R}^{d_{h}^{R}\\\\times d}$ is the matrix used to produce the decoupled key that carries Rotary Positional Embedding (RoPE) (Su et al., 2024); RoPE(¬∑) denotes the operation that applies RoPE matrices; and $[\\\\cdot;\\\\cdot]$ denotes concatenation. Note that for MLA, only the blue-boxed vectors (i.e., $\\\\mathbf{c}_{t}^{K V}$ and $\\\\mathbf{k}_{t}^{R}$ ) need to be cached during generation, which results in significantly reduced KV cache while maintaining performance comparable to standard Multi-Head Attention (MHA) (Vaswani et al., 2017). ', 'page_idx': 7}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'For the attention queries, we also perform a low-rank compression, which can reduce the activation memory during training: ', 'page_idx': 7}, children=[]), Content(type='equation', level=3, data={'type': 'equation', 'text': '$$\\n\\\\begin{array}{c}{{\\\\displaystyle{\\\\bf c}_{t}^{Q}=W^{D Q}{\\\\bf h}_{t},}}\\\\\\\\ {{\\\\displaystyle[{\\\\bf q}_{t,1}^{C};{\\\\bf q}_{t,2}^{C};...;{\\\\bf q}_{t,n_{h}}^{C}]={\\\\bf q}_{t}^{C}=W^{U Q}{\\\\bf c}_{t}^{Q},}}\\\\\\\\ {{\\\\displaystyle[{\\\\bf q}_{t,1}^{R};{\\\\bf q}_{t,2}^{R};...;{\\\\bf q}_{t,n_{h}}^{R}]={\\\\bf q}_{t}^{R}=\\\\mathrm{RoPE}(W^{Q R}{\\\\bf c}_{t}^{Q}),}}\\\\\\\\ {{\\\\displaystyle{\\\\bf q}_{t,i}=[{\\\\bf q}_{t,i}^{C};{\\\\bf q}_{t,i}^{R}],}}\\\\end{array}\\n$$', 'text_format': 'latex', 'page_idx': 7}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'where $\\\\mathbf{c}_{t}^{Q}\\\\in\\\\mathbb{R}^{d_{c}^{\\\\prime}}$ is the compressed latent vector for queries; $d_{c}^{\\\\prime}(\\\\ll~d_{h}n_{h})$ denotes the query compression dimension; $W^{D\\\\hat{Q}}\\\\in\\\\mathbb R^{d_{c}^{\\\\prime}\\\\times d},W^{U Q}\\\\in\\\\mathbb R^{d_{h}n_{h}\\\\times d_{c}^{\\\\prime}}$ are the down-projection and up-projection matrices for queries, respectively; and $W^{Q R}\\\\in\\\\mathbb{R}^{d_{h}^{R}n_{h}\\\\times d_{c}^{\\\\prime}}$ is the matrix to produce the decoupled queries that carry RoPE. ', 'page_idx': 7}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Ultimately, the attention queries $(\\\\mathbf{q}_{t,i})$ , keys $(\\\\mathbf{k}_{j,i})$ , and values $(\\\\mathbf{v}_{j,i}^{C})$ are combined to yield the final attention output ${\\\\bf u}_{t}$ : ', 'page_idx': 7}, children=[]), Content(type='equation', level=3, data={'type': 'equation', 'text': '$$\\n\\\\begin{array}{l}{{\\\\displaystyle{\\\\bf{o}}_{t,i}=\\\\sum_{j=1}^{t}{\\\\cal{S}}\\\\mathrm{oftmax}_{j}(\\\\frac{{\\\\bf{q}}_{t,i}^{T}{\\\\bf{k}}_{j,i}}{\\\\sqrt{d_{h}+d_{h}^{R}}}){\\\\bf{v}}_{j,i}^{C}},\\\\ ~}\\\\\\\\ {{\\\\bf{u}}_{t}={\\\\cal{W}}^{O}[{\\\\bf{o}}_{t,1};{\\\\bf{o}}_{t,2};...;{\\\\bf{o}}_{t,n_{h}}],\\\\ ~}\\\\end{array}\\n$$', 'text_format': 'latex', 'page_idx': 7}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'where $W^{O}\\\\in\\\\mathbb{R}^{d\\\\times d_{h}n_{h}}$ denotes the output projection matrix. ', 'page_idx': 7}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing ', 'text_level': 1, 'page_idx': 7}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'Basic Architecture of DeepSeekMoE. For Feed-Forward Networks (FFNs), DeepSeek-V3 employs the DeepSeekMoE architecture (Dai et al., 2024). Compared with traditional MoE architectures like GShard (Lepikhin et al., 2021), DeepSeekMoE uses finer-grained experts and isolates some experts as shared ones. Let ${\\\\bf u}_{t}$ denote the FFN input of the $t$ -th token, we compute the FFN output ${\\\\bf h}_{t}^{\\\\prime}$ as follows: ', 'page_idx': 7}, children=[]), Content(type='equation', level=3, data={'type': 'equation', 'text': '$$\\n\\\\begin{array}{l}{{\\\\displaystyle{\\\\bf h}_{t}^{\\\\prime}={\\\\bf u}_{t}+\\\\sum_{i=1}^{N_{s}}\\\\mathrm{FFN}_{i}^{(s)}\\\\left({\\\\bf u}_{t}\\\\right)+\\\\sum_{i=1}^{N_{r}}g_{i,t}\\\\mathrm{FFN}_{i}^{(r)}\\\\left({\\\\bf u}_{t}\\\\right)},}}\\\\\\\\ {{\\\\displaystyle g_{i,t}=\\\\frac{g_{i,t}^{\\\\prime}}{\\\\sum_{j=1}^{N_{r}}g_{j,t}^{\\\\prime}}},}\\\\\\\\ {{\\\\displaystyle g_{i,t}^{\\\\prime}=\\\\left\\\\{{s_{i,t}},\\\\quad s_{i},\\\\in\\\\mathrm{Topk}(\\\\{s_{j,t}|1\\\\leqslant j\\\\leqslant N_{r}\\\\},K_{r}),\\\\right.}}\\\\\\\\ {{\\\\displaystyle\\\\left.0,\\\\quad\\\\mathrm{otherwise}},\\\\right.}}\\\\\\\\ {{\\\\displaystyle s_{i,t}=\\\\mathrm{Sigmoid}\\\\left({\\\\bf u}_{t}^{,T}{\\\\bf e}_{i}\\\\right)},}\\\\end{array}\\n$$', 'text_format': 'latex', 'page_idx': 7}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'where $N_{s}$ and $N_{r}$ denote the numbers of shared experts and routed experts, respectively; $\\\\mathrm{FFN}_{i}^{(s)}(\\\\cdot)$ and $\\\\mathrm{FFN}_{i}^{(r)}(\\\\cdot)$ denote the ùëñ-th shared expert and the $i\\\\cdot$ -th routed expert, respectively; $K_{r}$ denotes the number of activated routed experts; $g_{i,t}$ is the gating value for the $i$ -th expert; $s_{i,t}$ is the token-to-expert affinity; $\\\\mathbf{e}_{i}$ is the centroid vector of the $i\\\\cdot$ -th routed expert; and $\\\\mathrm{Topk}(\\\\cdot,K)$ denotes the set comprising $K$ highest scores among the affinity scores calculated for the $t$ -th token and all routed experts. Slightly different from DeepSeek-V2, DeepSeek-V3 uses the sigmoid function to compute the affinity scores, and applies a normalization among all selected affinity scores to produce the gating values. ', 'page_idx': 8}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Auxiliary-Loss-Free Load Balancing. For MoE models, an unbalanced expert load will lead to routing collapse (Shazeer et al., 2017) and diminish computational efficiency in scenarios with expert parallelism. Conventional solutions usually rely on the auxiliary loss (Fedus et al., 2021; Lepikhin et al., 2021) to avoid unbalanced load. However, too large an auxiliary loss will impair the model performance (Wang et al., 2024a). To achieve a better trade-off between load balance and model performance, we pioneer an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) to ensure load balance. To be specific, we introduce a bias term $b_{i}$ for each expert and add it to the corresponding affinity scores $s_{i,t}$ to determine the top-K routing: ', 'page_idx': 8}, children=[]), Content(type='equation', level=3, data={'type': 'equation', 'text': '$$\\ng_{i,t}^{\\\\prime}=\\\\left\\\\{\\\\begin{array}{l l}{s_{i,t},\\\\quad s_{i,t}+b_{i}\\\\in\\\\mathrm{Topk}(\\\\{s_{j,t}+b_{j}|1\\\\leqslant j\\\\leqslant N_{r}\\\\},K_{r}),}\\\\\\\\ {0,\\\\quad\\\\mathrm{otherwise}.}\\\\end{array}\\\\right.\\n$$', 'text_format': 'latex', 'page_idx': 8}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Note that the bias term is only used for routing. The gating value, which will be multiplied with the FFN output, is still derived from the original affinity score $s_{i,t}$ . During training, we keep monitoring the expert load on the whole batch of each training step. At the end of each step, we will decrease the bias term by $\\\\gamma$ if its corresponding expert is overloaded, and increase it by $\\\\gamma$ if its corresponding expert is underloaded, where $\\\\gamma$ is a hyper-parameter called bias update speed. Through the dynamic adjustment, DeepSeek-V3 keeps balanced expert load during training, and achieves better performance than models that encourage load balance through pure auxiliary losses. ', 'page_idx': 8}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Complementary Sequence-Wise Auxiliary Loss. Although DeepSeek-V3 mainly relies on the auxiliary-loss-free strategy for load balance, to prevent extreme imbalance within any single sequence, we also employ a complementary sequence-wise balance loss: ', 'page_idx': 8}, children=[]), Content(type='equation', level=3, data={'type': 'equation', 'text': '$$\\n\\\\begin{array}{r l r}{\\\\lefteqn{\\\\mathcal{L}_{\\\\mathrm{Bal}}=\\\\alpha\\\\sum_{i=1}^{N_{r}}f_{i}P_{i},}}\\\\\\\\ &{f_{i}=\\\\frac{N_{r}}{K_{r}T}\\\\sum_{\\\\ell=1}^{T}\\\\mathbb{1}\\\\left(s_{i,\\\\ell}\\\\-\\\\mathrm{{T}}\\\\mathrm{{opk}}(\\\\{s_{j,\\\\ell}|1\\\\leqslant j\\\\leqslant N_{r}\\\\},K_{r})\\\\right),}\\\\\\\\ &{s_{i,\\\\ell}^{\\\\prime}=\\\\frac{s_{i,\\\\ell}}{\\\\sum_{j=1}^{N_{r}}s_{j,\\\\ell}},}\\\\\\\\ &{}&{P_{i}=\\\\frac{1}{T}\\\\sum_{\\\\ell=1}^{T}s_{i,\\\\ell}^{\\\\prime},}\\\\end{array}\\n$$', 'text_format': 'latex', 'page_idx': 8}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'where the balance factor $\\\\alpha$ is a hyper-parameter, which will be assigned an extremely small value for DeepSeek-V3; $\\\\Im(\\\\cdot)$ denotes the indicator function; and $T$ denotes the number of tokens in a sequence. The sequence-wise balance loss encourages the expert load on each sequence to be balanced. ', 'page_idx': 8}, children=[]), Content(type='image', level=3, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/6d0fae87957c43036f1578472f7c8b774d8aac67ba5a91dc447b9583e4c07b4b.jpg', 'img_caption': ['Figure 3 | Illustration of our Multi-Token Prediction (MTP) implementation. We keep the complete causal chain for the prediction of each token at each depth. '], 'img_footnote': [], 'page_idx': 9}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Node-Limited Routing. Like the device-limited routing used by DeepSeek-V2, DeepSeek-V3 also uses a restricted routing mechanism to limit communication costs during training. In short, we ensure that each token will be sent to at most ùëÄ nodes, which are selected according to the sum of the highest $\\\\frac{K_{r}}{M}$ affinity scores of the experts distributed on each node. Under this constraint, our MoE training framework can nearly achieve full computation-communication overlap. ', 'page_idx': 9}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'No Token-Dropping. Due to the effective load balancing strategy, DeepSeek-V3 keeps a good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training. In addition, we also implement specific deployment strategies to ensure inference load balance, so DeepSeek-V3 also does not drop tokens during inference. ', 'page_idx': 9}, children=[])])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '2.2. Multi-Token Prediction ', 'text_level': 1, 'page_idx': 9}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'Inspired by Gloeckle et al. (2024), we investigate and set a Multi-Token Prediction (MTP) objective for DeepSeek-V3, which extends the prediction scope to multiple future tokens at each position. On the one hand, an MTP objective densifies the training signals and may improve data efficiency. On the other hand, MTP may enable the model to pre-plan its representations for better prediction of future tokens. Figure 3 illustrates our implementation of MTP. Different from Gloeckle et al. (2024), which parallelly predicts $D$ additional tokens using independent output heads, we sequentially predict additional tokens and keep the complete causal chain at each prediction depth. We introduce the details of our MTP implementation in this section. ', 'page_idx': 9}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'MTP Modules. To be specific, our MTP implementation uses $D$ sequential modules to predict $D$ additional tokens. The $k$ -th MTP module consists of a shared embedding layer $\\\\operatorname{Emb}({\\\\cdot})$ , a shared output head OutHead $(\\\\cdot)$ , a Transformer block $\\\\mathrm{TRM}_{k}(\\\\cdot).$ , and a projection matrix $M_{k}\\\\in\\\\mathbb{R}^{d\\\\times2d}$ . For the $i\\\\cdot$ -th input token $t_{i},$ at the $k$ -th prediction depth, we first combine the representation of the $i$ -th token at the $(k-1)$ -th depth $\\\\mathbf{h}_{i}^{k-1}\\\\in\\\\mathbb{R}^{d}$ and the embedding of the $(i+k)$ -th token $E m b(t_{i+k})\\\\in\\\\mathbb{R}^{d}$ ', 'page_idx': 9}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'with the linear projection: ', 'page_idx': 10}, children=[]), Content(type='equation', level=2, data={'type': 'equation', 'text': '$$\\n\\\\mathbf{h}_{i}^{\\\\prime k}=M_{k}[\\\\mathrm{RMSNorm}(\\\\mathbf{h}_{i}^{k-1});\\\\mathrm{RMSNorm}(\\\\mathrm{Emb}(t_{i+k}))],\\n$$', 'text_format': 'latex', 'page_idx': 10}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'where $[\\\\cdot;\\\\cdot]$ denotes concatenation. Especially, when $k=1,\\\\mathbf{h}_{i}^{k-1}$ refers to the representation given by the main model. Note that for each MTP module, its embedding layer is shared with the main model. The combined $\\\\mathbf{h}_{i}^{\\\\prime k}$ serves as the input of the Transformer block at the $k$ -th depth to produce the output representation at the current depth $\\\\mathbf{h}_{i}^{k}$ : ', 'page_idx': 10}, children=[]), Content(type='equation', level=2, data={'type': 'equation', 'text': '$$\\n\\\\begin{array}{r}{\\\\mathbf{h}_{1:T-k}^{k}=\\\\mathrm{TRM}_{k}(\\\\mathbf{h}_{1:T-k}^{\\\\prime k}),}\\\\end{array}\\n$$', 'text_format': 'latex', 'page_idx': 10}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'where $T$ represents the input sequence length and $\\\\boldsymbol{i}{:}\\\\boldsymbol{j}$ denotes the slicing operation (inclusive of both the left and right boundaries). Finally, taking $\\\\mathbf{h}_{i}^{k}$ as the input, the shared output head will compute the probability distribution for the $k$ -th additional prediction token ùëÉùëñùëò 1 ùëò ‚àà Rùëâ , where $V$ is the vocabulary size: ', 'page_idx': 10}, children=[]), Content(type='equation', level=2, data={'type': 'equation', 'text': '$$\\nP_{i+k+1}^{k}=\\\\mathrm{OutHead}(\\\\mathbf{h}_{i}^{k}).\\n$$', 'text_format': 'latex', 'page_idx': 10}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'The output head OutHead $(\\\\cdot)$ linearly maps the representation to logits and subsequently applies the Softmax(¬∑) function to compute the prediction probabilities of the $k$ -th additional token. Also, for each MTP module, its output head is shared with the main model. Our principle of maintaining the causal chain of predictions is similar to that of EAGLE (Li et al., 2024b), but its primary objective is speculative decoding (Leviathan et al., 2023; Xia et al., 2023), whereas we utilize MTP to improve training. ', 'page_idx': 10}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': '$\\\\mathcal{L}_{\\\\mathrm{MTP}}^{k}$ ', 'page_idx': 10}, children=[]), Content(type='equation', level=2, data={'type': 'equation', 'text': '$$\\n\\\\mathcal{L}_{\\\\mathrm{MTP}}^{k}=\\\\mathrm{CrossEntropy}(P_{2+k:T+1}^{k},t_{2+k:T+1})=-\\\\frac{1}{T}\\\\sum_{i=2+k}^{T+1}\\\\log P_{i}^{k}[t_{i}],\\n$$', 'text_format': 'latex', 'page_idx': 10}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'where $T$ denotes the input sequence length, $t_{i}$ denotes the ground-truth token at the $i\\\\cdot$ -th position, and $P_{i}^{k}[t_{i}]$ denotes the corresponding prediction probability of $t_{i},$ given by the $k$ -th MTP module. Finally, we compute the average of the MTP losses across all depths and multiply it by a weighting factor $\\\\lambda$ to obtain the overall MTP loss $\\\\mathcal{L}_{\\\\mathrm{MTP}}$ , which serves as an additional training objective for DeepSeek-V3: ', 'page_idx': 10}, children=[]), Content(type='equation', level=2, data={'type': 'equation', 'text': '$$\\n\\\\mathcal{L}_{\\\\mathrm{MTP}}=\\\\frac{\\\\lambda}{D}\\\\sum_{k=1}^{D}\\\\mathcal{L}_{\\\\mathrm{MTP}}^{k}.\\n$$', 'text_format': 'latex', 'page_idx': 10}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'MTP in Inference. Our MTP strategy mainly aims to improve the performance of the main model, so during inference, we can directly discard the MTP modules and the main model can function independently and normally. Additionally, we can also repurpose these MTP modules for speculative decoding to further improve the generation latency. ', 'page_idx': 10}, children=[])])]), Content(type='chapter', level=0, data={'type': 'text', 'text': '3. Infrastructures ', 'text_level': 1, 'page_idx': 10}, children=[Content(type='chapter', level=1, data={'type': 'text', 'text': '3.1. Compute Clusters ', 'text_level': 1, 'page_idx': 10}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs. Each node in the H800 cluster contains 8 GPUs connected by NVLink and NVSwitch within nodes. Across different nodes, InfiniBand (IB) interconnects are utilized to facilitate communications. ', 'page_idx': 10}, children=[]), Content(type='image', level=2, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/a4465b20c980d401769d2328a06b9e892e925e081f435eff4c8cf6c4393990e1.jpg', 'img_caption': ['Figure 4 Overlapping strategy for a pair of individual forward and backward chunks (the boundaries of the transformer blocks are not aligned). Orange denotes forward, green denotes \"backward for input\", blue denotes \"backward for weights\", purple denotes PP communication, and red denotes barriers. Both all-to-all and PP communication can be fully hidden. '], 'img_footnote': [], 'page_idx': 11}, children=[])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '3.2. Training Framework ', 'text_level': 1, 'page_idx': 11}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up. On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Parallelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajbhandari et al., 2020). ', 'page_idx': 11}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'In order to facilitate efficient training of DeepSeek-V3, we implement meticulous engineering optimizations. Firstly, we design the DualPipe algorithm for efficient pipeline parallelism. Compared with existing PP methods, DualPipe has fewer pipeline bubbles. More importantly, it overlaps the computation and communication phases across forward and backward processes, thereby addressing the challenge of heavy communication overhead introduced by cross-node expert parallelism. Secondly, we develop efficient cross-node all-to-all communication kernels to fully utilize IB and NVLink bandwidths and conserve Streaming Multiprocessors (SMs) dedicated to communication. Finally, we meticulously optimize the memory footprint during training, thereby enabling us to train DeepSeek-V3 without using costly Tensor Parallelism (TP). ', 'page_idx': 11}, children=[]), Content(type='chapter', level=2, data={'type': 'text', 'text': '3.2.1. DualPipe and Computation-Communication Overlap ', 'text_level': 1, 'page_idx': 11}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'For DeepSeek-V3, the communication overhead introduced by cross-node expert parallelism results in an inefficient computation-to-communication ratio of approximately 1:1. To tackle this challenge, we design an innovative pipeline parallelism algorithm called DualPipe, which not only accelerates model training by effectively overlapping forward and backward computationcommunication phases, but also reduces the pipeline bubbles. ', 'page_idx': 11}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'The key idea of DualPipe is to overlap the computation and communication within a pair of individual forward and backward chunks. To be specific, we divide each chunk into four components: attention, all-to-all dispatch, MLP, and all-to-all combine. Specially, for a backward chunk, both attention and MLP are further split into two parts, backward for input and backward for weights, like in ZeroBubble (Qi et al., 2023b). In addition, we have a PP communication component. As illustrated in Figure 4, for a pair of forward and backward chunks, we rearrange these components and manually adjust the ratio of GPU SMs dedicated to communication versus computation. In this overlapping strategy, we can ensure that both all-to-all and PP communication can be fully hidden during execution. Given the efficient overlapping strategy, the full DualPipe scheduling is illustrated in Figure 5. It employs a bidirectional pipeline scheduling, which feeds micro-batches from both ends of the pipeline simultaneously and a significant portion of communications can be fully overlapped. This overlap also ensures that, as the model further scales up, as long as we maintain a constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead. ', 'page_idx': 11}, children=[]), Content(type='image', level=3, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/6fffbffe0474c0ffe9c917374fde2a3c8a46b6369cf79245b61302e1158aa67d.jpg', 'img_caption': ['Figure 5 | Example DualPipe scheduling for 8 PP ranks and 20 micro-batches in two directions. The micro-batches in the reverse direction are symmetric to those in the forward direction, so we omit their batch ID for illustration simplicity. Two cells enclosed by a shared black border have mutually overlapped computation and communication. '], 'img_footnote': [], 'page_idx': 12}, children=[]), Content(type='table', level=3, data={'type': 'table', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/7854d0a078215e4170ef86724e63cd844337966403b9f5f3e427db6c469470e0.jpg', 'table_caption': [], 'table_footnote': [], 'table_body': '\\n\\n<html><body><table><tr><td>Method</td><td>Bubble</td><td>Parameter</td><td>Activation</td></tr><tr><td>1F1B</td><td>(PP -1)(F+B)</td><td>1x</td><td>PP</td></tr><tr><td>ZB1P</td><td>(PP -1)(F+B-2W)</td><td>1x</td><td>PP</td></tr><tr><td>DualPipe (Ours)</td><td>(-1)(F&B+B-3W)</td><td>2x</td><td>PP+1</td></tr></table></body></html>\\n\\n', 'page_idx': 12}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Table 2 Comparison of pipeline bubbles and memory usage across different pipeline parallel methods. $F$ denotes the execution time of a forward chunk, $B$ denotes the execution time of a full backward chunk, ùëä denotes the execution time of a \"backward for weights\" chunk, and $F\\\\&B$ denotes the execution time of two mutually overlapped forward and backward chunks. ', 'page_idx': 12}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'In addition, even in more general scenarios without a heavy communication burden, DualPipe still exhibits efficiency advantages. In Table 2, we summarize the pipeline bubbles and memory usage across different PP methods. As shown in the table, compared with ZB1P (Qi et al., 2023b) and 1F1B (Harlap et al., 2018), DualPipe significantly reduces the pipeline bubbles while only increasing the peak activation memory by $\\\\scriptstyle{\\\\frac{1}{P P}}$ times. Although DualPipe requires keeping two copies of the model parameters, this does not significantly increase the memory consumption since we use a large EP size during training. Compared with Chimera (Li and Hoefler, 2021), DualPipe only requires that the pipeline stages and micro-batches be divisible by 2, without requiring micro-batches to be divisible by pipeline stages. In addition, for DualPipe, neither the bubbles nor activation memory will increase as the number of micro-batches grows. ', 'page_idx': 12}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '3.2.2. Efficient Implementation of Cross-Node All-to-All Communication ', 'text_level': 1, 'page_idx': 12}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'In order to ensure sufficient computational performance for DualPipe, we customize efficient cross-node all-to-all communication kernels (including dispatching and combining) to conserve the number of SMs dedicated to communication. The implementation of the kernels is codesigned with the MoE gating algorithm and the network topology of our cluster. To be specific, in our cluster, cross-node GPUs are fully interconnected with IB, and intra-node communications are handled via NVLink. NVLink offers a bandwidth of 160 GB/s, roughly 3.2 times that of IB $(50\\\\mathrm{GB}/\\\\mathrm{s})$ . To effectively leverage the different bandwidths of IB and NVLink, we limit each token to be dispatched to at most 4 nodes, thereby reducing IB traffic. For each token, when its routing decision is made, it will first be transmitted via IB to the GPUs with the same in-node index on its target nodes. Once it reaches the target nodes, we will endeavor to ensure that it is instantaneously forwarded via NVLink to specific GPUs that host their target experts, without being blocked by subsequently arriving tokens. In this way, communications via IB and NVLink are fully overlapped, and each token can efficiently select an average of 3.2 experts per node without incurring additional overhead from NVLink. This implies that, although DeepSeek-V3 selects only 8 routed experts in practice, it can scale up this number to a maximum of 13 experts (4 nodes $\\\\times3.2$ experts/node) while preserving the same communication cost. Overall, under such a communication strategy, only 20 SMs are sufficient to fully utilize the bandwidths of IB and NVLink. ', 'page_idx': 12}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'In detail, we employ the warp specialization technique (Bauer et al., 2014) and partition 20 SMs into 10 communication channels. During the dispatching process, (1) IB sending, (2) IB-to-NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The number of warps allocated to each communication task is dynamically adjusted according to the actual workload across all SMs. Similarly, during the combining process, (1) NVLink sending, (2) NVLink-to-IB forwarding and accumulation, and (3) IB receiving and accumulation are also handled by dynamically adjusted warps. In addition, both dispatching and combining kernels overlap with the computation stream, so we also consider their impact on other SM computation kernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and auto-tune the communication chunk size, which significantly reduces the use of the L2 cache and the interference to other SMs. ', 'page_idx': 13}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '3.2.3. Extremely Memory Saving with Minimal Overhead ', 'text_level': 1, 'page_idx': 13}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'In order to reduce the memory footprint during training, we employ the following techniques. ', 'page_idx': 13}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Recomputation of RMSNorm and MLA Up-Projection. We recompute all RMSNorm operations and MLA up-projections during back-propagation, thereby eliminating the need to persistently store their output activations. With a minor overhead, this strategy significantly reduces memory requirements for storing activations. ', 'page_idx': 13}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Exponential Moving Average in CPU. During training, we preserve the Exponential Moving Average (EMA) of the model parameters for early estimation of the model performance after learning rate decay. The EMA parameters are stored in CPU memory and are updated asynchronously after each training step. This method allows us to maintain EMA parameters without incurring additional memory or time overhead. ', 'page_idx': 13}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Shared Embedding and Output Head for Multi-Token Prediction. With the DualPipe strategy, we deploy the shallowest layers (including the embedding layer) and deepest layers (including the output head) of the model on the same PP rank. This arrangement enables the physical sharing of parameters and gradients, of the shared embedding and output head, between the MTP module and the main model. This physical sharing mechanism further enhances our memory efficiency. ', 'page_idx': 13}, children=[])])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '3.3. FP8 Training ', 'text_level': 1, 'page_idx': 13}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'Inspired by recent advances in low-precision training (Dettmers et al., 2022; Noune et al., 2022; Peng et al., 2023b), we propose a fine-grained mixed precision framework utilizing the FP8 data format for training DeepSeek-V3. While low-precision training holds great promise, it is often limited by the presence of outliers in activations, weights, and gradients (Fishman et al., 2024; He et al.; Sun et al., 2024). Although significant progress has been made in inference quantization (Frantar et al., 2022; Xiao et al., 2023), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model pre-training (Fishman et al., 2024). To address thiÊàñs chËÄÖalIlenpgeuat-nd>eAffcectitivealtyi eoxnte_ndLthe dynamic range of the FP8 format, we introduce a fine-grained quantization strategy: tile-wise grouping with $1\\\\times N_{c}$ elements or block-wise grouping with $N_{c}\\\\times N_{c}$ uetlepmuetn-t>s.ATchteiavssaotciioatned_{dLeq+u1an}tization overhead is largely mitigated under our increased-precision accumulation process, a critical aspect for achieving accurate FP8 General Matrix Multiplication (GEMM). Moreover, to further reduce memory and communication overhead in MoE training, we cache and dispatch activations in FP8, while storing low-precision optimizer states in BF16. We validate the proposed FP8 mixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeekV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably, compared with the BF16 baseline, the relative loss error of our FP8-training model remains consistently below $0.25\\\\%$ , a level well within the acceptable range of training randomness. ', 'page_idx': 13}, children=[]), Content(type='image', level=2, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/ec4d0e65b5484cb0a5fc05ae43186a6ed454d109a9645a64bf7349b9abd4e3ae.jpg', 'img_caption': ['Figure 6 | The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. '], 'img_footnote': [], 'page_idx': 14}, children=[]), Content(type='chapter', level=2, data={'type': 'text', 'text': '3.3.1. Mixed Precision Framework ', 'text_level': 1, 'page_idx': 14}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'Building upon widely adopted techniques in low-precision training (Kalamkar et al., 2019; Narang et al., 2017), we propose a mixed precision framework for FP8 training. In this framework, most compute-density operations are conducted in FP8, while a few key operations are strategically maintained in their original data formats to balance training efficiency and numerical stability. The overall framework is illustrated in Figure 6. ', 'page_idx': 14}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Firstly, in order to accelerate model training, the majority of core computation kernels, i.e., GEMM operations, are implemented in FP8 precision. These GEMM operations accept FP8 tensors as inputs and produce outputs in BF16 or FP32. As depicted in Figure 6, all three GEMMs associated with the Linear operator, namely Fprop (forward pass), Dgrad (activation backward pass), and Wgrad (weight backward pass), are executed in FP8. This design theoretically doubles the computational speed compared with the original BF16 method. Additionally, the FP8 Wgrad GEMM allows activations to be stored in FP8 for use in the backward pass. This significantly reduces memory consumption. ', 'page_idx': 14}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Despite the efficiency advantage of the FP8 format, certain operators still require a higher precision due to their sensitivity to low-precision computations. Besides, some low-cost operators can also utilize a higher precision with a negligible overhead to the overall training cost. For this reason, after careful investigations, we maintain the original precision (e.g., BF16 or FP32) for the following components: the embedding module, the output head, MoE gating modules, normalization operators, and attention operators. These targeted retentions of high precision ensure stable training dynamics for DeepSeek-V3. To further guarantee numerical stability, we store the master weights, weight gradients, and optimizer states in higher precision. While these high-precision components incur some memory overheads, their impact can be minimized through efficient sharding across multiple DP ranks in our distributed training system. ', 'page_idx': 14}, children=[]), Content(type='image', level=3, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/4838bb048c259d0985856e0b4fbdec39bbdbf7fa33517c90d7f2c5d59d8bfc2a.jpg', 'img_caption': ['Figure 7 | (a) We propose a fine-grained quantization method to mitigate quantization errors caused by feature outliers; for illustration simplicity, only Fprop is illustrated. (b) In conjunction with our quantization strategy, we improve the FP8 GEMM precision by promoting to CUDA Cores at an interval of $N_{C}=128$ elements MMA for the high-precision accumulation. '], 'img_footnote': [], 'page_idx': 15}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '3.3.2. Improved Precision from Quantization and Multiplication ', 'text_level': 1, 'page_idx': 15}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'Based on our mixed precision FP8 framework, we introduce several strategies to enhance lowprecision training accuracy, focusing on both the quantization method and the multiplication process. ', 'page_idx': 15}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Fine-Grained Quantization. In low-precision training frameworks, overflows and underflows are common challenges due to the limited dynamic range of the FP8 format, which is constrained by its reduced exponent bits. As a standard practice, the input distribution is aligned to the representable range of the FP8 format by scaling the maximum absolute value of the input tensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes lowprecision training highly sensitive to activation outliers, which can heavily degrade quantization accuracy. To solve this, we propose a fine-grained quantization method that applies scaling at a more granular level. As illustrated in Figure 7 (a), (1) for activations, we group and scale elements on a 1x128 tile basis (i.e., per token per 128 channels); and (2) for weights, we group and scale elements on a 128x128 block basis (i.e., per 128 input channels per 128 output channels). This approach ensures that the quantization process can better accommodate outliers by adapting the scale according to smaller groups of elements. In Appendix B.2, we further discuss the training instability when we group and scale activations on a block basis in the same way as weights quantization. ', 'page_idx': 15}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'One key modification in our method is the introduction of per-group scaling factors along the inner dimension of GEMM operations. This functionality is not directly supported in the standard FP8 GEMM. However, combined with our precise FP32 accumulation strategy, it can ', 'page_idx': 15}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'be efficiently implemented. ', 'page_idx': 16}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Notably, our fine-grained quantization strategy is highly consistent with the idea of microscaling formats (Rouhani et al., 2023b), while the Tensor Cores of NVIDIA next-generation GPUs (Blackwell series) have announced the support for microscaling formats with smaller quantization granularity (NVIDIA, 2024a). We hope our design can serve as a reference for future work to keep pace with the latest GPU architectures. ', 'page_idx': 16}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Increasing Accumulation Precision. Low-precision GEMM operations often suffer from underflow issues, and their accuracy largely depends on high-precision accumulation, which is commonly performed in an FP32 precision (Kalamkar et al., 2019; Narang et al., 2017). However, we observe that the accumulation precision of FP8 GEMM on NVIDIA H800 GPUs is limited to retaining around 14 bits, which is significantly lower than FP32 accumulation precision. This problem will become more pronounced when the inner dimension K is large (Wortsman et al., 2023), a typical scenario in large-scale model training where the batch size and model width are increased. Taking GEMM operations of two random matrices with $\\\\mathtt{K}=4096$ for example, in our preliminary test, the limited accumulation precision in Tensor Cores results in a maximum relative error of nearly $2\\\\%$ . Despite these problems, the limited accumulation precision is still the default option in a few FP8 frameworks (NVIDIA, 2024b), severely constraining the training accuracy. ', 'page_idx': 16}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'In order to address this issue, we adopt the strategy of promotion to CUDA Cores for higher precision (Thakkar et al., 2023). The process is illustrated in Figure 7 (b). To be specific, during MMA (Matrix Multiply-Accumulate) execution on Tensor Cores, intermediate results are accumulated using the limited bit width. Once an interval of $N_{C}$ is reached, these partial results will be copied to FP32 registers on CUDA Cores, where full-precision FP32 accumulation is performed. As mentioned before, our fine-grained quantization applies per-group scaling factors along the inner dimension K. These scaling factors can be efficiently multiplied on the CUDA Cores as the dequantization process with minimal additional computational cost. ', 'page_idx': 16}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'It is worth noting that this modification reduces the WGMMA (Warpgroup-level Matrix Multiply-Accumulate) instruction issue rate for a single warpgroup. However, on the H800 architecture, it is typical for two WGMMA to persist concurrently: while one warpgroup performs the promotion operation, the other is able to execute the MMA operation. This design enables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based on our experiments, setting $N_{C}=128$ elements, equivalent to 4 WGMMAs, represents the minimal accumulation interval that can significantly improve precision without introducing substantial overhead. ', 'page_idx': 16}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Mantissa over Exponents. In contrast to the hybrid FP8 format adopted by prior work (NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses E4M3 (4-bit exponent and 3-bit mantissa) in Fprop and E5M2 (5-bit exponent and 2-bit mantissa) in Dgrad and Wgrad, we adopt the E4M3 format on all tensors for higher precision. We attribute the feasibility of this approach to our fine-grained quantization strategy, i.e., tile and block-wise scaling. By operating on smaller element groups, our methodology effectively shares exponent bits among these grouped elements, mitigating the impact of the limited dynamic range. ', 'page_idx': 16}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Online Quantization. Delayed quantization is employed in tensor-wise quantization frameworks (NVIDIA, 2024b; Peng et al., 2023b), which maintains a history of the maximum absolute values across prior iterations to infer the current value. In order to ensure accurate scales and simplify the framework, we calculate the maximum absolute value online for each 1x128 activation tile or $128\\\\mathrm{x}128$ weight block. Based on it, we derive the scaling factor and then quantize the activation or weight online into the FP8 format. ', 'page_idx': 16}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '3.3.3. Low-Precision Storage and Communication ', 'text_level': 1, 'page_idx': 17}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'In conjunction with our FP8 training framework, we further reduce the memory consumption and communication overhead by compressing cached activations and optimizer states into lower-precision formats. ', 'page_idx': 17}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Low-Precision Optimizer States. We adopt the BF16 data format instead of FP32 to track the first and second moments in the AdamW (Loshchilov and Hutter, 2017) optimizer, without incurring observable performance degradation. However, the master weights (stored by the optimizer) and gradients (used for batch size accumulation) are still retained in FP32 to ensure numerical stability throughout training. ', 'page_idx': 17}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Low-Precision Activation. As illustrated in Figure 6, the Wgrad operation is performed in FP8. To reduce the memory consumption, it is a natural choice to cache activations in FP8 format for the backward pass of the Linear operator. However, special considerations are taken on several operators for low-cost high-precision training: ', 'page_idx': 17}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': '(1) Inputs of the Linear after the attention operator. These activations are also used in the backward pass of the attention operator, which makes it sensitive to precision. We adopt a customized E5M6 data format exclusively for these activations. Additionally, these activations will be converted from an 1x128 quantization tile to an $128\\\\mathbf{x}\\\\mathbf{1}$ tile in the backward pass. To avoid introducing extra quantization error, all the scaling factors are round scaled, i.e., integral power of 2. ', 'page_idx': 17}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': '(2) Inputs of the SwiGLU operator in MoE. To further reduce the memory cost, we cache the inputs of the SwiGLU operator and recompute its output in the backward pass. These activations are also stored in FP8 with our fine-grained quantization method, striking a balance between memory efficiency and computational accuracy. ', 'page_idx': 17}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Low-Precision Communication. Communication bandwidth is a critical bottleneck in the training of MoE models. To alleviate this challenge, we quantize the activation before MoE up-projections into FP8 and then apply dispatch components, which is compatible with FP8 Fprop in MoE up-projections. Like the inputs of the Linear after the attention operator, scaling factors for this activation are integral power of 2. A similar strategy is applied to the activation gradient before MoE down-projections. For both the forward and backward combine components, we retain them in BF16 to preserve training precision in critical parts of the training pipeline. ', 'page_idx': 17}, children=[])])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '3.4. Inference and Deployment ', 'text_level': 1, 'page_idx': 17}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected using NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously ensure both the Service-Level Objective (SLO) for online services and high throughput, we employ the following deployment strategy that separates the prefilling and decoding stages. ', 'page_idx': 17}, children=[]), Content(type='chapter', level=2, data={'type': 'text', 'text': '3.4.1. Prefilling ', 'text_level': 1, 'page_idx': 18}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'The minimum deployment unit of the prefilling stage consists of 4 nodes with 32 GPUs. The attention part employs 4-way Tensor Parallelism (TP4) with Sequence Parallelism (SP), combined with 8-way Data Parallelism (DP8). Its small TP size of 4 limits the overhead of TP communication. For the MoE part, we use 32-way Expert Parallelism (EP32), which ensures that each expert processes a sufficiently large batch size, thereby enhancing computational efficiency. For the MoE all-to-all communication, we use the same method as in training: first transferring tokens across nodes via IB, and then forwarding among the intra-node GPUs via NVLink. In particular, we use 1-way Tensor Parallelism for the dense MLPs in shallow layers to save TP communication. ', 'page_idx': 18}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'To achieve load balancing among different experts in the MoE part, we need to ensure that each GPU processes approximately the same number of tokens. To this end, we introduce a deployment strategy of redundant experts, which duplicates high-load experts and deploys them redundantly. The high-load experts are detected based on statistics collected during the online deployment and are adjusted periodically (e.g., every 10 minutes). After determining the set of redundant experts, we carefully rearrange experts among GPUs within a node based on the observed loads, striving to balance the load across GPUs as much as possible without increasing the cross-node all-to-all communication overhead. For the deployment of DeepSeek-V3, we set 32 redundant experts for the prefilling stage. For each GPU, besides the original 8 experts it hosts, it will also host one additional redundant expert. ', 'page_idx': 18}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Furthermore, in the prefilling stage, to improve the throughput and hide the overhead of all-to-all and TP communication, we simultaneously process two micro-batches with similar computational workloads, overlapping the attention and MoE of one micro-batch with the dispatch and combine of another. ', 'page_idx': 18}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Finally, we are exploring a dynamic redundancy strategy for experts, where each GPU hosts more experts (e.g., 16 experts), but only 9 will be activated during each inference step. Before the all-to-all operation at each layer begins, we compute the globally optimal routing scheme on the fly. Given the substantial computation involved in the prefilling stage, the overhead of computing this routing scheme is almost negligible. ', 'page_idx': 18}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '3.4.2. Decoding ', 'text_level': 1, 'page_idx': 18}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'During decoding, we treat the shared expert as a routed one. From this perspective, each token will select 9 experts during routing, where the shared expert is regarded as a heavy-load one that will always be selected. The minimum deployment unit of the decoding stage consists of 40 nodes with 320 GPUs. The attention part employs TP4 with SP, combined with DP80, while the MoE part uses EP320. For the MoE part, each GPU hosts only one expert, and 64 GPUs are responsible for hosting redundant experts and shared experts. All-to-all communication of the dispatch and combine parts is performed via direct point-to-point transfers over IB to achieve low latency. Additionally, we leverage the IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency. ', 'page_idx': 18}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Similar to prefilling, we periodically determine the set of redundant experts in a certain interval, based on the statistical expert load from our online service. However, we do not need to rearrange experts since each GPU only hosts one expert. We are also exploring the dynamic redundancy strategy for decoding. However, this requires more careful optimization of the algorithm that computes the globally optimal routing scheme and the fusion with the dispatch kernel to reduce overhead. ', 'page_idx': 18}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Additionally, to enhance throughput and hide the overhead of all-to-all communication, we are also exploring processing two micro-batches with similar computational workloads simultaneously in the decoding stage. Unlike prefilling, attention consumes a larger portion of time in the decoding stage. Therefore, we overlap the attention of one micro-batch with the dispatch+MoE $+$ combine of another. In the decoding stage, the batch size per expert is relatively small (usually within 256 tokens), and the bottleneck is memory access rather than computation. Since the MoE part only needs to load the parameters of one expert, the memory access overhead is minimal, so using fewer SMs will not significantly affect the overall performance. Therefore, to avoid impacting the computation speed of the attention part, we can allocate only a small portion of SMs to dispatch+MoE+combine. ', 'page_idx': 19}, children=[])])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '3.5. Suggestions on Hardware Design ', 'text_level': 1, 'page_idx': 19}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'Based on our implementation of the all-to-all communication and FP8 training scheme, we propose the following suggestions on chip design to AI hardware vendors. ', 'page_idx': 19}, children=[]), Content(type='chapter', level=2, data={'type': 'text', 'text': '3.5.1. Communication Hardware ', 'text_level': 1, 'page_idx': 19}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'In DeepSeek-V3, we implement the overlap between computation and communication to hide the communication latency during computation. This significantly reduces the dependency on communication bandwidth compared to serial computation and communication. However, the current communication implementation relies on expensive SMs (e.g., we allocate 20 out of the 132 SMs available in the H800 GPU for this purpose), which will limit the computational throughput. Moreover, using SMs for communication results in significant inefficiencies, as tensor cores remain entirely under-utilized. ', 'page_idx': 19}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Currently, the SMs primarily perform the following tasks for all-to-all communication: ', 'page_idx': 19}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': '‚Ä¢ Forwarding data between the IB (InfiniBand) and NVLink domain while aggregating IB traffic destined for multiple GPUs within the same node from a single GPU.   \\n‚Ä¢ Transporting data between RDMA buffers (registered GPU memory regions) and input/output buffers.   \\n‚Ä¢ Executing reduce operations for all-to-all combine.   \\n‚Ä¢ Managing fine-grained memory layout during chunked data transferring to multiple experts across the IB and NVLink domain. ', 'page_idx': 19}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'We aspire to see future vendors developing hardware that offloads these communication tasks from the valuable computation unit SM, serving as a GPU co-processor or a network co-processor like NVIDIA SHARP Graham et al. (2016). Furthermore, to reduce application programming complexity, we aim for this hardware to unify the IB (scale-out) and NVLink (scale-up) networks from the perspective of the computation units. With this unified interface, computation units can easily accomplish operations such as read, write, multicast, and reduce across the entire IB-NVLink-unified domain via submitting communication requests based on simple primitives. ', 'page_idx': 19}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '3.5.2. Compute Hardware ', 'text_level': 1, 'page_idx': 19}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'Higher FP8 GEMM Accumulation Precision in Tensor Cores. In the current Tensor Core implementation of the NVIDIA Hopper architecture, FP8 GEMM suffers from limited accumulation precision. After aligning 32 mantissa products by right-shifting based on the maximum exponent, the Tensor Core only uses the highest 14 bits of each mantissa product for addition, and truncates bits exceeding this range. The accumulation of addition results into registers also employs 14-bit precision. Our implementation partially mitigates the limitation by accumulating the addition results of 128 FP8 $\\\\times$ FP8 multiplications into registers with FP32 precision in the CUDA core. Although helpful in achieving successful FP8 training, it is merely a compromise due to the Hopper architecture‚Äôs hardware deficiency in FP8 GEMM accumulation precision. Future chips need to adopt higher precision. ', 'page_idx': 19}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Support for Tile- and Block-Wise Quantization. Current GPUs only support per-tensor quantization, lacking the native support for fine-grained quantization like our tile- and blockwise quantization. In the current implementation, when the $N_{C}$ interval is reached, the partial results will be copied from Tensor Cores to CUDA cores, multiplied by the scaling factors, and added to FP32 registers on CUDA cores. Although the dequantization overhead is significantly mitigated combined with our precise FP32 accumulation strategy, the frequent data movements between Tensor Cores and CUDA cores still limit the computational efficiency. Therefore, we recommend future chips to support fine-grained quantization by enabling Tensor Cores to receive scaling factors and implement MMA with group scaling. In this way, the whole partial sum accumulation and dequantization can be completed directly inside Tensor Cores until the final result is produced, avoiding frequent data movements. ', 'page_idx': 20}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Support for Online Quantization. The current implementations struggle to effectively support online quantization, despite its effectiveness demonstrated in our research. In the existing process, we need to read 128 BF16 activation values (the output of the previous computation) from HBM (High Bandwidth Memory) for quantization, and the quantized FP8 values are then written back to HBM, only to be read again for MMA. To address this inefficiency, we recommend that future chips integrate FP8 cast and TMA (Tensor Memory Accelerator) access into a single fused operation, so quantization can be completed during the transfer of activations from global memory to shared memory, avoiding frequent memory reads and writes. We also recommend supporting a warp-level cast instruction for speedup, which further facilitates the better fusion of layer normalization and FP8 cast. Alternatively, a near-memory computing approach can be adopted, where compute logic is placed near the HBM. In this case, BF16 elements can be cast to FP8 directly as they are read from HBM into the GPU, reducing off-chip memory access by roughly $50\\\\%$ . ', 'page_idx': 20}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Support for Transposed GEMM Operations. The current architecture makes it cumbersome to fuse matrix transposition with GEMM operations. In our workflow, activations during the forward pass are quantized into 1x128 FP8 tiles and stored. During the backward pass, the matrix needs to be read out, dequantized, transposed, re-quantized into 128x1 tiles, and stored in HBM. To reduce memory operations, we recommend future chips to enable direct transposed reads of matrices from shared memory before MMA operation, for those precisions required in both training and inference. Combined with the fusion of FP8 format conversion and TMA access, this enhancement will significantly streamline the quantization workflow. ', 'page_idx': 20}, children=[])])])]), Content(type='chapter', level=0, data={'type': 'text', 'text': '4. Pre-Training ', 'text_level': 1, 'page_idx': 20}, children=[Content(type='chapter', level=1, data={'type': 'text', 'text': '4.1. Data Construction ', 'text_level': 1, 'page_idx': 20}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond ', 'page_idx': 20}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity. Inspired by Ding et al. (2024), we implement the document packing method for data integrity but do not incorporate cross-sample attention masking during training. Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer. ', 'page_idx': 21}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'In the training process of DeepSeekCoder-V2 (DeepSeek-AI, 2024a), we observe that the Fill-in-Middle (FIM) strategy does not compromise the next-token prediction capability while enabling the model to accurately predict middle text based on contextual cues. In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3. To be specific, we employ the Prefix-Suffix-Middle (PSM) framework to structure data as follows: ', 'page_idx': 21}, children=[]), Content(type='equation', level=2, data={'type': 'equation', 'text': '$$\\n<|\\\\mathbf{f}\\\\mathrm{i}\\\\mathrm{i}\\\\mathrm{m}_{-}\\\\mathrm{begin}|>f_{\\\\mathrm{pre}}<|\\\\mathbf{f}\\\\mathrm{i}\\\\mathrm{i}\\\\mathrm{m}_{-}\\\\mathrm{hole}|>f_{\\\\mathrm{suf}}<|\\\\mathbf{f}\\\\mathrm{i}\\\\mathrm{i}\\\\mathrm{m}_{-}\\\\mathrm{end}|>f_{\\\\mathrm{middle}}<|\\\\mathbf{e}\\\\mathrm{os}_{-}\\\\mathbf{t}\\\\mathrm{oken}|>.\\n$$', 'text_format': 'latex', 'page_idx': 21}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'This structure is applied at the document level as a part of the pre-packing process. The FIM strategy is applied at a rate of 0.1, consistent with the PSM framework. ', 'page_idx': 21}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens. The pretokenizer and training data for our tokenizer are modified to optimize multilingual compression efficiency. In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks. However, this trick may introduce the token boundary bias (Lundberg, 2023) when the model processes multi-line prompts without terminal line breaks, particularly for few-shot evaluation prompts. To address this issue, we randomly split a certain proportion of such combined tokens during training, which exposes the model to a wider array of special cases and mitigates this bias. ', 'page_idx': 21}, children=[])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '4.2. Hyper-Parameters ', 'text_level': 1, 'page_idx': 21}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'Model Hyper-Parameters. We set the number of Transformer layers to 61 and the hidden dimension to 7168. All learnable parameters are randomly initialized with a standard deviation of 0.006. In MLA, we set the number of attention heads $n_{h}$ to 128 and the per-head dimension $d_{h}$ to 128. The KV compression dimension $d_{c}$ is set to 512, and the query compression dimension $d_{c}^{\\\\prime}$ is set to 1536. For the decoupled queries and key, we set the per-head dimension $d_{h}^{R}$ to 64. We substitute all FFNs except for the first three layers with MoE layers. Each MoE layer consists of 1 shared expert and 256 routed experts, where the intermediate hidden dimension of each expert is 2048. Among the routed experts, 8 experts will be activated for each token, and each token will be ensured to be sent to at most 4 nodes. The multi-token prediction depth $D$ is set to 1, i.e., besides the exact next token, each token will predict one additional token. As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token. ', 'page_idx': 21}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'Training Hyper-Parameters. We employ the AdamW optimizer (Loshchilov and Hutter, 2017) with hyper-parameters set to $\\\\beta_{1}=0.9$ , $\\\\beta_{2}=0.95$ , and weight_decay $=0.1$ . We set the maximum sequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens. As for the learning rate scheduling, we first linearly increase it from 0 to $2.2\\\\times10^{-4}$ during the first 2K steps. Then, we keep a constant learning rate of $2.2\\\\times10^{-4}$ until the model consumes 10T training tokens. Subsequently, we gradually decay the learning rate to $2.2\\\\times10^{-5}$ in $4.3\\\\mathrm{T}$ tokens, following a cosine decay curve. During the training of the final 500B tokens, we keep a constant learning rate of $2.2\\\\times10^{-5}$ in the first 333B tokens, and switch to another constant learning rate of $7.3\\\\times10^{-6}$ in the remaining 167B tokens. The gradient clipping norm is set to 1.0. We employ a batch size scheduling strategy, where the batch size is gradually increased from 3072 to 15360 in the training of the first 469B tokens, and then keeps 15360 in the remaining training. We leverage pipeline parallelism to deploy different layers of a model on different GPUs, and for each layer, the routed experts will be uniformly deployed on 64 GPUs belonging to 8 nodes. As for the node-limited routing, each token will be sent to at most 4 nodes (i.e., $M=4$ ). For auxiliary-loss-free load balancing, we set the bias update speed ùõæ to 0.001 for the first $14.3\\\\mathrm{T}$ tokens, and to 0.0 for the remaining 500B tokens. For the balance loss, we set $\\\\alpha$ to 0.0001, just to avoid extreme imbalance within any single sequence. The MTP loss weight $\\\\lambda$ is set to 0.3 for the first 10T tokens, and to 0.1 for the remaining 4.8T tokens. ', 'page_idx': 21}, children=[]), Content(type='image', level=2, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/994846ca2a80bbe6297f6a9f75672fcf45337ab0224fb66ddde155a093c48072.jpg', 'img_caption': ['Figure 8 | Evaluation results on the ‚ÄùNeedle In A Haystack‚Äù (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to 128K. '], 'img_footnote': [], 'page_idx': 22}, children=[])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '4.3. Long Context Extension ', 'text_level': 1, 'page_idx': 22}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K. The YaRN configuration is consistent with that used in DeepSeek-V2, being applied exclusively to the decoupled shared key $\\\\mathbf{k}_{t}^{R}$ . The hyper-parameters remain identical across both phases, with the scale $s=40$ , $\\\\alpha=1$ , $\\\\beta=32$ , and the scaling factor $\\\\sqrt{t}=0.1\\\\ln s+1.$ . In the first phase, the sequence length is set to 32K, and the batch size is 1920. During the second phase, the sequence length is increased to $128\\\\mathsf{K},$ and the batch size is reduced to 480. The learning rate for both phases is set to $7.3\\\\times10^{-6}$ , matching the final learning rate from the pre-training stage. ', 'page_idx': 22}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'Through this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to 128K in length while maintaining strong performance. Figure 8 illustrates that DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the \"Needle In A Haystack\" (NIAH) test, demonstrating consistent robustness across context window lengths up to 128K. ', 'page_idx': 22}, children=[])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '4.4. Evaluations ', 'text_level': 1, 'page_idx': 23}, children=[Content(type='chapter', level=2, data={'type': 'text', 'text': '4.4.1. Evaluation Benchmarks ', 'text_level': 1, 'page_idx': 23}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'The base model of DeepSeek-V3 is pretrained on a multilingual corpus with English and Chinese constituting the majority, so we evaluate its performance on a series of benchmarks primarily in English and Chinese, as well as on a multilingual benchmark. Our evaluation is based on our internal evaluation framework integrated in our HAI-LLM framework. Considered benchmarks are categorized and listed as follows, where underlined benchmarks are in Chinese and double-underlined benchmarks are multilingual ones: ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Multi-subject multiple-choice datasets include MMLU (Hendrycks et al., 2020), MMLURedux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024b), MMMLU (OpenAI, 2024b), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023). ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Language understanding and reasoning datasets include HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and BigBench Hard (BBH) (Suzgun et al., 2022). ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Closed-book question answering datasets include TriviaQA (Joshi et al., 2017) and NaturalQuestions (Kwiatkowski et al., 2019). ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Reading comprehension datasets include RACE Lai et al. (2017), DROP (Dua et al., 2019), 3 (Sun et al., 2019a), and CMRC (Cui et al., 2019). ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Reference disambiguation datasets include CLUEWSC (Xu et al., 2020) and WinoGrande Sakaguchi et al. (2019). ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Language modeling datasets include Pile (Gao et al., 2020). ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Chinese understanding and culture datasets include CCPM (Li et al., 2021). ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Math datasets include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MGSM (Shi et al., 2023), and CMath (Wei et al., 2023). ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Code datasets include HumanEval (Chen et al., 2021), LiveCodeBench-Base (0801-1101) (Jain et al., 2024), MBPP (Austin et al., 2021), and CRUXEval (Gu et al., 2024). ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Standardized exams include AGIEval (Zhong et al., 2023). Note that AGIEval includes both English and Chinese subsets. ', 'page_idx': 23}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Following our previous work (DeepSeek-AI, $2024\\\\mathrm{b},\\\\mathrm{c})$ ), we adopt perplexity-based evaluation for datasets including HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High, MMLU, MMLU-Redux, MMLU-Pro, MMMLU, ARC-Easy, ARC-Challenge, C-Eval, CMMLU, C3, and CCPM, and adopt generation-based evaluation for TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, MGSM, HumanEval, MBPP, LiveCodeBench-Base, CRUXEval, BBH, AGIEval, CLUEWSC, CMRC, and CMath. In addition, we perform language-modeling-based evaluation for Pile-test and use Bits-Per-Byte (BPB) as the metric to guarantee fair comparison among models using different tokenizers. ', 'page_idx': 23}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '4.4.2. Evaluation Results ', 'text_level': 1, 'page_idx': 23}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B Base (Qwen, 2024b), and LLaMA-3.1 405B Base (AI@Meta, 2024b). We evaluate all these models with our internal evaluation framework, and ensure that they share the same evaluation setting. Note that due to the changes in our evaluation framework over the past months, the performance of DeepSeek-V2-Base exhibits a slight difference from our previously reported results. Overall, DeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base, and surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the strongest open-source model. ', 'page_idx': 23}, children=[]), Content(type='table', level=3, data={'type': 'table', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/6a84148e6f9ec21234912515ca7454f225a10cccedef7f00755b2287a605e537.jpg', 'table_caption': ['Table 3 Comparison among DeepSeek-V3-Base and other representative open-source base models. All models are evaluated in our internal framework and share the same evaluation setting. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeekV3-Base achieves the best performance on most benchmarks, especially on math and code tasks. '], 'table_footnote': [], 'table_body': '\\n\\n<html><body><table><tr><td></td><td>Benchmark (Metric)</td><td># Shots</td><td>DeepSeek-V2 Base</td><td>Qwen2.5 72B Base</td><td>LLaMA-3.1 405B Base</td><td>DeepSeek-V3 Base</td></tr><tr><td rowspan=\"5\"></td><td>Architecture</td><td></td><td>MoE</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td># Activated Params</td><td></td><td>21B</td><td>72B</td><td>405B</td><td>37B</td></tr><tr><td># Total Params</td><td></td><td>236B</td><td>72B</td><td>405B</td><td>671B</td></tr><tr><td>Pile-test (BPB)</td><td></td><td>0.606</td><td>0.638</td><td>0.542</td><td>0.548</td></tr><tr><td>BBH (EM) MMLU (EM)</td><td>3-shot</td><td>78.8</td><td>79.8</td><td>82.9</td><td></td></tr><tr><td rowspan=\"12\">English</td><td></td><td>5-shot</td><td>78.4</td><td>85.0</td><td>84.4</td><td>87.5 87.1</td></tr><tr><td>MMLU-Redux (EM)</td><td>5-shot</td><td>75.6</td><td>83.2</td><td>81.3</td><td></td></tr><tr><td>MMLU-Pro (EM)</td><td>5-shot</td><td>51.4</td><td>58.3</td><td></td><td>86.2</td></tr><tr><td></td><td></td><td>80.4</td><td>80.6</td><td>52.8</td><td>64.4</td></tr><tr><td>DROP (F1) ARC-Easy (EM)</td><td>3-shot 25-shot</td><td>97.6</td><td>98.4</td><td>86.0</td><td>89.0</td></tr><tr><td>ARC-Challenge (EM)</td><td>25-shot</td><td>92.2</td><td>94.5</td><td>98.4 95.3</td><td>98.9</td></tr><tr><td>HellaSwag (EM)</td><td>10-shot</td><td>87.1</td><td>84.8</td><td>89.2</td><td>95.3 88.9</td></tr><tr><td>PIQA (EM)</td><td>O-shot</td><td>83.9</td><td>82.6</td><td>85.9</td><td>84.7</td></tr><tr><td>WinoGrande (EM)</td><td>5-shot</td><td>86.3</td><td>82.3</td><td>85.2</td><td></td></tr><tr><td>RACE-Middle (EM)</td><td>5-shot</td><td>73.1</td><td>68.1</td><td>74.2</td><td>84.9</td></tr><tr><td>RACE-High (EM)</td><td>5-shot</td><td>52.6</td><td>50.3</td><td>56.8</td><td>67.1 51.3</td></tr><tr><td>TriviaQA (EM)</td><td>5-shot</td><td>80.0</td><td>71.9</td><td>82.7</td><td>82.9</td></tr><tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>38.6</td><td>33.2</td><td>41.5</td><td>40.0</td></tr><tr><td>AGIEval (EM)</td><td>O-shot</td><td>57.5</td><td>75.8</td><td>60.6</td><td>79.6</td></tr><tr><td>HumanEval (Pass@1) Code</td><td>O-shot</td><td>43.3</td><td>53.0</td><td>54.9</td><td>65.2</td></tr><tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>65.0</td><td>72.6</td><td>68.4</td><td>75.4</td></tr><tr><td>LiveCodeBench-Base (Pass@1) CRUXEval-I (EM)</td><td>3-shot</td><td>11.6</td><td>12.9</td><td>15.5</td><td>19.4</td></tr><tr><td></td><td></td><td>2-shot 52.5</td><td>59.1</td><td>58.5</td><td>67.3</td><td></td></tr><tr><td rowspan=\"4\">Math</td><td>CRUXEval-O (EM)</td><td>2-shot</td><td>49.8</td><td>59.9</td><td>59.9</td><td>69.8</td></tr><tr><td>GSM8K (EM)</td><td>8-shot</td><td>81.6</td><td>88.3</td><td>83.5</td><td>89.3</td></tr><tr><td>MATH (EM)</td><td>4-shot</td><td>43.4</td><td>54.4</td><td>49.0</td><td>61.6</td></tr><tr><td>MGSM (EM)</td><td>8-shot</td><td>63.6</td><td>76.2</td><td>69.9</td><td>79.8</td></tr><tr><td rowspan=\"6\">Chinese</td><td>CMath (EM)</td><td>3-shot</td><td>78.7</td><td>84.5</td><td>77.3</td><td>90.7</td></tr><tr><td>CLUEWSC (EM)</td><td>5-shot</td><td>82.0</td><td>82.5</td><td>83.0</td><td>82.7</td></tr><tr><td>C-Eval (EM)</td><td>5-shot</td><td>81.4</td><td>89.2</td><td>72.5</td><td>90.1</td></tr><tr><td>CMMLU (EM)</td><td>5-shot</td><td>84.0</td><td>89.5</td><td>73.7</td><td>88.8</td></tr><tr><td>CMRC (EM)</td><td>1-shot</td><td>77.4</td><td>75.8</td><td>76.0</td><td>76.3</td></tr><tr><td>C3 (EM) CCPM (EM)</td><td>O-shot O-shot</td><td>77.4 93.0</td><td>76.7 88.5</td><td>79.7 78.6</td><td>78.6 92.0</td></tr><tr><td>Multilingual</td><td>MMMLU-non-English (EM)</td><td>5-shot</td><td>64.0</td><td>74.8</td><td>73.8</td><td>79.4</td></tr></table></body></html>\\n\\n', 'page_idx': 24}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'From a more detailed perspective, we compare DeepSeek-V3-Base with the other open-source base models individually. (1) Compared with DeepSeek-V2-Base, due to the improvements in our model architecture, the scale-up of the model size and training tokens, and the enhancement of data quality, DeepSeek-V3-Base achieves significantly better performance as expected. (2) Compared with Qwen2.5 72B Base, the state-of-the-art Chinese open-source model, with only half of the activated parameters, DeepSeek-V3-Base also demonstrates remarkable advantages, especially on English, multilingual, code, and math benchmarks. As for Chinese benchmarks, except for CMMLU, a Chinese multi-subject multiple-choice task, DeepSeek-V3-Base also shows better performance than Qwen2.5 72B. (3) Compared with LLaMA-3.1 405B Base, the largest open-source model with 11 times the activated parameters, DeepSeek-V3-Base also exhibits much better performance on multilingual, code, and math benchmarks. As for English and Chinese language benchmarks, DeepSeek-V3-Base shows competitive or better performance, and is especially good on BBH, MMLU-series, DROP, C-Eval, CMMLU, and CCPM. ', 'page_idx': 24}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Due to our efficient architectures and comprehensive engineering optimizations, DeepSeekV3 achieves extremely high training efficiency. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models. ', 'page_idx': 25}, children=[]), Content(type='table', level=3, data={'type': 'table', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/8db174ccb145521f9496f42bfc1c6000bf6fbe09e88bc54c229f4c8b0b183f79.jpg', 'table_caption': [], 'table_footnote': [], 'table_body': '\\n\\n<html><body><table><tr><td>Benchmark (Metric)</td><td># Shots</td><td>Small MoE Baseline</td><td>Small MoE w/ MTP</td><td>Large MoE Baseline</td><td>Large MoE w/ MTP</td></tr><tr><td># Activated Params (Inference)</td><td>‰∏Ä</td><td>2.4B</td><td>2.4B</td><td>20.9B</td><td>20.9B</td></tr><tr><td># Total Params (Inference)</td><td>-</td><td>15.7B</td><td>15.7B</td><td>228.7B</td><td>228.7B</td></tr><tr><td># Training Tokens</td><td>-</td><td>1.33T</td><td>1.33T</td><td>540B</td><td>540B</td></tr><tr><td>Pile-test (BPB)</td><td>‰∏Ä</td><td>0.729</td><td>0.729</td><td>0.658</td><td>0.657</td></tr><tr><td>BBH (EM)</td><td>3-shot</td><td>39.0</td><td>41.4</td><td>70.0</td><td>70.7</td></tr><tr><td>MMLU (EM)</td><td>5-shot</td><td>50.0</td><td>53.3</td><td>67.5</td><td>66.6</td></tr><tr><td>DROP (F1)</td><td>1-shot</td><td>39.2</td><td>41.3</td><td>68.5</td><td>70.6</td></tr><tr><td>TriviaQA (EM)</td><td>5-shot</td><td>56.9</td><td>57.7</td><td>67.0</td><td>67.3</td></tr><tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>22.7</td><td>22.3</td><td>27.2</td><td>28.5</td></tr><tr><td>HumanEval (Pass@1)</td><td>O-shot</td><td>20.7</td><td>26.8</td><td>44.5</td><td>53.7</td></tr><tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>35.8</td><td>36.8</td><td>61.6</td><td>62.2</td></tr><tr><td>GSM8K (EM)</td><td>8-shot</td><td>25.4</td><td>31.4</td><td>72.3</td><td>74.0</td></tr><tr><td>MATH (EM)</td><td>4-shot</td><td>10.7</td><td>12.6</td><td>38.6</td><td>39.8</td></tr></table></body></html>\\n\\n', 'page_idx': 25}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Table 4 | Ablation results for the MTP strategy. The MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. ', 'page_idx': 25}, children=[])])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '4.5. Discussion ', 'text_level': 1, 'page_idx': 25}, children=[Content(type='chapter', level=2, data={'type': 'text', 'text': '4.5.1. Ablation Studies for Multi-Token Prediction ', 'text_level': 1, 'page_idx': 25}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'In Table 4, we show the ablation results for the MTP strategy. To be specific, we validate the MTP strategy on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising 228.7B total parameters on 540B tokens. On top of them, keeping the training data and the other architectures the same, we append a 1-depth MTP module onto them and train two models with the MTP strategy for comparison. Note that during inference, we directly discard the MTP module, so the inference costs of the compared models are exactly the same. From the table, we can observe that the MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. ', 'page_idx': 25}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy ', 'text_level': 1, 'page_idx': 25}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'In Table 5, we show the ablation results for the auxiliary-loss-free balancing strategy. We validate this strategy on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising 228.7B total parameters on 578B tokens. ', 'page_idx': 25}, children=[]), Content(type='table', level=3, data={'type': 'table', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0b5c2bac8bb47af03f3d59035131c1afeb68ab2f2c7c43b749aff4fa45b47bd9.jpg', 'table_caption': ['Table 5 | Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. '], 'table_footnote': [], 'table_body': '\\n\\n<html><body><table><tr><td>Benchmark (Metric)</td><td># Shots</td><td>Small MoE Aux-Loss-Based</td><td>Small MoE Aux-Loss-Free</td><td>Large MoE Aux-Loss-Based</td><td>Large MoE Aux-Loss-Free</td></tr><tr><td># Activated Params</td><td></td><td>2.4B</td><td>2.4B</td><td>20.9B</td><td>20.9B</td></tr><tr><td># Total Params</td><td></td><td>15.7B</td><td>15.7B</td><td>228.7B</td><td>228.7B</td></tr><tr><td># Training Tokens</td><td>-</td><td>1.33T</td><td>1.33T</td><td>578B</td><td>578B</td></tr><tr><td>Pile-test (BPB)</td><td></td><td>0.727</td><td>0.724</td><td>0.656</td><td>0.652</td></tr><tr><td>BBH (EM)</td><td>3-shot</td><td>37.3</td><td>39.3</td><td>66.7</td><td>67.9</td></tr><tr><td>MMLU (EM)</td><td>5-shot</td><td>51.0</td><td>51.8</td><td>68.3</td><td>67.2</td></tr><tr><td>DROP (F1)</td><td>1-shot</td><td>38.1</td><td>39.0</td><td>67.1</td><td>67.1</td></tr><tr><td>TriviaQA (EM)</td><td>5-shot</td><td>58.3</td><td>58.5</td><td>66.7</td><td>67.7</td></tr><tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>23.2</td><td>23.4</td><td>27.1</td><td>28.1</td></tr><tr><td>HumanEval (Pass@1)</td><td>O-shot</td><td>22.0</td><td>22.6</td><td>40.2</td><td>46.3</td></tr><tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>36.6</td><td>35.8</td><td>59.2</td><td>61.2</td></tr><tr><td>GSM8K (EM)</td><td>8-shot</td><td>27.1</td><td>29.6</td><td>70.7</td><td>74.5</td></tr><tr><td>MATH (EM)</td><td>4-shot</td><td>10.9</td><td>11.1</td><td>37.2</td><td>39.6</td></tr></table></body></html>\\n\\n', 'page_idx': 26}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Both of the baseline models purely use auxiliary losses to encourage load balance, and use the sigmoid gating function with top-K affinity normalization. Their hyper-parameters to control the strength of auxiliary losses are the same as DeepSeek-V2-Lite and DeepSeek-V2, respectively. On top of these two baseline models, keeping the training data and the other architectures the same, we remove all auxiliary losses and introduce the auxiliary-loss-free balancing strategy for comparison. From the table, we can observe that the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. ', 'page_idx': 26}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance ', 'text_level': 1, 'page_idx': 26}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'The key distinction between auxiliary-loss-free balancing and sequence-wise auxiliary loss lies in their balancing scope: batch-wise versus sequence-wise. Compared with the sequence-wise auxiliary loss, batch-wise balancing imposes a more flexible constraint, as it does not enforce in-domain balance on each sequence. This flexibility allows experts to better specialize in different domains. To validate this, we record and analyze the expert load of a 16B auxiliaryloss-based baseline and a 16B auxiliary-loss-free model on different domains in the Pile test set. As illustrated in Figure 9, we observe that the auxiliary-loss-free model demonstrates greater expert specialization patterns as expected. ', 'page_idx': 26}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'To further investigate the correlation between this flexibility and the advantage in model performance, we additionally design and validate a batch-wise auxiliary loss that encourages load balance on each training batch instead of on each sequence. The experimental results show that, when achieving a similar level of batch-wise load balance, the batch-wise auxiliary loss can also achieve similar model performance to the auxiliary-loss-free method. To be specific, in our experiments with 1B MoE models, the validation losses are: 2.258 (using a sequencewise auxiliary loss), 2.253 (using the auxiliary-loss-free method), and 2.253 (using a batch-wise auxiliary loss). We also observe similar results on 3B MoE models: the model using a sequencewise auxiliary loss achieves a validation loss of 2.085, and the models using the auxiliary-loss-free method or a batch-wise auxiliary loss achieve the same validation loss of 2.080. ', 'page_idx': 26}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'In addition, although the batch-wise load balancing methods show consistent performance advantages, they also face two potential challenges in efficiency: (1) load imbalance within certain sequences or small batches, and (2) domain-shift-induced load imbalance during inference. The first challenge is naturally addressed by our training framework that uses large-scale expert parallelism and data parallelism, which guarantees a large size of each micro-batch. For the second challenge, we also design and implement an efficient inference framework with redundant expert deployment, as described in Section 3.4, to overcome it. ', 'page_idx': 26}, children=[]), Content(type='image', level=3, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/4e04a59983212cc4b1707f1db88fcc93a62807adca76c455ef42c532b2c10714.jpg', 'img_caption': ['Figure 9 | Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. Due to space constraints, we only present the results of two layers as an example, with the results of all layers provided in Appendix C. '], 'img_footnote': [], 'page_idx': 27}, children=[])])])]), Content(type='chapter', level=0, data={'type': 'text', 'text': '5. Post-Training ', 'text_level': 1, 'page_idx': 27}, children=[Content(type='chapter', level=1, data={'type': 'text', 'text': '5.1. Supervised Fine-Tuning ', 'text_level': 1, 'page_idx': 27}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'We curate our instruction-tuning datasets to include 1.5M instances spanning multiple domains, with each domain employing distinct data creation methods tailored to its specific requirements. ', 'page_idx': 27}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model. Specifically, while the R1-generated data demonstrates strong accuracy, it suffers from issues such as overthinking, poor formatting, and excessive length. Our objective is to balance the high accuracy of R1-generated reasoning data and the clarity and conciseness of regularly formatted reasoning data. ', 'page_idx': 27}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'To establish our methodology, we begin by developing an expert model tailored to a specific domain, such as code, mathematics, or general reasoning, using a combined Supervised FineTuning (SFT) and Reinforcement Learning (RL) training pipeline. This expert model serves as a data generator for the final model. The training process involves generating two distinct types of SFT samples for each instance: the first couples the problem with its original response in the format of <problem, original response>, while the second incorporates a system prompt alongside the problem and the R1 response in the format of <system prompt, problem, R1 response>. ', 'page_idx': 27}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'The system prompt is meticulously designed to include instructions that guide the model toward producing responses enriched with mechanisms for reflection and verification. During the RL phase, the model leverages high-temperature sampling to generate responses that integrate patterns from both the R1-generated and original data, even in the absence of explicit system prompts. After hundreds of RL steps, the intermediate RL model learns to incorporate R1 patterns, thereby enhancing overall performance strategically. ', 'page_idx': 28}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'Upon completing the RL training phase, we implement rejection sampling to curate highquality SFT data for the final model, where the expert models are used as data generation sources. This method ensures that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective. ', 'page_idx': 28}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data. ', 'page_idx': 28}, children=[]), Content(type='text', level=2, data={'type': 'text', 'text': 'SFT Settings. We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at $5\\\\times10^{-6}$ and gradually decreases to $1\\\\times\\\\dot{1}0^{-6}$ . During training, each single sequence is packed from multiple samples. However, we adopt a sample masking strategy to ensure that these examples remain isolated and mutually invisible. ', 'page_idx': 28}, children=[])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '5.2. Reinforcement Learning ', 'text_level': 1, 'page_idx': 28}, children=[Content(type='chapter', level=2, data={'type': 'text', 'text': '5.2.1. Reward Model ', 'text_level': 1, 'page_idx': 28}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'We employ a rule-based Reward Model (RM) and a model-based RM in our RL process. ', 'page_idx': 28}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Rule-Based RM. For questions that can be validated using specific rules, we adopt a rulebased reward system to determine the feedback. For instance, certain math problems have deterministic results, and we require the model to provide the final answer within a designated format (e.g., in a box), allowing us to apply rules to verify the correctness. Similarly, for LeetCode problems, we can utilize a compiler to generate feedback based on test cases. By leveraging rule-based validation wherever possible, we ensure a higher level of reliability, as this approach is resistant to manipulation or exploitation. ', 'page_idx': 28}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Model-Based RM. For questions with free-form ground-truth answers, we rely on the reward model to determine whether the response matches the expected ground-truth. Conversely, for questions without a definitive ground-truth, such as those involving creative writing, the reward model is tasked with providing feedback based on the question and the corresponding answer as inputs. The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its reliability, we construct preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward. This approach helps mitigate the risk of reward hacking in specific tasks. ', 'page_idx': 28}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '5.2.2. Group Relative Policy Optimization ', 'text_level': 1, 'page_idx': 29}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'Similar to DeepSeek-V2 (DeepSeek-AI, $2024\\\\mathrm{c}$ ), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q,$ GRPO samples a group of outputs $\\\\{o1,o2,\\\\cdots,o_{G}\\\\}$ from the old policy model $\\\\pi_{\\\\theta_{o l d}}$ and then optimizes the policy model $\\\\scriptstyle\\\\pi_{\\\\theta}$ by maximizing the following objective: ', 'page_idx': 29}, children=[]), Content(type='equation', level=3, data={'type': 'equation', 'text': '$$\\n\\\\begin{array}{l}{\\\\displaystyle\\\\mathcal{G}_{G R P O}(\\\\theta)=\\\\mathbb{E}[q\\\\sim P(Q),\\\\{\\\\boldsymbol{o}_{i}\\\\}_{i=1}^{G}\\\\sim\\\\pi_{\\\\theta_{o d d}}(O|{q})]}\\\\\\\\ {\\\\displaystyle\\\\frac{1}{G}\\\\sum_{i=1}^{G}\\\\left(\\\\operatorname*{min}\\\\left(\\\\frac{\\\\pi_{\\\\theta}(\\\\boldsymbol{o}_{i}|{q})}{\\\\pi_{\\\\theta_{o d d}}(\\\\boldsymbol{o}_{i}|{q})}A_{i},\\\\mathrm{clip}\\\\left(\\\\frac{\\\\pi_{\\\\theta}(\\\\boldsymbol{o}_{i}|{q})}{\\\\pi_{\\\\theta_{o d d}}(\\\\boldsymbol{o}_{i}|{q})},1-\\\\varepsilon,1+\\\\varepsilon\\\\right)A_{i}\\\\right)-\\\\beta\\\\mathbb{D}_{K L}\\\\left(\\\\pi_{\\\\theta}||\\\\boldsymbol{\\\\pi}_{r e f}\\\\right)\\\\right),}\\\\end{array}\\n$$', 'text_format': 'latex', 'page_idx': 29}, children=[]), Content(type='equation', level=3, data={'type': 'equation', 'text': '$$\\n\\\\mathbb{D}_{K L}\\\\left(\\\\pi_{\\\\theta}||\\\\pi_{r e f}\\\\right)=\\\\frac{\\\\pi_{r e f}(o_{i}|q)}{\\\\pi_{\\\\theta}(o_{i}|q)}-\\\\log\\\\frac{\\\\pi_{r e f}(o_{i}|q)}{\\\\pi_{\\\\theta}(o_{i}|q)}-1,\\n$$', 'text_format': 'latex', 'page_idx': 29}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'where $\\\\varepsilon$ and $\\\\beta$ are hyper-parameters; $\\\\pi_{r e f}$ is the reference model; and $A_{i}$ is the advantage, derived from the rewards $\\\\{r_{1},r_{2},\\\\hdots,r_{G}\\\\}$ corresponding to the outputs within each group: ', 'page_idx': 29}, children=[]), Content(type='equation', level=3, data={'type': 'equation', 'text': '$$\\nA_{i}={\\\\frac{r_{i}-\\\\operatorname*{mean}(\\\\{r_{1},r_{2},\\\\cdot\\\\cdot\\\\cdot,r_{G}\\\\})}{\\\\operatorname{std}(\\\\{r_{1},r_{2},\\\\cdot\\\\cdot\\\\cdot,r_{G}\\\\})}}.\\n$$', 'text_format': 'latex', 'page_idx': 29}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process. This approach not only aligns the model more closely with human preferences but also enhances performance on benchmarks, especially in scenarios where available SFT data are limited. ', 'page_idx': 29}, children=[])])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '5.3. Evaluations ', 'text_level': 1, 'page_idx': 29}, children=[Content(type='chapter', level=2, data={'type': 'text', 'text': '5.3.1. Evaluation Settings ', 'text_level': 1, 'page_idx': 29}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'Evaluation Benchmarks. Apart from the benchmark we used for base model testing, we further evaluate instructed models on IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), LongBench v2 (Bai et al., 2024), GPQA (Rein et al., 2023), SimpleQA (OpenAI, 2024c), CSimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (questions from August 2024 to November 2024), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). ', 'page_idx': 29}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Compared Baselines. We conduct comprehensive evaluations of our chat model against several strong baselines, including DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct, LLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, and GPT-4o-0513. For the DeepSeek-V2 model series, we select the most representative variants for comparison. For closed-source models, evaluations are performed through their respective APIs. ', 'page_idx': 29}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Detailed Evaluation Configurations. For standard benchmarks including MMLU, DROP, GPQA, and SimpleQA, we adopt the evaluation prompts from the simple-evals framework4. ', 'page_idx': 29}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'We utilize the Zero-Eval prompt format (Lin, 2024) for MMLU-Redux in a zero-shot setting. For other datasets, we follow their original evaluation protocols with default prompts as provided by the dataset creators. For code and math benchmarks, the HumanEval-Mul dataset includes 8 mainstream programming languages (Python, Java, Cpp, C#, JavaScript, TypeScript, PHP, and Bash) in total. We use CoT and non-CoT methods to evaluate model performance on LiveCodeBench, where the data are collected from August 2024 to November 2024. The Codeforces dataset is measured using the percentage of competitors. SWE-Bench verified is evaluated using the agentless framework (Xia et al., 2024). We use the ‚Äúdiff‚Äù format to evaluate the Aider-related benchmarks. For mathematical assessments, AIME and CNMO 2024 are evaluated with a temperature of 0.7, and the results are averaged over 16 runs, while MATH-500 employs greedy decoding. We allow all models to output a maximum of 8192 tokens for each benchmark. ', 'page_idx': 30}, children=[]), Content(type='table', level=3, data={'type': 'table', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/7a76ee39b10dd3db2c84be68fc0de9cf3dcadd9496d72fedd38eceabaa452b9c.jpg', 'table_caption': [], 'table_footnote': [], 'table_body': '\\n\\n<html><body><table><tr><td>Benchmark (Metric)</td><td></td><td>DeepSeek DeepSeek|Qwen2.5 LLaMA-3.1 Claude-3.5- GPT-4o|DeepSeek V2-0506</td><td>V2.5-0905</td><td>72B-Inst. 405B-Inst. Sonnet-1022</td><td></td><td></td><td>0513</td><td>V3</td></tr><tr><td rowspan=\"4\"></td><td>Architecture</td><td>MoE</td><td>MoE</td><td>Dense</td><td>Dense</td><td></td><td></td><td>MoE</td></tr><tr><td># Activated Params</td><td>21B</td><td>21B</td><td>72B</td><td>405B</td><td></td><td></td><td>37B</td></tr><tr><td># Total Params</td><td>236B</td><td>236B</td><td>72B</td><td>405B</td><td></td><td></td><td>671B</td></tr><tr><td>MMLU (EM)</td><td>78.2</td><td>80.6</td><td>85.3</td><td>88.6</td><td>88.3</td><td>87.2</td><td>88.5</td></tr><tr><td rowspan=\"10\">English</td><td>MMLU-Redux (EM)</td><td>77.9</td><td>80.3</td><td>85.6</td><td>86.2</td><td>88.9</td><td>88.0</td><td>89.1</td></tr><tr><td>MMLU-Pro (EM)</td><td>58.5</td><td>66.2</td><td>71.6</td><td>73.3</td><td>78.0</td><td>72.6</td><td>75.9</td></tr><tr><td>DROP (3-shot F1)</td><td>83.0</td><td></td><td>76.7</td><td></td><td></td><td></td><td></td></tr><tr><td>IF-Eval (Prompt Strict)</td><td>57.7</td><td>87.8 80.6</td><td>84.1</td><td>88.7 86.0</td><td>88.3 86.5</td><td>83.7 84.3</td><td>91.6 86.1</td></tr><tr><td>GPQA-Diamond (Pass@1)</td><td>35.3</td><td>41.3</td><td>49.0</td><td>51.1</td><td>65.0</td><td>49.9</td><td>59.1</td></tr><tr><td>SimpleQA (Correct)</td><td>9.0</td><td>10.2</td><td>9.1</td><td>17.1</td><td>28.4</td><td>38.2</td><td>24.9</td></tr><tr><td>FRAMES (Acc.)</td><td>66.9</td><td>65.4</td><td>69.8</td><td>70.0</td><td>72.5</td><td>80.5</td><td>73.3</td></tr><tr><td>LongBench v2 (Acc.)</td><td>31.6</td><td>35.4</td><td>39.4</td><td>36.1</td><td>41.0</td><td>48.1</td><td>48.7</td></tr><tr><td>HumanEval-Mul (Pass@1)</td><td>69.3</td><td>77.4</td><td>77.3</td><td></td><td></td><td></td><td></td></tr><tr><td>LiveCodeBench (Pass@1-COT)</td><td>18.8</td><td>29.2</td><td>31.1</td><td>77.2 28.4</td><td>81.7 36.3</td><td>80.5 33.4</td><td>82.6 40.5</td></tr><tr><td>Code LiveCodeBench (Pass@1)</td><td></td><td>20.3</td><td>28.4</td><td>28.7</td><td>30.1</td><td>32.8</td><td>34.2</td><td>37.6</td></tr><tr><td>Codeforces (Percentile)</td><td>17.5</td><td>35.6</td><td>24.8</td><td></td><td>25.3</td><td>20.3</td><td>23.6</td><td>51.6</td></tr><tr><td>SWE Verified (Resolved)</td><td></td><td>22.6</td><td>23.8</td><td></td><td>24.5</td><td>50.8</td><td>38.8</td><td>42.0</td></tr><tr><td>Aider-Edit (Acc.)</td><td>60.3</td><td>71.6</td><td>65.4</td><td></td><td>63.9</td><td>84.2</td><td>72.9</td><td>79.7</td></tr><tr><td>Aider-Polyglot (Acc.)</td><td>‰∏Ä</td><td>18.2</td><td>7.6</td><td>5.8</td><td></td><td>45.3</td><td>16.0</td><td>49.6</td></tr><tr><td rowspan=\"3\"></td><td>AIME 2024 (Pass@1)</td><td>4.6</td><td>16.7</td><td>23.3</td><td>23.3</td><td>16.0</td><td></td><td></td><td></td></tr><tr><td>Math MATH-500 (EM)</td><td>56.3</td><td>74.7</td><td>80.0</td><td>73.8</td><td>78.3</td><td></td><td>9.3 74.6</td><td>39.2</td></tr><tr><td>CNMO 2024 (Pass@1)</td><td>2.8</td><td>10.8</td><td>15.9</td><td>6.8</td><td>13.1</td><td></td><td>10.8</td><td>90.2 43.2</td></tr><tr><td rowspan=\"3\">Chinese C-Eval (EM)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CLUEWSC (EM)</td><td>89.9</td><td>90.4</td><td>91.4</td><td>84.7</td><td></td><td>85.4</td><td>87.9</td><td>90.9</td></tr><tr><td>C-SimpleQA (Correct)</td><td>78.6 48.5</td><td>79.5 54.1</td><td>86.1 48.4</td><td>61.5 50.4</td><td></td><td>76.7 51.3</td><td>76.0 59.3</td><td>86.5 64.8</td></tr></table></body></html>\\n\\n', 'page_idx': 30}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Table 6 Comparison between DeepSeek-V3 and other representative chat models. All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models. ', 'page_idx': 30}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '5.3.2. Standard Evaluation ', 'text_level': 1, 'page_idx': 30}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'Table 6 presents the evaluation results, showcasing that DeepSeek-V3 stands as the bestperforming open-source model. Additionally, it is competitive against frontier closed-source models like GPT-4o and Claude-3.5-Sonnet. ', 'page_idx': 30}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'English Benchmarks. MMLU is a widely recognized benchmark designed to assess the performance of large language models, across diverse knowledge domains and tasks. DeepSeek-V3 demonstrates competitive performance, standing on par with top-tier models such as LLaMA3.1-405B, GPT-4o, and Claude-Sonnet 3.5, while significantly outperforming Qwen2.5 72B. Moreover, DeepSeek-V3 excels in MMLU-Pro, a more challenging educational knowledge benchmark, where it closely trails Claude-Sonnet 3.5. On MMLU-Redux, a refined version of MMLU with corrected labels, DeepSeek-V3 surpasses its peers. In addition, on GPQA-Diamond, a PhD-level evaluation testbed, DeepSeek-V3 achieves remarkable results, ranking just behind Claude 3.5 Sonnet and outperforming all other competitors by a substantial margin. ', 'page_idx': 31}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'In long-context understanding benchmarks such as DROP, LongBench v2, and FRAMES, DeepSeek-V3 continues to demonstrate its position as a top-tier model. It achieves an impressive 91.6 F1 score in the 3-shot setting on DROP, outperforming all other models in this category. On FRAMES, a benchmark requiring question-answering over 100k token contexts, DeepSeekV3 closely trails GPT-4o while outperforming all other models by a significant margin. This demonstrates the strong capability of DeepSeek-V3 in handling extremely long-context tasks. The long-context capability of DeepSeek-V3 is further validated by its best-in-class performance on LongBench v2, a dataset that was released just a few weeks before the launch of DeepSeek V3. On the factual knowledge benchmark, SimpleQA, DeepSeek-V3 falls behind GPT-4o and Claude-Sonnet, primarily due to its design focus and resource allocation. DeepSeek-V3 assigns more training tokens to learn Chinese knowledge, leading to exceptional performance on the C-SimpleQA. On the instruction-following benchmark, DeepSeek-V3 significantly outperforms its predecessor, DeepSeek-V2-series, highlighting its improved ability to understand and adhere to user-defined format constraints. ', 'page_idx': 31}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Code and Math Benchmarks. Coding is a challenging and practical task for LLMs, encompassing engineering-focused tasks like SWE-Bench-Verified and Aider, as well as algorithmic tasks such as HumanEval and LiveCodeBench. In engineering tasks, DeepSeek-V3 trails behind Claude-Sonnet-3.5-1022 but significantly outperforms open-source models. The open-source DeepSeek-V3 is expected to foster advancements in coding-related engineering tasks. By providing access to its robust capabilities, DeepSeek-V3 can drive innovation and improvement in areas such as software engineering and algorithm development, empowering developers and researchers to push the boundaries of what open-source models can achieve in coding tasks. In algorithmic tasks, DeepSeek-V3 demonstrates superior performance, outperforming all baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be attributed to its advanced knowledge distillation technique, which effectively enhances its code generation and problem-solving capabilities in algorithm-focused tasks. ', 'page_idx': 31}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'On math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly surpassing baselines and setting a new state-of-the-art for non-o1-like models. Specifically, on AIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5 72B, by approximately $10\\\\%$ in absolute scores, which is a substantial margin for such challenging benchmarks. This remarkable capability highlights the effectiveness of the distillation technique from DeepSeek-R1, which has been proven highly beneficial for non-o1-like models. ', 'page_idx': 31}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Chinese Benchmarks. Qwen and DeepSeek are two representative model series with robust support for both Chinese and English. On the factual benchmark Chinese SimpleQA, DeepSeekV3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus compromising 18T tokens, which are $20\\\\%$ more than the $14.8\\\\mathrm{T}$ tokens that DeepSeek-V3 is ', 'page_idx': 31}, children=[]), Content(type='table', level=3, data={'type': 'table', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/c37f5046875e424ff7860735dd500227c5142a1630e000ff48bb7ce04be5e88f.jpg', 'table_caption': [], 'table_footnote': ['Table 7 English open-ended conversation evaluations. For AlpacaEval 2.0, we use the lengthcontrolled win rate as the metric. '], 'table_body': '\\n\\n<html><body><table><tr><td>Model</td><td>Arena-Hard</td><td>AlpacaEval 2.0</td></tr><tr><td>DeepSeek-V2.5-0905</td><td>76.2</td><td>50.5</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>81.2</td><td>49.1</td></tr><tr><td>LLaMA-3.1 405B</td><td>69.3</td><td>40.5</td></tr><tr><td>GPT-40-0513</td><td>80.4</td><td>51.1</td></tr><tr><td>Claude-Sonnet-3.5-1022</td><td>85.2</td><td>52.0</td></tr><tr><td>DeepSeek-V3</td><td>85.5</td><td>70.0</td></tr></table></body></html>\\n\\n', 'page_idx': 32}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'pre-trained on. ', 'page_idx': 32}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'On C-Eval, a representative benchmark for Chinese educational knowledge evaluation, and CLUEWSC (Chinese Winograd Schema Challenge), DeepSeek-V3 and Qwen2.5-72B exhibit similar performance levels, indicating that both models are well-optimized for challenging Chinese-language reasoning and educational tasks. ', 'page_idx': 32}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '5.3.3. Open-Ended Evaluation ', 'text_level': 1, 'page_idx': 32}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges, with the results shown in Table 7. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024a), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. On Arena-Hard, DeepSeek-V3 achieves an impressive win rate of over $86\\\\%$ against the baseline GPT-4-0314, performing on par with top-tier models like Claude-Sonnet-3.5-1022. This underscores the robust capabilities of DeepSeek-V3, especially in dealing with complex prompts, including coding and debugging tasks. Furthermore, DeepSeek-V3 achieves a groundbreaking milestone as the first open-source model to surpass $85\\\\%$ on the Arena-Hard benchmark. This achievement significantly bridges the performance gap between open-source and closed-source models, setting a new standard for what open-source models can accomplish in challenging domains. ', 'page_idx': 32}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Similarly, DeepSeek-V3 showcases exceptional performance on AlpacaEval 2.0, outperforming both closed-source and open-source models. This demonstrates its outstanding proficiency in writing tasks and handling straightforward question-answering scenarios. Notably, it surpasses DeepSeek-V2.5-0905 by a significant margin of $20\\\\%$ , highlighting substantial improvements in tackling simple tasks and showcasing the effectiveness of its advancements. ', 'page_idx': 32}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '5.3.4. DeepSeek-V3 as a Generative Reward Model ', 'text_level': 1, 'page_idx': 32}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'We compare the judgment ability of DeepSeek-V3 with state-of-the-art models, namely GPT-4o and Claude-3.5. Table 8 presents the performance of these models in RewardBench (Lambert et al., 2024). DeepSeek-V3 achieves performance on par with the best versions of GPT-4o-0806 and Claude-3.5-Sonnet-1022, while surpassing other versions. Additionally, the judgment ability of DeepSeek-V3 can also be enhanced by the voting technique. Therefore, we employ DeepSeekV3 along with voting to offer self-feedback on open-ended questions, thereby improving the effectiveness and robustness of the alignment process. ', 'page_idx': 32}, children=[]), Content(type='table', level=3, data={'type': 'table', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0b07a5c810c734387321e9fb236709a509378526205175534042267f9bc58722.jpg', 'table_caption': ['Table 8 | Performances of GPT-4o, Claude-3.5-sonnet and DeepSeek-V3 on RewardBench. '], 'table_footnote': [], 'table_body': '\\n\\n<html><body><table><tr><td>Model</td><td>Chat</td><td>Chat-Hard</td><td>Safety</td><td>Reasoning</td><td>Average</td></tr><tr><td>GPT-40-0513</td><td>96.6</td><td>70.4</td><td>86.7</td><td>84.9</td><td>84.7</td></tr><tr><td>GPT-40-0806</td><td>96.1</td><td>76.1</td><td>88.1</td><td>86.6</td><td>86.7</td></tr><tr><td>GPT-40-1120</td><td>95.8</td><td>71.3</td><td>86.2</td><td>85.2</td><td>84.6</td></tr><tr><td>Claude-3.5-sonnet-0620</td><td>96.4</td><td>74.0</td><td>81.6</td><td>84.7</td><td>84.2</td></tr><tr><td>Claude-3.5-sonnet-1022</td><td>96.4</td><td>79.7</td><td>91.1</td><td>87.6</td><td>88.7</td></tr><tr><td>DeepSeek-V3</td><td>96.9</td><td>79.8</td><td>87.0</td><td>84.3</td><td>87.0</td></tr><tr><td>DeepSeek-V3 (maj@6)</td><td>96.9</td><td>82.6</td><td>89.5</td><td>89.2</td><td>89.6</td></tr></table></body></html>\\n\\n', 'page_idx': 33}, children=[]), Content(type='table', level=3, data={'type': 'table', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0700f9f4a277d248543728c8d67e12011d7ca905620b8f1be4fa1406b33ee64d.jpg', 'table_caption': [], 'table_footnote': ['Table 9 | The contribution of distillation from DeepSeek-R1. The evaluation settings of LiveCodeBench and MATH-500 are the same as in Table 6. '], 'table_body': '\\n\\n<html><body><table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">LiveCodeBench-CoT</td><td colspan=\"2\">MATH-500</td></tr><tr><td>Pass@1</td><td>Length</td><td>Pass@1</td><td>Length</td></tr><tr><td>DeepSeek-V2.5 Baseline</td><td>31.1</td><td>718</td><td>74.6</td><td>769</td></tr><tr><td>DeepSeek-V2.5 +R1 Distill</td><td>37.4</td><td>783</td><td>83.2</td><td>1510</td></tr></table></body></html>\\n\\n', 'page_idx': 33}, children=[])])]), Content(type='chapter', level=1, data={'type': 'text', 'text': '5.4. Discussion ', 'text_level': 1, 'page_idx': 33}, children=[Content(type='chapter', level=2, data={'type': 'text', 'text': '5.4.1. Distillation from DeepSeek-R1 ', 'text_level': 1, 'page_idx': 33}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'We ablate the contribution of distillation from DeepSeek-R1 based on DeepSeek-V2.5. The baseline is trained on short CoT data, whereas its competitor uses data generated by the expert checkpoints described above. ', 'page_idx': 33}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Table 9 demonstrates the effectiveness of the distillation data, showing significant improvements in both LiveCodeBench and MATH-500 benchmarks. Our experiments reveal an interesting trade-off: the distillation leads to better performance but also substantially increases the average response length. To maintain a balance between model accuracy and computational efficiency, we carefully selected optimal settings for DeepSeek-V3 in distillation. ', 'page_idx': 33}, children=[]), Content(type='text', level=3, data={'type': 'text', 'text': 'Our research suggests that knowledge distillation from reasoning models presents a promising direction for post-training optimization. While our current work focuses on distilling data from mathematics and coding domains, this approach shows potential for broader applications across various task domains. The effectiveness demonstrated in these specific areas indicates that long-CoT distillation could be valuable for enhancing model performance in other cognitive tasks requiring complex reasoning. Further exploration of this approach across different domains remains an important direction for future research. ', 'page_idx': 33}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '5.4.2. Self-Rewarding ', 'text_level': 1, 'page_idx': 33}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'Rewards play a pivotal role in RL, steering the optimization process. In domains where verification through external tools is straightforward, such as some coding or mathematics scenarios, RL demonstrates exceptional efficacy. However, in more general scenarios, constructing a feedback mechanism through hard coding is impractical. During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source. This method has produced notable alignment effects, significantly enhancing the performance of DeepSeek-V3 in subjective evaluations. By integrating additional constitutional inputs, DeepSeek-V3 can optimize towards the constitutional direction. We believe that this paradigm, which combines supplementary information with LLMs as a feedback source, is of paramount importance. The LLM serves as a versatile processor capable of transforming unstructured information from diverse scenarios into rewards, ultimately facilitating the self-improvement of LLMs. Beyond self-rewarding, we are also dedicated to uncovering other general and scalable rewarding methods to consistently advance the model capabilities in general scenarios. ', 'page_idx': 33}, children=[])]), Content(type='chapter', level=2, data={'type': 'text', 'text': '5.4.3. Multi-Token Prediction Evaluation ', 'text_level': 1, 'page_idx': 34}, children=[Content(type='text', level=3, data={'type': 'text', 'text': 'Instead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through the MTP technique. Combined with the framework of speculative decoding (Leviathan et al., 2023; Xia et al., 2023), it can significantly accelerate the decoding speed of the model. A natural question arises concerning the acceptance rate of the additionally predicted token. Based on our evaluation, the acceptance rate of the second token prediction ranges between $85\\\\%$ and $90\\\\%$ across various generation topics, demonstrating consistent reliability. This high acceptance rate enables DeepSeek-V3 to achieve a significantly improved decoding speed, delivering 1.8 times TPS (Tokens Per Second). ', 'page_idx': 34}, children=[])])])]), Content(type='chapter', level=0, data={'type': 'text', 'text': '6. Conclusion, Limitations, and Future Directions ', 'text_level': 1, 'page_idx': 34}, children=[Content(type='text', level=1, data={'type': 'text', 'text': 'In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations. The post-training also makes a success in distilling the reasoning capability from the DeepSeek-R1 series of models. Comprehensive evaluations demonstrate that DeepSeek-V3 has emerged as the strongest open-source model currently available, and achieves performance comparable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. Despite its strong performance, it also maintains economical training costs. It requires only 2.788M H800 GPU hours for its full training, including pre-training, context length extension, and post-training. ', 'page_idx': 34}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'While acknowledging its strong performance and cost-effectiveness, we also recognize that DeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient inference, the recommended deployment unit for DeepSeek-V3 is relatively large, which might pose a burden for small-sized teams. Secondly, although our deployment strategy for DeepSeekV3 has achieved an end-to-end generation speed of more than two times that of DeepSeek-V2, there still remains potential for further enhancement. Fortunately, these limitations are expected to be naturally addressed with the development of more advanced hardware. ', 'page_idx': 34}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'DeepSeek consistently adheres to the route of open-source models with longtermism, aiming to steadily approach the ultimate goal of AGI (Artificial General Intelligence). In the future, we plan to strategically invest in research across the following directions. ', 'page_idx': 34}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': '‚Ä¢ We will consistently study and refine our model architectures, aiming to further improve both the training and inference efficiency, striving to approach efficient support for infinite context length. Additionally, we will try to break through the architectural limitations of Transformer, thereby pushing the boundaries of its modeling capabilities. ', 'page_idx': 34}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': '‚Ä¢ We will continuously iterate on the quantity and quality of our training data, and explore the incorporation of additional training signal sources, aiming to drive data scaling across a more comprehensive range of dimensions.   \\n‚Ä¢ We will consistently explore and iterate on the deep thinking capabilities of our models, aiming to enhance their intelligence and problem-solving abilities by expanding their reasoning length and depth.   \\n‚Ä¢ We will explore more comprehensive and multi-dimensional model evaluation methods to prevent the tendency towards optimizing a fixed set of benchmarks during research, which may create a misleading impression of the model capabilities and affect our foundational assessment. ', 'page_idx': 35}, children=[])]), Content(type='chapter', level=0, data={'type': 'text', 'text': 'References ', 'text_level': 1, 'page_idx': 35}, children=[Content(type='text', level=1, data={'type': 'text', 'text': 'AI@Meta. Llama 3 model card, 2024a. URL https://github.com/meta-llama/llama3/bl ob/main/MODEL_CARD.md.   \\nAI@Meta. Llama 3.1 model card, 2024b. URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md.   \\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3 -5-sonnet.   \\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.   \\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022.   \\nY. Bai, S. Tu, J. Zhang, H. Peng, X. Wang, X. Lv, S. Cao, J. Xu, L. Hou, Y. Dong, J. Tang, and J. Li. LongBench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024.   \\nM. Bauer, S. Treichler, and A. Aiken. Singe: leveraging warp specialization for high performance on GPUs. In Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP ‚Äô14, page 119‚Äì130, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450326568. doi: 10.1145/2555243.2555258. URL https://doi.org/10.1145/2555243.2555258.   \\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432‚Äì7439. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.   \\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.   \\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.   \\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,   \\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \\nY. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, and G. Hu. A span-extraction dataset for Chinese machine reading comprehension. In K. Inui, J. Jiang, V. $\\\\mathrm{Ng,}$ and X. Wan,   \\neditors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5883‚Äì5889, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1600. URL https://aclanthology.org/D19-1 600.   \\nD. Dai, C. Deng, C. Zhao, R. X. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, Z. Xie, Y. K. Li, P. Huang, F. Luo, C. Ruan, Z. Sui, and W. Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. CoRR, abs/2401.06066, 2024. URL https://doi.org/10.48550/arXiv.2401.06066.   \\nDeepSeek-AI. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. CoRR, abs/2406.11931, 2024a. URL https://doi.org/10.48550/arXiv.2406.11 931.   \\nDeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR, abs/2401.02954, 2024b. URL https://doi.org/10.48550/arXiv.2401.02954.   \\nDeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. CoRR, abs/2405.04434, 2024c. URL https://doi.org/10.48550/arXiv.2405. 04434.   \\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318‚Äì 30332, 2022.   \\nH. Ding, Z. Wang, G. Paolini, V. Kumar, A. Deoras, D. Roth, and S. Soatto. Fewer truncations improve language modeling. arXiv preprint arXiv:2404.10830, 2024.   \\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368‚Äì 2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL https://doi.org/10.18653/v1/n19-1246.   \\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. ', 'page_idx': 35}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https://arxiv.org/ abs/2101.03961. ', 'page_idx': 37}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'M. Fishman, B. Chmiel, R. Banner, and D. Soudry. Scaling FP8 training to trillion-token llms. arXiv preprint arXiv:2409.12517, 2024.   \\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.   \\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127.   \\nF. Gloeckle, B. Y. Idrissi, B. Rozi√®re, D. Lopez-Paz, and G. Synnaeve. Better & faster large language models via multi-token prediction. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=pEWAcejiU2.   \\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024.   \\nR. L. Graham, D. Bureddy, P. Lui, H. Rosenstock, G. Shainer, G. Bloch, D. Goldenerg, M. Dubman, S. Kotchubievsky, V. Koushnir, et al. Scalable hierarchical aggregation protocol (SHArP): A hardware architecture for efficient data reduction. In 2016 First International Workshop on Communication Optimizations in HPC (COMHPC), pages 1‚Äì10. IEEE, 2016.   \\nA. Gu, B. Rozi√®re, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang. Cruxeval: A benchmark for code reasoning, understanding and execution, 2024.   \\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. CoRR, abs/2401.14196, 2024. URL https://doi.org/10.485 50/arXiv.2401.14196.   \\nA. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger, and P. Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training, 2018. URL https://arxiv.or g/abs/1806.03377.   \\nB. He, L. Noci, D. Paliotta, I. Schlag, and T. Hofmann. Understanding and minimising outlier features in transformer training. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.   \\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chinese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024.   \\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.   \\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.   \\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.   \\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601‚Äì1611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.   \\nD. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen, et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019.   \\nS. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR, abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941.   \\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452‚Äì466, 2019. doi: 10.1162/tacl\\\\_a\\\\_00276. URL https://doi.org/10.1162/tacl_a_00276.   \\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. RACE: large-scale reading comprehension dataset from examinations. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 785‚Äì794. Association for Computational Linguistics, 2017. doi: 10.18653/V1/D17-1082. URL https://doi.org/10.18653/v1/d1 7-1082.   \\nN. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar, T. Zick, Y. Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.   \\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id $\\\\c=$ qrwe7XHTmYb.   \\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 19274‚Äì19286. PMLR, 2023. URL https://proceedings.mlr.press/v202/leviathan23 a.html.   \\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023.   \\nS. Li and T. Hoefler. Chimera: efficiently training large-scale neural networks with bidirectional pipelines. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC ‚Äô21, page 1‚Äì14. ACM, Nov. 2021. doi: 10.1145/345881 7.3476145. URL http://dx.doi.org/10.1145/3458817.3476145.   \\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024a.   \\nW. Li, F. Qi, M. Sun, X. Yi, and J. Zhang. Ccpm: A chinese classical poetry matching dataset, 2021.   \\nY. Li, F. Wei, C. Zhang, and H. Zhang. EAGLE: speculative sampling requires rethinking feature uncertainty. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024b. URL https://openreview.net /forum?id $\\\\c=$ 1NdN7eXyb4.   \\nB. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval.   \\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \\nS. Lundberg. The art of prompt design: Prompt boundaries and token healing, 2023. URL https://towardsdatascience.com/the-art-of-prompt-design-prompt-bound aries-and-token-healing-3b2448b0be38.   \\nY. Luo, Z. Zhang, R. Wu, H. Liu, Y. Jin, K. Zheng, M. Wang, Z. He, G. Hu, L. Chen, et al. Ascend HiFloat8 format for deep learning. arXiv preprint arXiv:2409.16626, 2024.   \\nMAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime.   \\nP. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisenthwaite, S. Ha, A. Heinecke, P. Judd, J. Kamalu, et al. FP8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.   \\nMistral. Cheaper, better, faster, stronger: Continuing to push the frontier of ai and making it accessible to all, 2024. URL https://mistral.ai/news/mixtral-8x22b.   \\nS. Narang, G. Diamos, E. Elsen, P. Micikevicius, J. Alben, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. In Int. Conf. on Learning Representation, 2017. ', 'page_idx': 37}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'B. Noune, P. Jones, D. Justus, D. Masters, and C. Luschi. 8-bit numerical formats for deep neural networks. arXiv preprint arXiv:2206.02915, 2022. ', 'page_idx': 40}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'NVIDIA. Improving network performance of HPC systems using NVIDIA Magnum IO NVSHMEM and GPUDirect Async. https://developer.nvidia.com/blog/improving-net work-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-g pudirect-async, 2022. ', 'page_idx': 40}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'NVIDIA. Blackwell architecture. https://www.nvidia.com/en-us/data-center/tech nologies/blackwell-architecture/, 2024a.   \\nNVIDIA. TransformerEngine, 2024b. URL https://github.com/NVIDIA/TransformerE ngine. Accessed: 2024-11-19.   \\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.   \\nOpenAI. Multilingual massive multitask language understanding (mmmlu), 2024b. URL https://huggingface.co/datasets/openai/MMMLU.   \\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing   \\n-simpleqa/.   \\nOpenAI. Introducing SWE-bench verified we‚Äôre releasing a human-validated subset of swebench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/.   \\nB. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023a.   \\nH. Peng, K. Wu, Y. Wei, G. Zhao, Y. Yang, Z. Liu, Y. Xiong, Z. Yang, B. Ni, J. Hu, et al. FP8-LM: Training FP8 large language models. arXiv preprint arXiv:2310.18313, 2023b.   \\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble pipeline parallelism. arXiv preprint arXiv:2401.10241, 2023a.   \\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble pipeline parallelism, 2023b. URL https: //arxiv.org/abs/2401.10241.   \\nQwen. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \\nQwen. Introducing Qwen1.5, 2024a. URL https://qwenlm.github.io/blog/qwen1.5.   \\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5.   \\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1‚Äì16. IEEE, 2020.   \\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.   \\nB. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea, E. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023a.   \\nB. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea, E. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023b.   \\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.   \\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017. OpenReview.net, 2017. URL https: //openreview.net/forum?id $\\\\c=$ B1ckMDqlg.   \\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?i d=fR3wGCk-IXp.   \\nY. Shibata, T. Kida, S. Fukamachi, M. Takeda, A. Shinohara, T. Shinohara, and S. Arikawa. Byte pair encoding: A text compression scheme that accelerates pattern matching. 1999.   \\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \\nK. Sun, D. Yu, D. Yu, and C. Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension, 2019a.   \\nM. Sun, X. Chen, J. Z. Kolter, and Z. Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024.   \\nX. Sun, J. Choi, C.-Y. Chen, N. Wang, S. Venkataramani, V. V. Srinivasan, X. Cui, W. Zhang, and K. Gopalakrishnan. Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks. Advances in neural information processing systems, 32, 2019b.   \\nM. Suzgun, N. Scales, N. Sch√§rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.   \\nV. Thakkar, P. Ramani, C. Cecka, A. Shivam, H. Lu, E. Yan, J. Kosaian, M. Hoemmen, H. Wu, A. Kerr, M. Nicely, D. Merrill, D. Blasig, F. Qiao, P. Majcher, P. Springer, M. Hohnerbach, J. Wang, and M. Gupta. CUTLASS, Jan. 2023. URL https://github.com/NVIDIA/cutlas s.   \\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi√®re, N. Goyal, E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.   \\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307. 09288.   \\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \\nL. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. CoRR, abs/2408.15664, 2024a. URL https://doi.org/10.48550/arX iv.2408.15664.   \\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024b. URL https://doi.org/10.48550/arXiv.2406.01574.   \\nT. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023.   \\nM. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and low-precision training for large-scale vision-language models. Advances in Neural Information Processing Systems, 36:10271‚Äì10298, 2023.   \\nH. Xi, C. Li, J. Chen, and J. Zhu. Training transformers with 4-bit integers. Advances in Neural Information Processing Systems, 36:49146‚Äì49168, 2023.   \\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024.   \\nH. Xia, T. Ge, P. Wang, S. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 3909‚Äì3925. Association for Computational Linguistics, 2023. URL https://doi.org/10.18653/v1/ 2023.findings-emnlp.257.   \\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087‚Äì38099. PMLR, 2023.   \\nL. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, Y. Tian, Q. Dong, W. Liu, B. Shi, Y. Cui, J. Li, J. Zeng, R. Wang, W. Xie, Y. Li, Y. Patterson, Z. Tian, Y. Zhang, H. Zhou, S. Liu, Z. Zhao, Q. Zhao, C. Yue, X. Zhang, Z. Yang, K. Richardson, and Z. Lan. CLUE: A chinese language understanding evaluation benchmark. In D. Scott, N. Bel, and C. Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4762‚Äì4772. International Committee on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.419. URL https://doi.org/10.18653/v1/2020.coling-main.419. ', 'page_idx': 40}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. M√†rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4791‚Äì4800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1 9-1472. W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023. doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. ', 'page_idx': 43}, children=[])]), Content(type='chapter', level=0, data={'type': 'text', 'text': 'Appendix ', 'text_level': 1, 'page_idx': 44}, children=[]), Content(type='chapter', level=0, data={'type': 'text', 'text': 'A. Contributions and Acknowledgments ', 'text_level': 1, 'page_idx': 44}, children=[Content(type='text', level=1, data={'type': 'text', 'text': 'Research & Engine   \\nAixin Liu   \\nBing Xue   \\nBingxuan Wang   \\nBochao Wu   \\nChengda Lu   \\nChenggang Zhao   \\nChengqi Deng   \\nChenyu Zhang\\\\*   \\nChong Ruan   \\nDamai Dai   \\nDaya Guo   \\nDejian Yang   \\nDeli Chen   \\nErhang Li   \\nFangyun Lin   \\nFucong Dai   \\nFuli Luo\\\\*   \\nGuangbo Hao   \\nGuanting Chen   \\nGuowei Li   \\nH. Zhang   \\nHan Bao\\\\*   \\nHanwei Xu   \\nHaocheng Wang\\\\*   \\nHaowei Zhang   \\nHonghui Ding   \\nHuajian Xin\\\\*   \\nHuazuo Gao   \\nHui Qu   \\nJianzhong Guo   \\nJiashi Li   \\nJiawei Wang\\\\*   \\nJingchang Chen   \\nJingyang Yuan   \\nJunjie Qiu   \\nJunlong Li   \\nJunxiao Song   \\nKai Dong   \\nKai Hu\\\\*   \\nKaige Gao   \\nKang Guan   \\nKexin Huang   \\nKuai Yu   \\nLean Wang   \\nLecong Zhang   \\nLiang Zhao   \\nLitong Wang   \\nLiyue Zhang   \\nMingchuan Zhang   \\nMinghua Zhang   \\nMinghui Tang   \\nPanpan Huang   \\nPeiyi Wang   \\nQiancheng Wang   \\nQihao Zhu   \\nQinyu Chen   \\nQiushi Du   \\nRuiqi Ge   \\nRuisong Zhang   \\nRuizhe Pan   \\nRunji Wang   \\nRunxin Xu   \\nRuoyu Zhang   \\nShanghao Lu   \\nShangyan Zhou   \\nShanhuang Chen   \\nShengfeng Ye   \\nShirong Ma   \\nShiyu Wang   \\nShuiping Yu   \\nShunfeng Zhou   \\nShuting Pan   \\nTao Yun   \\nTian Pei   \\nWangding Zeng   \\nWanjia Zhao\\\\*   \\nWen Liu   \\nWenfeng Liang   \\nWenjun Gao   \\nWenqin Yu   \\nWentao Zhang   \\nXiao Bi   \\nXiaodong Liu   \\nXiaohan Wang   \\nXiaokang Chen   \\nXiaokang Zhang   \\nXiaotao Nie   \\nXin Cheng   \\nXin Liu   \\nXin Xie   \\nXingchao Liu   \\nXingkai Yu   \\nXinyu Yang   \\nXinyuan Li   \\nXuecheng Su   \\nXuheng Lin   \\nY.K. Li   \\nY.Q. Wang   \\nY.X. Wei   \\nYang Zhang   \\nYanhong Xu   \\nYao Li   \\nYao Zhao   \\nYaofeng Sun   \\nYaohui Wang   \\nYi Yu   \\nYichao Zhang   \\nYifan Shi   \\nYiliang Xiong   \\nYing He   \\nYishi Piao   \\nYisong Wang   \\nYixuan Tan   \\nYiyang Ma\\\\*   \\nYiyuan Liu   \\nYongqiang Guo   \\nYu Wu   \\nYuan Ou   \\nYuduan Wang   \\nYue Gong   \\nYuheng Zou   \\nYujia He   \\nYunfan Xiong   \\nYuxiang Luo   \\nYuxiang You   \\nYuxuan Liu   \\nYuyang Zhou   \\nZ.F. Wu   \\nZ.Z. Ren   \\nZehui Ren   \\nZhangli Sha   \\nZhe Fu   \\nZhean Xu   \\nZhenda Xie   \\nZhengyan Zhan   \\nZhewen Hao   \\nZhibin Gou   \\nZhicheng Ma ', 'page_idx': 44}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'Zhigang Yan Zhihong Shao Zhiyu Wu Zhuoshu Li Zihui Gu Zijia Zhu Zijun Liu\\\\* Zilin Li Ziwei Xie Ziyang Song Ziyi Gao Zizheng Pan ', 'page_idx': 45}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'Data Annotation   \\nBei Feng   \\nHui Li   \\nJ.L. Cai   \\nJiaqi Ni   \\nLei Xu   \\nMeng Li   \\nNing Tian   \\nR.J. Chen   \\nR.L. Jin   \\nRuyi Chen   \\nS.S. Li   \\nShuang Zhou   \\nTianyu Sun   \\nX.Q. Li   \\nXiangyue Jin   \\nXiaojin Shen   \\nXiaosha Chen   \\nXiaowen Sun   \\nXiaoxiang Wang   \\nXinnan Song   \\nXinyi Zhou   \\nY.X. Zhu   \\nYanhong Xu   \\nYanping Huang   \\nYaohui Li   \\nYi Zheng   \\nYuchen Zhu   \\nYunxian Ma   \\nZhen Huang   \\nZhipeng Xu   \\nZhongyu Zhang ', 'page_idx': 45}, children=[])]), Content(type='chapter', level=0, data={'type': 'text', 'text': 'Business & Compliance Dongjie Ji ', 'text_level': 1, 'page_idx': 45}, children=[Content(type='text', level=1, data={'type': 'text', 'text': 'Jian Liang   \\nJin Chen   \\nLeyi Xia   \\nMiaojun Wang Mingming Li Peng Zhang   \\nShaoqing Wu Shengfeng Ye T. Wang ', 'page_idx': 46}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'W.L. Xiao Wei An Xianzu Wang Xinxia Shan Ying Tang Yukun Zha Yuting Yan Zhen Zhang ', 'page_idx': 46}, children=[]), Content(type='text', level=1, data={'type': 'text', 'text': 'Within each role, authors are listed alphabetically by the first name. Names marked with \\\\* denote individuals who have departed from our team. ', 'page_idx': 46}, children=[])]), Content(type='chapter', level=0, data={'type': 'text', 'text': 'B. Ablation Studies for Low-Precision Training ', 'text_level': 1, 'page_idx': 46}, children=[Content(type='image', level=1, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/b0ad5ed91f42c8d54b3c818553783cbd94b8e018e59a93200aea26c539e58a1d.jpg', 'img_caption': ['Figure 10 | Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9. '], 'img_footnote': [], 'page_idx': 46}, children=[]), Content(type='chapter', level=1, data={'type': 'text', 'text': 'B.1. FP8 v.s. BF16 Training ', 'text_level': 1, 'page_idx': 46}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'We validate our FP8 mixed precision framework with a comparison to BF16 training on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising approximately 16B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising approximately 230B total parameters on around $0.9\\\\mathrm{T}$ tokens. We show the training curves in Figure 10 and demonstrate that the relative error remains below $0.25\\\\%$ with our high-precision accumulation and fine-grained quantization strategies. ', 'page_idx': 46}, children=[])]), Content(type='chapter', level=1, data={'type': 'text', 'text': 'B.2. Discussion About Block-Wise Quantization ', 'text_level': 1, 'page_idx': 46}, children=[Content(type='text', level=2, data={'type': 'text', 'text': 'Although our tile-wise fine-grained quantization effectively mitigates the error introduced by feature outliers, it requires different groupings for activation quantization, i.e., $\\\\mathtt{1x128}$ in forward pass and $128\\\\mathbf{x}\\\\mathbf{1}$ for backward pass. A similar process is also required for the activation gradient. A straightforward strategy is to apply block-wise quantization per $128\\\\mathrm{x}128$ elements like the way we quantize the model weights. In this way, only transposition is required for backward. Therefore, we conduct an experiment where all tensors associated with Dgrad are quantized on a block-wise basis. The results reveal that the Dgrad operation which computes the activation gradients and back-propagates to shallow layers in a chain-like manner, is highly sensitive to precision. Specifically, block-wise quantization of activation gradients leads to model divergence on an MoE model comprising approximately 16B total parameters, trained for around 300B tokens. We hypothesize that this sensitivity arises because activation gradients are highly imbalanced among tokens, resulting in token-correlated outliers (Xi et al., 2023). These outliers cannot be effectively managed by a block-wise quantization approach. ', 'page_idx': 46}, children=[])])]), Content(type='chapter', level=0, data={'type': 'text', 'text': 'C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models ', 'text_level': 1, 'page_idx': 47}, children=[Content(type='text', level=1, data={'type': 'text', 'text': 'We record the expert load of the 16B auxiliary-loss-based baseline and the auxiliary-loss-free model on the Pile test set. The auxiliary-loss-free model tends to have greater expert specialization across all layers, as demonstrated in Figure 10. ', 'page_idx': 47}, children=[]), Content(type='image', level=1, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/df414ba6ec58ecdea8399c32970505fd62815fab4090c286317901838eb29684.jpg', 'img_caption': [], 'img_footnote': [], 'page_idx': 48}, children=[]), Content(type='image', level=1, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/ce6b240feda49244a4a0806c8852a8a67ba9428299564763e225999cf132ff86.jpg', 'img_caption': [], 'img_footnote': [], 'page_idx': 49}, children=[]), Content(type='image', level=1, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/3883c917c521a40a6dc026f2ea4a5be3fcce25df9d142f0ea4eb9ec79aaed32a.jpg', 'img_caption': [], 'img_footnote': [], 'page_idx': 50}, children=[]), Content(type='image', level=1, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/935ea04ae9b644bdc56695b8bee3a4a02c590d8ad7e185c711726649f5a80605.jpg', 'img_caption': [], 'img_footnote': [], 'page_idx': 51}, children=[]), Content(type='image', level=1, data={'type': 'image', 'img_path': './arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/aac7de5cd380235acb5843d45956d36baf8f6c532611c0bee9a72195ee967845.jpg', 'img_caption': ['Figure 10 | Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. '], 'img_footnote': [], 'page_idx': 52}, children=[])])])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rolling.processing import list_processed, load_document\n",
    "processed = list_processed()\n",
    "idx = [i for i, p in enumerate(processed) if \"deepseek-v3\" in p]\n",
    "print(processed[idx[0]])\n",
    "doc = load_document(processed[idx[0]])\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"document\", \"level\": -1, \"data\": {}, \"children\": [{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"DeepSeek-V3 Technical Report \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"DeepSeek-AI \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"research@deepseek.com \"}}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"Abstract \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3. \"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/700c3b3526a18f86057af07c28c75896ba00dd4edc85c8f924fc81e859e84d80.jpg\", \"img_caption\": [\"Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts. \"]}}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"Contents \"}, \"children\": []}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"1 Introduction 4 \"}, \"children\": []}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"2 Architecture 6 \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"2.1 Basic Architecture 6   \\n2.1.1 Multi-Head Latent Attention 7   \\n2.1.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing 8   \\n2.2 Multi-Token Prediction . 10 \"}}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"3 Infrastructures 11 \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"3.1 Compute Clusters . 11   \\n3.2 Training Framework 12   \\n3.2.1 DualPipe and Computation-Communication Overlap . 12   \\n3.2.2 Efficient Implementation of Cross-Node All-to-All Communication . . 13   \\n3.2.3 Extremely Memory Saving with Minimal Overhead . 14   \\n3.3 FP8 Training 14   \\n3.3.1 Mixed Precision Framework 15   \\n3.3.2 Improved Precision from Quantization and Multiplication 16   \\n3.3.3 Low-Precision Storage and Communication 18   \\n3.4 Inference and Deployment . 18   \\n3.4.1 Prefilling . 19   \\n3.4.2 Decoding 19   \\n3.5 Suggestions on Hardware Design 20   \\n3.5.1 Communication Hardware 20   \\n3.5.2 Compute Hardware 20 \"}}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"4 Pre-Training 21 \"}, \"children\": [{\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.1 Data Construction . 21 \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"4.2 Hyper-Parameters . 22   \\n4.3 Long Context Extension 23   \\n4.4 Evaluations 24   \\n4.4.1 Evaluation Benchmarks 24   \\n4.4.2 Evaluation Results 24   \\n4.5 Discussion 26   \\n4.5.1 Ablation Studies for Multi-Token Prediction 26   \\n4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy 26 \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance . 27 \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"5 Post-Training 28   \\n5.1 Supervised Fine-Tuning 28   \\n5.2 Reinforcement Learning . 29   \\n5.2.1 Reward Model 29   \\n5.2.2 Group Relative Policy Optimization 30   \\n5.3 Evaluations 30   \\n5.3.1 Evaluation Settings 30   \\n5.3.2 Standard Evaluation 31   \\n5.3.3 Open-Ended Evaluation 33   \\n5.3.4 DeepSeek-V3 as a Generative Reward Model 33   \\n5.4 Discussion 34   \\n5.4.1 Distillation from DeepSeek-R1 34   \\n5.4.2 Self-Rewarding 34   \\n5.4.3 Multi-Token Prediction Evaluation 35 \"}}]}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"6 Conclusion, Limitations, and Future Directions 35 \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"A Contributions and Acknowledgments 45   \\nB Ablation Studies for Low-Precision Training 47   \\nB.1 FP8 v.s. BF16 Training 47   \\nB.2 Discussion About Block-Wise Quantization 47 \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"C Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-Loss-Free Models 48 \"}}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"1. Introduction \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Beyond closed-source models, open-source models, including DeepSeek series (DeepSeek-AI, 2024a,b,c; Guo et al., 2024), LLaMA series (AI@Meta, $2024\\\\mathsf{a},\\\\mathsf{b},$ ; Touvron et al., 2023a,b), Qwen series (Qwen, 2023, 2024a,b), and Mistral series (Jiang et al., 2023; Mistral, 2024), are also making significant strides, endeavoring to close the gap with their closed-source counterparts. To further push the boundaries of open-source model capabilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"With a forward-looking perspective, we consistently strive for strong model performance and economical costs. Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for cost-effective training. These two architectures have been validated in DeepSeekV2 (DeepSeek-AI, 2024c), demonstrating their capability to maintain robust model performance while achieving efficient training and inference. Beyond the basic architecture, we implement two additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy (Wang et al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. Secondly, DeepSeek-V3 employs a multi-token prediction training objective, which we have observed to enhance the overall performance on evaluation benchmarks. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"In order to achieve efficient training, we support the FP8 mixed precision training and implement comprehensive optimizations for the training framework. Low-precision training has emerged as a promising solution for efficient training (Dettmers et al., 2022; Kalamkar et al., 2019; Narang et al., 2017; Peng et al., 2023b), its evolution being closely tied to advancements in hardware capabilities (Luo et al., 2024; Micikevicius et al., 2022; Rouhani et al., 2023a). In this work, we introduce an FP8 mixed precision training framework and, for the first time, validate its effectiveness on an extremely large-scale model. Through the support for FP8 computation and storage, we achieve both accelerated training and reduced GPU memory usage. As for the training framework, we design the DualPipe algorithm for efficient pipeline parallelism, which has fewer pipeline bubbles and hides most of the communication during training through computation-communication overlap. This overlap ensures that, as the model further scales up, as long as we maintain a constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead. In addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize InfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory footprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism. Combining these efforts, we achieve high training efficiency. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The pre-training process is remarkably stable. Throughout the entire training process, we did not encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage context length extension for DeepSeek-V3. In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K. Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential. During the post-training stage, we distill the reasoning capability from the DeepSeekR1 series of models, and meanwhile carefully maintain the balance between model accuracy \"}}, {\"type\": \"table\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/11b8e875de1519088afb45aab53bb45a5c61507dc80cebf8f2ce4ab6e24b70bc.jpg\", \"table_body\": \"\\n\\n<html><body><table><tr><td>Training Costs</td><td>Pre-Training</td><td>Context Extension</td><td>Post-Training</td><td>Total</td></tr><tr><td> in H800 GPU Hours</td><td>2664K</td><td>119K</td><td>5K</td><td>2788K</td></tr><tr><td>in USD</td><td>$5.328M</td><td>$0.238M</td><td>$0.01M</td><td>$5.576M</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Table 1 | Training costs of DeepSeek-V3, assuming the rental price of H800 is $\\\\$2$ per GPU hour. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"and generation length. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"We evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the strongest open-source base model currently available, especially in code and math. Its chat version also outperforms other open-source models and achieves performance comparable to leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard and open-ended benchmarks. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware. During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pretraining stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post-training, DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the $\\\\mathrm{{H800GPU}}$ is $\\\\$2$ per GPU hour, our total training costs amount to only $\\\\$5760$ . Note that the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Our main contribution includes: \"}}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"Architecture: Innovative Load Balancing Strategy and Training Objective \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"\\u2022 On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing. \\u2022 We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"Pre-Training: Towards Ultimate Training Efficiency \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"\\u2022 We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model. \\u2022 Through the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computationcommunication overlap. This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead. \\u2022 At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"Post-Training: Knowledge Distillation from DeepSeek-R1 \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"\\u2022 We introduce an innovative methodology to distill reasoning capabilities from the longChain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain control over the output style and length of DeepSeek-V3. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"Summary of Core Evaluation Results \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"\\u2022 Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA, DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9 on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3 demonstrates superior performance among open-source models on both SimpleQA and Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese SimpleQA), highlighting its strength in Chinese factual knowledge. \\u2022 Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on math-related benchmarks among all non-long-CoT open-source and closed-source models. Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500, demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks, DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks, such as LiveCodeBench, solidifying its position as the leading model in this domain. For engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5, it still outpaces all other models by a significant margin, demonstrating its competitiveness across diverse technical benchmarks. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3 model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing our compute clusters, the training framework, the support for FP8 training, the inference deployment strategy, and our suggestions on future hardware design. Next, we describe our pre-training process, including the construction of training data, hyper-parameter settings, longcontext extension techniques, the associated evaluations, as well as some discussions (Section 4). Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly, we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential directions for future research (Section 6). \"}}]}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"2. Architecture \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for economical training. Then, we present a Multi-Token Prediction (MTP) training objective, which we have observed to enhance the overall performance on evaluation benchmarks. For other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeekV2 (DeepSeek-AI, 2024c). \"}}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"2.1. Basic Architecture \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017) framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) for DeepSeekMoE to mitigate the performance degradation induced by the effort to ensure load balance. Figure 2 illustrates the basic architecture of DeepSeek-V3, and we will briefly review the details of MLA and DeepSeekMoE in this section. \"}}, {\"type\": \"image\", \"level\": 2, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0a839f8095c46c85a9c566ea17c4b8aad3d84635b3051071b2cd9fed03bb13d7.jpg\", \"img_caption\": [\"Figure 2 | Illustration of the basic architecture of DeepSeek-V3. Following DeepSeek-V2, we adopt MLA and DeepSeekMoE for efficient inference and economical training. \"]}}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"2.1.1. Multi-Head Latent Attention \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"For attention, DeepSeek-V3 adopts the MLA architecture. Let $d$ denote the embedding dimension, $n_{h}$ denote the number of attention heads, $d_{h}$ denote the dimension per head, and $\\\\mathbf h_{t}\\\\in\\\\mathbb R^{d}$ denote the attention input for the $t$ -th token at a given attention layer. The core of MLA is the low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during inference: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{c}{{{\\\\displaystyle\\\\left[{\\\\bf{\\\\overline{{c}}}}_{t}^{K V}}\\\\right]=W^{D K V}{\\\\bf h}_{t},}}\\\\\\\\ {{{\\\\displaystyle[{\\\\bf{k}}_{t,1}^{C};{\\\\bf{k}}_{t,2}^{C};...;{\\\\bf{k}}_{t,n_{h}}^{C}]={\\\\bf{k}}_{t}^{C}=W^{U K}{\\\\bf c}_{t}^{K V}},}}\\\\\\\\ {{{\\\\displaystyle\\\\left[{\\\\bf{\\\\overline{{k}}}}_{t}^{R}\\\\right]={\\\\mathrm{RoPE}}(W^{K R}{\\\\bf h}_{t})},}}\\\\\\\\ {{{\\\\displaystyle{\\\\bf{k}}_{t,i}=[{\\\\bf k}_{t,i}^{C};{\\\\bf k}_{t}^{R}]},}}\\\\\\\\ {{{\\\\displaystyle[{\\\\bf v}_{t,1}^{C};{\\\\bf v}_{t,2}^{C};...;{\\\\bf v}_{t,n_{h}}^{C}]={\\\\bf v}_{t}^{C}=W^{U V}{\\\\bf c}_{t}^{K V}},}}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where $\\\\mathbf{c}_{t}^{K V}\\\\in\\\\mathbb{R}^{d_{c}}$ is the compressed latent vector for keys and values; $d_{c}(\\\\ll d_{h}n_{h})$ indicates the KV compression dimension; $\\\\dot{W^{D K V}}\\\\in\\\\mathbb{R}^{d_{c}\\\\times d}$ denotes the down-projection matrix; $W^{U K}$ , $W^{U V}\\\\in\\\\mathbb{R}^{d_{h}n_{h}\\\\times d_{c}}$ are the up-projection matrices for keys and values, respectively; $W^{K R}\\\\in\\\\mathbb{R}^{d_{h}^{R}\\\\times d}$ is the matrix used to produce the decoupled key that carries Rotary Positional Embedding (RoPE) (Su et al., 2024); RoPE(\\u00b7) denotes the operation that applies RoPE matrices; and $[\\\\cdot;\\\\cdot]$ denotes concatenation. Note that for MLA, only the blue-boxed vectors (i.e., $\\\\mathbf{c}_{t}^{K V}$ and $\\\\mathbf{k}_{t}^{R}$ ) need to be cached during generation, which results in significantly reduced KV cache while maintaining performance comparable to standard Multi-Head Attention (MHA) (Vaswani et al., 2017). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"For the attention queries, we also perform a low-rank compression, which can reduce the activation memory during training: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{c}{{\\\\displaystyle{\\\\bf c}_{t}^{Q}=W^{D Q}{\\\\bf h}_{t},}}\\\\\\\\ {{\\\\displaystyle[{\\\\bf q}_{t,1}^{C};{\\\\bf q}_{t,2}^{C};...;{\\\\bf q}_{t,n_{h}}^{C}]={\\\\bf q}_{t}^{C}=W^{U Q}{\\\\bf c}_{t}^{Q},}}\\\\\\\\ {{\\\\displaystyle[{\\\\bf q}_{t,1}^{R};{\\\\bf q}_{t,2}^{R};...;{\\\\bf q}_{t,n_{h}}^{R}]={\\\\bf q}_{t}^{R}=\\\\mathrm{RoPE}(W^{Q R}{\\\\bf c}_{t}^{Q}),}}\\\\\\\\ {{\\\\displaystyle{\\\\bf q}_{t,i}=[{\\\\bf q}_{t,i}^{C};{\\\\bf q}_{t,i}^{R}],}}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where $\\\\mathbf{c}_{t}^{Q}\\\\in\\\\mathbb{R}^{d_{c}^{\\\\prime}}$ is the compressed latent vector for queries; $d_{c}^{\\\\prime}(\\\\ll~d_{h}n_{h})$ denotes the query compression dimension; $W^{D\\\\hat{Q}}\\\\in\\\\mathbb R^{d_{c}^{\\\\prime}\\\\times d},W^{U Q}\\\\in\\\\mathbb R^{d_{h}n_{h}\\\\times d_{c}^{\\\\prime}}$ are the down-projection and up-projection matrices for queries, respectively; and $W^{Q R}\\\\in\\\\mathbb{R}^{d_{h}^{R}n_{h}\\\\times d_{c}^{\\\\prime}}$ is the matrix to produce the decoupled queries that carry RoPE. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Ultimately, the attention queries $(\\\\mathbf{q}_{t,i})$ , keys $(\\\\mathbf{k}_{j,i})$ , and values $(\\\\mathbf{v}_{j,i}^{C})$ are combined to yield the final attention output ${\\\\bf u}_{t}$ : \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{l}{{\\\\displaystyle{\\\\bf{o}}_{t,i}=\\\\sum_{j=1}^{t}{\\\\cal{S}}\\\\mathrm{oftmax}_{j}(\\\\frac{{\\\\bf{q}}_{t,i}^{T}{\\\\bf{k}}_{j,i}}{\\\\sqrt{d_{h}+d_{h}^{R}}}){\\\\bf{v}}_{j,i}^{C}},\\\\ ~}\\\\\\\\ {{\\\\bf{u}}_{t}={\\\\cal{W}}^{O}[{\\\\bf{o}}_{t,1};{\\\\bf{o}}_{t,2};...;{\\\\bf{o}}_{t,n_{h}}],\\\\ ~}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where $W^{O}\\\\in\\\\mathbb{R}^{d\\\\times d_{h}n_{h}}$ denotes the output projection matrix. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Basic Architecture of DeepSeekMoE. For Feed-Forward Networks (FFNs), DeepSeek-V3 employs the DeepSeekMoE architecture (Dai et al., 2024). Compared with traditional MoE architectures like GShard (Lepikhin et al., 2021), DeepSeekMoE uses finer-grained experts and isolates some experts as shared ones. Let ${\\\\bf u}_{t}$ denote the FFN input of the $t$ -th token, we compute the FFN output ${\\\\bf h}_{t}^{\\\\prime}$ as follows: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{l}{{\\\\displaystyle{\\\\bf h}_{t}^{\\\\prime}={\\\\bf u}_{t}+\\\\sum_{i=1}^{N_{s}}\\\\mathrm{FFN}_{i}^{(s)}\\\\left({\\\\bf u}_{t}\\\\right)+\\\\sum_{i=1}^{N_{r}}g_{i,t}\\\\mathrm{FFN}_{i}^{(r)}\\\\left({\\\\bf u}_{t}\\\\right)},}}\\\\\\\\ {{\\\\displaystyle g_{i,t}=\\\\frac{g_{i,t}^{\\\\prime}}{\\\\sum_{j=1}^{N_{r}}g_{j,t}^{\\\\prime}}},}\\\\\\\\ {{\\\\displaystyle g_{i,t}^{\\\\prime}=\\\\left\\\\{{s_{i,t}},\\\\quad s_{i},\\\\in\\\\mathrm{Topk}(\\\\{s_{j,t}|1\\\\leqslant j\\\\leqslant N_{r}\\\\},K_{r}),\\\\right.}}\\\\\\\\ {{\\\\displaystyle\\\\left.0,\\\\quad\\\\mathrm{otherwise}},\\\\right.}}\\\\\\\\ {{\\\\displaystyle s_{i,t}=\\\\mathrm{Sigmoid}\\\\left({\\\\bf u}_{t}^{,T}{\\\\bf e}_{i}\\\\right)},}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where $N_{s}$ and $N_{r}$ denote the numbers of shared experts and routed experts, respectively; $\\\\mathrm{FFN}_{i}^{(s)}(\\\\cdot)$ and $\\\\mathrm{FFN}_{i}^{(r)}(\\\\cdot)$ denote the \\ud835\\udc56-th shared expert and the $i\\\\cdot$ -th routed expert, respectively; $K_{r}$ denotes the number of activated routed experts; $g_{i,t}$ is the gating value for the $i$ -th expert; $s_{i,t}$ is the token-to-expert affinity; $\\\\mathbf{e}_{i}$ is the centroid vector of the $i\\\\cdot$ -th routed expert; and $\\\\mathrm{Topk}(\\\\cdot,K)$ denotes the set comprising $K$ highest scores among the affinity scores calculated for the $t$ -th token and all routed experts. Slightly different from DeepSeek-V2, DeepSeek-V3 uses the sigmoid function to compute the affinity scores, and applies a normalization among all selected affinity scores to produce the gating values. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Auxiliary-Loss-Free Load Balancing. For MoE models, an unbalanced expert load will lead to routing collapse (Shazeer et al., 2017) and diminish computational efficiency in scenarios with expert parallelism. Conventional solutions usually rely on the auxiliary loss (Fedus et al., 2021; Lepikhin et al., 2021) to avoid unbalanced load. However, too large an auxiliary loss will impair the model performance (Wang et al., 2024a). To achieve a better trade-off between load balance and model performance, we pioneer an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) to ensure load balance. To be specific, we introduce a bias term $b_{i}$ for each expert and add it to the corresponding affinity scores $s_{i,t}$ to determine the top-K routing: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\ng_{i,t}^{\\\\prime}=\\\\left\\\\{\\\\begin{array}{l l}{s_{i,t},\\\\quad s_{i,t}+b_{i}\\\\in\\\\mathrm{Topk}(\\\\{s_{j,t}+b_{j}|1\\\\leqslant j\\\\leqslant N_{r}\\\\},K_{r}),}\\\\\\\\ {0,\\\\quad\\\\mathrm{otherwise}.}\\\\end{array}\\\\right.\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Note that the bias term is only used for routing. The gating value, which will be multiplied with the FFN output, is still derived from the original affinity score $s_{i,t}$ . During training, we keep monitoring the expert load on the whole batch of each training step. At the end of each step, we will decrease the bias term by $\\\\gamma$ if its corresponding expert is overloaded, and increase it by $\\\\gamma$ if its corresponding expert is underloaded, where $\\\\gamma$ is a hyper-parameter called bias update speed. Through the dynamic adjustment, DeepSeek-V3 keeps balanced expert load during training, and achieves better performance than models that encourage load balance through pure auxiliary losses. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Complementary Sequence-Wise Auxiliary Loss. Although DeepSeek-V3 mainly relies on the auxiliary-loss-free strategy for load balance, to prevent extreme imbalance within any single sequence, we also employ a complementary sequence-wise balance loss: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{r l r}{\\\\lefteqn{\\\\mathcal{L}_{\\\\mathrm{Bal}}=\\\\alpha\\\\sum_{i=1}^{N_{r}}f_{i}P_{i},}}\\\\\\\\ &{f_{i}=\\\\frac{N_{r}}{K_{r}T}\\\\sum_{\\\\ell=1}^{T}\\\\mathbb{1}\\\\left(s_{i,\\\\ell}\\\\-\\\\mathrm{{T}}\\\\mathrm{{opk}}(\\\\{s_{j,\\\\ell}|1\\\\leqslant j\\\\leqslant N_{r}\\\\},K_{r})\\\\right),}\\\\\\\\ &{s_{i,\\\\ell}^{\\\\prime}=\\\\frac{s_{i,\\\\ell}}{\\\\sum_{j=1}^{N_{r}}s_{j,\\\\ell}},}\\\\\\\\ &{}&{P_{i}=\\\\frac{1}{T}\\\\sum_{\\\\ell=1}^{T}s_{i,\\\\ell}^{\\\\prime},}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where the balance factor $\\\\alpha$ is a hyper-parameter, which will be assigned an extremely small value for DeepSeek-V3; $\\\\Im(\\\\cdot)$ denotes the indicator function; and $T$ denotes the number of tokens in a sequence. The sequence-wise balance loss encourages the expert load on each sequence to be balanced. \"}}, {\"type\": \"image\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/6d0fae87957c43036f1578472f7c8b774d8aac67ba5a91dc447b9583e4c07b4b.jpg\", \"img_caption\": [\"Figure 3 | Illustration of our Multi-Token Prediction (MTP) implementation. We keep the complete causal chain for the prediction of each token at each depth. \"]}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Node-Limited Routing. Like the device-limited routing used by DeepSeek-V2, DeepSeek-V3 also uses a restricted routing mechanism to limit communication costs during training. In short, we ensure that each token will be sent to at most \\ud835\\udc40 nodes, which are selected according to the sum of the highest $\\\\frac{K_{r}}{M}$ affinity scores of the experts distributed on each node. Under this constraint, our MoE training framework can nearly achieve full computation-communication overlap. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"No Token-Dropping. Due to the effective load balancing strategy, DeepSeek-V3 keeps a good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training. In addition, we also implement specific deployment strategies to ensure inference load balance, so DeepSeek-V3 also does not drop tokens during inference. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"2.2. Multi-Token Prediction \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Inspired by Gloeckle et al. (2024), we investigate and set a Multi-Token Prediction (MTP) objective for DeepSeek-V3, which extends the prediction scope to multiple future tokens at each position. On the one hand, an MTP objective densifies the training signals and may improve data efficiency. On the other hand, MTP may enable the model to pre-plan its representations for better prediction of future tokens. Figure 3 illustrates our implementation of MTP. Different from Gloeckle et al. (2024), which parallelly predicts $D$ additional tokens using independent output heads, we sequentially predict additional tokens and keep the complete causal chain at each prediction depth. We introduce the details of our MTP implementation in this section. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"MTP Modules. To be specific, our MTP implementation uses $D$ sequential modules to predict $D$ additional tokens. The $k$ -th MTP module consists of a shared embedding layer $\\\\operatorname{Emb}({\\\\cdot})$ , a shared output head OutHead $(\\\\cdot)$ , a Transformer block $\\\\mathrm{TRM}_{k}(\\\\cdot).$ , and a projection matrix $M_{k}\\\\in\\\\mathbb{R}^{d\\\\times2d}$ . For the $i\\\\cdot$ -th input token $t_{i},$ at the $k$ -th prediction depth, we first combine the representation of the $i$ -th token at the $(k-1)$ -th depth $\\\\mathbf{h}_{i}^{k-1}\\\\in\\\\mathbb{R}^{d}$ and the embedding of the $(i+k)$ -th token $E m b(t_{i+k})\\\\in\\\\mathbb{R}^{d}$ \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"with the linear projection: \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\n\\\\mathbf{h}_{i}^{\\\\prime k}=M_{k}[\\\\mathrm{RMSNorm}(\\\\mathbf{h}_{i}^{k-1});\\\\mathrm{RMSNorm}(\\\\mathrm{Emb}(t_{i+k}))],\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"where $[\\\\cdot;\\\\cdot]$ denotes concatenation. Especially, when $k=1,\\\\mathbf{h}_{i}^{k-1}$ refers to the representation given by the main model. Note that for each MTP module, its embedding layer is shared with the main model. The combined $\\\\mathbf{h}_{i}^{\\\\prime k}$ serves as the input of the Transformer block at the $k$ -th depth to produce the output representation at the current depth $\\\\mathbf{h}_{i}^{k}$ : \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\n\\\\begin{array}{r}{\\\\mathbf{h}_{1:T-k}^{k}=\\\\mathrm{TRM}_{k}(\\\\mathbf{h}_{1:T-k}^{\\\\prime k}),}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"where $T$ represents the input sequence length and $\\\\boldsymbol{i}{:}\\\\boldsymbol{j}$ denotes the slicing operation (inclusive of both the left and right boundaries). Finally, taking $\\\\mathbf{h}_{i}^{k}$ as the input, the shared output head will compute the probability distribution for the $k$ -th additional prediction token \\ud835\\udc43\\ud835\\udc56\\ud835\\udc58 1 \\ud835\\udc58 \\u2208 R\\ud835\\udc49 , where $V$ is the vocabulary size: \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\nP_{i+k+1}^{k}=\\\\mathrm{OutHead}(\\\\mathbf{h}_{i}^{k}).\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"The output head OutHead $(\\\\cdot)$ linearly maps the representation to logits and subsequently applies the Softmax(\\u00b7) function to compute the prediction probabilities of the $k$ -th additional token. Also, for each MTP module, its output head is shared with the main model. Our principle of maintaining the causal chain of predictions is similar to that of EAGLE (Li et al., 2024b), but its primary objective is speculative decoding (Leviathan et al., 2023; Xia et al., 2023), whereas we utilize MTP to improve training. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"$\\\\mathcal{L}_{\\\\mathrm{MTP}}^{k}$ \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\n\\\\mathcal{L}_{\\\\mathrm{MTP}}^{k}=\\\\mathrm{CrossEntropy}(P_{2+k:T+1}^{k},t_{2+k:T+1})=-\\\\frac{1}{T}\\\\sum_{i=2+k}^{T+1}\\\\log P_{i}^{k}[t_{i}],\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"where $T$ denotes the input sequence length, $t_{i}$ denotes the ground-truth token at the $i\\\\cdot$ -th position, and $P_{i}^{k}[t_{i}]$ denotes the corresponding prediction probability of $t_{i},$ given by the $k$ -th MTP module. Finally, we compute the average of the MTP losses across all depths and multiply it by a weighting factor $\\\\lambda$ to obtain the overall MTP loss $\\\\mathcal{L}_{\\\\mathrm{MTP}}$ , which serves as an additional training objective for DeepSeek-V3: \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\n\\\\mathcal{L}_{\\\\mathrm{MTP}}=\\\\frac{\\\\lambda}{D}\\\\sum_{k=1}^{D}\\\\mathcal{L}_{\\\\mathrm{MTP}}^{k}.\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"MTP in Inference. Our MTP strategy mainly aims to improve the performance of the main model, so during inference, we can directly discard the MTP modules and the main model can function independently and normally. Additionally, we can also repurpose these MTP modules for speculative decoding to further improve the generation latency. \"}}]}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"3. Infrastructures \"}, \"children\": [{\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"3.1. Compute Clusters \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs. Each node in the H800 cluster contains 8 GPUs connected by NVLink and NVSwitch within nodes. Across different nodes, InfiniBand (IB) interconnects are utilized to facilitate communications. \"}}, {\"type\": \"image\", \"level\": 2, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/a4465b20c980d401769d2328a06b9e892e925e081f435eff4c8cf6c4393990e1.jpg\", \"img_caption\": [\"Figure 4 Overlapping strategy for a pair of individual forward and backward chunks (the boundaries of the transformer blocks are not aligned). Orange denotes forward, green denotes \\\"backward for input\\\", blue denotes \\\"backward for weights\\\", purple denotes PP communication, and red denotes barriers. Both all-to-all and PP communication can be fully hidden. \"]}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"3.2. Training Framework \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up. On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Parallelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajbhandari et al., 2020). \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"In order to facilitate efficient training of DeepSeek-V3, we implement meticulous engineering optimizations. Firstly, we design the DualPipe algorithm for efficient pipeline parallelism. Compared with existing PP methods, DualPipe has fewer pipeline bubbles. More importantly, it overlaps the computation and communication phases across forward and backward processes, thereby addressing the challenge of heavy communication overhead introduced by cross-node expert parallelism. Secondly, we develop efficient cross-node all-to-all communication kernels to fully utilize IB and NVLink bandwidths and conserve Streaming Multiprocessors (SMs) dedicated to communication. Finally, we meticulously optimize the memory footprint during training, thereby enabling us to train DeepSeek-V3 without using costly Tensor Parallelism (TP). \"}}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.2.1. DualPipe and Computation-Communication Overlap \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"For DeepSeek-V3, the communication overhead introduced by cross-node expert parallelism results in an inefficient computation-to-communication ratio of approximately 1:1. To tackle this challenge, we design an innovative pipeline parallelism algorithm called DualPipe, which not only accelerates model training by effectively overlapping forward and backward computationcommunication phases, but also reduces the pipeline bubbles. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"The key idea of DualPipe is to overlap the computation and communication within a pair of individual forward and backward chunks. To be specific, we divide each chunk into four components: attention, all-to-all dispatch, MLP, and all-to-all combine. Specially, for a backward chunk, both attention and MLP are further split into two parts, backward for input and backward for weights, like in ZeroBubble (Qi et al., 2023b). In addition, we have a PP communication component. As illustrated in Figure 4, for a pair of forward and backward chunks, we rearrange these components and manually adjust the ratio of GPU SMs dedicated to communication versus computation. In this overlapping strategy, we can ensure that both all-to-all and PP communication can be fully hidden during execution. Given the efficient overlapping strategy, the full DualPipe scheduling is illustrated in Figure 5. It employs a bidirectional pipeline scheduling, which feeds micro-batches from both ends of the pipeline simultaneously and a significant portion of communications can be fully overlapped. This overlap also ensures that, as the model further scales up, as long as we maintain a constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead. \"}}, {\"type\": \"image\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/6fffbffe0474c0ffe9c917374fde2a3c8a46b6369cf79245b61302e1158aa67d.jpg\", \"img_caption\": [\"Figure 5 | Example DualPipe scheduling for 8 PP ranks and 20 micro-batches in two directions. The micro-batches in the reverse direction are symmetric to those in the forward direction, so we omit their batch ID for illustration simplicity. Two cells enclosed by a shared black border have mutually overlapped computation and communication. \"]}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/7854d0a078215e4170ef86724e63cd844337966403b9f5f3e427db6c469470e0.jpg\", \"table_body\": \"\\n\\n<html><body><table><tr><td>Method</td><td>Bubble</td><td>Parameter</td><td>Activation</td></tr><tr><td>1F1B</td><td>(PP -1)(F+B)</td><td>1x</td><td>PP</td></tr><tr><td>ZB1P</td><td>(PP -1)(F+B-2W)</td><td>1x</td><td>PP</td></tr><tr><td>DualPipe (Ours)</td><td>(-1)(F&B+B-3W)</td><td>2x</td><td>PP+1</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Table 2 Comparison of pipeline bubbles and memory usage across different pipeline parallel methods. $F$ denotes the execution time of a forward chunk, $B$ denotes the execution time of a full backward chunk, \\ud835\\udc4a denotes the execution time of a \\\"backward for weights\\\" chunk, and $F\\\\&B$ denotes the execution time of two mutually overlapped forward and backward chunks. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In addition, even in more general scenarios without a heavy communication burden, DualPipe still exhibits efficiency advantages. In Table 2, we summarize the pipeline bubbles and memory usage across different PP methods. As shown in the table, compared with ZB1P (Qi et al., 2023b) and 1F1B (Harlap et al., 2018), DualPipe significantly reduces the pipeline bubbles while only increasing the peak activation memory by $\\\\scriptstyle{\\\\frac{1}{P P}}$ times. Although DualPipe requires keeping two copies of the model parameters, this does not significantly increase the memory consumption since we use a large EP size during training. Compared with Chimera (Li and Hoefler, 2021), DualPipe only requires that the pipeline stages and micro-batches be divisible by 2, without requiring micro-batches to be divisible by pipeline stages. In addition, for DualPipe, neither the bubbles nor activation memory will increase as the number of micro-batches grows. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In order to ensure sufficient computational performance for DualPipe, we customize efficient cross-node all-to-all communication kernels (including dispatching and combining) to conserve the number of SMs dedicated to communication. The implementation of the kernels is codesigned with the MoE gating algorithm and the network topology of our cluster. To be specific, in our cluster, cross-node GPUs are fully interconnected with IB, and intra-node communications are handled via NVLink. NVLink offers a bandwidth of 160 GB/s, roughly 3.2 times that of IB $(50\\\\mathrm{GB}/\\\\mathrm{s})$ . To effectively leverage the different bandwidths of IB and NVLink, we limit each token to be dispatched to at most 4 nodes, thereby reducing IB traffic. For each token, when its routing decision is made, it will first be transmitted via IB to the GPUs with the same in-node index on its target nodes. Once it reaches the target nodes, we will endeavor to ensure that it is instantaneously forwarded via NVLink to specific GPUs that host their target experts, without being blocked by subsequently arriving tokens. In this way, communications via IB and NVLink are fully overlapped, and each token can efficiently select an average of 3.2 experts per node without incurring additional overhead from NVLink. This implies that, although DeepSeek-V3 selects only 8 routed experts in practice, it can scale up this number to a maximum of 13 experts (4 nodes $\\\\times3.2$ experts/node) while preserving the same communication cost. Overall, under such a communication strategy, only 20 SMs are sufficient to fully utilize the bandwidths of IB and NVLink. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In detail, we employ the warp specialization technique (Bauer et al., 2014) and partition 20 SMs into 10 communication channels. During the dispatching process, (1) IB sending, (2) IB-to-NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The number of warps allocated to each communication task is dynamically adjusted according to the actual workload across all SMs. Similarly, during the combining process, (1) NVLink sending, (2) NVLink-to-IB forwarding and accumulation, and (3) IB receiving and accumulation are also handled by dynamically adjusted warps. In addition, both dispatching and combining kernels overlap with the computation stream, so we also consider their impact on other SM computation kernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and auto-tune the communication chunk size, which significantly reduces the use of the L2 cache and the interference to other SMs. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.2.3. Extremely Memory Saving with Minimal Overhead \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In order to reduce the memory footprint during training, we employ the following techniques. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Recomputation of RMSNorm and MLA Up-Projection. We recompute all RMSNorm operations and MLA up-projections during back-propagation, thereby eliminating the need to persistently store their output activations. With a minor overhead, this strategy significantly reduces memory requirements for storing activations. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Exponential Moving Average in CPU. During training, we preserve the Exponential Moving Average (EMA) of the model parameters for early estimation of the model performance after learning rate decay. The EMA parameters are stored in CPU memory and are updated asynchronously after each training step. This method allows us to maintain EMA parameters without incurring additional memory or time overhead. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Shared Embedding and Output Head for Multi-Token Prediction. With the DualPipe strategy, we deploy the shallowest layers (including the embedding layer) and deepest layers (including the output head) of the model on the same PP rank. This arrangement enables the physical sharing of parameters and gradients, of the shared embedding and output head, between the MTP module and the main model. This physical sharing mechanism further enhances our memory efficiency. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"3.3. FP8 Training \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Inspired by recent advances in low-precision training (Dettmers et al., 2022; Noune et al., 2022; Peng et al., 2023b), we propose a fine-grained mixed precision framework utilizing the FP8 data format for training DeepSeek-V3. While low-precision training holds great promise, it is often limited by the presence of outliers in activations, weights, and gradients (Fishman et al., 2024; He et al.; Sun et al., 2024). Although significant progress has been made in inference quantization (Frantar et al., 2022; Xiao et al., 2023), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model pre-training (Fishman et al., 2024). To address thi\\u6216s ch\\u8005alIlenpgeuat-nd>eAffcectitivealtyi eoxnte_ndLthe dynamic range of the FP8 format, we introduce a fine-grained quantization strategy: tile-wise grouping with $1\\\\times N_{c}$ elements or block-wise grouping with $N_{c}\\\\times N_{c}$ uetlepmuetn-t>s.ATchteiavssaotciioatned_{dLeq+u1an}tization overhead is largely mitigated under our increased-precision accumulation process, a critical aspect for achieving accurate FP8 General Matrix Multiplication (GEMM). Moreover, to further reduce memory and communication overhead in MoE training, we cache and dispatch activations in FP8, while storing low-precision optimizer states in BF16. We validate the proposed FP8 mixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeekV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably, compared with the BF16 baseline, the relative loss error of our FP8-training model remains consistently below $0.25\\\\%$ , a level well within the acceptable range of training randomness. \"}}, {\"type\": \"image\", \"level\": 2, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/ec4d0e65b5484cb0a5fc05ae43186a6ed454d109a9645a64bf7349b9abd4e3ae.jpg\", \"img_caption\": [\"Figure 6 | The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. \"]}}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.3.1. Mixed Precision Framework \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Building upon widely adopted techniques in low-precision training (Kalamkar et al., 2019; Narang et al., 2017), we propose a mixed precision framework for FP8 training. In this framework, most compute-density operations are conducted in FP8, while a few key operations are strategically maintained in their original data formats to balance training efficiency and numerical stability. The overall framework is illustrated in Figure 6. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Firstly, in order to accelerate model training, the majority of core computation kernels, i.e., GEMM operations, are implemented in FP8 precision. These GEMM operations accept FP8 tensors as inputs and produce outputs in BF16 or FP32. As depicted in Figure 6, all three GEMMs associated with the Linear operator, namely Fprop (forward pass), Dgrad (activation backward pass), and Wgrad (weight backward pass), are executed in FP8. This design theoretically doubles the computational speed compared with the original BF16 method. Additionally, the FP8 Wgrad GEMM allows activations to be stored in FP8 for use in the backward pass. This significantly reduces memory consumption. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Despite the efficiency advantage of the FP8 format, certain operators still require a higher precision due to their sensitivity to low-precision computations. Besides, some low-cost operators can also utilize a higher precision with a negligible overhead to the overall training cost. For this reason, after careful investigations, we maintain the original precision (e.g., BF16 or FP32) for the following components: the embedding module, the output head, MoE gating modules, normalization operators, and attention operators. These targeted retentions of high precision ensure stable training dynamics for DeepSeek-V3. To further guarantee numerical stability, we store the master weights, weight gradients, and optimizer states in higher precision. While these high-precision components incur some memory overheads, their impact can be minimized through efficient sharding across multiple DP ranks in our distributed training system. \"}}, {\"type\": \"image\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/4838bb048c259d0985856e0b4fbdec39bbdbf7fa33517c90d7f2c5d59d8bfc2a.jpg\", \"img_caption\": [\"Figure 7 | (a) We propose a fine-grained quantization method to mitigate quantization errors caused by feature outliers; for illustration simplicity, only Fprop is illustrated. (b) In conjunction with our quantization strategy, we improve the FP8 GEMM precision by promoting to CUDA Cores at an interval of $N_{C}=128$ elements MMA for the high-precision accumulation. \"]}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.3.2. Improved Precision from Quantization and Multiplication \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Based on our mixed precision FP8 framework, we introduce several strategies to enhance lowprecision training accuracy, focusing on both the quantization method and the multiplication process. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Fine-Grained Quantization. In low-precision training frameworks, overflows and underflows are common challenges due to the limited dynamic range of the FP8 format, which is constrained by its reduced exponent bits. As a standard practice, the input distribution is aligned to the representable range of the FP8 format by scaling the maximum absolute value of the input tensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes lowprecision training highly sensitive to activation outliers, which can heavily degrade quantization accuracy. To solve this, we propose a fine-grained quantization method that applies scaling at a more granular level. As illustrated in Figure 7 (a), (1) for activations, we group and scale elements on a 1x128 tile basis (i.e., per token per 128 channels); and (2) for weights, we group and scale elements on a 128x128 block basis (i.e., per 128 input channels per 128 output channels). This approach ensures that the quantization process can better accommodate outliers by adapting the scale according to smaller groups of elements. In Appendix B.2, we further discuss the training instability when we group and scale activations on a block basis in the same way as weights quantization. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"One key modification in our method is the introduction of per-group scaling factors along the inner dimension of GEMM operations. This functionality is not directly supported in the standard FP8 GEMM. However, combined with our precise FP32 accumulation strategy, it can \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"be efficiently implemented. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Notably, our fine-grained quantization strategy is highly consistent with the idea of microscaling formats (Rouhani et al., 2023b), while the Tensor Cores of NVIDIA next-generation GPUs (Blackwell series) have announced the support for microscaling formats with smaller quantization granularity (NVIDIA, 2024a). We hope our design can serve as a reference for future work to keep pace with the latest GPU architectures. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Increasing Accumulation Precision. Low-precision GEMM operations often suffer from underflow issues, and their accuracy largely depends on high-precision accumulation, which is commonly performed in an FP32 precision (Kalamkar et al., 2019; Narang et al., 2017). However, we observe that the accumulation precision of FP8 GEMM on NVIDIA H800 GPUs is limited to retaining around 14 bits, which is significantly lower than FP32 accumulation precision. This problem will become more pronounced when the inner dimension K is large (Wortsman et al., 2023), a typical scenario in large-scale model training where the batch size and model width are increased. Taking GEMM operations of two random matrices with $\\\\mathtt{K}=4096$ for example, in our preliminary test, the limited accumulation precision in Tensor Cores results in a maximum relative error of nearly $2\\\\%$ . Despite these problems, the limited accumulation precision is still the default option in a few FP8 frameworks (NVIDIA, 2024b), severely constraining the training accuracy. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In order to address this issue, we adopt the strategy of promotion to CUDA Cores for higher precision (Thakkar et al., 2023). The process is illustrated in Figure 7 (b). To be specific, during MMA (Matrix Multiply-Accumulate) execution on Tensor Cores, intermediate results are accumulated using the limited bit width. Once an interval of $N_{C}$ is reached, these partial results will be copied to FP32 registers on CUDA Cores, where full-precision FP32 accumulation is performed. As mentioned before, our fine-grained quantization applies per-group scaling factors along the inner dimension K. These scaling factors can be efficiently multiplied on the CUDA Cores as the dequantization process with minimal additional computational cost. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"It is worth noting that this modification reduces the WGMMA (Warpgroup-level Matrix Multiply-Accumulate) instruction issue rate for a single warpgroup. However, on the H800 architecture, it is typical for two WGMMA to persist concurrently: while one warpgroup performs the promotion operation, the other is able to execute the MMA operation. This design enables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based on our experiments, setting $N_{C}=128$ elements, equivalent to 4 WGMMAs, represents the minimal accumulation interval that can significantly improve precision without introducing substantial overhead. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Mantissa over Exponents. In contrast to the hybrid FP8 format adopted by prior work (NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses E4M3 (4-bit exponent and 3-bit mantissa) in Fprop and E5M2 (5-bit exponent and 2-bit mantissa) in Dgrad and Wgrad, we adopt the E4M3 format on all tensors for higher precision. We attribute the feasibility of this approach to our fine-grained quantization strategy, i.e., tile and block-wise scaling. By operating on smaller element groups, our methodology effectively shares exponent bits among these grouped elements, mitigating the impact of the limited dynamic range. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Online Quantization. Delayed quantization is employed in tensor-wise quantization frameworks (NVIDIA, 2024b; Peng et al., 2023b), which maintains a history of the maximum absolute values across prior iterations to infer the current value. In order to ensure accurate scales and simplify the framework, we calculate the maximum absolute value online for each 1x128 activation tile or $128\\\\mathrm{x}128$ weight block. Based on it, we derive the scaling factor and then quantize the activation or weight online into the FP8 format. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.3.3. Low-Precision Storage and Communication \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In conjunction with our FP8 training framework, we further reduce the memory consumption and communication overhead by compressing cached activations and optimizer states into lower-precision formats. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Low-Precision Optimizer States. We adopt the BF16 data format instead of FP32 to track the first and second moments in the AdamW (Loshchilov and Hutter, 2017) optimizer, without incurring observable performance degradation. However, the master weights (stored by the optimizer) and gradients (used for batch size accumulation) are still retained in FP32 to ensure numerical stability throughout training. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Low-Precision Activation. As illustrated in Figure 6, the Wgrad operation is performed in FP8. To reduce the memory consumption, it is a natural choice to cache activations in FP8 format for the backward pass of the Linear operator. However, special considerations are taken on several operators for low-cost high-precision training: \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"(1) Inputs of the Linear after the attention operator. These activations are also used in the backward pass of the attention operator, which makes it sensitive to precision. We adopt a customized E5M6 data format exclusively for these activations. Additionally, these activations will be converted from an 1x128 quantization tile to an $128\\\\mathbf{x}\\\\mathbf{1}$ tile in the backward pass. To avoid introducing extra quantization error, all the scaling factors are round scaled, i.e., integral power of 2. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"(2) Inputs of the SwiGLU operator in MoE. To further reduce the memory cost, we cache the inputs of the SwiGLU operator and recompute its output in the backward pass. These activations are also stored in FP8 with our fine-grained quantization method, striking a balance between memory efficiency and computational accuracy. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Low-Precision Communication. Communication bandwidth is a critical bottleneck in the training of MoE models. To alleviate this challenge, we quantize the activation before MoE up-projections into FP8 and then apply dispatch components, which is compatible with FP8 Fprop in MoE up-projections. Like the inputs of the Linear after the attention operator, scaling factors for this activation are integral power of 2. A similar strategy is applied to the activation gradient before MoE down-projections. For both the forward and backward combine components, we retain them in BF16 to preserve training precision in critical parts of the training pipeline. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"3.4. Inference and Deployment \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected using NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously ensure both the Service-Level Objective (SLO) for online services and high throughput, we employ the following deployment strategy that separates the prefilling and decoding stages. \"}}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.4.1. Prefilling \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"The minimum deployment unit of the prefilling stage consists of 4 nodes with 32 GPUs. The attention part employs 4-way Tensor Parallelism (TP4) with Sequence Parallelism (SP), combined with 8-way Data Parallelism (DP8). Its small TP size of 4 limits the overhead of TP communication. For the MoE part, we use 32-way Expert Parallelism (EP32), which ensures that each expert processes a sufficiently large batch size, thereby enhancing computational efficiency. For the MoE all-to-all communication, we use the same method as in training: first transferring tokens across nodes via IB, and then forwarding among the intra-node GPUs via NVLink. In particular, we use 1-way Tensor Parallelism for the dense MLPs in shallow layers to save TP communication. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"To achieve load balancing among different experts in the MoE part, we need to ensure that each GPU processes approximately the same number of tokens. To this end, we introduce a deployment strategy of redundant experts, which duplicates high-load experts and deploys them redundantly. The high-load experts are detected based on statistics collected during the online deployment and are adjusted periodically (e.g., every 10 minutes). After determining the set of redundant experts, we carefully rearrange experts among GPUs within a node based on the observed loads, striving to balance the load across GPUs as much as possible without increasing the cross-node all-to-all communication overhead. For the deployment of DeepSeek-V3, we set 32 redundant experts for the prefilling stage. For each GPU, besides the original 8 experts it hosts, it will also host one additional redundant expert. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Furthermore, in the prefilling stage, to improve the throughput and hide the overhead of all-to-all and TP communication, we simultaneously process two micro-batches with similar computational workloads, overlapping the attention and MoE of one micro-batch with the dispatch and combine of another. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Finally, we are exploring a dynamic redundancy strategy for experts, where each GPU hosts more experts (e.g., 16 experts), but only 9 will be activated during each inference step. Before the all-to-all operation at each layer begins, we compute the globally optimal routing scheme on the fly. Given the substantial computation involved in the prefilling stage, the overhead of computing this routing scheme is almost negligible. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.4.2. Decoding \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"During decoding, we treat the shared expert as a routed one. From this perspective, each token will select 9 experts during routing, where the shared expert is regarded as a heavy-load one that will always be selected. The minimum deployment unit of the decoding stage consists of 40 nodes with 320 GPUs. The attention part employs TP4 with SP, combined with DP80, while the MoE part uses EP320. For the MoE part, each GPU hosts only one expert, and 64 GPUs are responsible for hosting redundant experts and shared experts. All-to-all communication of the dispatch and combine parts is performed via direct point-to-point transfers over IB to achieve low latency. Additionally, we leverage the IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Similar to prefilling, we periodically determine the set of redundant experts in a certain interval, based on the statistical expert load from our online service. However, we do not need to rearrange experts since each GPU only hosts one expert. We are also exploring the dynamic redundancy strategy for decoding. However, this requires more careful optimization of the algorithm that computes the globally optimal routing scheme and the fusion with the dispatch kernel to reduce overhead. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Additionally, to enhance throughput and hide the overhead of all-to-all communication, we are also exploring processing two micro-batches with similar computational workloads simultaneously in the decoding stage. Unlike prefilling, attention consumes a larger portion of time in the decoding stage. Therefore, we overlap the attention of one micro-batch with the dispatch+MoE $+$ combine of another. In the decoding stage, the batch size per expert is relatively small (usually within 256 tokens), and the bottleneck is memory access rather than computation. Since the MoE part only needs to load the parameters of one expert, the memory access overhead is minimal, so using fewer SMs will not significantly affect the overall performance. Therefore, to avoid impacting the computation speed of the attention part, we can allocate only a small portion of SMs to dispatch+MoE+combine. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"3.5. Suggestions on Hardware Design \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Based on our implementation of the all-to-all communication and FP8 training scheme, we propose the following suggestions on chip design to AI hardware vendors. \"}}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.5.1. Communication Hardware \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In DeepSeek-V3, we implement the overlap between computation and communication to hide the communication latency during computation. This significantly reduces the dependency on communication bandwidth compared to serial computation and communication. However, the current communication implementation relies on expensive SMs (e.g., we allocate 20 out of the 132 SMs available in the H800 GPU for this purpose), which will limit the computational throughput. Moreover, using SMs for communication results in significant inefficiencies, as tensor cores remain entirely under-utilized. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Currently, the SMs primarily perform the following tasks for all-to-all communication: \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"\\u2022 Forwarding data between the IB (InfiniBand) and NVLink domain while aggregating IB traffic destined for multiple GPUs within the same node from a single GPU.   \\n\\u2022 Transporting data between RDMA buffers (registered GPU memory regions) and input/output buffers.   \\n\\u2022 Executing reduce operations for all-to-all combine.   \\n\\u2022 Managing fine-grained memory layout during chunked data transferring to multiple experts across the IB and NVLink domain. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We aspire to see future vendors developing hardware that offloads these communication tasks from the valuable computation unit SM, serving as a GPU co-processor or a network co-processor like NVIDIA SHARP Graham et al. (2016). Furthermore, to reduce application programming complexity, we aim for this hardware to unify the IB (scale-out) and NVLink (scale-up) networks from the perspective of the computation units. With this unified interface, computation units can easily accomplish operations such as read, write, multicast, and reduce across the entire IB-NVLink-unified domain via submitting communication requests based on simple primitives. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.5.2. Compute Hardware \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Higher FP8 GEMM Accumulation Precision in Tensor Cores. In the current Tensor Core implementation of the NVIDIA Hopper architecture, FP8 GEMM suffers from limited accumulation precision. After aligning 32 mantissa products by right-shifting based on the maximum exponent, the Tensor Core only uses the highest 14 bits of each mantissa product for addition, and truncates bits exceeding this range. The accumulation of addition results into registers also employs 14-bit precision. Our implementation partially mitigates the limitation by accumulating the addition results of 128 FP8 $\\\\times$ FP8 multiplications into registers with FP32 precision in the CUDA core. Although helpful in achieving successful FP8 training, it is merely a compromise due to the Hopper architecture\\u2019s hardware deficiency in FP8 GEMM accumulation precision. Future chips need to adopt higher precision. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Support for Tile- and Block-Wise Quantization. Current GPUs only support per-tensor quantization, lacking the native support for fine-grained quantization like our tile- and blockwise quantization. In the current implementation, when the $N_{C}$ interval is reached, the partial results will be copied from Tensor Cores to CUDA cores, multiplied by the scaling factors, and added to FP32 registers on CUDA cores. Although the dequantization overhead is significantly mitigated combined with our precise FP32 accumulation strategy, the frequent data movements between Tensor Cores and CUDA cores still limit the computational efficiency. Therefore, we recommend future chips to support fine-grained quantization by enabling Tensor Cores to receive scaling factors and implement MMA with group scaling. In this way, the whole partial sum accumulation and dequantization can be completed directly inside Tensor Cores until the final result is produced, avoiding frequent data movements. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Support for Online Quantization. The current implementations struggle to effectively support online quantization, despite its effectiveness demonstrated in our research. In the existing process, we need to read 128 BF16 activation values (the output of the previous computation) from HBM (High Bandwidth Memory) for quantization, and the quantized FP8 values are then written back to HBM, only to be read again for MMA. To address this inefficiency, we recommend that future chips integrate FP8 cast and TMA (Tensor Memory Accelerator) access into a single fused operation, so quantization can be completed during the transfer of activations from global memory to shared memory, avoiding frequent memory reads and writes. We also recommend supporting a warp-level cast instruction for speedup, which further facilitates the better fusion of layer normalization and FP8 cast. Alternatively, a near-memory computing approach can be adopted, where compute logic is placed near the HBM. In this case, BF16 elements can be cast to FP8 directly as they are read from HBM into the GPU, reducing off-chip memory access by roughly $50\\\\%$ . \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Support for Transposed GEMM Operations. The current architecture makes it cumbersome to fuse matrix transposition with GEMM operations. In our workflow, activations during the forward pass are quantized into 1x128 FP8 tiles and stored. During the backward pass, the matrix needs to be read out, dequantized, transposed, re-quantized into 128x1 tiles, and stored in HBM. To reduce memory operations, we recommend future chips to enable direct transposed reads of matrices from shared memory before MMA operation, for those precisions required in both training and inference. Combined with the fusion of FP8 format conversion and TMA access, this enhancement will significantly streamline the quantization workflow. \"}}]}]}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"4. Pre-Training \"}, \"children\": [{\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.1. Data Construction \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity. Inspired by Ding et al. (2024), we implement the document packing method for data integrity but do not incorporate cross-sample attention masking during training. Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"In the training process of DeepSeekCoder-V2 (DeepSeek-AI, 2024a), we observe that the Fill-in-Middle (FIM) strategy does not compromise the next-token prediction capability while enabling the model to accurately predict middle text based on contextual cues. In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3. To be specific, we employ the Prefix-Suffix-Middle (PSM) framework to structure data as follows: \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\n<|\\\\mathbf{f}\\\\mathrm{i}\\\\mathrm{i}\\\\mathrm{m}_{-}\\\\mathrm{begin}|>f_{\\\\mathrm{pre}}<|\\\\mathbf{f}\\\\mathrm{i}\\\\mathrm{i}\\\\mathrm{m}_{-}\\\\mathrm{hole}|>f_{\\\\mathrm{suf}}<|\\\\mathbf{f}\\\\mathrm{i}\\\\mathrm{i}\\\\mathrm{m}_{-}\\\\mathrm{end}|>f_{\\\\mathrm{middle}}<|\\\\mathbf{e}\\\\mathrm{os}_{-}\\\\mathbf{t}\\\\mathrm{oken}|>.\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"This structure is applied at the document level as a part of the pre-packing process. The FIM strategy is applied at a rate of 0.1, consistent with the PSM framework. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens. The pretokenizer and training data for our tokenizer are modified to optimize multilingual compression efficiency. In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks. However, this trick may introduce the token boundary bias (Lundberg, 2023) when the model processes multi-line prompts without terminal line breaks, particularly for few-shot evaluation prompts. To address this issue, we randomly split a certain proportion of such combined tokens during training, which exposes the model to a wider array of special cases and mitigates this bias. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.2. Hyper-Parameters \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Model Hyper-Parameters. We set the number of Transformer layers to 61 and the hidden dimension to 7168. All learnable parameters are randomly initialized with a standard deviation of 0.006. In MLA, we set the number of attention heads $n_{h}$ to 128 and the per-head dimension $d_{h}$ to 128. The KV compression dimension $d_{c}$ is set to 512, and the query compression dimension $d_{c}^{\\\\prime}$ is set to 1536. For the decoupled queries and key, we set the per-head dimension $d_{h}^{R}$ to 64. We substitute all FFNs except for the first three layers with MoE layers. Each MoE layer consists of 1 shared expert and 256 routed experts, where the intermediate hidden dimension of each expert is 2048. Among the routed experts, 8 experts will be activated for each token, and each token will be ensured to be sent to at most 4 nodes. The multi-token prediction depth $D$ is set to 1, i.e., besides the exact next token, each token will predict one additional token. As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Training Hyper-Parameters. We employ the AdamW optimizer (Loshchilov and Hutter, 2017) with hyper-parameters set to $\\\\beta_{1}=0.9$ , $\\\\beta_{2}=0.95$ , and weight_decay $=0.1$ . We set the maximum sequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens. As for the learning rate scheduling, we first linearly increase it from 0 to $2.2\\\\times10^{-4}$ during the first 2K steps. Then, we keep a constant learning rate of $2.2\\\\times10^{-4}$ until the model consumes 10T training tokens. Subsequently, we gradually decay the learning rate to $2.2\\\\times10^{-5}$ in $4.3\\\\mathrm{T}$ tokens, following a cosine decay curve. During the training of the final 500B tokens, we keep a constant learning rate of $2.2\\\\times10^{-5}$ in the first 333B tokens, and switch to another constant learning rate of $7.3\\\\times10^{-6}$ in the remaining 167B tokens. The gradient clipping norm is set to 1.0. We employ a batch size scheduling strategy, where the batch size is gradually increased from 3072 to 15360 in the training of the first 469B tokens, and then keeps 15360 in the remaining training. We leverage pipeline parallelism to deploy different layers of a model on different GPUs, and for each layer, the routed experts will be uniformly deployed on 64 GPUs belonging to 8 nodes. As for the node-limited routing, each token will be sent to at most 4 nodes (i.e., $M=4$ ). For auxiliary-loss-free load balancing, we set the bias update speed \\ud835\\udefe to 0.001 for the first $14.3\\\\mathrm{T}$ tokens, and to 0.0 for the remaining 500B tokens. For the balance loss, we set $\\\\alpha$ to 0.0001, just to avoid extreme imbalance within any single sequence. The MTP loss weight $\\\\lambda$ is set to 0.3 for the first 10T tokens, and to 0.1 for the remaining 4.8T tokens. \"}}, {\"type\": \"image\", \"level\": 2, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/994846ca2a80bbe6297f6a9f75672fcf45337ab0224fb66ddde155a093c48072.jpg\", \"img_caption\": [\"Figure 8 | Evaluation results on the \\u201dNeedle In A Haystack\\u201d (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to 128K. \"]}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.3. Long Context Extension \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K. The YaRN configuration is consistent with that used in DeepSeek-V2, being applied exclusively to the decoupled shared key $\\\\mathbf{k}_{t}^{R}$ . The hyper-parameters remain identical across both phases, with the scale $s=40$ , $\\\\alpha=1$ , $\\\\beta=32$ , and the scaling factor $\\\\sqrt{t}=0.1\\\\ln s+1.$ . In the first phase, the sequence length is set to 32K, and the batch size is 1920. During the second phase, the sequence length is increased to $128\\\\mathsf{K},$ and the batch size is reduced to 480. The learning rate for both phases is set to $7.3\\\\times10^{-6}$ , matching the final learning rate from the pre-training stage. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Through this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to 128K in length while maintaining strong performance. Figure 8 illustrates that DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the \\\"Needle In A Haystack\\\" (NIAH) test, demonstrating consistent robustness across context window lengths up to 128K. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.4. Evaluations \"}, \"children\": [{\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"4.4.1. Evaluation Benchmarks \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"The base model of DeepSeek-V3 is pretrained on a multilingual corpus with English and Chinese constituting the majority, so we evaluate its performance on a series of benchmarks primarily in English and Chinese, as well as on a multilingual benchmark. Our evaluation is based on our internal evaluation framework integrated in our HAI-LLM framework. Considered benchmarks are categorized and listed as follows, where underlined benchmarks are in Chinese and double-underlined benchmarks are multilingual ones: \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Multi-subject multiple-choice datasets include MMLU (Hendrycks et al., 2020), MMLURedux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024b), MMMLU (OpenAI, 2024b), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Language understanding and reasoning datasets include HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and BigBench Hard (BBH) (Suzgun et al., 2022). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Closed-book question answering datasets include TriviaQA (Joshi et al., 2017) and NaturalQuestions (Kwiatkowski et al., 2019). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Reading comprehension datasets include RACE Lai et al. (2017), DROP (Dua et al., 2019), 3 (Sun et al., 2019a), and CMRC (Cui et al., 2019). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Reference disambiguation datasets include CLUEWSC (Xu et al., 2020) and WinoGrande Sakaguchi et al. (2019). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Language modeling datasets include Pile (Gao et al., 2020). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Chinese understanding and culture datasets include CCPM (Li et al., 2021). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Math datasets include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MGSM (Shi et al., 2023), and CMath (Wei et al., 2023). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Code datasets include HumanEval (Chen et al., 2021), LiveCodeBench-Base (0801-1101) (Jain et al., 2024), MBPP (Austin et al., 2021), and CRUXEval (Gu et al., 2024). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Standardized exams include AGIEval (Zhong et al., 2023). Note that AGIEval includes both English and Chinese subsets. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Following our previous work (DeepSeek-AI, $2024\\\\mathrm{b},\\\\mathrm{c})$ ), we adopt perplexity-based evaluation for datasets including HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High, MMLU, MMLU-Redux, MMLU-Pro, MMMLU, ARC-Easy, ARC-Challenge, C-Eval, CMMLU, C3, and CCPM, and adopt generation-based evaluation for TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, MGSM, HumanEval, MBPP, LiveCodeBench-Base, CRUXEval, BBH, AGIEval, CLUEWSC, CMRC, and CMath. In addition, we perform language-modeling-based evaluation for Pile-test and use Bits-Per-Byte (BPB) as the metric to guarantee fair comparison among models using different tokenizers. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"4.4.2. Evaluation Results \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B Base (Qwen, 2024b), and LLaMA-3.1 405B Base (AI@Meta, 2024b). We evaluate all these models with our internal evaluation framework, and ensure that they share the same evaluation setting. Note that due to the changes in our evaluation framework over the past months, the performance of DeepSeek-V2-Base exhibits a slight difference from our previously reported results. Overall, DeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base, and surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the strongest open-source model. \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/6a84148e6f9ec21234912515ca7454f225a10cccedef7f00755b2287a605e537.jpg\", \"table_caption\": [\"Table 3 Comparison among DeepSeek-V3-Base and other representative open-source base models. All models are evaluated in our internal framework and share the same evaluation setting. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeekV3-Base achieves the best performance on most benchmarks, especially on math and code tasks. \"], \"table_body\": \"\\n\\n<html><body><table><tr><td></td><td>Benchmark (Metric)</td><td># Shots</td><td>DeepSeek-V2 Base</td><td>Qwen2.5 72B Base</td><td>LLaMA-3.1 405B Base</td><td>DeepSeek-V3 Base</td></tr><tr><td rowspan=\\\"5\\\"></td><td>Architecture</td><td></td><td>MoE</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td># Activated Params</td><td></td><td>21B</td><td>72B</td><td>405B</td><td>37B</td></tr><tr><td># Total Params</td><td></td><td>236B</td><td>72B</td><td>405B</td><td>671B</td></tr><tr><td>Pile-test (BPB)</td><td></td><td>0.606</td><td>0.638</td><td>0.542</td><td>0.548</td></tr><tr><td>BBH (EM) MMLU (EM)</td><td>3-shot</td><td>78.8</td><td>79.8</td><td>82.9</td><td></td></tr><tr><td rowspan=\\\"12\\\">English</td><td></td><td>5-shot</td><td>78.4</td><td>85.0</td><td>84.4</td><td>87.5 87.1</td></tr><tr><td>MMLU-Redux (EM)</td><td>5-shot</td><td>75.6</td><td>83.2</td><td>81.3</td><td></td></tr><tr><td>MMLU-Pro (EM)</td><td>5-shot</td><td>51.4</td><td>58.3</td><td></td><td>86.2</td></tr><tr><td></td><td></td><td>80.4</td><td>80.6</td><td>52.8</td><td>64.4</td></tr><tr><td>DROP (F1) ARC-Easy (EM)</td><td>3-shot 25-shot</td><td>97.6</td><td>98.4</td><td>86.0</td><td>89.0</td></tr><tr><td>ARC-Challenge (EM)</td><td>25-shot</td><td>92.2</td><td>94.5</td><td>98.4 95.3</td><td>98.9</td></tr><tr><td>HellaSwag (EM)</td><td>10-shot</td><td>87.1</td><td>84.8</td><td>89.2</td><td>95.3 88.9</td></tr><tr><td>PIQA (EM)</td><td>O-shot</td><td>83.9</td><td>82.6</td><td>85.9</td><td>84.7</td></tr><tr><td>WinoGrande (EM)</td><td>5-shot</td><td>86.3</td><td>82.3</td><td>85.2</td><td></td></tr><tr><td>RACE-Middle (EM)</td><td>5-shot</td><td>73.1</td><td>68.1</td><td>74.2</td><td>84.9</td></tr><tr><td>RACE-High (EM)</td><td>5-shot</td><td>52.6</td><td>50.3</td><td>56.8</td><td>67.1 51.3</td></tr><tr><td>TriviaQA (EM)</td><td>5-shot</td><td>80.0</td><td>71.9</td><td>82.7</td><td>82.9</td></tr><tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>38.6</td><td>33.2</td><td>41.5</td><td>40.0</td></tr><tr><td>AGIEval (EM)</td><td>O-shot</td><td>57.5</td><td>75.8</td><td>60.6</td><td>79.6</td></tr><tr><td>HumanEval (Pass@1) Code</td><td>O-shot</td><td>43.3</td><td>53.0</td><td>54.9</td><td>65.2</td></tr><tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>65.0</td><td>72.6</td><td>68.4</td><td>75.4</td></tr><tr><td>LiveCodeBench-Base (Pass@1) CRUXEval-I (EM)</td><td>3-shot</td><td>11.6</td><td>12.9</td><td>15.5</td><td>19.4</td></tr><tr><td></td><td></td><td>2-shot 52.5</td><td>59.1</td><td>58.5</td><td>67.3</td><td></td></tr><tr><td rowspan=\\\"4\\\">Math</td><td>CRUXEval-O (EM)</td><td>2-shot</td><td>49.8</td><td>59.9</td><td>59.9</td><td>69.8</td></tr><tr><td>GSM8K (EM)</td><td>8-shot</td><td>81.6</td><td>88.3</td><td>83.5</td><td>89.3</td></tr><tr><td>MATH (EM)</td><td>4-shot</td><td>43.4</td><td>54.4</td><td>49.0</td><td>61.6</td></tr><tr><td>MGSM (EM)</td><td>8-shot</td><td>63.6</td><td>76.2</td><td>69.9</td><td>79.8</td></tr><tr><td rowspan=\\\"6\\\">Chinese</td><td>CMath (EM)</td><td>3-shot</td><td>78.7</td><td>84.5</td><td>77.3</td><td>90.7</td></tr><tr><td>CLUEWSC (EM)</td><td>5-shot</td><td>82.0</td><td>82.5</td><td>83.0</td><td>82.7</td></tr><tr><td>C-Eval (EM)</td><td>5-shot</td><td>81.4</td><td>89.2</td><td>72.5</td><td>90.1</td></tr><tr><td>CMMLU (EM)</td><td>5-shot</td><td>84.0</td><td>89.5</td><td>73.7</td><td>88.8</td></tr><tr><td>CMRC (EM)</td><td>1-shot</td><td>77.4</td><td>75.8</td><td>76.0</td><td>76.3</td></tr><tr><td>C3 (EM) CCPM (EM)</td><td>O-shot O-shot</td><td>77.4 93.0</td><td>76.7 88.5</td><td>79.7 78.6</td><td>78.6 92.0</td></tr><tr><td>Multilingual</td><td>MMMLU-non-English (EM)</td><td>5-shot</td><td>64.0</td><td>74.8</td><td>73.8</td><td>79.4</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"From a more detailed perspective, we compare DeepSeek-V3-Base with the other open-source base models individually. (1) Compared with DeepSeek-V2-Base, due to the improvements in our model architecture, the scale-up of the model size and training tokens, and the enhancement of data quality, DeepSeek-V3-Base achieves significantly better performance as expected. (2) Compared with Qwen2.5 72B Base, the state-of-the-art Chinese open-source model, with only half of the activated parameters, DeepSeek-V3-Base also demonstrates remarkable advantages, especially on English, multilingual, code, and math benchmarks. As for Chinese benchmarks, except for CMMLU, a Chinese multi-subject multiple-choice task, DeepSeek-V3-Base also shows better performance than Qwen2.5 72B. (3) Compared with LLaMA-3.1 405B Base, the largest open-source model with 11 times the activated parameters, DeepSeek-V3-Base also exhibits much better performance on multilingual, code, and math benchmarks. As for English and Chinese language benchmarks, DeepSeek-V3-Base shows competitive or better performance, and is especially good on BBH, MMLU-series, DROP, C-Eval, CMMLU, and CCPM. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Due to our efficient architectures and comprehensive engineering optimizations, DeepSeekV3 achieves extremely high training efficiency. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models. \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/8db174ccb145521f9496f42bfc1c6000bf6fbe09e88bc54c229f4c8b0b183f79.jpg\", \"table_body\": \"\\n\\n<html><body><table><tr><td>Benchmark (Metric)</td><td># Shots</td><td>Small MoE Baseline</td><td>Small MoE w/ MTP</td><td>Large MoE Baseline</td><td>Large MoE w/ MTP</td></tr><tr><td># Activated Params (Inference)</td><td>\\u4e00</td><td>2.4B</td><td>2.4B</td><td>20.9B</td><td>20.9B</td></tr><tr><td># Total Params (Inference)</td><td>-</td><td>15.7B</td><td>15.7B</td><td>228.7B</td><td>228.7B</td></tr><tr><td># Training Tokens</td><td>-</td><td>1.33T</td><td>1.33T</td><td>540B</td><td>540B</td></tr><tr><td>Pile-test (BPB)</td><td>\\u4e00</td><td>0.729</td><td>0.729</td><td>0.658</td><td>0.657</td></tr><tr><td>BBH (EM)</td><td>3-shot</td><td>39.0</td><td>41.4</td><td>70.0</td><td>70.7</td></tr><tr><td>MMLU (EM)</td><td>5-shot</td><td>50.0</td><td>53.3</td><td>67.5</td><td>66.6</td></tr><tr><td>DROP (F1)</td><td>1-shot</td><td>39.2</td><td>41.3</td><td>68.5</td><td>70.6</td></tr><tr><td>TriviaQA (EM)</td><td>5-shot</td><td>56.9</td><td>57.7</td><td>67.0</td><td>67.3</td></tr><tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>22.7</td><td>22.3</td><td>27.2</td><td>28.5</td></tr><tr><td>HumanEval (Pass@1)</td><td>O-shot</td><td>20.7</td><td>26.8</td><td>44.5</td><td>53.7</td></tr><tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>35.8</td><td>36.8</td><td>61.6</td><td>62.2</td></tr><tr><td>GSM8K (EM)</td><td>8-shot</td><td>25.4</td><td>31.4</td><td>72.3</td><td>74.0</td></tr><tr><td>MATH (EM)</td><td>4-shot</td><td>10.7</td><td>12.6</td><td>38.6</td><td>39.8</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Table 4 | Ablation results for the MTP strategy. The MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.5. Discussion \"}, \"children\": [{\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"4.5.1. Ablation Studies for Multi-Token Prediction \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In Table 4, we show the ablation results for the MTP strategy. To be specific, we validate the MTP strategy on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising 228.7B total parameters on 540B tokens. On top of them, keeping the training data and the other architectures the same, we append a 1-depth MTP module onto them and train two models with the MTP strategy for comparison. Note that during inference, we directly discard the MTP module, so the inference costs of the compared models are exactly the same. From the table, we can observe that the MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In Table 5, we show the ablation results for the auxiliary-loss-free balancing strategy. We validate this strategy on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising 228.7B total parameters on 578B tokens. \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0b5c2bac8bb47af03f3d59035131c1afeb68ab2f2c7c43b749aff4fa45b47bd9.jpg\", \"table_caption\": [\"Table 5 | Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. \"], \"table_body\": \"\\n\\n<html><body><table><tr><td>Benchmark (Metric)</td><td># Shots</td><td>Small MoE Aux-Loss-Based</td><td>Small MoE Aux-Loss-Free</td><td>Large MoE Aux-Loss-Based</td><td>Large MoE Aux-Loss-Free</td></tr><tr><td># Activated Params</td><td></td><td>2.4B</td><td>2.4B</td><td>20.9B</td><td>20.9B</td></tr><tr><td># Total Params</td><td></td><td>15.7B</td><td>15.7B</td><td>228.7B</td><td>228.7B</td></tr><tr><td># Training Tokens</td><td>-</td><td>1.33T</td><td>1.33T</td><td>578B</td><td>578B</td></tr><tr><td>Pile-test (BPB)</td><td></td><td>0.727</td><td>0.724</td><td>0.656</td><td>0.652</td></tr><tr><td>BBH (EM)</td><td>3-shot</td><td>37.3</td><td>39.3</td><td>66.7</td><td>67.9</td></tr><tr><td>MMLU (EM)</td><td>5-shot</td><td>51.0</td><td>51.8</td><td>68.3</td><td>67.2</td></tr><tr><td>DROP (F1)</td><td>1-shot</td><td>38.1</td><td>39.0</td><td>67.1</td><td>67.1</td></tr><tr><td>TriviaQA (EM)</td><td>5-shot</td><td>58.3</td><td>58.5</td><td>66.7</td><td>67.7</td></tr><tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>23.2</td><td>23.4</td><td>27.1</td><td>28.1</td></tr><tr><td>HumanEval (Pass@1)</td><td>O-shot</td><td>22.0</td><td>22.6</td><td>40.2</td><td>46.3</td></tr><tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>36.6</td><td>35.8</td><td>59.2</td><td>61.2</td></tr><tr><td>GSM8K (EM)</td><td>8-shot</td><td>27.1</td><td>29.6</td><td>70.7</td><td>74.5</td></tr><tr><td>MATH (EM)</td><td>4-shot</td><td>10.9</td><td>11.1</td><td>37.2</td><td>39.6</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Both of the baseline models purely use auxiliary losses to encourage load balance, and use the sigmoid gating function with top-K affinity normalization. Their hyper-parameters to control the strength of auxiliary losses are the same as DeepSeek-V2-Lite and DeepSeek-V2, respectively. On top of these two baseline models, keeping the training data and the other architectures the same, we remove all auxiliary losses and introduce the auxiliary-loss-free balancing strategy for comparison. From the table, we can observe that the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"The key distinction between auxiliary-loss-free balancing and sequence-wise auxiliary loss lies in their balancing scope: batch-wise versus sequence-wise. Compared with the sequence-wise auxiliary loss, batch-wise balancing imposes a more flexible constraint, as it does not enforce in-domain balance on each sequence. This flexibility allows experts to better specialize in different domains. To validate this, we record and analyze the expert load of a 16B auxiliaryloss-based baseline and a 16B auxiliary-loss-free model on different domains in the Pile test set. As illustrated in Figure 9, we observe that the auxiliary-loss-free model demonstrates greater expert specialization patterns as expected. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"To further investigate the correlation between this flexibility and the advantage in model performance, we additionally design and validate a batch-wise auxiliary loss that encourages load balance on each training batch instead of on each sequence. The experimental results show that, when achieving a similar level of batch-wise load balance, the batch-wise auxiliary loss can also achieve similar model performance to the auxiliary-loss-free method. To be specific, in our experiments with 1B MoE models, the validation losses are: 2.258 (using a sequencewise auxiliary loss), 2.253 (using the auxiliary-loss-free method), and 2.253 (using a batch-wise auxiliary loss). We also observe similar results on 3B MoE models: the model using a sequencewise auxiliary loss achieves a validation loss of 2.085, and the models using the auxiliary-loss-free method or a batch-wise auxiliary loss achieve the same validation loss of 2.080. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In addition, although the batch-wise load balancing methods show consistent performance advantages, they also face two potential challenges in efficiency: (1) load imbalance within certain sequences or small batches, and (2) domain-shift-induced load imbalance during inference. The first challenge is naturally addressed by our training framework that uses large-scale expert parallelism and data parallelism, which guarantees a large size of each micro-batch. For the second challenge, we also design and implement an efficient inference framework with redundant expert deployment, as described in Section 3.4, to overcome it. \"}}, {\"type\": \"image\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/4e04a59983212cc4b1707f1db88fcc93a62807adca76c455ef42c532b2c10714.jpg\", \"img_caption\": [\"Figure 9 | Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. Due to space constraints, we only present the results of two layers as an example, with the results of all layers provided in Appendix C. \"]}}]}]}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"5. Post-Training \"}, \"children\": [{\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"5.1. Supervised Fine-Tuning \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"We curate our instruction-tuning datasets to include 1.5M instances spanning multiple domains, with each domain employing distinct data creation methods tailored to its specific requirements. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model. Specifically, while the R1-generated data demonstrates strong accuracy, it suffers from issues such as overthinking, poor formatting, and excessive length. Our objective is to balance the high accuracy of R1-generated reasoning data and the clarity and conciseness of regularly formatted reasoning data. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"To establish our methodology, we begin by developing an expert model tailored to a specific domain, such as code, mathematics, or general reasoning, using a combined Supervised FineTuning (SFT) and Reinforcement Learning (RL) training pipeline. This expert model serves as a data generator for the final model. The training process involves generating two distinct types of SFT samples for each instance: the first couples the problem with its original response in the format of <problem, original response>, while the second incorporates a system prompt alongside the problem and the R1 response in the format of <system prompt, problem, R1 response>. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"The system prompt is meticulously designed to include instructions that guide the model toward producing responses enriched with mechanisms for reflection and verification. During the RL phase, the model leverages high-temperature sampling to generate responses that integrate patterns from both the R1-generated and original data, even in the absence of explicit system prompts. After hundreds of RL steps, the intermediate RL model learns to incorporate R1 patterns, thereby enhancing overall performance strategically. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Upon completing the RL training phase, we implement rejection sampling to curate highquality SFT data for the final model, where the expert models are used as data generation sources. This method ensures that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"SFT Settings. We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at $5\\\\times10^{-6}$ and gradually decreases to $1\\\\times\\\\dot{1}0^{-6}$ . During training, each single sequence is packed from multiple samples. However, we adopt a sample masking strategy to ensure that these examples remain isolated and mutually invisible. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"5.2. Reinforcement Learning \"}, \"children\": [{\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.2.1. Reward Model \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We employ a rule-based Reward Model (RM) and a model-based RM in our RL process. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Rule-Based RM. For questions that can be validated using specific rules, we adopt a rulebased reward system to determine the feedback. For instance, certain math problems have deterministic results, and we require the model to provide the final answer within a designated format (e.g., in a box), allowing us to apply rules to verify the correctness. Similarly, for LeetCode problems, we can utilize a compiler to generate feedback based on test cases. By leveraging rule-based validation wherever possible, we ensure a higher level of reliability, as this approach is resistant to manipulation or exploitation. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Model-Based RM. For questions with free-form ground-truth answers, we rely on the reward model to determine whether the response matches the expected ground-truth. Conversely, for questions without a definitive ground-truth, such as those involving creative writing, the reward model is tasked with providing feedback based on the question and the corresponding answer as inputs. The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its reliability, we construct preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward. This approach helps mitigate the risk of reward hacking in specific tasks. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.2.2. Group Relative Policy Optimization \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Similar to DeepSeek-V2 (DeepSeek-AI, $2024\\\\mathrm{c}$ ), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q,$ GRPO samples a group of outputs $\\\\{o1,o2,\\\\cdots,o_{G}\\\\}$ from the old policy model $\\\\pi_{\\\\theta_{o l d}}$ and then optimizes the policy model $\\\\scriptstyle\\\\pi_{\\\\theta}$ by maximizing the following objective: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{l}{\\\\displaystyle\\\\mathcal{G}_{G R P O}(\\\\theta)=\\\\mathbb{E}[q\\\\sim P(Q),\\\\{\\\\boldsymbol{o}_{i}\\\\}_{i=1}^{G}\\\\sim\\\\pi_{\\\\theta_{o d d}}(O|{q})]}\\\\\\\\ {\\\\displaystyle\\\\frac{1}{G}\\\\sum_{i=1}^{G}\\\\left(\\\\operatorname*{min}\\\\left(\\\\frac{\\\\pi_{\\\\theta}(\\\\boldsymbol{o}_{i}|{q})}{\\\\pi_{\\\\theta_{o d d}}(\\\\boldsymbol{o}_{i}|{q})}A_{i},\\\\mathrm{clip}\\\\left(\\\\frac{\\\\pi_{\\\\theta}(\\\\boldsymbol{o}_{i}|{q})}{\\\\pi_{\\\\theta_{o d d}}(\\\\boldsymbol{o}_{i}|{q})},1-\\\\varepsilon,1+\\\\varepsilon\\\\right)A_{i}\\\\right)-\\\\beta\\\\mathbb{D}_{K L}\\\\left(\\\\pi_{\\\\theta}||\\\\boldsymbol{\\\\pi}_{r e f}\\\\right)\\\\right),}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\mathbb{D}_{K L}\\\\left(\\\\pi_{\\\\theta}||\\\\pi_{r e f}\\\\right)=\\\\frac{\\\\pi_{r e f}(o_{i}|q)}{\\\\pi_{\\\\theta}(o_{i}|q)}-\\\\log\\\\frac{\\\\pi_{r e f}(o_{i}|q)}{\\\\pi_{\\\\theta}(o_{i}|q)}-1,\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where $\\\\varepsilon$ and $\\\\beta$ are hyper-parameters; $\\\\pi_{r e f}$ is the reference model; and $A_{i}$ is the advantage, derived from the rewards $\\\\{r_{1},r_{2},\\\\hdots,r_{G}\\\\}$ corresponding to the outputs within each group: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\nA_{i}={\\\\frac{r_{i}-\\\\operatorname*{mean}(\\\\{r_{1},r_{2},\\\\cdot\\\\cdot\\\\cdot,r_{G}\\\\})}{\\\\operatorname{std}(\\\\{r_{1},r_{2},\\\\cdot\\\\cdot\\\\cdot,r_{G}\\\\})}}.\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process. This approach not only aligns the model more closely with human preferences but also enhances performance on benchmarks, especially in scenarios where available SFT data are limited. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"5.3. Evaluations \"}, \"children\": [{\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.3.1. Evaluation Settings \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Evaluation Benchmarks. Apart from the benchmark we used for base model testing, we further evaluate instructed models on IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), LongBench v2 (Bai et al., 2024), GPQA (Rein et al., 2023), SimpleQA (OpenAI, 2024c), CSimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (questions from August 2024 to November 2024), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Compared Baselines. We conduct comprehensive evaluations of our chat model against several strong baselines, including DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct, LLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, and GPT-4o-0513. For the DeepSeek-V2 model series, we select the most representative variants for comparison. For closed-source models, evaluations are performed through their respective APIs. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Detailed Evaluation Configurations. For standard benchmarks including MMLU, DROP, GPQA, and SimpleQA, we adopt the evaluation prompts from the simple-evals framework4. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We utilize the Zero-Eval prompt format (Lin, 2024) for MMLU-Redux in a zero-shot setting. For other datasets, we follow their original evaluation protocols with default prompts as provided by the dataset creators. For code and math benchmarks, the HumanEval-Mul dataset includes 8 mainstream programming languages (Python, Java, Cpp, C#, JavaScript, TypeScript, PHP, and Bash) in total. We use CoT and non-CoT methods to evaluate model performance on LiveCodeBench, where the data are collected from August 2024 to November 2024. The Codeforces dataset is measured using the percentage of competitors. SWE-Bench verified is evaluated using the agentless framework (Xia et al., 2024). We use the \\u201cdiff\\u201d format to evaluate the Aider-related benchmarks. For mathematical assessments, AIME and CNMO 2024 are evaluated with a temperature of 0.7, and the results are averaged over 16 runs, while MATH-500 employs greedy decoding. We allow all models to output a maximum of 8192 tokens for each benchmark. \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/7a76ee39b10dd3db2c84be68fc0de9cf3dcadd9496d72fedd38eceabaa452b9c.jpg\", \"table_body\": \"\\n\\n<html><body><table><tr><td>Benchmark (Metric)</td><td></td><td>DeepSeek DeepSeek|Qwen2.5 LLaMA-3.1 Claude-3.5- GPT-4o|DeepSeek V2-0506</td><td>V2.5-0905</td><td>72B-Inst. 405B-Inst. Sonnet-1022</td><td></td><td></td><td>0513</td><td>V3</td></tr><tr><td rowspan=\\\"4\\\"></td><td>Architecture</td><td>MoE</td><td>MoE</td><td>Dense</td><td>Dense</td><td></td><td></td><td>MoE</td></tr><tr><td># Activated Params</td><td>21B</td><td>21B</td><td>72B</td><td>405B</td><td></td><td></td><td>37B</td></tr><tr><td># Total Params</td><td>236B</td><td>236B</td><td>72B</td><td>405B</td><td></td><td></td><td>671B</td></tr><tr><td>MMLU (EM)</td><td>78.2</td><td>80.6</td><td>85.3</td><td>88.6</td><td>88.3</td><td>87.2</td><td>88.5</td></tr><tr><td rowspan=\\\"10\\\">English</td><td>MMLU-Redux (EM)</td><td>77.9</td><td>80.3</td><td>85.6</td><td>86.2</td><td>88.9</td><td>88.0</td><td>89.1</td></tr><tr><td>MMLU-Pro (EM)</td><td>58.5</td><td>66.2</td><td>71.6</td><td>73.3</td><td>78.0</td><td>72.6</td><td>75.9</td></tr><tr><td>DROP (3-shot F1)</td><td>83.0</td><td></td><td>76.7</td><td></td><td></td><td></td><td></td></tr><tr><td>IF-Eval (Prompt Strict)</td><td>57.7</td><td>87.8 80.6</td><td>84.1</td><td>88.7 86.0</td><td>88.3 86.5</td><td>83.7 84.3</td><td>91.6 86.1</td></tr><tr><td>GPQA-Diamond (Pass@1)</td><td>35.3</td><td>41.3</td><td>49.0</td><td>51.1</td><td>65.0</td><td>49.9</td><td>59.1</td></tr><tr><td>SimpleQA (Correct)</td><td>9.0</td><td>10.2</td><td>9.1</td><td>17.1</td><td>28.4</td><td>38.2</td><td>24.9</td></tr><tr><td>FRAMES (Acc.)</td><td>66.9</td><td>65.4</td><td>69.8</td><td>70.0</td><td>72.5</td><td>80.5</td><td>73.3</td></tr><tr><td>LongBench v2 (Acc.)</td><td>31.6</td><td>35.4</td><td>39.4</td><td>36.1</td><td>41.0</td><td>48.1</td><td>48.7</td></tr><tr><td>HumanEval-Mul (Pass@1)</td><td>69.3</td><td>77.4</td><td>77.3</td><td></td><td></td><td></td><td></td></tr><tr><td>LiveCodeBench (Pass@1-COT)</td><td>18.8</td><td>29.2</td><td>31.1</td><td>77.2 28.4</td><td>81.7 36.3</td><td>80.5 33.4</td><td>82.6 40.5</td></tr><tr><td>Code LiveCodeBench (Pass@1)</td><td></td><td>20.3</td><td>28.4</td><td>28.7</td><td>30.1</td><td>32.8</td><td>34.2</td><td>37.6</td></tr><tr><td>Codeforces (Percentile)</td><td>17.5</td><td>35.6</td><td>24.8</td><td></td><td>25.3</td><td>20.3</td><td>23.6</td><td>51.6</td></tr><tr><td>SWE Verified (Resolved)</td><td></td><td>22.6</td><td>23.8</td><td></td><td>24.5</td><td>50.8</td><td>38.8</td><td>42.0</td></tr><tr><td>Aider-Edit (Acc.)</td><td>60.3</td><td>71.6</td><td>65.4</td><td></td><td>63.9</td><td>84.2</td><td>72.9</td><td>79.7</td></tr><tr><td>Aider-Polyglot (Acc.)</td><td>\\u4e00</td><td>18.2</td><td>7.6</td><td>5.8</td><td></td><td>45.3</td><td>16.0</td><td>49.6</td></tr><tr><td rowspan=\\\"3\\\"></td><td>AIME 2024 (Pass@1)</td><td>4.6</td><td>16.7</td><td>23.3</td><td>23.3</td><td>16.0</td><td></td><td></td><td></td></tr><tr><td>Math MATH-500 (EM)</td><td>56.3</td><td>74.7</td><td>80.0</td><td>73.8</td><td>78.3</td><td></td><td>9.3 74.6</td><td>39.2</td></tr><tr><td>CNMO 2024 (Pass@1)</td><td>2.8</td><td>10.8</td><td>15.9</td><td>6.8</td><td>13.1</td><td></td><td>10.8</td><td>90.2 43.2</td></tr><tr><td rowspan=\\\"3\\\">Chinese C-Eval (EM)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CLUEWSC (EM)</td><td>89.9</td><td>90.4</td><td>91.4</td><td>84.7</td><td></td><td>85.4</td><td>87.9</td><td>90.9</td></tr><tr><td>C-SimpleQA (Correct)</td><td>78.6 48.5</td><td>79.5 54.1</td><td>86.1 48.4</td><td>61.5 50.4</td><td></td><td>76.7 51.3</td><td>76.0 59.3</td><td>86.5 64.8</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Table 6 Comparison between DeepSeek-V3 and other representative chat models. All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.3.2. Standard Evaluation \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Table 6 presents the evaluation results, showcasing that DeepSeek-V3 stands as the bestperforming open-source model. Additionally, it is competitive against frontier closed-source models like GPT-4o and Claude-3.5-Sonnet. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"English Benchmarks. MMLU is a widely recognized benchmark designed to assess the performance of large language models, across diverse knowledge domains and tasks. DeepSeek-V3 demonstrates competitive performance, standing on par with top-tier models such as LLaMA3.1-405B, GPT-4o, and Claude-Sonnet 3.5, while significantly outperforming Qwen2.5 72B. Moreover, DeepSeek-V3 excels in MMLU-Pro, a more challenging educational knowledge benchmark, where it closely trails Claude-Sonnet 3.5. On MMLU-Redux, a refined version of MMLU with corrected labels, DeepSeek-V3 surpasses its peers. In addition, on GPQA-Diamond, a PhD-level evaluation testbed, DeepSeek-V3 achieves remarkable results, ranking just behind Claude 3.5 Sonnet and outperforming all other competitors by a substantial margin. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In long-context understanding benchmarks such as DROP, LongBench v2, and FRAMES, DeepSeek-V3 continues to demonstrate its position as a top-tier model. It achieves an impressive 91.6 F1 score in the 3-shot setting on DROP, outperforming all other models in this category. On FRAMES, a benchmark requiring question-answering over 100k token contexts, DeepSeekV3 closely trails GPT-4o while outperforming all other models by a significant margin. This demonstrates the strong capability of DeepSeek-V3 in handling extremely long-context tasks. The long-context capability of DeepSeek-V3 is further validated by its best-in-class performance on LongBench v2, a dataset that was released just a few weeks before the launch of DeepSeek V3. On the factual knowledge benchmark, SimpleQA, DeepSeek-V3 falls behind GPT-4o and Claude-Sonnet, primarily due to its design focus and resource allocation. DeepSeek-V3 assigns more training tokens to learn Chinese knowledge, leading to exceptional performance on the C-SimpleQA. On the instruction-following benchmark, DeepSeek-V3 significantly outperforms its predecessor, DeepSeek-V2-series, highlighting its improved ability to understand and adhere to user-defined format constraints. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Code and Math Benchmarks. Coding is a challenging and practical task for LLMs, encompassing engineering-focused tasks like SWE-Bench-Verified and Aider, as well as algorithmic tasks such as HumanEval and LiveCodeBench. In engineering tasks, DeepSeek-V3 trails behind Claude-Sonnet-3.5-1022 but significantly outperforms open-source models. The open-source DeepSeek-V3 is expected to foster advancements in coding-related engineering tasks. By providing access to its robust capabilities, DeepSeek-V3 can drive innovation and improvement in areas such as software engineering and algorithm development, empowering developers and researchers to push the boundaries of what open-source models can achieve in coding tasks. In algorithmic tasks, DeepSeek-V3 demonstrates superior performance, outperforming all baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be attributed to its advanced knowledge distillation technique, which effectively enhances its code generation and problem-solving capabilities in algorithm-focused tasks. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"On math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly surpassing baselines and setting a new state-of-the-art for non-o1-like models. Specifically, on AIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5 72B, by approximately $10\\\\%$ in absolute scores, which is a substantial margin for such challenging benchmarks. This remarkable capability highlights the effectiveness of the distillation technique from DeepSeek-R1, which has been proven highly beneficial for non-o1-like models. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Chinese Benchmarks. Qwen and DeepSeek are two representative model series with robust support for both Chinese and English. On the factual benchmark Chinese SimpleQA, DeepSeekV3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus compromising 18T tokens, which are $20\\\\%$ more than the $14.8\\\\mathrm{T}$ tokens that DeepSeek-V3 is \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/c37f5046875e424ff7860735dd500227c5142a1630e000ff48bb7ce04be5e88f.jpg\", \"table_footnote\": [\"Table 7 English open-ended conversation evaluations. For AlpacaEval 2.0, we use the lengthcontrolled win rate as the metric. \"], \"table_body\": \"\\n\\n<html><body><table><tr><td>Model</td><td>Arena-Hard</td><td>AlpacaEval 2.0</td></tr><tr><td>DeepSeek-V2.5-0905</td><td>76.2</td><td>50.5</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>81.2</td><td>49.1</td></tr><tr><td>LLaMA-3.1 405B</td><td>69.3</td><td>40.5</td></tr><tr><td>GPT-40-0513</td><td>80.4</td><td>51.1</td></tr><tr><td>Claude-Sonnet-3.5-1022</td><td>85.2</td><td>52.0</td></tr><tr><td>DeepSeek-V3</td><td>85.5</td><td>70.0</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"pre-trained on. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"On C-Eval, a representative benchmark for Chinese educational knowledge evaluation, and CLUEWSC (Chinese Winograd Schema Challenge), DeepSeek-V3 and Qwen2.5-72B exhibit similar performance levels, indicating that both models are well-optimized for challenging Chinese-language reasoning and educational tasks. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.3.3. Open-Ended Evaluation \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges, with the results shown in Table 7. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024a), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. On Arena-Hard, DeepSeek-V3 achieves an impressive win rate of over $86\\\\%$ against the baseline GPT-4-0314, performing on par with top-tier models like Claude-Sonnet-3.5-1022. This underscores the robust capabilities of DeepSeek-V3, especially in dealing with complex prompts, including coding and debugging tasks. Furthermore, DeepSeek-V3 achieves a groundbreaking milestone as the first open-source model to surpass $85\\\\%$ on the Arena-Hard benchmark. This achievement significantly bridges the performance gap between open-source and closed-source models, setting a new standard for what open-source models can accomplish in challenging domains. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Similarly, DeepSeek-V3 showcases exceptional performance on AlpacaEval 2.0, outperforming both closed-source and open-source models. This demonstrates its outstanding proficiency in writing tasks and handling straightforward question-answering scenarios. Notably, it surpasses DeepSeek-V2.5-0905 by a significant margin of $20\\\\%$ , highlighting substantial improvements in tackling simple tasks and showcasing the effectiveness of its advancements. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.3.4. DeepSeek-V3 as a Generative Reward Model \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We compare the judgment ability of DeepSeek-V3 with state-of-the-art models, namely GPT-4o and Claude-3.5. Table 8 presents the performance of these models in RewardBench (Lambert et al., 2024). DeepSeek-V3 achieves performance on par with the best versions of GPT-4o-0806 and Claude-3.5-Sonnet-1022, while surpassing other versions. Additionally, the judgment ability of DeepSeek-V3 can also be enhanced by the voting technique. Therefore, we employ DeepSeekV3 along with voting to offer self-feedback on open-ended questions, thereby improving the effectiveness and robustness of the alignment process. \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0b07a5c810c734387321e9fb236709a509378526205175534042267f9bc58722.jpg\", \"table_caption\": [\"Table 8 | Performances of GPT-4o, Claude-3.5-sonnet and DeepSeek-V3 on RewardBench. \"], \"table_body\": \"\\n\\n<html><body><table><tr><td>Model</td><td>Chat</td><td>Chat-Hard</td><td>Safety</td><td>Reasoning</td><td>Average</td></tr><tr><td>GPT-40-0513</td><td>96.6</td><td>70.4</td><td>86.7</td><td>84.9</td><td>84.7</td></tr><tr><td>GPT-40-0806</td><td>96.1</td><td>76.1</td><td>88.1</td><td>86.6</td><td>86.7</td></tr><tr><td>GPT-40-1120</td><td>95.8</td><td>71.3</td><td>86.2</td><td>85.2</td><td>84.6</td></tr><tr><td>Claude-3.5-sonnet-0620</td><td>96.4</td><td>74.0</td><td>81.6</td><td>84.7</td><td>84.2</td></tr><tr><td>Claude-3.5-sonnet-1022</td><td>96.4</td><td>79.7</td><td>91.1</td><td>87.6</td><td>88.7</td></tr><tr><td>DeepSeek-V3</td><td>96.9</td><td>79.8</td><td>87.0</td><td>84.3</td><td>87.0</td></tr><tr><td>DeepSeek-V3 (maj@6)</td><td>96.9</td><td>82.6</td><td>89.5</td><td>89.2</td><td>89.6</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0700f9f4a277d248543728c8d67e12011d7ca905620b8f1be4fa1406b33ee64d.jpg\", \"table_footnote\": [\"Table 9 | The contribution of distillation from DeepSeek-R1. The evaluation settings of LiveCodeBench and MATH-500 are the same as in Table 6. \"], \"table_body\": \"\\n\\n<html><body><table><tr><td rowspan=\\\"2\\\">Model</td><td colspan=\\\"2\\\">LiveCodeBench-CoT</td><td colspan=\\\"2\\\">MATH-500</td></tr><tr><td>Pass@1</td><td>Length</td><td>Pass@1</td><td>Length</td></tr><tr><td>DeepSeek-V2.5 Baseline</td><td>31.1</td><td>718</td><td>74.6</td><td>769</td></tr><tr><td>DeepSeek-V2.5 +R1 Distill</td><td>37.4</td><td>783</td><td>83.2</td><td>1510</td></tr></table></body></html>\\n\\n\"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"5.4. Discussion \"}, \"children\": [{\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.4.1. Distillation from DeepSeek-R1 \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We ablate the contribution of distillation from DeepSeek-R1 based on DeepSeek-V2.5. The baseline is trained on short CoT data, whereas its competitor uses data generated by the expert checkpoints described above. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Table 9 demonstrates the effectiveness of the distillation data, showing significant improvements in both LiveCodeBench and MATH-500 benchmarks. Our experiments reveal an interesting trade-off: the distillation leads to better performance but also substantially increases the average response length. To maintain a balance between model accuracy and computational efficiency, we carefully selected optimal settings for DeepSeek-V3 in distillation. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Our research suggests that knowledge distillation from reasoning models presents a promising direction for post-training optimization. While our current work focuses on distilling data from mathematics and coding domains, this approach shows potential for broader applications across various task domains. The effectiveness demonstrated in these specific areas indicates that long-CoT distillation could be valuable for enhancing model performance in other cognitive tasks requiring complex reasoning. Further exploration of this approach across different domains remains an important direction for future research. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.4.2. Self-Rewarding \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Rewards play a pivotal role in RL, steering the optimization process. In domains where verification through external tools is straightforward, such as some coding or mathematics scenarios, RL demonstrates exceptional efficacy. However, in more general scenarios, constructing a feedback mechanism through hard coding is impractical. During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source. This method has produced notable alignment effects, significantly enhancing the performance of DeepSeek-V3 in subjective evaluations. By integrating additional constitutional inputs, DeepSeek-V3 can optimize towards the constitutional direction. We believe that this paradigm, which combines supplementary information with LLMs as a feedback source, is of paramount importance. The LLM serves as a versatile processor capable of transforming unstructured information from diverse scenarios into rewards, ultimately facilitating the self-improvement of LLMs. Beyond self-rewarding, we are also dedicated to uncovering other general and scalable rewarding methods to consistently advance the model capabilities in general scenarios. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.4.3. Multi-Token Prediction Evaluation \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Instead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through the MTP technique. Combined with the framework of speculative decoding (Leviathan et al., 2023; Xia et al., 2023), it can significantly accelerate the decoding speed of the model. A natural question arises concerning the acceptance rate of the additionally predicted token. Based on our evaluation, the acceptance rate of the second token prediction ranges between $85\\\\%$ and $90\\\\%$ across various generation topics, demonstrating consistent reliability. This high acceptance rate enables DeepSeek-V3 to achieve a significantly improved decoding speed, delivering 1.8 times TPS (Tokens Per Second). \"}}]}]}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"6. Conclusion, Limitations, and Future Directions \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations. The post-training also makes a success in distilling the reasoning capability from the DeepSeek-R1 series of models. Comprehensive evaluations demonstrate that DeepSeek-V3 has emerged as the strongest open-source model currently available, and achieves performance comparable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. Despite its strong performance, it also maintains economical training costs. It requires only 2.788M H800 GPU hours for its full training, including pre-training, context length extension, and post-training. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"While acknowledging its strong performance and cost-effectiveness, we also recognize that DeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient inference, the recommended deployment unit for DeepSeek-V3 is relatively large, which might pose a burden for small-sized teams. Secondly, although our deployment strategy for DeepSeekV3 has achieved an end-to-end generation speed of more than two times that of DeepSeek-V2, there still remains potential for further enhancement. Fortunately, these limitations are expected to be naturally addressed with the development of more advanced hardware. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"DeepSeek consistently adheres to the route of open-source models with longtermism, aiming to steadily approach the ultimate goal of AGI (Artificial General Intelligence). In the future, we plan to strategically invest in research across the following directions. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"\\u2022 We will consistently study and refine our model architectures, aiming to further improve both the training and inference efficiency, striving to approach efficient support for infinite context length. Additionally, we will try to break through the architectural limitations of Transformer, thereby pushing the boundaries of its modeling capabilities. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"\\u2022 We will continuously iterate on the quantity and quality of our training data, and explore the incorporation of additional training signal sources, aiming to drive data scaling across a more comprehensive range of dimensions.   \\n\\u2022 We will consistently explore and iterate on the deep thinking capabilities of our models, aiming to enhance their intelligence and problem-solving abilities by expanding their reasoning length and depth.   \\n\\u2022 We will explore more comprehensive and multi-dimensional model evaluation methods to prevent the tendency towards optimizing a fixed set of benchmarks during research, which may create a misleading impression of the model capabilities and affect our foundational assessment. \"}}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"References \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"AI@Meta. Llama 3 model card, 2024a. URL https://github.com/meta-llama/llama3/bl ob/main/MODEL_CARD.md.   \\nAI@Meta. Llama 3.1 model card, 2024b. URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md.   \\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3 -5-sonnet.   \\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.   \\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022.   \\nY. Bai, S. Tu, J. Zhang, H. Peng, X. Wang, X. Lv, S. Cao, J. Xu, L. Hou, Y. Dong, J. Tang, and J. Li. LongBench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024.   \\nM. Bauer, S. Treichler, and A. Aiken. Singe: leveraging warp specialization for high performance on GPUs. In Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP \\u201914, page 119\\u2013130, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450326568. doi: 10.1145/2555243.2555258. URL https://doi.org/10.1145/2555243.2555258.   \\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432\\u20137439. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.   \\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.   \\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.   \\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,   \\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \\nY. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, and G. Hu. A span-extraction dataset for Chinese machine reading comprehension. In K. Inui, J. Jiang, V. $\\\\mathrm{Ng,}$ and X. Wan,   \\neditors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5883\\u20135889, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1600. URL https://aclanthology.org/D19-1 600.   \\nD. Dai, C. Deng, C. Zhao, R. X. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, Z. Xie, Y. K. Li, P. Huang, F. Luo, C. Ruan, Z. Sui, and W. Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. CoRR, abs/2401.06066, 2024. URL https://doi.org/10.48550/arXiv.2401.06066.   \\nDeepSeek-AI. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. CoRR, abs/2406.11931, 2024a. URL https://doi.org/10.48550/arXiv.2406.11 931.   \\nDeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR, abs/2401.02954, 2024b. URL https://doi.org/10.48550/arXiv.2401.02954.   \\nDeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. CoRR, abs/2405.04434, 2024c. URL https://doi.org/10.48550/arXiv.2405. 04434.   \\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318\\u2013 30332, 2022.   \\nH. Ding, Z. Wang, G. Paolini, V. Kumar, A. Deoras, D. Roth, and S. Soatto. Fewer truncations improve language modeling. arXiv preprint arXiv:2404.10830, 2024.   \\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368\\u2013 2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL https://doi.org/10.18653/v1/n19-1246.   \\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https://arxiv.org/ abs/2101.03961. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"M. Fishman, B. Chmiel, R. Banner, and D. Soudry. Scaling FP8 training to trillion-token llms. arXiv preprint arXiv:2409.12517, 2024.   \\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.   \\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127.   \\nF. Gloeckle, B. Y. Idrissi, B. Rozi\\u00e8re, D. Lopez-Paz, and G. Synnaeve. Better & faster large language models via multi-token prediction. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=pEWAcejiU2.   \\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024.   \\nR. L. Graham, D. Bureddy, P. Lui, H. Rosenstock, G. Shainer, G. Bloch, D. Goldenerg, M. Dubman, S. Kotchubievsky, V. Koushnir, et al. Scalable hierarchical aggregation protocol (SHArP): A hardware architecture for efficient data reduction. In 2016 First International Workshop on Communication Optimizations in HPC (COMHPC), pages 1\\u201310. IEEE, 2016.   \\nA. Gu, B. Rozi\\u00e8re, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang. Cruxeval: A benchmark for code reasoning, understanding and execution, 2024.   \\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. CoRR, abs/2401.14196, 2024. URL https://doi.org/10.485 50/arXiv.2401.14196.   \\nA. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger, and P. Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training, 2018. URL https://arxiv.or g/abs/1806.03377.   \\nB. He, L. Noci, D. Paliotta, I. Schlag, and T. Hofmann. Understanding and minimising outlier features in transformer training. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.   \\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chinese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024.   \\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.   \\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.   \\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.   \\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.   \\nD. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen, et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019.   \\nS. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR, abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941.   \\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452\\u2013466, 2019. doi: 10.1162/tacl\\\\_a\\\\_00276. URL https://doi.org/10.1162/tacl_a_00276.   \\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. RACE: large-scale reading comprehension dataset from examinations. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 785\\u2013794. Association for Computational Linguistics, 2017. doi: 10.18653/V1/D17-1082. URL https://doi.org/10.18653/v1/d1 7-1082.   \\nN. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar, T. Zick, Y. Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.   \\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id $\\\\c=$ qrwe7XHTmYb.   \\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 19274\\u201319286. PMLR, 2023. URL https://proceedings.mlr.press/v202/leviathan23 a.html.   \\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023.   \\nS. Li and T. Hoefler. Chimera: efficiently training large-scale neural networks with bidirectional pipelines. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC \\u201921, page 1\\u201314. ACM, Nov. 2021. doi: 10.1145/345881 7.3476145. URL http://dx.doi.org/10.1145/3458817.3476145.   \\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024a.   \\nW. Li, F. Qi, M. Sun, X. Yi, and J. Zhang. Ccpm: A chinese classical poetry matching dataset, 2021.   \\nY. Li, F. Wei, C. Zhang, and H. Zhang. EAGLE: speculative sampling requires rethinking feature uncertainty. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024b. URL https://openreview.net /forum?id $\\\\c=$ 1NdN7eXyb4.   \\nB. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval.   \\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \\nS. Lundberg. The art of prompt design: Prompt boundaries and token healing, 2023. URL https://towardsdatascience.com/the-art-of-prompt-design-prompt-bound aries-and-token-healing-3b2448b0be38.   \\nY. Luo, Z. Zhang, R. Wu, H. Liu, Y. Jin, K. Zheng, M. Wang, Z. He, G. Hu, L. Chen, et al. Ascend HiFloat8 format for deep learning. arXiv preprint arXiv:2409.16626, 2024.   \\nMAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime.   \\nP. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisenthwaite, S. Ha, A. Heinecke, P. Judd, J. Kamalu, et al. FP8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.   \\nMistral. Cheaper, better, faster, stronger: Continuing to push the frontier of ai and making it accessible to all, 2024. URL https://mistral.ai/news/mixtral-8x22b.   \\nS. Narang, G. Diamos, E. Elsen, P. Micikevicius, J. Alben, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. In Int. Conf. on Learning Representation, 2017. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"B. Noune, P. Jones, D. Justus, D. Masters, and C. Luschi. 8-bit numerical formats for deep neural networks. arXiv preprint arXiv:2206.02915, 2022. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"NVIDIA. Improving network performance of HPC systems using NVIDIA Magnum IO NVSHMEM and GPUDirect Async. https://developer.nvidia.com/blog/improving-net work-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-g pudirect-async, 2022. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"NVIDIA. Blackwell architecture. https://www.nvidia.com/en-us/data-center/tech nologies/blackwell-architecture/, 2024a.   \\nNVIDIA. TransformerEngine, 2024b. URL https://github.com/NVIDIA/TransformerE ngine. Accessed: 2024-11-19.   \\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.   \\nOpenAI. Multilingual massive multitask language understanding (mmmlu), 2024b. URL https://huggingface.co/datasets/openai/MMMLU.   \\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing   \\n-simpleqa/.   \\nOpenAI. Introducing SWE-bench verified we\\u2019re releasing a human-validated subset of swebench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/.   \\nB. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023a.   \\nH. Peng, K. Wu, Y. Wei, G. Zhao, Y. Yang, Z. Liu, Y. Xiong, Z. Yang, B. Ni, J. Hu, et al. FP8-LM: Training FP8 large language models. arXiv preprint arXiv:2310.18313, 2023b.   \\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble pipeline parallelism. arXiv preprint arXiv:2401.10241, 2023a.   \\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble pipeline parallelism, 2023b. URL https: //arxiv.org/abs/2401.10241.   \\nQwen. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \\nQwen. Introducing Qwen1.5, 2024a. URL https://qwenlm.github.io/blog/qwen1.5.   \\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5.   \\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\\u201316. IEEE, 2020.   \\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.   \\nB. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea, E. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023a.   \\nB. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea, E. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023b.   \\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.   \\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017. OpenReview.net, 2017. URL https: //openreview.net/forum?id $\\\\c=$ B1ckMDqlg.   \\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?i d=fR3wGCk-IXp.   \\nY. Shibata, T. Kida, S. Fukamachi, M. Takeda, A. Shinohara, T. Shinohara, and S. Arikawa. Byte pair encoding: A text compression scheme that accelerates pattern matching. 1999.   \\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \\nK. Sun, D. Yu, D. Yu, and C. Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension, 2019a.   \\nM. Sun, X. Chen, J. Z. Kolter, and Z. Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024.   \\nX. Sun, J. Choi, C.-Y. Chen, N. Wang, S. Venkataramani, V. V. Srinivasan, X. Cui, W. Zhang, and K. Gopalakrishnan. Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks. Advances in neural information processing systems, 32, 2019b.   \\nM. Suzgun, N. Scales, N. Sch\\u00e4rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.   \\nV. Thakkar, P. Ramani, C. Cecka, A. Shivam, H. Lu, E. Yan, J. Kosaian, M. Hoemmen, H. Wu, A. Kerr, M. Nicely, D. Merrill, D. Blasig, F. Qiao, P. Majcher, P. Springer, M. Hohnerbach, J. Wang, and M. Gupta. CUTLASS, Jan. 2023. URL https://github.com/NVIDIA/cutlas s.   \\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.   \\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307. 09288.   \\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \\u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \\nL. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. CoRR, abs/2408.15664, 2024a. URL https://doi.org/10.48550/arX iv.2408.15664.   \\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024b. URL https://doi.org/10.48550/arXiv.2406.01574.   \\nT. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023.   \\nM. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and low-precision training for large-scale vision-language models. Advances in Neural Information Processing Systems, 36:10271\\u201310298, 2023.   \\nH. Xi, C. Li, J. Chen, and J. Zhu. Training transformers with 4-bit integers. Advances in Neural Information Processing Systems, 36:49146\\u201349168, 2023.   \\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024.   \\nH. Xia, T. Ge, P. Wang, S. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 3909\\u20133925. Association for Computational Linguistics, 2023. URL https://doi.org/10.18653/v1/ 2023.findings-emnlp.257.   \\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087\\u201338099. PMLR, 2023.   \\nL. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, Y. Tian, Q. Dong, W. Liu, B. Shi, Y. Cui, J. Li, J. Zeng, R. Wang, W. Xie, Y. Li, Y. Patterson, Z. Tian, Y. Zhang, H. Zhou, S. Liu, Z. Zhao, Q. Zhao, C. Yue, X. Zhang, Z. Yang, K. Richardson, and Z. Lan. CLUE: A chinese language understanding evaluation benchmark. In D. Scott, N. Bel, and C. Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4762\\u20134772. International Committee on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.419. URL https://doi.org/10.18653/v1/2020.coling-main.419. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. M\\u00e0rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4791\\u20134800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1 9-1472. W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023. doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. \"}}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"Appendix \"}, \"children\": []}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"A. Contributions and Acknowledgments \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Research & Engine   \\nAixin Liu   \\nBing Xue   \\nBingxuan Wang   \\nBochao Wu   \\nChengda Lu   \\nChenggang Zhao   \\nChengqi Deng   \\nChenyu Zhang\\\\*   \\nChong Ruan   \\nDamai Dai   \\nDaya Guo   \\nDejian Yang   \\nDeli Chen   \\nErhang Li   \\nFangyun Lin   \\nFucong Dai   \\nFuli Luo\\\\*   \\nGuangbo Hao   \\nGuanting Chen   \\nGuowei Li   \\nH. Zhang   \\nHan Bao\\\\*   \\nHanwei Xu   \\nHaocheng Wang\\\\*   \\nHaowei Zhang   \\nHonghui Ding   \\nHuajian Xin\\\\*   \\nHuazuo Gao   \\nHui Qu   \\nJianzhong Guo   \\nJiashi Li   \\nJiawei Wang\\\\*   \\nJingchang Chen   \\nJingyang Yuan   \\nJunjie Qiu   \\nJunlong Li   \\nJunxiao Song   \\nKai Dong   \\nKai Hu\\\\*   \\nKaige Gao   \\nKang Guan   \\nKexin Huang   \\nKuai Yu   \\nLean Wang   \\nLecong Zhang   \\nLiang Zhao   \\nLitong Wang   \\nLiyue Zhang   \\nMingchuan Zhang   \\nMinghua Zhang   \\nMinghui Tang   \\nPanpan Huang   \\nPeiyi Wang   \\nQiancheng Wang   \\nQihao Zhu   \\nQinyu Chen   \\nQiushi Du   \\nRuiqi Ge   \\nRuisong Zhang   \\nRuizhe Pan   \\nRunji Wang   \\nRunxin Xu   \\nRuoyu Zhang   \\nShanghao Lu   \\nShangyan Zhou   \\nShanhuang Chen   \\nShengfeng Ye   \\nShirong Ma   \\nShiyu Wang   \\nShuiping Yu   \\nShunfeng Zhou   \\nShuting Pan   \\nTao Yun   \\nTian Pei   \\nWangding Zeng   \\nWanjia Zhao\\\\*   \\nWen Liu   \\nWenfeng Liang   \\nWenjun Gao   \\nWenqin Yu   \\nWentao Zhang   \\nXiao Bi   \\nXiaodong Liu   \\nXiaohan Wang   \\nXiaokang Chen   \\nXiaokang Zhang   \\nXiaotao Nie   \\nXin Cheng   \\nXin Liu   \\nXin Xie   \\nXingchao Liu   \\nXingkai Yu   \\nXinyu Yang   \\nXinyuan Li   \\nXuecheng Su   \\nXuheng Lin   \\nY.K. Li   \\nY.Q. Wang   \\nY.X. Wei   \\nYang Zhang   \\nYanhong Xu   \\nYao Li   \\nYao Zhao   \\nYaofeng Sun   \\nYaohui Wang   \\nYi Yu   \\nYichao Zhang   \\nYifan Shi   \\nYiliang Xiong   \\nYing He   \\nYishi Piao   \\nYisong Wang   \\nYixuan Tan   \\nYiyang Ma\\\\*   \\nYiyuan Liu   \\nYongqiang Guo   \\nYu Wu   \\nYuan Ou   \\nYuduan Wang   \\nYue Gong   \\nYuheng Zou   \\nYujia He   \\nYunfan Xiong   \\nYuxiang Luo   \\nYuxiang You   \\nYuxuan Liu   \\nYuyang Zhou   \\nZ.F. Wu   \\nZ.Z. Ren   \\nZehui Ren   \\nZhangli Sha   \\nZhe Fu   \\nZhean Xu   \\nZhenda Xie   \\nZhengyan Zhan   \\nZhewen Hao   \\nZhibin Gou   \\nZhicheng Ma \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Zhigang Yan Zhihong Shao Zhiyu Wu Zhuoshu Li Zihui Gu Zijia Zhu Zijun Liu\\\\* Zilin Li Ziwei Xie Ziyang Song Ziyi Gao Zizheng Pan \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Data Annotation   \\nBei Feng   \\nHui Li   \\nJ.L. Cai   \\nJiaqi Ni   \\nLei Xu   \\nMeng Li   \\nNing Tian   \\nR.J. Chen   \\nR.L. Jin   \\nRuyi Chen   \\nS.S. Li   \\nShuang Zhou   \\nTianyu Sun   \\nX.Q. Li   \\nXiangyue Jin   \\nXiaojin Shen   \\nXiaosha Chen   \\nXiaowen Sun   \\nXiaoxiang Wang   \\nXinnan Song   \\nXinyi Zhou   \\nY.X. Zhu   \\nYanhong Xu   \\nYanping Huang   \\nYaohui Li   \\nYi Zheng   \\nYuchen Zhu   \\nYunxian Ma   \\nZhen Huang   \\nZhipeng Xu   \\nZhongyu Zhang \"}}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"Business & Compliance Dongjie Ji \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Jian Liang   \\nJin Chen   \\nLeyi Xia   \\nMiaojun Wang Mingming Li Peng Zhang   \\nShaoqing Wu Shengfeng Ye T. Wang \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"W.L. Xiao Wei An Xianzu Wang Xinxia Shan Ying Tang Yukun Zha Yuting Yan Zhen Zhang \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Within each role, authors are listed alphabetically by the first name. Names marked with \\\\* denote individuals who have departed from our team. \"}}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"B. Ablation Studies for Low-Precision Training \"}, \"children\": [{\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/b0ad5ed91f42c8d54b3c818553783cbd94b8e018e59a93200aea26c539e58a1d.jpg\", \"img_caption\": [\"Figure 10 | Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9. \"]}}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"B.1. FP8 v.s. BF16 Training \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"We validate our FP8 mixed precision framework with a comparison to BF16 training on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising approximately 16B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising approximately 230B total parameters on around $0.9\\\\mathrm{T}$ tokens. We show the training curves in Figure 10 and demonstrate that the relative error remains below $0.25\\\\%$ with our high-precision accumulation and fine-grained quantization strategies. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"B.2. Discussion About Block-Wise Quantization \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Although our tile-wise fine-grained quantization effectively mitigates the error introduced by feature outliers, it requires different groupings for activation quantization, i.e., $\\\\mathtt{1x128}$ in forward pass and $128\\\\mathbf{x}\\\\mathbf{1}$ for backward pass. A similar process is also required for the activation gradient. A straightforward strategy is to apply block-wise quantization per $128\\\\mathrm{x}128$ elements like the way we quantize the model weights. In this way, only transposition is required for backward. Therefore, we conduct an experiment where all tensors associated with Dgrad are quantized on a block-wise basis. The results reveal that the Dgrad operation which computes the activation gradients and back-propagates to shallow layers in a chain-like manner, is highly sensitive to precision. Specifically, block-wise quantization of activation gradients leads to model divergence on an MoE model comprising approximately 16B total parameters, trained for around 300B tokens. We hypothesize that this sensitivity arises because activation gradients are highly imbalanced among tokens, resulting in token-correlated outliers (Xi et al., 2023). These outliers cannot be effectively managed by a block-wise quantization approach. \"}}]}]}, {\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"We record the expert load of the 16B auxiliary-loss-based baseline and the auxiliary-loss-free model on the Pile test set. The auxiliary-loss-free model tends to have greater expert specialization across all layers, as demonstrated in Figure 10. \"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/df414ba6ec58ecdea8399c32970505fd62815fab4090c286317901838eb29684.jpg\"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/ce6b240feda49244a4a0806c8852a8a67ba9428299564763e225999cf132ff86.jpg\"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/3883c917c521a40a6dc026f2ea4a5be3fcce25df9d142f0ea4eb9ec79aaed32a.jpg\"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/935ea04ae9b644bdc56695b8bee3a4a02c590d8ad7e185c711726649f5a80605.jpg\"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/aac7de5cd380235acb5843d45956d36baf8f6c532611c0bee9a72195ee967845.jpg\", \"img_caption\": [\"Figure 10 | Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. \"]}}]}]}\n"
     ]
    }
   ],
   "source": [
    "from rolling.processing import printable_content, get_content_hierarchy, Content\n",
    "print(printable_content(doc, indent=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[]```\n",
      "* Current Chapter Title: ``DeepSeek-V3 Technical Report `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"DeepSeek-V3 Technical Report \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"DeepSeek-AI \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"research@deepseek.com \"}}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 954753625\n",
      "{'chapter_title': 'DeepSeek-V3 Technical Report ', 'summaries': []}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}]```\n",
      "* Current Chapter Title: ``Abstract `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"Abstract \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3. \"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/700c3b3526a18f86057af07c28c75896ba00dd4edc85c8f924fc81e859e84d80.jpg\", \"img_caption\": [\"Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts. \"]}}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 243145053\n",
      "{'chapter_title': 'Abstract', 'summaries': ['DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.', 'The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.', 'DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.', 'Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.', 'Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.', 'Model checkpoints are available on GitHub.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}]```\n",
      "* Current Chapter Title: ``2 Architecture 6 `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"2 Architecture 6 \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"2.1 Basic Architecture 6   \\n2.1.1 Multi-Head Latent Attention 7   \\n2.1.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing 8   \\n2.2 Multi-Token Prediction . 10 \"}}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 850206842\n",
      "{'chapter_title': '2 Architecture 6', 'summaries': ['The architecture incorporates Multi-Head Latent Attention (MLA).', 'DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.', 'The training objective includes multi-token prediction.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}]```\n",
      "* Current Chapter Title: ``3 Infrastructures 11 `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"3 Infrastructures 11 \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"3.1 Compute Clusters . 11   \\n3.2 Training Framework 12   \\n3.2.1 DualPipe and Computation-Communication Overlap . 12   \\n3.2.2 Efficient Implementation of Cross-Node All-to-All Communication . . 13   \\n3.2.3 Extremely Memory Saving with Minimal Overhead . 14   \\n3.3 FP8 Training 14   \\n3.3.1 Mixed Precision Framework 15   \\n3.3.2 Improved Precision from Quantization and Multiplication 16   \\n3.3.3 Low-Precision Storage and Communication 18   \\n3.4 Inference and Deployment . 18   \\n3.4.1 Prefilling . 19   \\n3.4.2 Decoding 19   \\n3.5 Suggestions on Hardware Design 20   \\n3.5.1 Communication Hardware 20   \\n3.5.2 Compute Hardware 20 \"}}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 970228772\n",
      "{'chapter_title': '3 Infrastructures 11', 'summaries': ['DualPipe enables computation-communication overlap.', 'An efficient cross-node all-to-all communication implementation was developed.', 'The training framework prioritizes extreme memory saving with minimal overhead.', 'A mixed-precision framework (FP8) improves precision through quantization and multiplication.', 'Low-precision storage and communication are employed.', 'Inference includes prefilling and decoding steps.', 'Hardware design suggestions include considerations for communication and compute hardware.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}]```\n",
      "* Current Chapter Title: ``4 Pre-Training 21 `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"4 Pre-Training 21 \"}, \"children\": [{\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.1 Data Construction . 21 \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"4.2 Hyper-Parameters . 22   \\n4.3 Long Context Extension 23   \\n4.4 Evaluations 24   \\n4.4.1 Evaluation Benchmarks 24   \\n4.4.2 Evaluation Results 24   \\n4.5 Discussion 26   \\n4.5.1 Ablation Studies for Multi-Token Prediction 26   \\n4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy 26 \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance . 27 \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"5 Post-Training 28   \\n5.1 Supervised Fine-Tuning 28   \\n5.2 Reinforcement Learning . 29   \\n5.2.1 Reward Model 29   \\n5.2.2 Group Relative Policy Optimization 30   \\n5.3 Evaluations 30   \\n5.3.1 Evaluation Settings 30   \\n5.3.2 Standard Evaluation 31   \\n5.3.3 Open-Ended Evaluation 33   \\n5.3.4 DeepSeek-V3 as a Generative Reward Model 33   \\n5.4 Discussion 34   \\n5.4.1 Distillation from DeepSeek-R1 34   \\n5.4.2 Self-Rewarding 34   \\n5.4.3 Multi-Token Prediction Evaluation 35 \"}}]}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 347848077\n",
      "{'chapter_title': '4 Pre-Training 21', 'summaries': ['The pre-training process involved constructing a dataset and optimizing hyperparameters.', 'A long context extension technique was employed.', 'Evaluations were conducted using various benchmarks, revealing performance results.', 'Ablation studies explored the impact of multi-token prediction.', 'Ablation studies examined the auxiliary-loss-free balancing strategy.', 'The effectiveness of batch-wise versus sequence-wise load balancing was investigated.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}]```\n",
      "* Current Chapter Title: ``6 Conclusion, Limitations, and Future Directions 35 `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"6 Conclusion, Limitations, and Future Directions 35 \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"A Contributions and Acknowledgments 45   \\nB Ablation Studies for Low-Precision Training 47   \\nB.1 FP8 v.s. BF16 Training 47   \\nB.2 Discussion About Block-Wise Quantization 47 \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"C Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-Loss-Free Models 48 \"}}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 191056283\n",
      "{'chapter_title': '6 Conclusion, Limitations, and Future Directions 35', 'summaries': []}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}]```\n",
      "* Current Chapter Title: ``1. Introduction `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"1. Introduction \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap towards Artificial General Intelligence (AGI). Beyond closed-source models, open-source models, including DeepSeek series (DeepSeek-AI, 2024a,b,c; Guo et al., 2024), LLaMA series (AI@Meta, $2024\\\\mathsf{a},\\\\mathsf{b},$ ; Touvron et al., 2023a,b), Qwen series (Qwen, 2023, 2024a,b), and Mistral series (Jiang et al., 2023; Mistral, 2024), are also making significant strides, endeavoring to close the gap with their closed-source counterparts. To further push the boundaries of open-source model capabilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"With a forward-looking perspective, we consistently strive for strong model performance and economical costs. Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for cost-effective training. These two architectures have been validated in DeepSeekV2 (DeepSeek-AI, 2024c), demonstrating their capability to maintain robust model performance while achieving efficient training and inference. Beyond the basic architecture, we implement two additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pioneers an auxiliary-loss-free strategy (Wang et al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. Secondly, DeepSeek-V3 employs a multi-token prediction training objective, which we have observed to enhance the overall performance on evaluation benchmarks. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"In order to achieve efficient training, we support the FP8 mixed precision training and implement comprehensive optimizations for the training framework. Low-precision training has emerged as a promising solution for efficient training (Dettmers et al., 2022; Kalamkar et al., 2019; Narang et al., 2017; Peng et al., 2023b), its evolution being closely tied to advancements in hardware capabilities (Luo et al., 2024; Micikevicius et al., 2022; Rouhani et al., 2023a). In this work, we introduce an FP8 mixed precision training framework and, for the first time, validate its effectiveness on an extremely large-scale model. Through the support for FP8 computation and storage, we achieve both accelerated training and reduced GPU memory usage. As for the training framework, we design the DualPipe algorithm for efficient pipeline parallelism, which has fewer pipeline bubbles and hides most of the communication during training through computation-communication overlap. This overlap ensures that, as the model further scales up, as long as we maintain a constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead. In addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize InfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory footprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism. Combining these efforts, we achieve high training efficiency. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The pre-training process is remarkably stable. Throughout the entire training process, we did not encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage context length extension for DeepSeek-V3. In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K. Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential. During the post-training stage, we distill the reasoning capability from the DeepSeekR1 series of models, and meanwhile carefully maintain the balance between model accuracy \"}}, {\"type\": \"table\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/11b8e875de1519088afb45aab53bb45a5c61507dc80cebf8f2ce4ab6e24b70bc.jpg\", \"table_body\": \"\\n\\n<html><body><table><tr><td>Training Costs</td><td>Pre-Training</td><td>Context Extension</td><td>Post-Training</td><td>Total</td></tr><tr><td> in H800 GPU Hours</td><td>2664K</td><td>119K</td><td>5K</td><td>2788K</td></tr><tr><td>in USD</td><td>$5.328M</td><td>$0.238M</td><td>$0.01M</td><td>$5.576M</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Table 1 | Training costs of DeepSeek-V3, assuming the rental price of H800 is $\\\\$2$ per GPU hour. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"and generation length. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"We evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the strongest open-source base model currently available, especially in code and math. Its chat version also outperforms other open-source models and achieves performance comparable to leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard and open-ended benchmarks. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware. During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pretraining stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post-training, DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the $\\\\mathrm{{H800GPU}}$ is $\\\\$2$ per GPU hour, our total training costs amount to only $\\\\$5760$ . Note that the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Our main contribution includes: \"}}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"Architecture: Innovative Load Balancing Strategy and Training Objective \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"\\u2022 On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing. \\u2022 We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"Pre-Training: Towards Ultimate Training Efficiency \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"\\u2022 We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model. \\u2022 Through the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computationcommunication overlap. This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead. \\u2022 At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"Post-Training: Knowledge Distillation from DeepSeek-R1 \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"\\u2022 We introduce an innovative methodology to distill reasoning capabilities from the longChain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain control over the output style and length of DeepSeek-V3. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"Summary of Core Evaluation Results \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"\\u2022 Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA, DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9 on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3 demonstrates superior performance among open-source models on both SimpleQA and Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese SimpleQA), highlighting its strength in Chinese factual knowledge. \\u2022 Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on math-related benchmarks among all non-long-CoT open-source and closed-source models. Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500, demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks, DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks, such as LiveCodeBench, solidifying its position as the leading model in this domain. For engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5, it still outpaces all other models by a significant margin, demonstrating its competitiveness across diverse technical benchmarks. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3 model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing our compute clusters, the training framework, the support for FP8 training, the inference deployment strategy, and our suggestions on future hardware design. Next, we describe our pre-training process, including the construction of training data, hyper-parameter settings, longcontext extension techniques, the associated evaluations, as well as some discussions (Section 4). Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly, we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential directions for future research (Section 6). \"}}]}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 63806094\n",
      "{'chapter_title': '1. Introduction ', 'summaries': ['DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.', 'The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.', 'FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.', 'The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.', 'DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.', 'Training cost was 2.788M H800 GPU hours.', 'Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.', 'DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}, {\"chapter_title\": \"1. Introduction \", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\", \"The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\", \"FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\", \"The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\", \"Training cost was 2.788M H800 GPU hours.\", \"Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\", \"DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\"]}]```\n",
      "* Current Chapter Title: ``2. Architecture `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"2. Architecture \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for economical training. Then, we present a Multi-Token Prediction (MTP) training objective, which we have observed to enhance the overall performance on evaluation benchmarks. For other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeekV2 (DeepSeek-AI, 2024c). \"}}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"2.1. Basic Architecture \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017) framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) for DeepSeekMoE to mitigate the performance degradation induced by the effort to ensure load balance. Figure 2 illustrates the basic architecture of DeepSeek-V3, and we will briefly review the details of MLA and DeepSeekMoE in this section. \"}}, {\"type\": \"image\", \"level\": 2, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0a839f8095c46c85a9c566ea17c4b8aad3d84635b3051071b2cd9fed03bb13d7.jpg\", \"img_caption\": [\"Figure 2 | Illustration of the basic architecture of DeepSeek-V3. Following DeepSeek-V2, we adopt MLA and DeepSeekMoE for efficient inference and economical training. \"]}}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"2.1.1. Multi-Head Latent Attention \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"For attention, DeepSeek-V3 adopts the MLA architecture. Let $d$ denote the embedding dimension, $n_{h}$ denote the number of attention heads, $d_{h}$ denote the dimension per head, and $\\\\mathbf h_{t}\\\\in\\\\mathbb R^{d}$ denote the attention input for the $t$ -th token at a given attention layer. The core of MLA is the low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during inference: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{c}{{{\\\\displaystyle\\\\left[{\\\\bf{\\\\overline{{c}}}}_{t}^{K V}}\\\\right]=W^{D K V}{\\\\bf h}_{t},}}\\\\\\\\ {{{\\\\displaystyle[{\\\\bf{k}}_{t,1}^{C};{\\\\bf{k}}_{t,2}^{C};...;{\\\\bf{k}}_{t,n_{h}}^{C}]={\\\\bf{k}}_{t}^{C}=W^{U K}{\\\\bf c}_{t}^{K V}},}}\\\\\\\\ {{{\\\\displaystyle\\\\left[{\\\\bf{\\\\overline{{k}}}}_{t}^{R}\\\\right]={\\\\mathrm{RoPE}}(W^{K R}{\\\\bf h}_{t})},}}\\\\\\\\ {{{\\\\displaystyle{\\\\bf{k}}_{t,i}=[{\\\\bf k}_{t,i}^{C};{\\\\bf k}_{t}^{R}]},}}\\\\\\\\ {{{\\\\displaystyle[{\\\\bf v}_{t,1}^{C};{\\\\bf v}_{t,2}^{C};...;{\\\\bf v}_{t,n_{h}}^{C}]={\\\\bf v}_{t}^{C}=W^{U V}{\\\\bf c}_{t}^{K V}},}}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where $\\\\mathbf{c}_{t}^{K V}\\\\in\\\\mathbb{R}^{d_{c}}$ is the compressed latent vector for keys and values; $d_{c}(\\\\ll d_{h}n_{h})$ indicates the KV compression dimension; $\\\\dot{W^{D K V}}\\\\in\\\\mathbb{R}^{d_{c}\\\\times d}$ denotes the down-projection matrix; $W^{U K}$ , $W^{U V}\\\\in\\\\mathbb{R}^{d_{h}n_{h}\\\\times d_{c}}$ are the up-projection matrices for keys and values, respectively; $W^{K R}\\\\in\\\\mathbb{R}^{d_{h}^{R}\\\\times d}$ is the matrix used to produce the decoupled key that carries Rotary Positional Embedding (RoPE) (Su et al., 2024); RoPE(\\u00b7) denotes the operation that applies RoPE matrices; and $[\\\\cdot;\\\\cdot]$ denotes concatenation. Note that for MLA, only the blue-boxed vectors (i.e., $\\\\mathbf{c}_{t}^{K V}$ and $\\\\mathbf{k}_{t}^{R}$ ) need to be cached during generation, which results in significantly reduced KV cache while maintaining performance comparable to standard Multi-Head Attention (MHA) (Vaswani et al., 2017). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"For the attention queries, we also perform a low-rank compression, which can reduce the activation memory during training: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{c}{{\\\\displaystyle{\\\\bf c}_{t}^{Q}=W^{D Q}{\\\\bf h}_{t},}}\\\\\\\\ {{\\\\displaystyle[{\\\\bf q}_{t,1}^{C};{\\\\bf q}_{t,2}^{C};...;{\\\\bf q}_{t,n_{h}}^{C}]={\\\\bf q}_{t}^{C}=W^{U Q}{\\\\bf c}_{t}^{Q},}}\\\\\\\\ {{\\\\displaystyle[{\\\\bf q}_{t,1}^{R};{\\\\bf q}_{t,2}^{R};...;{\\\\bf q}_{t,n_{h}}^{R}]={\\\\bf q}_{t}^{R}=\\\\mathrm{RoPE}(W^{Q R}{\\\\bf c}_{t}^{Q}),}}\\\\\\\\ {{\\\\displaystyle{\\\\bf q}_{t,i}=[{\\\\bf q}_{t,i}^{C};{\\\\bf q}_{t,i}^{R}],}}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where $\\\\mathbf{c}_{t}^{Q}\\\\in\\\\mathbb{R}^{d_{c}^{\\\\prime}}$ is the compressed latent vector for queries; $d_{c}^{\\\\prime}(\\\\ll~d_{h}n_{h})$ denotes the query compression dimension; $W^{D\\\\hat{Q}}\\\\in\\\\mathbb R^{d_{c}^{\\\\prime}\\\\times d},W^{U Q}\\\\in\\\\mathbb R^{d_{h}n_{h}\\\\times d_{c}^{\\\\prime}}$ are the down-projection and up-projection matrices for queries, respectively; and $W^{Q R}\\\\in\\\\mathbb{R}^{d_{h}^{R}n_{h}\\\\times d_{c}^{\\\\prime}}$ is the matrix to produce the decoupled queries that carry RoPE. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Ultimately, the attention queries $(\\\\mathbf{q}_{t,i})$ , keys $(\\\\mathbf{k}_{j,i})$ , and values $(\\\\mathbf{v}_{j,i}^{C})$ are combined to yield the final attention output ${\\\\bf u}_{t}$ : \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{l}{{\\\\displaystyle{\\\\bf{o}}_{t,i}=\\\\sum_{j=1}^{t}{\\\\cal{S}}\\\\mathrm{oftmax}_{j}(\\\\frac{{\\\\bf{q}}_{t,i}^{T}{\\\\bf{k}}_{j,i}}{\\\\sqrt{d_{h}+d_{h}^{R}}}){\\\\bf{v}}_{j,i}^{C}},\\\\ ~}\\\\\\\\ {{\\\\bf{u}}_{t}={\\\\cal{W}}^{O}[{\\\\bf{o}}_{t,1};{\\\\bf{o}}_{t,2};...;{\\\\bf{o}}_{t,n_{h}}],\\\\ ~}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where $W^{O}\\\\in\\\\mathbb{R}^{d\\\\times d_{h}n_{h}}$ denotes the output projection matrix. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Basic Architecture of DeepSeekMoE. For Feed-Forward Networks (FFNs), DeepSeek-V3 employs the DeepSeekMoE architecture (Dai et al., 2024). Compared with traditional MoE architectures like GShard (Lepikhin et al., 2021), DeepSeekMoE uses finer-grained experts and isolates some experts as shared ones. Let ${\\\\bf u}_{t}$ denote the FFN input of the $t$ -th token, we compute the FFN output ${\\\\bf h}_{t}^{\\\\prime}$ as follows: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{l}{{\\\\displaystyle{\\\\bf h}_{t}^{\\\\prime}={\\\\bf u}_{t}+\\\\sum_{i=1}^{N_{s}}\\\\mathrm{FFN}_{i}^{(s)}\\\\left({\\\\bf u}_{t}\\\\right)+\\\\sum_{i=1}^{N_{r}}g_{i,t}\\\\mathrm{FFN}_{i}^{(r)}\\\\left({\\\\bf u}_{t}\\\\right)},}}\\\\\\\\ {{\\\\displaystyle g_{i,t}=\\\\frac{g_{i,t}^{\\\\prime}}{\\\\sum_{j=1}^{N_{r}}g_{j,t}^{\\\\prime}}},}\\\\\\\\ {{\\\\displaystyle g_{i,t}^{\\\\prime}=\\\\left\\\\{{s_{i,t}},\\\\quad s_{i},\\\\in\\\\mathrm{Topk}(\\\\{s_{j,t}|1\\\\leqslant j\\\\leqslant N_{r}\\\\},K_{r}),\\\\right.}}\\\\\\\\ {{\\\\displaystyle\\\\left.0,\\\\quad\\\\mathrm{otherwise}},\\\\right.}}\\\\\\\\ {{\\\\displaystyle s_{i,t}=\\\\mathrm{Sigmoid}\\\\left({\\\\bf u}_{t}^{,T}{\\\\bf e}_{i}\\\\right)},}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where $N_{s}$ and $N_{r}$ denote the numbers of shared experts and routed experts, respectively; $\\\\mathrm{FFN}_{i}^{(s)}(\\\\cdot)$ and $\\\\mathrm{FFN}_{i}^{(r)}(\\\\cdot)$ denote the \\ud835\\udc56-th shared expert and the $i\\\\cdot$ -th routed expert, respectively; $K_{r}$ denotes the number of activated routed experts; $g_{i,t}$ is the gating value for the $i$ -th expert; $s_{i,t}$ is the token-to-expert affinity; $\\\\mathbf{e}_{i}$ is the centroid vector of the $i\\\\cdot$ -th routed expert; and $\\\\mathrm{Topk}(\\\\cdot,K)$ denotes the set comprising $K$ highest scores among the affinity scores calculated for the $t$ -th token and all routed experts. Slightly different from DeepSeek-V2, DeepSeek-V3 uses the sigmoid function to compute the affinity scores, and applies a normalization among all selected affinity scores to produce the gating values. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Auxiliary-Loss-Free Load Balancing. For MoE models, an unbalanced expert load will lead to routing collapse (Shazeer et al., 2017) and diminish computational efficiency in scenarios with expert parallelism. Conventional solutions usually rely on the auxiliary loss (Fedus et al., 2021; Lepikhin et al., 2021) to avoid unbalanced load. However, too large an auxiliary loss will impair the model performance (Wang et al., 2024a). To achieve a better trade-off between load balance and model performance, we pioneer an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) to ensure load balance. To be specific, we introduce a bias term $b_{i}$ for each expert and add it to the corresponding affinity scores $s_{i,t}$ to determine the top-K routing: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\ng_{i,t}^{\\\\prime}=\\\\left\\\\{\\\\begin{array}{l l}{s_{i,t},\\\\quad s_{i,t}+b_{i}\\\\in\\\\mathrm{Topk}(\\\\{s_{j,t}+b_{j}|1\\\\leqslant j\\\\leqslant N_{r}\\\\},K_{r}),}\\\\\\\\ {0,\\\\quad\\\\mathrm{otherwise}.}\\\\end{array}\\\\right.\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Note that the bias term is only used for routing. The gating value, which will be multiplied with the FFN output, is still derived from the original affinity score $s_{i,t}$ . During training, we keep monitoring the expert load on the whole batch of each training step. At the end of each step, we will decrease the bias term by $\\\\gamma$ if its corresponding expert is overloaded, and increase it by $\\\\gamma$ if its corresponding expert is underloaded, where $\\\\gamma$ is a hyper-parameter called bias update speed. Through the dynamic adjustment, DeepSeek-V3 keeps balanced expert load during training, and achieves better performance than models that encourage load balance through pure auxiliary losses. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Complementary Sequence-Wise Auxiliary Loss. Although DeepSeek-V3 mainly relies on the auxiliary-loss-free strategy for load balance, to prevent extreme imbalance within any single sequence, we also employ a complementary sequence-wise balance loss: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{r l r}{\\\\lefteqn{\\\\mathcal{L}_{\\\\mathrm{Bal}}=\\\\alpha\\\\sum_{i=1}^{N_{r}}f_{i}P_{i},}}\\\\\\\\ &{f_{i}=\\\\frac{N_{r}}{K_{r}T}\\\\sum_{\\\\ell=1}^{T}\\\\mathbb{1}\\\\left(s_{i,\\\\ell}\\\\-\\\\mathrm{{T}}\\\\mathrm{{opk}}(\\\\{s_{j,\\\\ell}|1\\\\leqslant j\\\\leqslant N_{r}\\\\},K_{r})\\\\right),}\\\\\\\\ &{s_{i,\\\\ell}^{\\\\prime}=\\\\frac{s_{i,\\\\ell}}{\\\\sum_{j=1}^{N_{r}}s_{j,\\\\ell}},}\\\\\\\\ &{}&{P_{i}=\\\\frac{1}{T}\\\\sum_{\\\\ell=1}^{T}s_{i,\\\\ell}^{\\\\prime},}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where the balance factor $\\\\alpha$ is a hyper-parameter, which will be assigned an extremely small value for DeepSeek-V3; $\\\\Im(\\\\cdot)$ denotes the indicator function; and $T$ denotes the number of tokens in a sequence. The sequence-wise balance loss encourages the expert load on each sequence to be balanced. \"}}, {\"type\": \"image\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/6d0fae87957c43036f1578472f7c8b774d8aac67ba5a91dc447b9583e4c07b4b.jpg\", \"img_caption\": [\"Figure 3 | Illustration of our Multi-Token Prediction (MTP) implementation. We keep the complete causal chain for the prediction of each token at each depth. \"]}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Node-Limited Routing. Like the device-limited routing used by DeepSeek-V2, DeepSeek-V3 also uses a restricted routing mechanism to limit communication costs during training. In short, we ensure that each token will be sent to at most \\ud835\\udc40 nodes, which are selected according to the sum of the highest $\\\\frac{K_{r}}{M}$ affinity scores of the experts distributed on each node. Under this constraint, our MoE training framework can nearly achieve full computation-communication overlap. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"No Token-Dropping. Due to the effective load balancing strategy, DeepSeek-V3 keeps a good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training. In addition, we also implement specific deployment strategies to ensure inference load balance, so DeepSeek-V3 also does not drop tokens during inference. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"2.2. Multi-Token Prediction \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Inspired by Gloeckle et al. (2024), we investigate and set a Multi-Token Prediction (MTP) objective for DeepSeek-V3, which extends the prediction scope to multiple future tokens at each position. On the one hand, an MTP objective densifies the training signals and may improve data efficiency. On the other hand, MTP may enable the model to pre-plan its representations for better prediction of future tokens. Figure 3 illustrates our implementation of MTP. Different from Gloeckle et al. (2024), which parallelly predicts $D$ additional tokens using independent output heads, we sequentially predict additional tokens and keep the complete causal chain at each prediction depth. We introduce the details of our MTP implementation in this section. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"MTP Modules. To be specific, our MTP implementation uses $D$ sequential modules to predict $D$ additional tokens. The $k$ -th MTP module consists of a shared embedding layer $\\\\operatorname{Emb}({\\\\cdot})$ , a shared output head OutHead $(\\\\cdot)$ , a Transformer block $\\\\mathrm{TRM}_{k}(\\\\cdot).$ , and a projection matrix $M_{k}\\\\in\\\\mathbb{R}^{d\\\\times2d}$ . For the $i\\\\cdot$ -th input token $t_{i},$ at the $k$ -th prediction depth, we first combine the representation of the $i$ -th token at the $(k-1)$ -th depth $\\\\mathbf{h}_{i}^{k-1}\\\\in\\\\mathbb{R}^{d}$ and the embedding of the $(i+k)$ -th token $E m b(t_{i+k})\\\\in\\\\mathbb{R}^{d}$ \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"with the linear projection: \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\n\\\\mathbf{h}_{i}^{\\\\prime k}=M_{k}[\\\\mathrm{RMSNorm}(\\\\mathbf{h}_{i}^{k-1});\\\\mathrm{RMSNorm}(\\\\mathrm{Emb}(t_{i+k}))],\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"where $[\\\\cdot;\\\\cdot]$ denotes concatenation. Especially, when $k=1,\\\\mathbf{h}_{i}^{k-1}$ refers to the representation given by the main model. Note that for each MTP module, its embedding layer is shared with the main model. The combined $\\\\mathbf{h}_{i}^{\\\\prime k}$ serves as the input of the Transformer block at the $k$ -th depth to produce the output representation at the current depth $\\\\mathbf{h}_{i}^{k}$ : \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\n\\\\begin{array}{r}{\\\\mathbf{h}_{1:T-k}^{k}=\\\\mathrm{TRM}_{k}(\\\\mathbf{h}_{1:T-k}^{\\\\prime k}),}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"where $T$ represents the input sequence length and $\\\\boldsymbol{i}{:}\\\\boldsymbol{j}$ denotes the slicing operation (inclusive of both the left and right boundaries). Finally, taking $\\\\mathbf{h}_{i}^{k}$ as the input, the shared output head will compute the probability distribution for the $k$ -th additional prediction token \\ud835\\udc43\\ud835\\udc56\\ud835\\udc58 1 \\ud835\\udc58 \\u2208 R\\ud835\\udc49 , where $V$ is the vocabulary size: \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\nP_{i+k+1}^{k}=\\\\mathrm{OutHead}(\\\\mathbf{h}_{i}^{k}).\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"The output head OutHead $(\\\\cdot)$ linearly maps the representation to logits and subsequently applies the Softmax(\\u00b7) function to compute the prediction probabilities of the $k$ -th additional token. Also, for each MTP module, its output head is shared with the main model. Our principle of maintaining the causal chain of predictions is similar to that of EAGLE (Li et al., 2024b), but its primary objective is speculative decoding (Leviathan et al., 2023; Xia et al., 2023), whereas we utilize MTP to improve training. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"$\\\\mathcal{L}_{\\\\mathrm{MTP}}^{k}$ \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\n\\\\mathcal{L}_{\\\\mathrm{MTP}}^{k}=\\\\mathrm{CrossEntropy}(P_{2+k:T+1}^{k},t_{2+k:T+1})=-\\\\frac{1}{T}\\\\sum_{i=2+k}^{T+1}\\\\log P_{i}^{k}[t_{i}],\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"where $T$ denotes the input sequence length, $t_{i}$ denotes the ground-truth token at the $i\\\\cdot$ -th position, and $P_{i}^{k}[t_{i}]$ denotes the corresponding prediction probability of $t_{i},$ given by the $k$ -th MTP module. Finally, we compute the average of the MTP losses across all depths and multiply it by a weighting factor $\\\\lambda$ to obtain the overall MTP loss $\\\\mathcal{L}_{\\\\mathrm{MTP}}$ , which serves as an additional training objective for DeepSeek-V3: \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\n\\\\mathcal{L}_{\\\\mathrm{MTP}}=\\\\frac{\\\\lambda}{D}\\\\sum_{k=1}^{D}\\\\mathcal{L}_{\\\\mathrm{MTP}}^{k}.\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"MTP in Inference. Our MTP strategy mainly aims to improve the performance of the main model, so during inference, we can directly discard the MTP modules and the main model can function independently and normally. Additionally, we can also repurpose these MTP modules for speculative decoding to further improve the generation latency. \"}}]}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 332936193\n",
      "{'chapter_title': '2. Architecture', 'summaries': ['The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.', 'An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.', 'Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.', 'The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.', 'During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}, {\"chapter_title\": \"1. Introduction \", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\", \"The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\", \"FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\", \"The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\", \"Training cost was 2.788M H800 GPU hours.\", \"Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\", \"DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\"]}, {\"chapter_title\": \"2. Architecture\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.\", \"An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.\", \"Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.\", \"The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.\", \"During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.\"]}]```\n",
      "* Current Chapter Title: ``3. Infrastructures `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"3. Infrastructures \"}, \"children\": [{\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"3.1. Compute Clusters \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs. Each node in the H800 cluster contains 8 GPUs connected by NVLink and NVSwitch within nodes. Across different nodes, InfiniBand (IB) interconnects are utilized to facilitate communications. \"}}, {\"type\": \"image\", \"level\": 2, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/a4465b20c980d401769d2328a06b9e892e925e081f435eff4c8cf6c4393990e1.jpg\", \"img_caption\": [\"Figure 4 Overlapping strategy for a pair of individual forward and backward chunks (the boundaries of the transformer blocks are not aligned). Orange denotes forward, green denotes \\\"backward for input\\\", blue denotes \\\"backward for weights\\\", purple denotes PP communication, and red denotes barriers. Both all-to-all and PP communication can be fully hidden. \"]}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"3.2. Training Framework \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up. On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Parallelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajbhandari et al., 2020). \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"In order to facilitate efficient training of DeepSeek-V3, we implement meticulous engineering optimizations. Firstly, we design the DualPipe algorithm for efficient pipeline parallelism. Compared with existing PP methods, DualPipe has fewer pipeline bubbles. More importantly, it overlaps the computation and communication phases across forward and backward processes, thereby addressing the challenge of heavy communication overhead introduced by cross-node expert parallelism. Secondly, we develop efficient cross-node all-to-all communication kernels to fully utilize IB and NVLink bandwidths and conserve Streaming Multiprocessors (SMs) dedicated to communication. Finally, we meticulously optimize the memory footprint during training, thereby enabling us to train DeepSeek-V3 without using costly Tensor Parallelism (TP). \"}}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.2.1. DualPipe and Computation-Communication Overlap \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"For DeepSeek-V3, the communication overhead introduced by cross-node expert parallelism results in an inefficient computation-to-communication ratio of approximately 1:1. To tackle this challenge, we design an innovative pipeline parallelism algorithm called DualPipe, which not only accelerates model training by effectively overlapping forward and backward computationcommunication phases, but also reduces the pipeline bubbles. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"The key idea of DualPipe is to overlap the computation and communication within a pair of individual forward and backward chunks. To be specific, we divide each chunk into four components: attention, all-to-all dispatch, MLP, and all-to-all combine. Specially, for a backward chunk, both attention and MLP are further split into two parts, backward for input and backward for weights, like in ZeroBubble (Qi et al., 2023b). In addition, we have a PP communication component. As illustrated in Figure 4, for a pair of forward and backward chunks, we rearrange these components and manually adjust the ratio of GPU SMs dedicated to communication versus computation. In this overlapping strategy, we can ensure that both all-to-all and PP communication can be fully hidden during execution. Given the efficient overlapping strategy, the full DualPipe scheduling is illustrated in Figure 5. It employs a bidirectional pipeline scheduling, which feeds micro-batches from both ends of the pipeline simultaneously and a significant portion of communications can be fully overlapped. This overlap also ensures that, as the model further scales up, as long as we maintain a constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead. \"}}, {\"type\": \"image\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/6fffbffe0474c0ffe9c917374fde2a3c8a46b6369cf79245b61302e1158aa67d.jpg\", \"img_caption\": [\"Figure 5 | Example DualPipe scheduling for 8 PP ranks and 20 micro-batches in two directions. The micro-batches in the reverse direction are symmetric to those in the forward direction, so we omit their batch ID for illustration simplicity. Two cells enclosed by a shared black border have mutually overlapped computation and communication. \"]}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/7854d0a078215e4170ef86724e63cd844337966403b9f5f3e427db6c469470e0.jpg\", \"table_body\": \"\\n\\n<html><body><table><tr><td>Method</td><td>Bubble</td><td>Parameter</td><td>Activation</td></tr><tr><td>1F1B</td><td>(PP -1)(F+B)</td><td>1x</td><td>PP</td></tr><tr><td>ZB1P</td><td>(PP -1)(F+B-2W)</td><td>1x</td><td>PP</td></tr><tr><td>DualPipe (Ours)</td><td>(-1)(F&B+B-3W)</td><td>2x</td><td>PP+1</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Table 2 Comparison of pipeline bubbles and memory usage across different pipeline parallel methods. $F$ denotes the execution time of a forward chunk, $B$ denotes the execution time of a full backward chunk, \\ud835\\udc4a denotes the execution time of a \\\"backward for weights\\\" chunk, and $F\\\\&B$ denotes the execution time of two mutually overlapped forward and backward chunks. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In addition, even in more general scenarios without a heavy communication burden, DualPipe still exhibits efficiency advantages. In Table 2, we summarize the pipeline bubbles and memory usage across different PP methods. As shown in the table, compared with ZB1P (Qi et al., 2023b) and 1F1B (Harlap et al., 2018), DualPipe significantly reduces the pipeline bubbles while only increasing the peak activation memory by $\\\\scriptstyle{\\\\frac{1}{P P}}$ times. Although DualPipe requires keeping two copies of the model parameters, this does not significantly increase the memory consumption since we use a large EP size during training. Compared with Chimera (Li and Hoefler, 2021), DualPipe only requires that the pipeline stages and micro-batches be divisible by 2, without requiring micro-batches to be divisible by pipeline stages. In addition, for DualPipe, neither the bubbles nor activation memory will increase as the number of micro-batches grows. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In order to ensure sufficient computational performance for DualPipe, we customize efficient cross-node all-to-all communication kernels (including dispatching and combining) to conserve the number of SMs dedicated to communication. The implementation of the kernels is codesigned with the MoE gating algorithm and the network topology of our cluster. To be specific, in our cluster, cross-node GPUs are fully interconnected with IB, and intra-node communications are handled via NVLink. NVLink offers a bandwidth of 160 GB/s, roughly 3.2 times that of IB $(50\\\\mathrm{GB}/\\\\mathrm{s})$ . To effectively leverage the different bandwidths of IB and NVLink, we limit each token to be dispatched to at most 4 nodes, thereby reducing IB traffic. For each token, when its routing decision is made, it will first be transmitted via IB to the GPUs with the same in-node index on its target nodes. Once it reaches the target nodes, we will endeavor to ensure that it is instantaneously forwarded via NVLink to specific GPUs that host their target experts, without being blocked by subsequently arriving tokens. In this way, communications via IB and NVLink are fully overlapped, and each token can efficiently select an average of 3.2 experts per node without incurring additional overhead from NVLink. This implies that, although DeepSeek-V3 selects only 8 routed experts in practice, it can scale up this number to a maximum of 13 experts (4 nodes $\\\\times3.2$ experts/node) while preserving the same communication cost. Overall, under such a communication strategy, only 20 SMs are sufficient to fully utilize the bandwidths of IB and NVLink. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In detail, we employ the warp specialization technique (Bauer et al., 2014) and partition 20 SMs into 10 communication channels. During the dispatching process, (1) IB sending, (2) IB-to-NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The number of warps allocated to each communication task is dynamically adjusted according to the actual workload across all SMs. Similarly, during the combining process, (1) NVLink sending, (2) NVLink-to-IB forwarding and accumulation, and (3) IB receiving and accumulation are also handled by dynamically adjusted warps. In addition, both dispatching and combining kernels overlap with the computation stream, so we also consider their impact on other SM computation kernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and auto-tune the communication chunk size, which significantly reduces the use of the L2 cache and the interference to other SMs. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.2.3. Extremely Memory Saving with Minimal Overhead \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In order to reduce the memory footprint during training, we employ the following techniques. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Recomputation of RMSNorm and MLA Up-Projection. We recompute all RMSNorm operations and MLA up-projections during back-propagation, thereby eliminating the need to persistently store their output activations. With a minor overhead, this strategy significantly reduces memory requirements for storing activations. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Exponential Moving Average in CPU. During training, we preserve the Exponential Moving Average (EMA) of the model parameters for early estimation of the model performance after learning rate decay. The EMA parameters are stored in CPU memory and are updated asynchronously after each training step. This method allows us to maintain EMA parameters without incurring additional memory or time overhead. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Shared Embedding and Output Head for Multi-Token Prediction. With the DualPipe strategy, we deploy the shallowest layers (including the embedding layer) and deepest layers (including the output head) of the model on the same PP rank. This arrangement enables the physical sharing of parameters and gradients, of the shared embedding and output head, between the MTP module and the main model. This physical sharing mechanism further enhances our memory efficiency. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"3.3. FP8 Training \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Inspired by recent advances in low-precision training (Dettmers et al., 2022; Noune et al., 2022; Peng et al., 2023b), we propose a fine-grained mixed precision framework utilizing the FP8 data format for training DeepSeek-V3. While low-precision training holds great promise, it is often limited by the presence of outliers in activations, weights, and gradients (Fishman et al., 2024; He et al.; Sun et al., 2024). Although significant progress has been made in inference quantization (Frantar et al., 2022; Xiao et al., 2023), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model pre-training (Fishman et al., 2024). To address thi\\u6216s ch\\u8005alIlenpgeuat-nd>eAffcectitivealtyi eoxnte_ndLthe dynamic range of the FP8 format, we introduce a fine-grained quantization strategy: tile-wise grouping with $1\\\\times N_{c}$ elements or block-wise grouping with $N_{c}\\\\times N_{c}$ uetlepmuetn-t>s.ATchteiavssaotciioatned_{dLeq+u1an}tization overhead is largely mitigated under our increased-precision accumulation process, a critical aspect for achieving accurate FP8 General Matrix Multiplication (GEMM). Moreover, to further reduce memory and communication overhead in MoE training, we cache and dispatch activations in FP8, while storing low-precision optimizer states in BF16. We validate the proposed FP8 mixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeekV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably, compared with the BF16 baseline, the relative loss error of our FP8-training model remains consistently below $0.25\\\\%$ , a level well within the acceptable range of training randomness. \"}}, {\"type\": \"image\", \"level\": 2, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/ec4d0e65b5484cb0a5fc05ae43186a6ed454d109a9645a64bf7349b9abd4e3ae.jpg\", \"img_caption\": [\"Figure 6 | The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. \"]}}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.3.1. Mixed Precision Framework \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Building upon widely adopted techniques in low-precision training (Kalamkar et al., 2019; Narang et al., 2017), we propose a mixed precision framework for FP8 training. In this framework, most compute-density operations are conducted in FP8, while a few key operations are strategically maintained in their original data formats to balance training efficiency and numerical stability. The overall framework is illustrated in Figure 6. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Firstly, in order to accelerate model training, the majority of core computation kernels, i.e., GEMM operations, are implemented in FP8 precision. These GEMM operations accept FP8 tensors as inputs and produce outputs in BF16 or FP32. As depicted in Figure 6, all three GEMMs associated with the Linear operator, namely Fprop (forward pass), Dgrad (activation backward pass), and Wgrad (weight backward pass), are executed in FP8. This design theoretically doubles the computational speed compared with the original BF16 method. Additionally, the FP8 Wgrad GEMM allows activations to be stored in FP8 for use in the backward pass. This significantly reduces memory consumption. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Despite the efficiency advantage of the FP8 format, certain operators still require a higher precision due to their sensitivity to low-precision computations. Besides, some low-cost operators can also utilize a higher precision with a negligible overhead to the overall training cost. For this reason, after careful investigations, we maintain the original precision (e.g., BF16 or FP32) for the following components: the embedding module, the output head, MoE gating modules, normalization operators, and attention operators. These targeted retentions of high precision ensure stable training dynamics for DeepSeek-V3. To further guarantee numerical stability, we store the master weights, weight gradients, and optimizer states in higher precision. While these high-precision components incur some memory overheads, their impact can be minimized through efficient sharding across multiple DP ranks in our distributed training system. \"}}, {\"type\": \"image\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/4838bb048c259d0985856e0b4fbdec39bbdbf7fa33517c90d7f2c5d59d8bfc2a.jpg\", \"img_caption\": [\"Figure 7 | (a) We propose a fine-grained quantization method to mitigate quantization errors caused by feature outliers; for illustration simplicity, only Fprop is illustrated. (b) In conjunction with our quantization strategy, we improve the FP8 GEMM precision by promoting to CUDA Cores at an interval of $N_{C}=128$ elements MMA for the high-precision accumulation. \"]}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.3.2. Improved Precision from Quantization and Multiplication \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Based on our mixed precision FP8 framework, we introduce several strategies to enhance lowprecision training accuracy, focusing on both the quantization method and the multiplication process. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Fine-Grained Quantization. In low-precision training frameworks, overflows and underflows are common challenges due to the limited dynamic range of the FP8 format, which is constrained by its reduced exponent bits. As a standard practice, the input distribution is aligned to the representable range of the FP8 format by scaling the maximum absolute value of the input tensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes lowprecision training highly sensitive to activation outliers, which can heavily degrade quantization accuracy. To solve this, we propose a fine-grained quantization method that applies scaling at a more granular level. As illustrated in Figure 7 (a), (1) for activations, we group and scale elements on a 1x128 tile basis (i.e., per token per 128 channels); and (2) for weights, we group and scale elements on a 128x128 block basis (i.e., per 128 input channels per 128 output channels). This approach ensures that the quantization process can better accommodate outliers by adapting the scale according to smaller groups of elements. In Appendix B.2, we further discuss the training instability when we group and scale activations on a block basis in the same way as weights quantization. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"One key modification in our method is the introduction of per-group scaling factors along the inner dimension of GEMM operations. This functionality is not directly supported in the standard FP8 GEMM. However, combined with our precise FP32 accumulation strategy, it can \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"be efficiently implemented. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Notably, our fine-grained quantization strategy is highly consistent with the idea of microscaling formats (Rouhani et al., 2023b), while the Tensor Cores of NVIDIA next-generation GPUs (Blackwell series) have announced the support for microscaling formats with smaller quantization granularity (NVIDIA, 2024a). We hope our design can serve as a reference for future work to keep pace with the latest GPU architectures. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Increasing Accumulation Precision. Low-precision GEMM operations often suffer from underflow issues, and their accuracy largely depends on high-precision accumulation, which is commonly performed in an FP32 precision (Kalamkar et al., 2019; Narang et al., 2017). However, we observe that the accumulation precision of FP8 GEMM on NVIDIA H800 GPUs is limited to retaining around 14 bits, which is significantly lower than FP32 accumulation precision. This problem will become more pronounced when the inner dimension K is large (Wortsman et al., 2023), a typical scenario in large-scale model training where the batch size and model width are increased. Taking GEMM operations of two random matrices with $\\\\mathtt{K}=4096$ for example, in our preliminary test, the limited accumulation precision in Tensor Cores results in a maximum relative error of nearly $2\\\\%$ . Despite these problems, the limited accumulation precision is still the default option in a few FP8 frameworks (NVIDIA, 2024b), severely constraining the training accuracy. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In order to address this issue, we adopt the strategy of promotion to CUDA Cores for higher precision (Thakkar et al., 2023). The process is illustrated in Figure 7 (b). To be specific, during MMA (Matrix Multiply-Accumulate) execution on Tensor Cores, intermediate results are accumulated using the limited bit width. Once an interval of $N_{C}$ is reached, these partial results will be copied to FP32 registers on CUDA Cores, where full-precision FP32 accumulation is performed. As mentioned before, our fine-grained quantization applies per-group scaling factors along the inner dimension K. These scaling factors can be efficiently multiplied on the CUDA Cores as the dequantization process with minimal additional computational cost. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"It is worth noting that this modification reduces the WGMMA (Warpgroup-level Matrix Multiply-Accumulate) instruction issue rate for a single warpgroup. However, on the H800 architecture, it is typical for two WGMMA to persist concurrently: while one warpgroup performs the promotion operation, the other is able to execute the MMA operation. This design enables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based on our experiments, setting $N_{C}=128$ elements, equivalent to 4 WGMMAs, represents the minimal accumulation interval that can significantly improve precision without introducing substantial overhead. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Mantissa over Exponents. In contrast to the hybrid FP8 format adopted by prior work (NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses E4M3 (4-bit exponent and 3-bit mantissa) in Fprop and E5M2 (5-bit exponent and 2-bit mantissa) in Dgrad and Wgrad, we adopt the E4M3 format on all tensors for higher precision. We attribute the feasibility of this approach to our fine-grained quantization strategy, i.e., tile and block-wise scaling. By operating on smaller element groups, our methodology effectively shares exponent bits among these grouped elements, mitigating the impact of the limited dynamic range. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Online Quantization. Delayed quantization is employed in tensor-wise quantization frameworks (NVIDIA, 2024b; Peng et al., 2023b), which maintains a history of the maximum absolute values across prior iterations to infer the current value. In order to ensure accurate scales and simplify the framework, we calculate the maximum absolute value online for each 1x128 activation tile or $128\\\\mathrm{x}128$ weight block. Based on it, we derive the scaling factor and then quantize the activation or weight online into the FP8 format. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.3.3. Low-Precision Storage and Communication \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In conjunction with our FP8 training framework, we further reduce the memory consumption and communication overhead by compressing cached activations and optimizer states into lower-precision formats. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Low-Precision Optimizer States. We adopt the BF16 data format instead of FP32 to track the first and second moments in the AdamW (Loshchilov and Hutter, 2017) optimizer, without incurring observable performance degradation. However, the master weights (stored by the optimizer) and gradients (used for batch size accumulation) are still retained in FP32 to ensure numerical stability throughout training. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Low-Precision Activation. As illustrated in Figure 6, the Wgrad operation is performed in FP8. To reduce the memory consumption, it is a natural choice to cache activations in FP8 format for the backward pass of the Linear operator. However, special considerations are taken on several operators for low-cost high-precision training: \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"(1) Inputs of the Linear after the attention operator. These activations are also used in the backward pass of the attention operator, which makes it sensitive to precision. We adopt a customized E5M6 data format exclusively for these activations. Additionally, these activations will be converted from an 1x128 quantization tile to an $128\\\\mathbf{x}\\\\mathbf{1}$ tile in the backward pass. To avoid introducing extra quantization error, all the scaling factors are round scaled, i.e., integral power of 2. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"(2) Inputs of the SwiGLU operator in MoE. To further reduce the memory cost, we cache the inputs of the SwiGLU operator and recompute its output in the backward pass. These activations are also stored in FP8 with our fine-grained quantization method, striking a balance between memory efficiency and computational accuracy. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Low-Precision Communication. Communication bandwidth is a critical bottleneck in the training of MoE models. To alleviate this challenge, we quantize the activation before MoE up-projections into FP8 and then apply dispatch components, which is compatible with FP8 Fprop in MoE up-projections. Like the inputs of the Linear after the attention operator, scaling factors for this activation are integral power of 2. A similar strategy is applied to the activation gradient before MoE down-projections. For both the forward and backward combine components, we retain them in BF16 to preserve training precision in critical parts of the training pipeline. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"3.4. Inference and Deployment \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected using NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously ensure both the Service-Level Objective (SLO) for online services and high throughput, we employ the following deployment strategy that separates the prefilling and decoding stages. \"}}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.4.1. Prefilling \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"The minimum deployment unit of the prefilling stage consists of 4 nodes with 32 GPUs. The attention part employs 4-way Tensor Parallelism (TP4) with Sequence Parallelism (SP), combined with 8-way Data Parallelism (DP8). Its small TP size of 4 limits the overhead of TP communication. For the MoE part, we use 32-way Expert Parallelism (EP32), which ensures that each expert processes a sufficiently large batch size, thereby enhancing computational efficiency. For the MoE all-to-all communication, we use the same method as in training: first transferring tokens across nodes via IB, and then forwarding among the intra-node GPUs via NVLink. In particular, we use 1-way Tensor Parallelism for the dense MLPs in shallow layers to save TP communication. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"To achieve load balancing among different experts in the MoE part, we need to ensure that each GPU processes approximately the same number of tokens. To this end, we introduce a deployment strategy of redundant experts, which duplicates high-load experts and deploys them redundantly. The high-load experts are detected based on statistics collected during the online deployment and are adjusted periodically (e.g., every 10 minutes). After determining the set of redundant experts, we carefully rearrange experts among GPUs within a node based on the observed loads, striving to balance the load across GPUs as much as possible without increasing the cross-node all-to-all communication overhead. For the deployment of DeepSeek-V3, we set 32 redundant experts for the prefilling stage. For each GPU, besides the original 8 experts it hosts, it will also host one additional redundant expert. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Furthermore, in the prefilling stage, to improve the throughput and hide the overhead of all-to-all and TP communication, we simultaneously process two micro-batches with similar computational workloads, overlapping the attention and MoE of one micro-batch with the dispatch and combine of another. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Finally, we are exploring a dynamic redundancy strategy for experts, where each GPU hosts more experts (e.g., 16 experts), but only 9 will be activated during each inference step. Before the all-to-all operation at each layer begins, we compute the globally optimal routing scheme on the fly. Given the substantial computation involved in the prefilling stage, the overhead of computing this routing scheme is almost negligible. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.4.2. Decoding \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"During decoding, we treat the shared expert as a routed one. From this perspective, each token will select 9 experts during routing, where the shared expert is regarded as a heavy-load one that will always be selected. The minimum deployment unit of the decoding stage consists of 40 nodes with 320 GPUs. The attention part employs TP4 with SP, combined with DP80, while the MoE part uses EP320. For the MoE part, each GPU hosts only one expert, and 64 GPUs are responsible for hosting redundant experts and shared experts. All-to-all communication of the dispatch and combine parts is performed via direct point-to-point transfers over IB to achieve low latency. Additionally, we leverage the IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Similar to prefilling, we periodically determine the set of redundant experts in a certain interval, based on the statistical expert load from our online service. However, we do not need to rearrange experts since each GPU only hosts one expert. We are also exploring the dynamic redundancy strategy for decoding. However, this requires more careful optimization of the algorithm that computes the globally optimal routing scheme and the fusion with the dispatch kernel to reduce overhead. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Additionally, to enhance throughput and hide the overhead of all-to-all communication, we are also exploring processing two micro-batches with similar computational workloads simultaneously in the decoding stage. Unlike prefilling, attention consumes a larger portion of time in the decoding stage. Therefore, we overlap the attention of one micro-batch with the dispatch+MoE $+$ combine of another. In the decoding stage, the batch size per expert is relatively small (usually within 256 tokens), and the bottleneck is memory access rather than computation. Since the MoE part only needs to load the parameters of one expert, the memory access overhead is minimal, so using fewer SMs will not significantly affect the overall performance. Therefore, to avoid impacting the computation speed of the attention part, we can allocate only a small portion of SMs to dispatch+MoE+combine. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"3.5. Suggestions on Hardware Design \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Based on our implementation of the all-to-all communication and FP8 training scheme, we propose the following suggestions on chip design to AI hardware vendors. \"}}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.5.1. Communication Hardware \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In DeepSeek-V3, we implement the overlap between computation and communication to hide the communication latency during computation. This significantly reduces the dependency on communication bandwidth compared to serial computation and communication. However, the current communication implementation relies on expensive SMs (e.g., we allocate 20 out of the 132 SMs available in the H800 GPU for this purpose), which will limit the computational throughput. Moreover, using SMs for communication results in significant inefficiencies, as tensor cores remain entirely under-utilized. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Currently, the SMs primarily perform the following tasks for all-to-all communication: \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"\\u2022 Forwarding data between the IB (InfiniBand) and NVLink domain while aggregating IB traffic destined for multiple GPUs within the same node from a single GPU.   \\n\\u2022 Transporting data between RDMA buffers (registered GPU memory regions) and input/output buffers.   \\n\\u2022 Executing reduce operations for all-to-all combine.   \\n\\u2022 Managing fine-grained memory layout during chunked data transferring to multiple experts across the IB and NVLink domain. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We aspire to see future vendors developing hardware that offloads these communication tasks from the valuable computation unit SM, serving as a GPU co-processor or a network co-processor like NVIDIA SHARP Graham et al. (2016). Furthermore, to reduce application programming complexity, we aim for this hardware to unify the IB (scale-out) and NVLink (scale-up) networks from the perspective of the computation units. With this unified interface, computation units can easily accomplish operations such as read, write, multicast, and reduce across the entire IB-NVLink-unified domain via submitting communication requests based on simple primitives. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"3.5.2. Compute Hardware \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Higher FP8 GEMM Accumulation Precision in Tensor Cores. In the current Tensor Core implementation of the NVIDIA Hopper architecture, FP8 GEMM suffers from limited accumulation precision. After aligning 32 mantissa products by right-shifting based on the maximum exponent, the Tensor Core only uses the highest 14 bits of each mantissa product for addition, and truncates bits exceeding this range. The accumulation of addition results into registers also employs 14-bit precision. Our implementation partially mitigates the limitation by accumulating the addition results of 128 FP8 $\\\\times$ FP8 multiplications into registers with FP32 precision in the CUDA core. Although helpful in achieving successful FP8 training, it is merely a compromise due to the Hopper architecture\\u2019s hardware deficiency in FP8 GEMM accumulation precision. Future chips need to adopt higher precision. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Support for Tile- and Block-Wise Quantization. Current GPUs only support per-tensor quantization, lacking the native support for fine-grained quantization like our tile- and blockwise quantization. In the current implementation, when the $N_{C}$ interval is reached, the partial results will be copied from Tensor Cores to CUDA cores, multiplied by the scaling factors, and added to FP32 registers on CUDA cores. Although the dequantization overhead is significantly mitigated combined with our precise FP32 accumulation strategy, the frequent data movements between Tensor Cores and CUDA cores still limit the computational efficiency. Therefore, we recommend future chips to support fine-grained quantization by enabling Tensor Cores to receive scaling factors and implement MMA with group scaling. In this way, the whole partial sum accumulation and dequantization can be completed directly inside Tensor Cores until the final result is produced, avoiding frequent data movements. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Support for Online Quantization. The current implementations struggle to effectively support online quantization, despite its effectiveness demonstrated in our research. In the existing process, we need to read 128 BF16 activation values (the output of the previous computation) from HBM (High Bandwidth Memory) for quantization, and the quantized FP8 values are then written back to HBM, only to be read again for MMA. To address this inefficiency, we recommend that future chips integrate FP8 cast and TMA (Tensor Memory Accelerator) access into a single fused operation, so quantization can be completed during the transfer of activations from global memory to shared memory, avoiding frequent memory reads and writes. We also recommend supporting a warp-level cast instruction for speedup, which further facilitates the better fusion of layer normalization and FP8 cast. Alternatively, a near-memory computing approach can be adopted, where compute logic is placed near the HBM. In this case, BF16 elements can be cast to FP8 directly as they are read from HBM into the GPU, reducing off-chip memory access by roughly $50\\\\%$ . \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Support for Transposed GEMM Operations. The current architecture makes it cumbersome to fuse matrix transposition with GEMM operations. In our workflow, activations during the forward pass are quantized into 1x128 FP8 tiles and stored. During the backward pass, the matrix needs to be read out, dequantized, transposed, re-quantized into 128x1 tiles, and stored in HBM. To reduce memory operations, we recommend future chips to enable direct transposed reads of matrices from shared memory before MMA operation, for those precisions required in both training and inference. Combined with the fusion of FP8 format conversion and TMA access, this enhancement will significantly streamline the quantization workflow. \"}}]}]}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 19371 / Seed: 841694213\n",
      "{'chapter_title': '3. Infrastructures', 'summaries': ['DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.', 'The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.', 'DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.', 'Efficient cross-node all-to-all communication kernels minimize communication overhead.', 'The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.', 'FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.', 'The framework employs tile- and block-wise quantization to mitigate quantization errors.', 'The inference deployment separates prefilling and decoding stages for improved throughput.', 'Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}, {\"chapter_title\": \"1. Introduction \", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\", \"The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\", \"FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\", \"The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\", \"Training cost was 2.788M H800 GPU hours.\", \"Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\", \"DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\"]}, {\"chapter_title\": \"2. Architecture\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.\", \"An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.\", \"Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.\", \"The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.\", \"During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.\"]}, {\"chapter_title\": \"3. Infrastructures\", \"summaries\": [\"DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.\", \"The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.\", \"DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.\", \"Efficient cross-node all-to-all communication kernels minimize communication overhead.\", \"The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.\", \"FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.\", \"The framework employs tile- and block-wise quantization to mitigate quantization errors.\", \"The inference deployment separates prefilling and decoding stages for improved throughput.\", \"Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.\"]}]```\n",
      "* Current Chapter Title: ``4. Pre-Training `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"4. Pre-Training \"}, \"children\": [{\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.1. Data Construction \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity. Inspired by Ding et al. (2024), we implement the document packing method for data integrity but do not incorporate cross-sample attention masking during training. Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"In the training process of DeepSeekCoder-V2 (DeepSeek-AI, 2024a), we observe that the Fill-in-Middle (FIM) strategy does not compromise the next-token prediction capability while enabling the model to accurately predict middle text based on contextual cues. In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3. To be specific, we employ the Prefix-Suffix-Middle (PSM) framework to structure data as follows: \"}}, {\"type\": \"equation\", \"level\": 2, \"data\": {\"text\": \"$$\\n<|\\\\mathbf{f}\\\\mathrm{i}\\\\mathrm{i}\\\\mathrm{m}_{-}\\\\mathrm{begin}|>f_{\\\\mathrm{pre}}<|\\\\mathbf{f}\\\\mathrm{i}\\\\mathrm{i}\\\\mathrm{m}_{-}\\\\mathrm{hole}|>f_{\\\\mathrm{suf}}<|\\\\mathbf{f}\\\\mathrm{i}\\\\mathrm{i}\\\\mathrm{m}_{-}\\\\mathrm{end}|>f_{\\\\mathrm{middle}}<|\\\\mathbf{e}\\\\mathrm{os}_{-}\\\\mathbf{t}\\\\mathrm{oken}|>.\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"This structure is applied at the document level as a part of the pre-packing process. The FIM strategy is applied at a rate of 0.1, consistent with the PSM framework. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens. The pretokenizer and training data for our tokenizer are modified to optimize multilingual compression efficiency. In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks. However, this trick may introduce the token boundary bias (Lundberg, 2023) when the model processes multi-line prompts without terminal line breaks, particularly for few-shot evaluation prompts. To address this issue, we randomly split a certain proportion of such combined tokens during training, which exposes the model to a wider array of special cases and mitigates this bias. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.2. Hyper-Parameters \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Model Hyper-Parameters. We set the number of Transformer layers to 61 and the hidden dimension to 7168. All learnable parameters are randomly initialized with a standard deviation of 0.006. In MLA, we set the number of attention heads $n_{h}$ to 128 and the per-head dimension $d_{h}$ to 128. The KV compression dimension $d_{c}$ is set to 512, and the query compression dimension $d_{c}^{\\\\prime}$ is set to 1536. For the decoupled queries and key, we set the per-head dimension $d_{h}^{R}$ to 64. We substitute all FFNs except for the first three layers with MoE layers. Each MoE layer consists of 1 shared expert and 256 routed experts, where the intermediate hidden dimension of each expert is 2048. Among the routed experts, 8 experts will be activated for each token, and each token will be ensured to be sent to at most 4 nodes. The multi-token prediction depth $D$ is set to 1, i.e., besides the exact next token, each token will predict one additional token. As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Training Hyper-Parameters. We employ the AdamW optimizer (Loshchilov and Hutter, 2017) with hyper-parameters set to $\\\\beta_{1}=0.9$ , $\\\\beta_{2}=0.95$ , and weight_decay $=0.1$ . We set the maximum sequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens. As for the learning rate scheduling, we first linearly increase it from 0 to $2.2\\\\times10^{-4}$ during the first 2K steps. Then, we keep a constant learning rate of $2.2\\\\times10^{-4}$ until the model consumes 10T training tokens. Subsequently, we gradually decay the learning rate to $2.2\\\\times10^{-5}$ in $4.3\\\\mathrm{T}$ tokens, following a cosine decay curve. During the training of the final 500B tokens, we keep a constant learning rate of $2.2\\\\times10^{-5}$ in the first 333B tokens, and switch to another constant learning rate of $7.3\\\\times10^{-6}$ in the remaining 167B tokens. The gradient clipping norm is set to 1.0. We employ a batch size scheduling strategy, where the batch size is gradually increased from 3072 to 15360 in the training of the first 469B tokens, and then keeps 15360 in the remaining training. We leverage pipeline parallelism to deploy different layers of a model on different GPUs, and for each layer, the routed experts will be uniformly deployed on 64 GPUs belonging to 8 nodes. As for the node-limited routing, each token will be sent to at most 4 nodes (i.e., $M=4$ ). For auxiliary-loss-free load balancing, we set the bias update speed \\ud835\\udefe to 0.001 for the first $14.3\\\\mathrm{T}$ tokens, and to 0.0 for the remaining 500B tokens. For the balance loss, we set $\\\\alpha$ to 0.0001, just to avoid extreme imbalance within any single sequence. The MTP loss weight $\\\\lambda$ is set to 0.3 for the first 10T tokens, and to 0.1 for the remaining 4.8T tokens. \"}}, {\"type\": \"image\", \"level\": 2, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/994846ca2a80bbe6297f6a9f75672fcf45337ab0224fb66ddde155a093c48072.jpg\", \"img_caption\": [\"Figure 8 | Evaluation results on the \\u201dNeedle In A Haystack\\u201d (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to 128K. \"]}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.3. Long Context Extension \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K. The YaRN configuration is consistent with that used in DeepSeek-V2, being applied exclusively to the decoupled shared key $\\\\mathbf{k}_{t}^{R}$ . The hyper-parameters remain identical across both phases, with the scale $s=40$ , $\\\\alpha=1$ , $\\\\beta=32$ , and the scaling factor $\\\\sqrt{t}=0.1\\\\ln s+1.$ . In the first phase, the sequence length is set to 32K, and the batch size is 1920. During the second phase, the sequence length is increased to $128\\\\mathsf{K},$ and the batch size is reduced to 480. The learning rate for both phases is set to $7.3\\\\times10^{-6}$ , matching the final learning rate from the pre-training stage. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Through this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to 128K in length while maintaining strong performance. Figure 8 illustrates that DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the \\\"Needle In A Haystack\\\" (NIAH) test, demonstrating consistent robustness across context window lengths up to 128K. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.4. Evaluations \"}, \"children\": [{\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"4.4.1. Evaluation Benchmarks \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"The base model of DeepSeek-V3 is pretrained on a multilingual corpus with English and Chinese constituting the majority, so we evaluate its performance on a series of benchmarks primarily in English and Chinese, as well as on a multilingual benchmark. Our evaluation is based on our internal evaluation framework integrated in our HAI-LLM framework. Considered benchmarks are categorized and listed as follows, where underlined benchmarks are in Chinese and double-underlined benchmarks are multilingual ones: \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Multi-subject multiple-choice datasets include MMLU (Hendrycks et al., 2020), MMLURedux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024b), MMMLU (OpenAI, 2024b), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Language understanding and reasoning datasets include HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and BigBench Hard (BBH) (Suzgun et al., 2022). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Closed-book question answering datasets include TriviaQA (Joshi et al., 2017) and NaturalQuestions (Kwiatkowski et al., 2019). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Reading comprehension datasets include RACE Lai et al. (2017), DROP (Dua et al., 2019), 3 (Sun et al., 2019a), and CMRC (Cui et al., 2019). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Reference disambiguation datasets include CLUEWSC (Xu et al., 2020) and WinoGrande Sakaguchi et al. (2019). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Language modeling datasets include Pile (Gao et al., 2020). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Chinese understanding and culture datasets include CCPM (Li et al., 2021). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Math datasets include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MGSM (Shi et al., 2023), and CMath (Wei et al., 2023). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Code datasets include HumanEval (Chen et al., 2021), LiveCodeBench-Base (0801-1101) (Jain et al., 2024), MBPP (Austin et al., 2021), and CRUXEval (Gu et al., 2024). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Standardized exams include AGIEval (Zhong et al., 2023). Note that AGIEval includes both English and Chinese subsets. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Following our previous work (DeepSeek-AI, $2024\\\\mathrm{b},\\\\mathrm{c})$ ), we adopt perplexity-based evaluation for datasets including HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High, MMLU, MMLU-Redux, MMLU-Pro, MMMLU, ARC-Easy, ARC-Challenge, C-Eval, CMMLU, C3, and CCPM, and adopt generation-based evaluation for TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, MGSM, HumanEval, MBPP, LiveCodeBench-Base, CRUXEval, BBH, AGIEval, CLUEWSC, CMRC, and CMath. In addition, we perform language-modeling-based evaluation for Pile-test and use Bits-Per-Byte (BPB) as the metric to guarantee fair comparison among models using different tokenizers. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"4.4.2. Evaluation Results \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B Base (Qwen, 2024b), and LLaMA-3.1 405B Base (AI@Meta, 2024b). We evaluate all these models with our internal evaluation framework, and ensure that they share the same evaluation setting. Note that due to the changes in our evaluation framework over the past months, the performance of DeepSeek-V2-Base exhibits a slight difference from our previously reported results. Overall, DeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base, and surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the strongest open-source model. \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/6a84148e6f9ec21234912515ca7454f225a10cccedef7f00755b2287a605e537.jpg\", \"table_caption\": [\"Table 3 Comparison among DeepSeek-V3-Base and other representative open-source base models. All models are evaluated in our internal framework and share the same evaluation setting. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeekV3-Base achieves the best performance on most benchmarks, especially on math and code tasks. \"], \"table_body\": \"\\n\\n<html><body><table><tr><td></td><td>Benchmark (Metric)</td><td># Shots</td><td>DeepSeek-V2 Base</td><td>Qwen2.5 72B Base</td><td>LLaMA-3.1 405B Base</td><td>DeepSeek-V3 Base</td></tr><tr><td rowspan=\\\"5\\\"></td><td>Architecture</td><td></td><td>MoE</td><td>Dense</td><td>Dense</td><td>MoE</td></tr><tr><td># Activated Params</td><td></td><td>21B</td><td>72B</td><td>405B</td><td>37B</td></tr><tr><td># Total Params</td><td></td><td>236B</td><td>72B</td><td>405B</td><td>671B</td></tr><tr><td>Pile-test (BPB)</td><td></td><td>0.606</td><td>0.638</td><td>0.542</td><td>0.548</td></tr><tr><td>BBH (EM) MMLU (EM)</td><td>3-shot</td><td>78.8</td><td>79.8</td><td>82.9</td><td></td></tr><tr><td rowspan=\\\"12\\\">English</td><td></td><td>5-shot</td><td>78.4</td><td>85.0</td><td>84.4</td><td>87.5 87.1</td></tr><tr><td>MMLU-Redux (EM)</td><td>5-shot</td><td>75.6</td><td>83.2</td><td>81.3</td><td></td></tr><tr><td>MMLU-Pro (EM)</td><td>5-shot</td><td>51.4</td><td>58.3</td><td></td><td>86.2</td></tr><tr><td></td><td></td><td>80.4</td><td>80.6</td><td>52.8</td><td>64.4</td></tr><tr><td>DROP (F1) ARC-Easy (EM)</td><td>3-shot 25-shot</td><td>97.6</td><td>98.4</td><td>86.0</td><td>89.0</td></tr><tr><td>ARC-Challenge (EM)</td><td>25-shot</td><td>92.2</td><td>94.5</td><td>98.4 95.3</td><td>98.9</td></tr><tr><td>HellaSwag (EM)</td><td>10-shot</td><td>87.1</td><td>84.8</td><td>89.2</td><td>95.3 88.9</td></tr><tr><td>PIQA (EM)</td><td>O-shot</td><td>83.9</td><td>82.6</td><td>85.9</td><td>84.7</td></tr><tr><td>WinoGrande (EM)</td><td>5-shot</td><td>86.3</td><td>82.3</td><td>85.2</td><td></td></tr><tr><td>RACE-Middle (EM)</td><td>5-shot</td><td>73.1</td><td>68.1</td><td>74.2</td><td>84.9</td></tr><tr><td>RACE-High (EM)</td><td>5-shot</td><td>52.6</td><td>50.3</td><td>56.8</td><td>67.1 51.3</td></tr><tr><td>TriviaQA (EM)</td><td>5-shot</td><td>80.0</td><td>71.9</td><td>82.7</td><td>82.9</td></tr><tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>38.6</td><td>33.2</td><td>41.5</td><td>40.0</td></tr><tr><td>AGIEval (EM)</td><td>O-shot</td><td>57.5</td><td>75.8</td><td>60.6</td><td>79.6</td></tr><tr><td>HumanEval (Pass@1) Code</td><td>O-shot</td><td>43.3</td><td>53.0</td><td>54.9</td><td>65.2</td></tr><tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>65.0</td><td>72.6</td><td>68.4</td><td>75.4</td></tr><tr><td>LiveCodeBench-Base (Pass@1) CRUXEval-I (EM)</td><td>3-shot</td><td>11.6</td><td>12.9</td><td>15.5</td><td>19.4</td></tr><tr><td></td><td></td><td>2-shot 52.5</td><td>59.1</td><td>58.5</td><td>67.3</td><td></td></tr><tr><td rowspan=\\\"4\\\">Math</td><td>CRUXEval-O (EM)</td><td>2-shot</td><td>49.8</td><td>59.9</td><td>59.9</td><td>69.8</td></tr><tr><td>GSM8K (EM)</td><td>8-shot</td><td>81.6</td><td>88.3</td><td>83.5</td><td>89.3</td></tr><tr><td>MATH (EM)</td><td>4-shot</td><td>43.4</td><td>54.4</td><td>49.0</td><td>61.6</td></tr><tr><td>MGSM (EM)</td><td>8-shot</td><td>63.6</td><td>76.2</td><td>69.9</td><td>79.8</td></tr><tr><td rowspan=\\\"6\\\">Chinese</td><td>CMath (EM)</td><td>3-shot</td><td>78.7</td><td>84.5</td><td>77.3</td><td>90.7</td></tr><tr><td>CLUEWSC (EM)</td><td>5-shot</td><td>82.0</td><td>82.5</td><td>83.0</td><td>82.7</td></tr><tr><td>C-Eval (EM)</td><td>5-shot</td><td>81.4</td><td>89.2</td><td>72.5</td><td>90.1</td></tr><tr><td>CMMLU (EM)</td><td>5-shot</td><td>84.0</td><td>89.5</td><td>73.7</td><td>88.8</td></tr><tr><td>CMRC (EM)</td><td>1-shot</td><td>77.4</td><td>75.8</td><td>76.0</td><td>76.3</td></tr><tr><td>C3 (EM) CCPM (EM)</td><td>O-shot O-shot</td><td>77.4 93.0</td><td>76.7 88.5</td><td>79.7 78.6</td><td>78.6 92.0</td></tr><tr><td>Multilingual</td><td>MMMLU-non-English (EM)</td><td>5-shot</td><td>64.0</td><td>74.8</td><td>73.8</td><td>79.4</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"From a more detailed perspective, we compare DeepSeek-V3-Base with the other open-source base models individually. (1) Compared with DeepSeek-V2-Base, due to the improvements in our model architecture, the scale-up of the model size and training tokens, and the enhancement of data quality, DeepSeek-V3-Base achieves significantly better performance as expected. (2) Compared with Qwen2.5 72B Base, the state-of-the-art Chinese open-source model, with only half of the activated parameters, DeepSeek-V3-Base also demonstrates remarkable advantages, especially on English, multilingual, code, and math benchmarks. As for Chinese benchmarks, except for CMMLU, a Chinese multi-subject multiple-choice task, DeepSeek-V3-Base also shows better performance than Qwen2.5 72B. (3) Compared with LLaMA-3.1 405B Base, the largest open-source model with 11 times the activated parameters, DeepSeek-V3-Base also exhibits much better performance on multilingual, code, and math benchmarks. As for English and Chinese language benchmarks, DeepSeek-V3-Base shows competitive or better performance, and is especially good on BBH, MMLU-series, DROP, C-Eval, CMMLU, and CCPM. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Due to our efficient architectures and comprehensive engineering optimizations, DeepSeekV3 achieves extremely high training efficiency. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models. \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/8db174ccb145521f9496f42bfc1c6000bf6fbe09e88bc54c229f4c8b0b183f79.jpg\", \"table_body\": \"\\n\\n<html><body><table><tr><td>Benchmark (Metric)</td><td># Shots</td><td>Small MoE Baseline</td><td>Small MoE w/ MTP</td><td>Large MoE Baseline</td><td>Large MoE w/ MTP</td></tr><tr><td># Activated Params (Inference)</td><td>\\u4e00</td><td>2.4B</td><td>2.4B</td><td>20.9B</td><td>20.9B</td></tr><tr><td># Total Params (Inference)</td><td>-</td><td>15.7B</td><td>15.7B</td><td>228.7B</td><td>228.7B</td></tr><tr><td># Training Tokens</td><td>-</td><td>1.33T</td><td>1.33T</td><td>540B</td><td>540B</td></tr><tr><td>Pile-test (BPB)</td><td>\\u4e00</td><td>0.729</td><td>0.729</td><td>0.658</td><td>0.657</td></tr><tr><td>BBH (EM)</td><td>3-shot</td><td>39.0</td><td>41.4</td><td>70.0</td><td>70.7</td></tr><tr><td>MMLU (EM)</td><td>5-shot</td><td>50.0</td><td>53.3</td><td>67.5</td><td>66.6</td></tr><tr><td>DROP (F1)</td><td>1-shot</td><td>39.2</td><td>41.3</td><td>68.5</td><td>70.6</td></tr><tr><td>TriviaQA (EM)</td><td>5-shot</td><td>56.9</td><td>57.7</td><td>67.0</td><td>67.3</td></tr><tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>22.7</td><td>22.3</td><td>27.2</td><td>28.5</td></tr><tr><td>HumanEval (Pass@1)</td><td>O-shot</td><td>20.7</td><td>26.8</td><td>44.5</td><td>53.7</td></tr><tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>35.8</td><td>36.8</td><td>61.6</td><td>62.2</td></tr><tr><td>GSM8K (EM)</td><td>8-shot</td><td>25.4</td><td>31.4</td><td>72.3</td><td>74.0</td></tr><tr><td>MATH (EM)</td><td>4-shot</td><td>10.7</td><td>12.6</td><td>38.6</td><td>39.8</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Table 4 | Ablation results for the MTP strategy. The MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"4.5. Discussion \"}, \"children\": [{\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"4.5.1. Ablation Studies for Multi-Token Prediction \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In Table 4, we show the ablation results for the MTP strategy. To be specific, we validate the MTP strategy on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising 228.7B total parameters on 540B tokens. On top of them, keeping the training data and the other architectures the same, we append a 1-depth MTP module onto them and train two models with the MTP strategy for comparison. Note that during inference, we directly discard the MTP module, so the inference costs of the compared models are exactly the same. From the table, we can observe that the MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In Table 5, we show the ablation results for the auxiliary-loss-free balancing strategy. We validate this strategy on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising 228.7B total parameters on 578B tokens. \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0b5c2bac8bb47af03f3d59035131c1afeb68ab2f2c7c43b749aff4fa45b47bd9.jpg\", \"table_caption\": [\"Table 5 | Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. \"], \"table_body\": \"\\n\\n<html><body><table><tr><td>Benchmark (Metric)</td><td># Shots</td><td>Small MoE Aux-Loss-Based</td><td>Small MoE Aux-Loss-Free</td><td>Large MoE Aux-Loss-Based</td><td>Large MoE Aux-Loss-Free</td></tr><tr><td># Activated Params</td><td></td><td>2.4B</td><td>2.4B</td><td>20.9B</td><td>20.9B</td></tr><tr><td># Total Params</td><td></td><td>15.7B</td><td>15.7B</td><td>228.7B</td><td>228.7B</td></tr><tr><td># Training Tokens</td><td>-</td><td>1.33T</td><td>1.33T</td><td>578B</td><td>578B</td></tr><tr><td>Pile-test (BPB)</td><td></td><td>0.727</td><td>0.724</td><td>0.656</td><td>0.652</td></tr><tr><td>BBH (EM)</td><td>3-shot</td><td>37.3</td><td>39.3</td><td>66.7</td><td>67.9</td></tr><tr><td>MMLU (EM)</td><td>5-shot</td><td>51.0</td><td>51.8</td><td>68.3</td><td>67.2</td></tr><tr><td>DROP (F1)</td><td>1-shot</td><td>38.1</td><td>39.0</td><td>67.1</td><td>67.1</td></tr><tr><td>TriviaQA (EM)</td><td>5-shot</td><td>58.3</td><td>58.5</td><td>66.7</td><td>67.7</td></tr><tr><td>NaturalQuestions (EM)</td><td>5-shot</td><td>23.2</td><td>23.4</td><td>27.1</td><td>28.1</td></tr><tr><td>HumanEval (Pass@1)</td><td>O-shot</td><td>22.0</td><td>22.6</td><td>40.2</td><td>46.3</td></tr><tr><td>MBPP (Pass@1)</td><td>3-shot</td><td>36.6</td><td>35.8</td><td>59.2</td><td>61.2</td></tr><tr><td>GSM8K (EM)</td><td>8-shot</td><td>27.1</td><td>29.6</td><td>70.7</td><td>74.5</td></tr><tr><td>MATH (EM)</td><td>4-shot</td><td>10.9</td><td>11.1</td><td>37.2</td><td>39.6</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Both of the baseline models purely use auxiliary losses to encourage load balance, and use the sigmoid gating function with top-K affinity normalization. Their hyper-parameters to control the strength of auxiliary losses are the same as DeepSeek-V2-Lite and DeepSeek-V2, respectively. On top of these two baseline models, keeping the training data and the other architectures the same, we remove all auxiliary losses and introduce the auxiliary-loss-free balancing strategy for comparison. From the table, we can observe that the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"The key distinction between auxiliary-loss-free balancing and sequence-wise auxiliary loss lies in their balancing scope: batch-wise versus sequence-wise. Compared with the sequence-wise auxiliary loss, batch-wise balancing imposes a more flexible constraint, as it does not enforce in-domain balance on each sequence. This flexibility allows experts to better specialize in different domains. To validate this, we record and analyze the expert load of a 16B auxiliaryloss-based baseline and a 16B auxiliary-loss-free model on different domains in the Pile test set. As illustrated in Figure 9, we observe that the auxiliary-loss-free model demonstrates greater expert specialization patterns as expected. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"To further investigate the correlation between this flexibility and the advantage in model performance, we additionally design and validate a batch-wise auxiliary loss that encourages load balance on each training batch instead of on each sequence. The experimental results show that, when achieving a similar level of batch-wise load balance, the batch-wise auxiliary loss can also achieve similar model performance to the auxiliary-loss-free method. To be specific, in our experiments with 1B MoE models, the validation losses are: 2.258 (using a sequencewise auxiliary loss), 2.253 (using the auxiliary-loss-free method), and 2.253 (using a batch-wise auxiliary loss). We also observe similar results on 3B MoE models: the model using a sequencewise auxiliary loss achieves a validation loss of 2.085, and the models using the auxiliary-loss-free method or a batch-wise auxiliary loss achieve the same validation loss of 2.080. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In addition, although the batch-wise load balancing methods show consistent performance advantages, they also face two potential challenges in efficiency: (1) load imbalance within certain sequences or small batches, and (2) domain-shift-induced load imbalance during inference. The first challenge is naturally addressed by our training framework that uses large-scale expert parallelism and data parallelism, which guarantees a large size of each micro-batch. For the second challenge, we also design and implement an efficient inference framework with redundant expert deployment, as described in Section 3.4, to overcome it. \"}}, {\"type\": \"image\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/4e04a59983212cc4b1707f1db88fcc93a62807adca76c455ef42c532b2c10714.jpg\", \"img_caption\": [\"Figure 9 | Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. Due to space constraints, we only present the results of two layers as an example, with the results of all layers provided in Appendix C. \"]}}]}]}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 604500967\n",
      "{'chapter_title': '4. Pre-Training', 'summaries': ['The pre-training corpus was optimized by enhancing mathematical and programming samples and expanding multilingual coverage.', 'The data processing pipeline was refined to minimize redundancy while maintaining corpus diversity.', 'The Prefix-Suffix-Middle (PSM) framework, employing the Fill-in-Middle (FIM) strategy at a rate of 0.1, structures data.', 'A tokenizer with a 128K token vocabulary was used, with combined punctuation/line break tokens split to mitigate token boundary bias.', 'Hyperparameters include 61 Transformer layers, a hidden dimension of 3768, and 37B activated parameters per token.', 'Training involved a multi-stage learning rate schedule and a batch size scheduling strategy.', 'Long context extension used YaRN to progressively expand the context window to 128K.', 'The MTP strategy consistently enhances model performance on most benchmarks.', 'The auxiliary-loss-free balancing strategy demonstrates better model performance compared to purely auxiliary-loss-based methods.', 'Batch-wise balancing enables greater expert specialization patterns and faces efficiency challenges addressed by the training framework.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}, {\"chapter_title\": \"1. Introduction \", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\", \"The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\", \"FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\", \"The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\", \"Training cost was 2.788M H800 GPU hours.\", \"Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\", \"DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\"]}, {\"chapter_title\": \"2. Architecture\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.\", \"An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.\", \"Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.\", \"The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.\", \"During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.\"]}, {\"chapter_title\": \"3. Infrastructures\", \"summaries\": [\"DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.\", \"The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.\", \"DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.\", \"Efficient cross-node all-to-all communication kernels minimize communication overhead.\", \"The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.\", \"FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.\", \"The framework employs tile- and block-wise quantization to mitigate quantization errors.\", \"The inference deployment separates prefilling and decoding stages for improved throughput.\", \"Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.\"]}, {\"chapter_title\": \"4. Pre-Training\", \"summaries\": [\"The pre-training corpus was optimized by enhancing mathematical and programming samples and expanding multilingual coverage.\", \"The data processing pipeline was refined to minimize redundancy while maintaining corpus diversity.\", \"The Prefix-Suffix-Middle (PSM) framework, employing the Fill-in-Middle (FIM) strategy at a rate of 0.1, structures data.\", \"A tokenizer with a 128K token vocabulary was used, with combined punctuation/line break tokens split to mitigate token boundary bias.\", \"Hyperparameters include 61 Transformer layers, a hidden dimension of 3768, and 37B activated parameters per token.\", \"Training involved a multi-stage learning rate schedule and a batch size scheduling strategy.\", \"Long context extension used YaRN to progressively expand the context window to 128K.\", \"The MTP strategy consistently enhances model performance on most benchmarks.\", \"The auxiliary-loss-free balancing strategy demonstrates better model performance compared to purely auxiliary-loss-based methods.\", \"Batch-wise balancing enables greater expert specialization patterns and faces efficiency challenges addressed by the training framework.\"]}]```\n",
      "* Current Chapter Title: ``5. Post-Training `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"5. Post-Training \"}, \"children\": [{\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"5.1. Supervised Fine-Tuning \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"We curate our instruction-tuning datasets to include 1.5M instances spanning multiple domains, with each domain employing distinct data creation methods tailored to its specific requirements. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model. Specifically, while the R1-generated data demonstrates strong accuracy, it suffers from issues such as overthinking, poor formatting, and excessive length. Our objective is to balance the high accuracy of R1-generated reasoning data and the clarity and conciseness of regularly formatted reasoning data. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"To establish our methodology, we begin by developing an expert model tailored to a specific domain, such as code, mathematics, or general reasoning, using a combined Supervised FineTuning (SFT) and Reinforcement Learning (RL) training pipeline. This expert model serves as a data generator for the final model. The training process involves generating two distinct types of SFT samples for each instance: the first couples the problem with its original response in the format of <problem, original response>, while the second incorporates a system prompt alongside the problem and the R1 response in the format of <system prompt, problem, R1 response>. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"The system prompt is meticulously designed to include instructions that guide the model toward producing responses enriched with mechanisms for reflection and verification. During the RL phase, the model leverages high-temperature sampling to generate responses that integrate patterns from both the R1-generated and original data, even in the absence of explicit system prompts. After hundreds of RL steps, the intermediate RL model learns to incorporate R1 patterns, thereby enhancing overall performance strategically. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Upon completing the RL training phase, we implement rejection sampling to curate highquality SFT data for the final model, where the expert models are used as data generation sources. This method ensures that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data. \"}}, {\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"SFT Settings. We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at $5\\\\times10^{-6}$ and gradually decreases to $1\\\\times\\\\dot{1}0^{-6}$ . During training, each single sequence is packed from multiple samples. However, we adopt a sample masking strategy to ensure that these examples remain isolated and mutually invisible. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"5.2. Reinforcement Learning \"}, \"children\": [{\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.2.1. Reward Model \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We employ a rule-based Reward Model (RM) and a model-based RM in our RL process. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Rule-Based RM. For questions that can be validated using specific rules, we adopt a rulebased reward system to determine the feedback. For instance, certain math problems have deterministic results, and we require the model to provide the final answer within a designated format (e.g., in a box), allowing us to apply rules to verify the correctness. Similarly, for LeetCode problems, we can utilize a compiler to generate feedback based on test cases. By leveraging rule-based validation wherever possible, we ensure a higher level of reliability, as this approach is resistant to manipulation or exploitation. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Model-Based RM. For questions with free-form ground-truth answers, we rely on the reward model to determine whether the response matches the expected ground-truth. Conversely, for questions without a definitive ground-truth, such as those involving creative writing, the reward model is tasked with providing feedback based on the question and the corresponding answer as inputs. The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its reliability, we construct preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward. This approach helps mitigate the risk of reward hacking in specific tasks. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.2.2. Group Relative Policy Optimization \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Similar to DeepSeek-V2 (DeepSeek-AI, $2024\\\\mathrm{c}$ ), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question $q,$ GRPO samples a group of outputs $\\\\{o1,o2,\\\\cdots,o_{G}\\\\}$ from the old policy model $\\\\pi_{\\\\theta_{o l d}}$ and then optimizes the policy model $\\\\scriptstyle\\\\pi_{\\\\theta}$ by maximizing the following objective: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\begin{array}{l}{\\\\displaystyle\\\\mathcal{G}_{G R P O}(\\\\theta)=\\\\mathbb{E}[q\\\\sim P(Q),\\\\{\\\\boldsymbol{o}_{i}\\\\}_{i=1}^{G}\\\\sim\\\\pi_{\\\\theta_{o d d}}(O|{q})]}\\\\\\\\ {\\\\displaystyle\\\\frac{1}{G}\\\\sum_{i=1}^{G}\\\\left(\\\\operatorname*{min}\\\\left(\\\\frac{\\\\pi_{\\\\theta}(\\\\boldsymbol{o}_{i}|{q})}{\\\\pi_{\\\\theta_{o d d}}(\\\\boldsymbol{o}_{i}|{q})}A_{i},\\\\mathrm{clip}\\\\left(\\\\frac{\\\\pi_{\\\\theta}(\\\\boldsymbol{o}_{i}|{q})}{\\\\pi_{\\\\theta_{o d d}}(\\\\boldsymbol{o}_{i}|{q})},1-\\\\varepsilon,1+\\\\varepsilon\\\\right)A_{i}\\\\right)-\\\\beta\\\\mathbb{D}_{K L}\\\\left(\\\\pi_{\\\\theta}||\\\\boldsymbol{\\\\pi}_{r e f}\\\\right)\\\\right),}\\\\end{array}\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\n\\\\mathbb{D}_{K L}\\\\left(\\\\pi_{\\\\theta}||\\\\pi_{r e f}\\\\right)=\\\\frac{\\\\pi_{r e f}(o_{i}|q)}{\\\\pi_{\\\\theta}(o_{i}|q)}-\\\\log\\\\frac{\\\\pi_{r e f}(o_{i}|q)}{\\\\pi_{\\\\theta}(o_{i}|q)}-1,\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"where $\\\\varepsilon$ and $\\\\beta$ are hyper-parameters; $\\\\pi_{r e f}$ is the reference model; and $A_{i}$ is the advantage, derived from the rewards $\\\\{r_{1},r_{2},\\\\hdots,r_{G}\\\\}$ corresponding to the outputs within each group: \"}}, {\"type\": \"equation\", \"level\": 3, \"data\": {\"text\": \"$$\\nA_{i}={\\\\frac{r_{i}-\\\\operatorname*{mean}(\\\\{r_{1},r_{2},\\\\cdot\\\\cdot\\\\cdot,r_{G}\\\\})}{\\\\operatorname{std}(\\\\{r_{1},r_{2},\\\\cdot\\\\cdot\\\\cdot,r_{G}\\\\})}}.\\n$$\", \"text_format\": \"latex\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process. This approach not only aligns the model more closely with human preferences but also enhances performance on benchmarks, especially in scenarios where available SFT data are limited. \"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"5.3. Evaluations \"}, \"children\": [{\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.3.1. Evaluation Settings \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Evaluation Benchmarks. Apart from the benchmark we used for base model testing, we further evaluate instructed models on IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), LongBench v2 (Bai et al., 2024), GPQA (Rein et al., 2023), SimpleQA (OpenAI, 2024c), CSimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (questions from August 2024 to November 2024), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Compared Baselines. We conduct comprehensive evaluations of our chat model against several strong baselines, including DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct, LLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, and GPT-4o-0513. For the DeepSeek-V2 model series, we select the most representative variants for comparison. For closed-source models, evaluations are performed through their respective APIs. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Detailed Evaluation Configurations. For standard benchmarks including MMLU, DROP, GPQA, and SimpleQA, we adopt the evaluation prompts from the simple-evals framework4. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We utilize the Zero-Eval prompt format (Lin, 2024) for MMLU-Redux in a zero-shot setting. For other datasets, we follow their original evaluation protocols with default prompts as provided by the dataset creators. For code and math benchmarks, the HumanEval-Mul dataset includes 8 mainstream programming languages (Python, Java, Cpp, C#, JavaScript, TypeScript, PHP, and Bash) in total. We use CoT and non-CoT methods to evaluate model performance on LiveCodeBench, where the data are collected from August 2024 to November 2024. The Codeforces dataset is measured using the percentage of competitors. SWE-Bench verified is evaluated using the agentless framework (Xia et al., 2024). We use the \\u201cdiff\\u201d format to evaluate the Aider-related benchmarks. For mathematical assessments, AIME and CNMO 2024 are evaluated with a temperature of 0.7, and the results are averaged over 16 runs, while MATH-500 employs greedy decoding. We allow all models to output a maximum of 8192 tokens for each benchmark. \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/7a76ee39b10dd3db2c84be68fc0de9cf3dcadd9496d72fedd38eceabaa452b9c.jpg\", \"table_body\": \"\\n\\n<html><body><table><tr><td>Benchmark (Metric)</td><td></td><td>DeepSeek DeepSeek|Qwen2.5 LLaMA-3.1 Claude-3.5- GPT-4o|DeepSeek V2-0506</td><td>V2.5-0905</td><td>72B-Inst. 405B-Inst. Sonnet-1022</td><td></td><td></td><td>0513</td><td>V3</td></tr><tr><td rowspan=\\\"4\\\"></td><td>Architecture</td><td>MoE</td><td>MoE</td><td>Dense</td><td>Dense</td><td></td><td></td><td>MoE</td></tr><tr><td># Activated Params</td><td>21B</td><td>21B</td><td>72B</td><td>405B</td><td></td><td></td><td>37B</td></tr><tr><td># Total Params</td><td>236B</td><td>236B</td><td>72B</td><td>405B</td><td></td><td></td><td>671B</td></tr><tr><td>MMLU (EM)</td><td>78.2</td><td>80.6</td><td>85.3</td><td>88.6</td><td>88.3</td><td>87.2</td><td>88.5</td></tr><tr><td rowspan=\\\"10\\\">English</td><td>MMLU-Redux (EM)</td><td>77.9</td><td>80.3</td><td>85.6</td><td>86.2</td><td>88.9</td><td>88.0</td><td>89.1</td></tr><tr><td>MMLU-Pro (EM)</td><td>58.5</td><td>66.2</td><td>71.6</td><td>73.3</td><td>78.0</td><td>72.6</td><td>75.9</td></tr><tr><td>DROP (3-shot F1)</td><td>83.0</td><td></td><td>76.7</td><td></td><td></td><td></td><td></td></tr><tr><td>IF-Eval (Prompt Strict)</td><td>57.7</td><td>87.8 80.6</td><td>84.1</td><td>88.7 86.0</td><td>88.3 86.5</td><td>83.7 84.3</td><td>91.6 86.1</td></tr><tr><td>GPQA-Diamond (Pass@1)</td><td>35.3</td><td>41.3</td><td>49.0</td><td>51.1</td><td>65.0</td><td>49.9</td><td>59.1</td></tr><tr><td>SimpleQA (Correct)</td><td>9.0</td><td>10.2</td><td>9.1</td><td>17.1</td><td>28.4</td><td>38.2</td><td>24.9</td></tr><tr><td>FRAMES (Acc.)</td><td>66.9</td><td>65.4</td><td>69.8</td><td>70.0</td><td>72.5</td><td>80.5</td><td>73.3</td></tr><tr><td>LongBench v2 (Acc.)</td><td>31.6</td><td>35.4</td><td>39.4</td><td>36.1</td><td>41.0</td><td>48.1</td><td>48.7</td></tr><tr><td>HumanEval-Mul (Pass@1)</td><td>69.3</td><td>77.4</td><td>77.3</td><td></td><td></td><td></td><td></td></tr><tr><td>LiveCodeBench (Pass@1-COT)</td><td>18.8</td><td>29.2</td><td>31.1</td><td>77.2 28.4</td><td>81.7 36.3</td><td>80.5 33.4</td><td>82.6 40.5</td></tr><tr><td>Code LiveCodeBench (Pass@1)</td><td></td><td>20.3</td><td>28.4</td><td>28.7</td><td>30.1</td><td>32.8</td><td>34.2</td><td>37.6</td></tr><tr><td>Codeforces (Percentile)</td><td>17.5</td><td>35.6</td><td>24.8</td><td></td><td>25.3</td><td>20.3</td><td>23.6</td><td>51.6</td></tr><tr><td>SWE Verified (Resolved)</td><td></td><td>22.6</td><td>23.8</td><td></td><td>24.5</td><td>50.8</td><td>38.8</td><td>42.0</td></tr><tr><td>Aider-Edit (Acc.)</td><td>60.3</td><td>71.6</td><td>65.4</td><td></td><td>63.9</td><td>84.2</td><td>72.9</td><td>79.7</td></tr><tr><td>Aider-Polyglot (Acc.)</td><td>\\u4e00</td><td>18.2</td><td>7.6</td><td>5.8</td><td></td><td>45.3</td><td>16.0</td><td>49.6</td></tr><tr><td rowspan=\\\"3\\\"></td><td>AIME 2024 (Pass@1)</td><td>4.6</td><td>16.7</td><td>23.3</td><td>23.3</td><td>16.0</td><td></td><td></td><td></td></tr><tr><td>Math MATH-500 (EM)</td><td>56.3</td><td>74.7</td><td>80.0</td><td>73.8</td><td>78.3</td><td></td><td>9.3 74.6</td><td>39.2</td></tr><tr><td>CNMO 2024 (Pass@1)</td><td>2.8</td><td>10.8</td><td>15.9</td><td>6.8</td><td>13.1</td><td></td><td>10.8</td><td>90.2 43.2</td></tr><tr><td rowspan=\\\"3\\\">Chinese C-Eval (EM)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CLUEWSC (EM)</td><td>89.9</td><td>90.4</td><td>91.4</td><td>84.7</td><td></td><td>85.4</td><td>87.9</td><td>90.9</td></tr><tr><td>C-SimpleQA (Correct)</td><td>78.6 48.5</td><td>79.5 54.1</td><td>86.1 48.4</td><td>61.5 50.4</td><td></td><td>76.7 51.3</td><td>76.0 59.3</td><td>86.5 64.8</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Table 6 Comparison between DeepSeek-V3 and other representative chat models. All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.3.2. Standard Evaluation \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Table 6 presents the evaluation results, showcasing that DeepSeek-V3 stands as the bestperforming open-source model. Additionally, it is competitive against frontier closed-source models like GPT-4o and Claude-3.5-Sonnet. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"English Benchmarks. MMLU is a widely recognized benchmark designed to assess the performance of large language models, across diverse knowledge domains and tasks. DeepSeek-V3 demonstrates competitive performance, standing on par with top-tier models such as LLaMA3.1-405B, GPT-4o, and Claude-Sonnet 3.5, while significantly outperforming Qwen2.5 72B. Moreover, DeepSeek-V3 excels in MMLU-Pro, a more challenging educational knowledge benchmark, where it closely trails Claude-Sonnet 3.5. On MMLU-Redux, a refined version of MMLU with corrected labels, DeepSeek-V3 surpasses its peers. In addition, on GPQA-Diamond, a PhD-level evaluation testbed, DeepSeek-V3 achieves remarkable results, ranking just behind Claude 3.5 Sonnet and outperforming all other competitors by a substantial margin. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In long-context understanding benchmarks such as DROP, LongBench v2, and FRAMES, DeepSeek-V3 continues to demonstrate its position as a top-tier model. It achieves an impressive 91.6 F1 score in the 3-shot setting on DROP, outperforming all other models in this category. On FRAMES, a benchmark requiring question-answering over 100k token contexts, DeepSeekV3 closely trails GPT-4o while outperforming all other models by a significant margin. This demonstrates the strong capability of DeepSeek-V3 in handling extremely long-context tasks. The long-context capability of DeepSeek-V3 is further validated by its best-in-class performance on LongBench v2, a dataset that was released just a few weeks before the launch of DeepSeek V3. On the factual knowledge benchmark, SimpleQA, DeepSeek-V3 falls behind GPT-4o and Claude-Sonnet, primarily due to its design focus and resource allocation. DeepSeek-V3 assigns more training tokens to learn Chinese knowledge, leading to exceptional performance on the C-SimpleQA. On the instruction-following benchmark, DeepSeek-V3 significantly outperforms its predecessor, DeepSeek-V2-series, highlighting its improved ability to understand and adhere to user-defined format constraints. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Code and Math Benchmarks. Coding is a challenging and practical task for LLMs, encompassing engineering-focused tasks like SWE-Bench-Verified and Aider, as well as algorithmic tasks such as HumanEval and LiveCodeBench. In engineering tasks, DeepSeek-V3 trails behind Claude-Sonnet-3.5-1022 but significantly outperforms open-source models. The open-source DeepSeek-V3 is expected to foster advancements in coding-related engineering tasks. By providing access to its robust capabilities, DeepSeek-V3 can drive innovation and improvement in areas such as software engineering and algorithm development, empowering developers and researchers to push the boundaries of what open-source models can achieve in coding tasks. In algorithmic tasks, DeepSeek-V3 demonstrates superior performance, outperforming all baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be attributed to its advanced knowledge distillation technique, which effectively enhances its code generation and problem-solving capabilities in algorithm-focused tasks. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"On math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly surpassing baselines and setting a new state-of-the-art for non-o1-like models. Specifically, on AIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5 72B, by approximately $10\\\\%$ in absolute scores, which is a substantial margin for such challenging benchmarks. This remarkable capability highlights the effectiveness of the distillation technique from DeepSeek-R1, which has been proven highly beneficial for non-o1-like models. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Chinese Benchmarks. Qwen and DeepSeek are two representative model series with robust support for both Chinese and English. On the factual benchmark Chinese SimpleQA, DeepSeekV3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus compromising 18T tokens, which are $20\\\\%$ more than the $14.8\\\\mathrm{T}$ tokens that DeepSeek-V3 is \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/c37f5046875e424ff7860735dd500227c5142a1630e000ff48bb7ce04be5e88f.jpg\", \"table_footnote\": [\"Table 7 English open-ended conversation evaluations. For AlpacaEval 2.0, we use the lengthcontrolled win rate as the metric. \"], \"table_body\": \"\\n\\n<html><body><table><tr><td>Model</td><td>Arena-Hard</td><td>AlpacaEval 2.0</td></tr><tr><td>DeepSeek-V2.5-0905</td><td>76.2</td><td>50.5</td></tr><tr><td>Qwen2.5-72B-Instruct</td><td>81.2</td><td>49.1</td></tr><tr><td>LLaMA-3.1 405B</td><td>69.3</td><td>40.5</td></tr><tr><td>GPT-40-0513</td><td>80.4</td><td>51.1</td></tr><tr><td>Claude-Sonnet-3.5-1022</td><td>85.2</td><td>52.0</td></tr><tr><td>DeepSeek-V3</td><td>85.5</td><td>70.0</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"pre-trained on. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"On C-Eval, a representative benchmark for Chinese educational knowledge evaluation, and CLUEWSC (Chinese Winograd Schema Challenge), DeepSeek-V3 and Qwen2.5-72B exhibit similar performance levels, indicating that both models are well-optimized for challenging Chinese-language reasoning and educational tasks. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.3.3. Open-Ended Evaluation \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges, with the results shown in Table 7. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024a), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. On Arena-Hard, DeepSeek-V3 achieves an impressive win rate of over $86\\\\%$ against the baseline GPT-4-0314, performing on par with top-tier models like Claude-Sonnet-3.5-1022. This underscores the robust capabilities of DeepSeek-V3, especially in dealing with complex prompts, including coding and debugging tasks. Furthermore, DeepSeek-V3 achieves a groundbreaking milestone as the first open-source model to surpass $85\\\\%$ on the Arena-Hard benchmark. This achievement significantly bridges the performance gap between open-source and closed-source models, setting a new standard for what open-source models can accomplish in challenging domains. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Similarly, DeepSeek-V3 showcases exceptional performance on AlpacaEval 2.0, outperforming both closed-source and open-source models. This demonstrates its outstanding proficiency in writing tasks and handling straightforward question-answering scenarios. Notably, it surpasses DeepSeek-V2.5-0905 by a significant margin of $20\\\\%$ , highlighting substantial improvements in tackling simple tasks and showcasing the effectiveness of its advancements. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.3.4. DeepSeek-V3 as a Generative Reward Model \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We compare the judgment ability of DeepSeek-V3 with state-of-the-art models, namely GPT-4o and Claude-3.5. Table 8 presents the performance of these models in RewardBench (Lambert et al., 2024). DeepSeek-V3 achieves performance on par with the best versions of GPT-4o-0806 and Claude-3.5-Sonnet-1022, while surpassing other versions. Additionally, the judgment ability of DeepSeek-V3 can also be enhanced by the voting technique. Therefore, we employ DeepSeekV3 along with voting to offer self-feedback on open-ended questions, thereby improving the effectiveness and robustness of the alignment process. \"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0b07a5c810c734387321e9fb236709a509378526205175534042267f9bc58722.jpg\", \"table_caption\": [\"Table 8 | Performances of GPT-4o, Claude-3.5-sonnet and DeepSeek-V3 on RewardBench. \"], \"table_body\": \"\\n\\n<html><body><table><tr><td>Model</td><td>Chat</td><td>Chat-Hard</td><td>Safety</td><td>Reasoning</td><td>Average</td></tr><tr><td>GPT-40-0513</td><td>96.6</td><td>70.4</td><td>86.7</td><td>84.9</td><td>84.7</td></tr><tr><td>GPT-40-0806</td><td>96.1</td><td>76.1</td><td>88.1</td><td>86.6</td><td>86.7</td></tr><tr><td>GPT-40-1120</td><td>95.8</td><td>71.3</td><td>86.2</td><td>85.2</td><td>84.6</td></tr><tr><td>Claude-3.5-sonnet-0620</td><td>96.4</td><td>74.0</td><td>81.6</td><td>84.7</td><td>84.2</td></tr><tr><td>Claude-3.5-sonnet-1022</td><td>96.4</td><td>79.7</td><td>91.1</td><td>87.6</td><td>88.7</td></tr><tr><td>DeepSeek-V3</td><td>96.9</td><td>79.8</td><td>87.0</td><td>84.3</td><td>87.0</td></tr><tr><td>DeepSeek-V3 (maj@6)</td><td>96.9</td><td>82.6</td><td>89.5</td><td>89.2</td><td>89.6</td></tr></table></body></html>\\n\\n\"}}, {\"type\": \"table\", \"level\": 3, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/0700f9f4a277d248543728c8d67e12011d7ca905620b8f1be4fa1406b33ee64d.jpg\", \"table_footnote\": [\"Table 9 | The contribution of distillation from DeepSeek-R1. The evaluation settings of LiveCodeBench and MATH-500 are the same as in Table 6. \"], \"table_body\": \"\\n\\n<html><body><table><tr><td rowspan=\\\"2\\\">Model</td><td colspan=\\\"2\\\">LiveCodeBench-CoT</td><td colspan=\\\"2\\\">MATH-500</td></tr><tr><td>Pass@1</td><td>Length</td><td>Pass@1</td><td>Length</td></tr><tr><td>DeepSeek-V2.5 Baseline</td><td>31.1</td><td>718</td><td>74.6</td><td>769</td></tr><tr><td>DeepSeek-V2.5 +R1 Distill</td><td>37.4</td><td>783</td><td>83.2</td><td>1510</td></tr></table></body></html>\\n\\n\"}}]}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"5.4. Discussion \"}, \"children\": [{\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.4.1. Distillation from DeepSeek-R1 \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"We ablate the contribution of distillation from DeepSeek-R1 based on DeepSeek-V2.5. The baseline is trained on short CoT data, whereas its competitor uses data generated by the expert checkpoints described above. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Table 9 demonstrates the effectiveness of the distillation data, showing significant improvements in both LiveCodeBench and MATH-500 benchmarks. Our experiments reveal an interesting trade-off: the distillation leads to better performance but also substantially increases the average response length. To maintain a balance between model accuracy and computational efficiency, we carefully selected optimal settings for DeepSeek-V3 in distillation. \"}}, {\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Our research suggests that knowledge distillation from reasoning models presents a promising direction for post-training optimization. While our current work focuses on distilling data from mathematics and coding domains, this approach shows potential for broader applications across various task domains. The effectiveness demonstrated in these specific areas indicates that long-CoT distillation could be valuable for enhancing model performance in other cognitive tasks requiring complex reasoning. Further exploration of this approach across different domains remains an important direction for future research. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.4.2. Self-Rewarding \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Rewards play a pivotal role in RL, steering the optimization process. In domains where verification through external tools is straightforward, such as some coding or mathematics scenarios, RL demonstrates exceptional efficacy. However, in more general scenarios, constructing a feedback mechanism through hard coding is impractical. During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source. This method has produced notable alignment effects, significantly enhancing the performance of DeepSeek-V3 in subjective evaluations. By integrating additional constitutional inputs, DeepSeek-V3 can optimize towards the constitutional direction. We believe that this paradigm, which combines supplementary information with LLMs as a feedback source, is of paramount importance. The LLM serves as a versatile processor capable of transforming unstructured information from diverse scenarios into rewards, ultimately facilitating the self-improvement of LLMs. Beyond self-rewarding, we are also dedicated to uncovering other general and scalable rewarding methods to consistently advance the model capabilities in general scenarios. \"}}]}, {\"type\": \"chapter\", \"level\": 2, \"data\": {\"text\": \"5.4.3. Multi-Token Prediction Evaluation \"}, \"children\": [{\"type\": \"text\", \"level\": 3, \"data\": {\"text\": \"Instead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through the MTP technique. Combined with the framework of speculative decoding (Leviathan et al., 2023; Xia et al., 2023), it can significantly accelerate the decoding speed of the model. A natural question arises concerning the acceptance rate of the additionally predicted token. Based on our evaluation, the acceptance rate of the second token prediction ranges between $85\\\\%$ and $90\\\\%$ across various generation topics, demonstrating consistent reliability. This high acceptance rate enables DeepSeek-V3 to achieve a significantly improved decoding speed, delivering 1.8 times TPS (Tokens Per Second). \"}}]}]}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 790015773\n",
      "{'chapter_title': '5. Post-Training ', 'summaries': ['Curated instruction-tuning datasets include 1.5M instances spanning multiple domains, employing domain-specific data creation methods. Expert models generate reasoning data, balancing accuracy with clarity and conciseness. Supervised Fine-Tuning (SFT) uses system prompts to guide responses and incorporates chain-of-thought patterns. Reinforcement Learning (RL) utilizes Group Relative Policy Optimization (GRPO) to optimize policy models. A rule-based Reward Model (RM) is used for verifiable tasks, while a model-based RM is used for free-form responses, incorporating chain-of-thought into the reward model.', 'The reward model is trained from DeepSeek-V3 SFT checkpoints to enhance reliability.', 'DeepSeek-V3 demonstrates competitive performance against leading models like GPT-4o and Claude-3.5-Sonnet in code and math benchmarks.', 'Distillation from DeepSeek-R1 enhances model performance but increases response length, requiring careful setting of distillation parameters.', 'DeepSeek-V3 uses self-rewarding, leveraging its own voting results as feedback, leading to significant alignment effects.', 'Multi-Token Prediction enhances decoding speed, achieving a token acceptance rate of 2.', 'In open-ended evaluation DeepSeek-V3 shows strong performance against leading models']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}, {\"chapter_title\": \"1. Introduction \", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\", \"The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\", \"FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\", \"The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\", \"Training cost was 2.788M H800 GPU hours.\", \"Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\", \"DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\"]}, {\"chapter_title\": \"2. Architecture\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.\", \"An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.\", \"Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.\", \"The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.\", \"During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.\"]}, {\"chapter_title\": \"3. Infrastructures\", \"summaries\": [\"DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.\", \"The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.\", \"DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.\", \"Efficient cross-node all-to-all communication kernels minimize communication overhead.\", \"The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.\", \"FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.\", \"The framework employs tile- and block-wise quantization to mitigate quantization errors.\", \"The inference deployment separates prefilling and decoding stages for improved throughput.\", \"Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.\"]}, {\"chapter_title\": \"4. Pre-Training\", \"summaries\": [\"The pre-training corpus was optimized by enhancing mathematical and programming samples and expanding multilingual coverage.\", \"The data processing pipeline was refined to minimize redundancy while maintaining corpus diversity.\", \"The Prefix-Suffix-Middle (PSM) framework, employing the Fill-in-Middle (FIM) strategy at a rate of 0.1, structures data.\", \"A tokenizer with a 128K token vocabulary was used, with combined punctuation/line break tokens split to mitigate token boundary bias.\", \"Hyperparameters include 61 Transformer layers, a hidden dimension of 3768, and 37B activated parameters per token.\", \"Training involved a multi-stage learning rate schedule and a batch size scheduling strategy.\", \"Long context extension used YaRN to progressively expand the context window to 128K.\", \"The MTP strategy consistently enhances model performance on most benchmarks.\", \"The auxiliary-loss-free balancing strategy demonstrates better model performance compared to purely auxiliary-loss-based methods.\", \"Batch-wise balancing enables greater expert specialization patterns and faces efficiency challenges addressed by the training framework.\"]}, {\"chapter_title\": \"5. Post-Training \", \"summaries\": [\"Curated instruction-tuning datasets include 1.5M instances spanning multiple domains, employing domain-specific data creation methods. Expert models generate reasoning data, balancing accuracy with clarity and conciseness. Supervised Fine-Tuning (SFT) uses system prompts to guide responses and incorporates chain-of-thought patterns. Reinforcement Learning (RL) utilizes Group Relative Policy Optimization (GRPO) to optimize policy models. A rule-based Reward Model (RM) is used for verifiable tasks, while a model-based RM is used for free-form responses, incorporating chain-of-thought into the reward model.\", \"The reward model is trained from DeepSeek-V3 SFT checkpoints to enhance reliability.\", \"DeepSeek-V3 demonstrates competitive performance against leading models like GPT-4o and Claude-3.5-Sonnet in code and math benchmarks.\", \"Distillation from DeepSeek-R1 enhances model performance but increases response length, requiring careful setting of distillation parameters.\", \"DeepSeek-V3 uses self-rewarding, leveraging its own voting results as feedback, leading to significant alignment effects.\", \"Multi-Token Prediction enhances decoding speed, achieving a token acceptance rate of 2.\", \"In open-ended evaluation DeepSeek-V3 shows strong performance against leading models\"]}]```\n",
      "* Current Chapter Title: ``6. Conclusion, Limitations, and Future Directions `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"6. Conclusion, Limitations, and Future Directions \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations. The post-training also makes a success in distilling the reasoning capability from the DeepSeek-R1 series of models. Comprehensive evaluations demonstrate that DeepSeek-V3 has emerged as the strongest open-source model currently available, and achieves performance comparable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. Despite its strong performance, it also maintains economical training costs. It requires only 2.788M H800 GPU hours for its full training, including pre-training, context length extension, and post-training. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"While acknowledging its strong performance and cost-effectiveness, we also recognize that DeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient inference, the recommended deployment unit for DeepSeek-V3 is relatively large, which might pose a burden for small-sized teams. Secondly, although our deployment strategy for DeepSeekV3 has achieved an end-to-end generation speed of more than two times that of DeepSeek-V2, there still remains potential for further enhancement. Fortunately, these limitations are expected to be naturally addressed with the development of more advanced hardware. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"DeepSeek consistently adheres to the route of open-source models with longtermism, aiming to steadily approach the ultimate goal of AGI (Artificial General Intelligence). In the future, we plan to strategically invest in research across the following directions. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"\\u2022 We will consistently study and refine our model architectures, aiming to further improve both the training and inference efficiency, striving to approach efficient support for infinite context length. Additionally, we will try to break through the architectural limitations of Transformer, thereby pushing the boundaries of its modeling capabilities. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"\\u2022 We will continuously iterate on the quantity and quality of our training data, and explore the incorporation of additional training signal sources, aiming to drive data scaling across a more comprehensive range of dimensions.   \\n\\u2022 We will consistently explore and iterate on the deep thinking capabilities of our models, aiming to enhance their intelligence and problem-solving abilities by expanding their reasoning length and depth.   \\n\\u2022 We will explore more comprehensive and multi-dimensional model evaluation methods to prevent the tendency towards optimizing a fixed set of benchmarks during research, which may create a misleading impression of the model capabilities and affect our foundational assessment. \"}}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 347988509\n",
      "{'chapter_title': '6. Conclusion, Limitations, and Future Directions', 'summaries': ['DeepSeek-V3 is a 671B parameter MoE model trained on 14.8T tokens, incorporating MLA, DeepSeekMoE, auxiliary-loss-free load balancing, and a multi-token prediction training objective.', 'Training cost was 2.788M H800 GPU hours.', 'DeepSeek-V3 demonstrates performance comparable to leading closed-source models (GPT-4o, Claude-3.5-Sonnet) while remaining cost-effective.', 'A limitation is the relatively large deployment unit required for efficient inference.', 'Future research directions include architectural improvements, increased data quantity and quality, enhancement of deep thinking capabilities, and development of more comprehensive evaluation methods.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}, {\"chapter_title\": \"1. Introduction \", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\", \"The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\", \"FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\", \"The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\", \"Training cost was 2.788M H800 GPU hours.\", \"Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\", \"DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\"]}, {\"chapter_title\": \"2. Architecture\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.\", \"An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.\", \"Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.\", \"The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.\", \"During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.\"]}, {\"chapter_title\": \"3. Infrastructures\", \"summaries\": [\"DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.\", \"The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.\", \"DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.\", \"Efficient cross-node all-to-all communication kernels minimize communication overhead.\", \"The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.\", \"FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.\", \"The framework employs tile- and block-wise quantization to mitigate quantization errors.\", \"The inference deployment separates prefilling and decoding stages for improved throughput.\", \"Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.\"]}, {\"chapter_title\": \"4. Pre-Training\", \"summaries\": [\"The pre-training corpus was optimized by enhancing mathematical and programming samples and expanding multilingual coverage.\", \"The data processing pipeline was refined to minimize redundancy while maintaining corpus diversity.\", \"The Prefix-Suffix-Middle (PSM) framework, employing the Fill-in-Middle (FIM) strategy at a rate of 0.1, structures data.\", \"A tokenizer with a 128K token vocabulary was used, with combined punctuation/line break tokens split to mitigate token boundary bias.\", \"Hyperparameters include 61 Transformer layers, a hidden dimension of 3768, and 37B activated parameters per token.\", \"Training involved a multi-stage learning rate schedule and a batch size scheduling strategy.\", \"Long context extension used YaRN to progressively expand the context window to 128K.\", \"The MTP strategy consistently enhances model performance on most benchmarks.\", \"The auxiliary-loss-free balancing strategy demonstrates better model performance compared to purely auxiliary-loss-based methods.\", \"Batch-wise balancing enables greater expert specialization patterns and faces efficiency challenges addressed by the training framework.\"]}, {\"chapter_title\": \"5. Post-Training \", \"summaries\": [\"Curated instruction-tuning datasets include 1.5M instances spanning multiple domains, employing domain-specific data creation methods. Expert models generate reasoning data, balancing accuracy with clarity and conciseness. Supervised Fine-Tuning (SFT) uses system prompts to guide responses and incorporates chain-of-thought patterns. Reinforcement Learning (RL) utilizes Group Relative Policy Optimization (GRPO) to optimize policy models. A rule-based Reward Model (RM) is used for verifiable tasks, while a model-based RM is used for free-form responses, incorporating chain-of-thought into the reward model.\", \"The reward model is trained from DeepSeek-V3 SFT checkpoints to enhance reliability.\", \"DeepSeek-V3 demonstrates competitive performance against leading models like GPT-4o and Claude-3.5-Sonnet in code and math benchmarks.\", \"Distillation from DeepSeek-R1 enhances model performance but increases response length, requiring careful setting of distillation parameters.\", \"DeepSeek-V3 uses self-rewarding, leveraging its own voting results as feedback, leading to significant alignment effects.\", \"Multi-Token Prediction enhances decoding speed, achieving a token acceptance rate of 2.\", \"In open-ended evaluation DeepSeek-V3 shows strong performance against leading models\"]}, {\"chapter_title\": \"6. Conclusion, Limitations, and Future Directions\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter MoE model trained on 14.8T tokens, incorporating MLA, DeepSeekMoE, auxiliary-loss-free load balancing, and a multi-token prediction training objective.\", \"Training cost was 2.788M H800 GPU hours.\", \"DeepSeek-V3 demonstrates performance comparable to leading closed-source models (GPT-4o, Claude-3.5-Sonnet) while remaining cost-effective.\", \"A limitation is the relatively large deployment unit required for efficient inference.\", \"Future research directions include architectural improvements, increased data quantity and quality, enhancement of deep thinking capabilities, and development of more comprehensive evaluation methods.\"]}]```\n",
      "* Current Chapter Title: ``References `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"References \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"AI@Meta. Llama 3 model card, 2024a. URL https://github.com/meta-llama/llama3/bl ob/main/MODEL_CARD.md.   \\nAI@Meta. Llama 3.1 model card, 2024b. URL https://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md.   \\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3 -5-sonnet.   \\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.   \\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022.   \\nY. Bai, S. Tu, J. Zhang, H. Peng, X. Wang, X. Lv, S. Cao, J. Xu, L. Hou, Y. Dong, J. Tang, and J. Li. LongBench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024.   \\nM. Bauer, S. Treichler, and A. Aiken. Singe: leveraging warp specialization for high performance on GPUs. In Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP \\u201914, page 119\\u2013130, New York, NY, USA, 2014. Association for Computing Machinery. ISBN 9781450326568. doi: 10.1145/2555243.2555258. URL https://doi.org/10.1145/2555243.2555258.   \\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432\\u20137439. AAAI Press, 2020. doi: 10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.   \\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.   \\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.   \\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,   \\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   \\nY. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, and G. Hu. A span-extraction dataset for Chinese machine reading comprehension. In K. Inui, J. Jiang, V. $\\\\mathrm{Ng,}$ and X. Wan,   \\neditors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5883\\u20135889, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1600. URL https://aclanthology.org/D19-1 600.   \\nD. Dai, C. Deng, C. Zhao, R. X. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, Z. Xie, Y. K. Li, P. Huang, F. Luo, C. Ruan, Z. Sui, and W. Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. CoRR, abs/2401.06066, 2024. URL https://doi.org/10.48550/arXiv.2401.06066.   \\nDeepSeek-AI. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. CoRR, abs/2406.11931, 2024a. URL https://doi.org/10.48550/arXiv.2406.11 931.   \\nDeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR, abs/2401.02954, 2024b. URL https://doi.org/10.48550/arXiv.2401.02954.   \\nDeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. CoRR, abs/2405.04434, 2024c. URL https://doi.org/10.48550/arXiv.2405. 04434.   \\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318\\u2013 30332, 2022.   \\nH. Ding, Z. Wang, G. Paolini, V. Kumar, A. Deoras, D. Roth, and S. Soatto. Fewer truncations improve language modeling. arXiv preprint arXiv:2404.10830, 2024.   \\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368\\u2013 2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL https://doi.org/10.18653/v1/n19-1246.   \\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https://arxiv.org/ abs/2101.03961. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"M. Fishman, B. Chmiel, R. Banner, and D. Soudry. Scaling FP8 training to trillion-token llms. arXiv preprint arXiv:2409.12517, 2024.   \\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.   \\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.   \\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or g/10.48550/arXiv.2406.04127.   \\nF. Gloeckle, B. Y. Idrissi, B. Rozi\\u00e8re, D. Lopez-Paz, and G. Synnaeve. Better & faster large language models via multi-token prediction. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=pEWAcejiU2.   \\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024.   \\nR. L. Graham, D. Bureddy, P. Lui, H. Rosenstock, G. Shainer, G. Bloch, D. Goldenerg, M. Dubman, S. Kotchubievsky, V. Koushnir, et al. Scalable hierarchical aggregation protocol (SHArP): A hardware architecture for efficient data reduction. In 2016 First International Workshop on Communication Optimizations in HPC (COMHPC), pages 1\\u201310. IEEE, 2016.   \\nA. Gu, B. Rozi\\u00e8re, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang. Cruxeval: A benchmark for code reasoning, understanding and execution, 2024.   \\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo, Y. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming - the rise of code intelligence. CoRR, abs/2401.14196, 2024. URL https://doi.org/10.485 50/arXiv.2401.14196.   \\nA. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger, and P. Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training, 2018. URL https://arxiv.or g/abs/1806.03377.   \\nB. He, L. Noci, D. Paliotta, I. Schlag, and T. Hofmann. Understanding and minimising outlier features in transformer training. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.   \\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chinese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140, 2024.   \\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   \\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.   \\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023.   \\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.   \\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   \\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.   \\nD. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen, et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019.   \\nS. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR, abs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485 50/arXiv.2409.12941.   \\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452\\u2013466, 2019. doi: 10.1162/tacl\\\\_a\\\\_00276. URL https://doi.org/10.1162/tacl_a_00276.   \\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. RACE: large-scale reading comprehension dataset from examinations. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 785\\u2013794. Association for Computational Linguistics, 2017. doi: 10.18653/V1/D17-1082. URL https://doi.org/10.18653/v1/d1 7-1082.   \\nN. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar, T. Zick, Y. Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.   \\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id $\\\\c=$ qrwe7XHTmYb.   \\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 19274\\u201319286. PMLR, 2023. URL https://proceedings.mlr.press/v202/leviathan23 a.html.   \\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023.   \\nS. Li and T. Hoefler. Chimera: efficiently training large-scale neural networks with bidirectional pipelines. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC \\u201921, page 1\\u201314. ACM, Nov. 2021. doi: 10.1145/345881 7.3476145. URL http://dx.doi.org/10.1145/3458817.3476145.   \\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024a.   \\nW. Li, F. Qi, M. Sun, X. Yi, and J. Zhang. Ccpm: A chinese classical poetry matching dataset, 2021.   \\nY. Li, F. Wei, C. Zhang, and H. Zhang. EAGLE: speculative sampling requires rethinking feature uncertainty. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024b. URL https://openreview.net /forum?id $\\\\c=$ 1NdN7eXyb4.   \\nB. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL https://github.com/WildEval/ZeroEval.   \\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   \\nS. Lundberg. The art of prompt design: Prompt boundaries and token healing, 2023. URL https://towardsdatascience.com/the-art-of-prompt-design-prompt-bound aries-and-token-healing-3b2448b0be38.   \\nY. Luo, Z. Zhang, R. Wu, H. Liu, Y. Jin, K. Zheng, M. Wang, Z. He, G. Hu, L. Chen, et al. Ascend HiFloat8 format for deep learning. arXiv preprint arXiv:2409.16626, 2024.   \\nMAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime.   \\nP. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisenthwaite, S. Ha, A. Heinecke, P. Judd, J. Kamalu, et al. FP8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.   \\nMistral. Cheaper, better, faster, stronger: Continuing to push the frontier of ai and making it accessible to all, 2024. URL https://mistral.ai/news/mixtral-8x22b.   \\nS. Narang, G. Diamos, E. Elsen, P. Micikevicius, J. Alben, D. Garcia, B. Ginsburg, M. Houston, O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. In Int. Conf. on Learning Representation, 2017. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"B. Noune, P. Jones, D. Justus, D. Masters, and C. Luschi. 8-bit numerical formats for deep neural networks. arXiv preprint arXiv:2206.02915, 2022. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"NVIDIA. Improving network performance of HPC systems using NVIDIA Magnum IO NVSHMEM and GPUDirect Async. https://developer.nvidia.com/blog/improving-net work-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-g pudirect-async, 2022. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"NVIDIA. Blackwell architecture. https://www.nvidia.com/en-us/data-center/tech nologies/blackwell-architecture/, 2024a.   \\nNVIDIA. TransformerEngine, 2024b. URL https://github.com/NVIDIA/TransformerE ngine. Accessed: 2024-11-19.   \\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.   \\nOpenAI. Multilingual massive multitask language understanding (mmmlu), 2024b. URL https://huggingface.co/datasets/openai/MMMLU.   \\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing   \\n-simpleqa/.   \\nOpenAI. Introducing SWE-bench verified we\\u2019re releasing a human-validated subset of swebench that more, 2024d. URL https://openai.com/index/introducing-swe-bench -verified/.   \\nB. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023a.   \\nH. Peng, K. Wu, Y. Wei, G. Zhao, Y. Yang, Z. Liu, Y. Xiong, Z. Yang, B. Ni, J. Hu, et al. FP8-LM: Training FP8 large language models. arXiv preprint arXiv:2310.18313, 2023b.   \\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble pipeline parallelism. arXiv preprint arXiv:2401.10241, 2023a.   \\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble pipeline parallelism, 2023b. URL https: //arxiv.org/abs/2401.10241.   \\nQwen. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.   \\nQwen. Introducing Qwen1.5, 2024a. URL https://qwenlm.github.io/blog/qwen1.5.   \\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b log/qwen2.5.   \\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\\u201316. IEEE, 2020.   \\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.   \\nB. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea, E. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023a.   \\nB. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea, E. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023b.   \\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019.   \\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   \\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017. OpenReview.net, 2017. URL https: //openreview.net/forum?id $\\\\c=$ B1ckMDqlg.   \\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?i d=fR3wGCk-IXp.   \\nY. Shibata, T. Kida, S. Fukamachi, M. Takeda, A. Shinohara, T. Shinohara, and S. Arikawa. Byte pair encoding: A text compression scheme that accelerates pattern matching. 1999.   \\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   \\nK. Sun, D. Yu, D. Yu, and C. Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension, 2019a.   \\nM. Sun, X. Chen, J. Z. Kolter, and Z. Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024.   \\nX. Sun, J. Choi, C.-Y. Chen, N. Wang, S. Venkataramani, V. V. Srinivasan, X. Cui, W. Zhang, and K. Gopalakrishnan. Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks. Advances in neural information processing systems, 32, 2019b.   \\nM. Suzgun, N. Scales, N. Sch\\u00e4rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.   \\nV. Thakkar, P. Ramani, C. Cecka, A. Shivam, H. Lu, E. Yan, J. Kosaian, M. Hoemmen, H. Wu, A. Kerr, M. Nicely, D. Merrill, D. Blasig, F. Qiao, P. Majcher, P. Springer, M. Hohnerbach, J. Wang, and M. Gupta. CUTLASS, Jan. 2023. URL https://github.com/NVIDIA/cutlas s.   \\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\\u00e8re, N. Goyal, E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.   \\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307. 09288.   \\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \\u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.   \\nL. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. CoRR, abs/2408.15664, 2024a. URL https://doi.org/10.48550/arX iv.2408.15664.   \\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024b. URL https://doi.org/10.48550/arXiv.2406.01574.   \\nT. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023.   \\nM. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable and low-precision training for large-scale vision-language models. Advances in Neural Information Processing Systems, 36:10271\\u201310298, 2023.   \\nH. Xi, C. Li, J. Chen, and J. Zhu. Training transformers with 4-bit integers. Advances in Neural Information Processing Systems, 36:49146\\u201349168, 2023.   \\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint, 2024.   \\nH. Xia, T. Ge, P. Wang, S. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 3909\\u20133925. Association for Computational Linguistics, 2023. URL https://doi.org/10.18653/v1/ 2023.findings-emnlp.257.   \\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 38087\\u201338099. PMLR, 2023.   \\nL. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, Y. Tian, Q. Dong, W. Liu, B. Shi, Y. Cui, J. Li, J. Zeng, R. Wang, W. Xie, Y. Li, Y. Patterson, Z. Tian, Y. Zhang, H. Zhou, S. Liu, Z. Zhao, Q. Zhao, C. Yue, X. Zhang, Z. Yang, K. Richardson, and Z. Lan. CLUE: A chinese language understanding evaluation benchmark. In D. Scott, N. Bel, and C. Zong, editors, Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4762\\u20134772. International Committee on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.419. URL https://doi.org/10.18653/v1/2020.coling-main.419. \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish your sentence? In A. Korhonen, D. R. Traum, and L. M\\u00e0rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4791\\u20134800. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1 9-1472. W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A human-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023. doi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. \"}}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 391562132\n",
      "{'chapter_title': 'References', 'summaries': []}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}, {\"chapter_title\": \"1. Introduction \", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\", \"The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\", \"FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\", \"The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\", \"Training cost was 2.788M H800 GPU hours.\", \"Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\", \"DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\"]}, {\"chapter_title\": \"2. Architecture\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.\", \"An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.\", \"Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.\", \"The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.\", \"During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.\"]}, {\"chapter_title\": \"3. Infrastructures\", \"summaries\": [\"DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.\", \"The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.\", \"DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.\", \"Efficient cross-node all-to-all communication kernels minimize communication overhead.\", \"The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.\", \"FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.\", \"The framework employs tile- and block-wise quantization to mitigate quantization errors.\", \"The inference deployment separates prefilling and decoding stages for improved throughput.\", \"Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.\"]}, {\"chapter_title\": \"4. Pre-Training\", \"summaries\": [\"The pre-training corpus was optimized by enhancing mathematical and programming samples and expanding multilingual coverage.\", \"The data processing pipeline was refined to minimize redundancy while maintaining corpus diversity.\", \"The Prefix-Suffix-Middle (PSM) framework, employing the Fill-in-Middle (FIM) strategy at a rate of 0.1, structures data.\", \"A tokenizer with a 128K token vocabulary was used, with combined punctuation/line break tokens split to mitigate token boundary bias.\", \"Hyperparameters include 61 Transformer layers, a hidden dimension of 3768, and 37B activated parameters per token.\", \"Training involved a multi-stage learning rate schedule and a batch size scheduling strategy.\", \"Long context extension used YaRN to progressively expand the context window to 128K.\", \"The MTP strategy consistently enhances model performance on most benchmarks.\", \"The auxiliary-loss-free balancing strategy demonstrates better model performance compared to purely auxiliary-loss-based methods.\", \"Batch-wise balancing enables greater expert specialization patterns and faces efficiency challenges addressed by the training framework.\"]}, {\"chapter_title\": \"5. Post-Training \", \"summaries\": [\"Curated instruction-tuning datasets include 1.5M instances spanning multiple domains, employing domain-specific data creation methods. Expert models generate reasoning data, balancing accuracy with clarity and conciseness. Supervised Fine-Tuning (SFT) uses system prompts to guide responses and incorporates chain-of-thought patterns. Reinforcement Learning (RL) utilizes Group Relative Policy Optimization (GRPO) to optimize policy models. A rule-based Reward Model (RM) is used for verifiable tasks, while a model-based RM is used for free-form responses, incorporating chain-of-thought into the reward model.\", \"The reward model is trained from DeepSeek-V3 SFT checkpoints to enhance reliability.\", \"DeepSeek-V3 demonstrates competitive performance against leading models like GPT-4o and Claude-3.5-Sonnet in code and math benchmarks.\", \"Distillation from DeepSeek-R1 enhances model performance but increases response length, requiring careful setting of distillation parameters.\", \"DeepSeek-V3 uses self-rewarding, leveraging its own voting results as feedback, leading to significant alignment effects.\", \"Multi-Token Prediction enhances decoding speed, achieving a token acceptance rate of 2.\", \"In open-ended evaluation DeepSeek-V3 shows strong performance against leading models\"]}, {\"chapter_title\": \"6. Conclusion, Limitations, and Future Directions\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter MoE model trained on 14.8T tokens, incorporating MLA, DeepSeekMoE, auxiliary-loss-free load balancing, and a multi-token prediction training objective.\", \"Training cost was 2.788M H800 GPU hours.\", \"DeepSeek-V3 demonstrates performance comparable to leading closed-source models (GPT-4o, Claude-3.5-Sonnet) while remaining cost-effective.\", \"A limitation is the relatively large deployment unit required for efficient inference.\", \"Future research directions include architectural improvements, increased data quantity and quality, enhancement of deep thinking capabilities, and development of more comprehensive evaluation methods.\"]}, {\"chapter_title\": \"References\", \"summaries\": []}, {\"chapter_title\": \"Appendix \", \"summaries\": []}]```\n",
      "* Current Chapter Title: ``A. Contributions and Acknowledgments `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"A. Contributions and Acknowledgments \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Research & Engine   \\nAixin Liu   \\nBing Xue   \\nBingxuan Wang   \\nBochao Wu   \\nChengda Lu   \\nChenggang Zhao   \\nChengqi Deng   \\nChenyu Zhang\\\\*   \\nChong Ruan   \\nDamai Dai   \\nDaya Guo   \\nDejian Yang   \\nDeli Chen   \\nErhang Li   \\nFangyun Lin   \\nFucong Dai   \\nFuli Luo\\\\*   \\nGuangbo Hao   \\nGuanting Chen   \\nGuowei Li   \\nH. Zhang   \\nHan Bao\\\\*   \\nHanwei Xu   \\nHaocheng Wang\\\\*   \\nHaowei Zhang   \\nHonghui Ding   \\nHuajian Xin\\\\*   \\nHuazuo Gao   \\nHui Qu   \\nJianzhong Guo   \\nJiashi Li   \\nJiawei Wang\\\\*   \\nJingchang Chen   \\nJingyang Yuan   \\nJunjie Qiu   \\nJunlong Li   \\nJunxiao Song   \\nKai Dong   \\nKai Hu\\\\*   \\nKaige Gao   \\nKang Guan   \\nKexin Huang   \\nKuai Yu   \\nLean Wang   \\nLecong Zhang   \\nLiang Zhao   \\nLitong Wang   \\nLiyue Zhang   \\nMingchuan Zhang   \\nMinghua Zhang   \\nMinghui Tang   \\nPanpan Huang   \\nPeiyi Wang   \\nQiancheng Wang   \\nQihao Zhu   \\nQinyu Chen   \\nQiushi Du   \\nRuiqi Ge   \\nRuisong Zhang   \\nRuizhe Pan   \\nRunji Wang   \\nRunxin Xu   \\nRuoyu Zhang   \\nShanghao Lu   \\nShangyan Zhou   \\nShanhuang Chen   \\nShengfeng Ye   \\nShirong Ma   \\nShiyu Wang   \\nShuiping Yu   \\nShunfeng Zhou   \\nShuting Pan   \\nTao Yun   \\nTian Pei   \\nWangding Zeng   \\nWanjia Zhao\\\\*   \\nWen Liu   \\nWenfeng Liang   \\nWenjun Gao   \\nWenqin Yu   \\nWentao Zhang   \\nXiao Bi   \\nXiaodong Liu   \\nXiaohan Wang   \\nXiaokang Chen   \\nXiaokang Zhang   \\nXiaotao Nie   \\nXin Cheng   \\nXin Liu   \\nXin Xie   \\nXingchao Liu   \\nXingkai Yu   \\nXinyu Yang   \\nXinyuan Li   \\nXuecheng Su   \\nXuheng Lin   \\nY.K. Li   \\nY.Q. Wang   \\nY.X. Wei   \\nYang Zhang   \\nYanhong Xu   \\nYao Li   \\nYao Zhao   \\nYaofeng Sun   \\nYaohui Wang   \\nYi Yu   \\nYichao Zhang   \\nYifan Shi   \\nYiliang Xiong   \\nYing He   \\nYishi Piao   \\nYisong Wang   \\nYixuan Tan   \\nYiyang Ma\\\\*   \\nYiyuan Liu   \\nYongqiang Guo   \\nYu Wu   \\nYuan Ou   \\nYuduan Wang   \\nYue Gong   \\nYuheng Zou   \\nYujia He   \\nYunfan Xiong   \\nYuxiang Luo   \\nYuxiang You   \\nYuxuan Liu   \\nYuyang Zhou   \\nZ.F. Wu   \\nZ.Z. Ren   \\nZehui Ren   \\nZhangli Sha   \\nZhe Fu   \\nZhean Xu   \\nZhenda Xie   \\nZhengyan Zhan   \\nZhewen Hao   \\nZhibin Gou   \\nZhicheng Ma \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Zhigang Yan Zhihong Shao Zhiyu Wu Zhuoshu Li Zihui Gu Zijia Zhu Zijun Liu\\\\* Zilin Li Ziwei Xie Ziyang Song Ziyi Gao Zizheng Pan \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Data Annotation   \\nBei Feng   \\nHui Li   \\nJ.L. Cai   \\nJiaqi Ni   \\nLei Xu   \\nMeng Li   \\nNing Tian   \\nR.J. Chen   \\nR.L. Jin   \\nRuyi Chen   \\nS.S. Li   \\nShuang Zhou   \\nTianyu Sun   \\nX.Q. Li   \\nXiangyue Jin   \\nXiaojin Shen   \\nXiaosha Chen   \\nXiaowen Sun   \\nXiaoxiang Wang   \\nXinnan Song   \\nXinyi Zhou   \\nY.X. Zhu   \\nYanhong Xu   \\nYanping Huang   \\nYaohui Li   \\nYi Zheng   \\nYuchen Zhu   \\nYunxian Ma   \\nZhen Huang   \\nZhipeng Xu   \\nZhongyu Zhang \"}}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 991841529\n",
      "{'chapter_title': 'A. Contributions and Acknowledgments', 'summaries': ['A comprehensive list of individuals contributed to the research and engine efforts.', 'A separate list acknowledges the contributions of those involved in data annotation.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}, {\"chapter_title\": \"1. Introduction \", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\", \"The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\", \"FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\", \"The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\", \"Training cost was 2.788M H800 GPU hours.\", \"Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\", \"DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\"]}, {\"chapter_title\": \"2. Architecture\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.\", \"An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.\", \"Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.\", \"The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.\", \"During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.\"]}, {\"chapter_title\": \"3. Infrastructures\", \"summaries\": [\"DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.\", \"The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.\", \"DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.\", \"Efficient cross-node all-to-all communication kernels minimize communication overhead.\", \"The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.\", \"FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.\", \"The framework employs tile- and block-wise quantization to mitigate quantization errors.\", \"The inference deployment separates prefilling and decoding stages for improved throughput.\", \"Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.\"]}, {\"chapter_title\": \"4. Pre-Training\", \"summaries\": [\"The pre-training corpus was optimized by enhancing mathematical and programming samples and expanding multilingual coverage.\", \"The data processing pipeline was refined to minimize redundancy while maintaining corpus diversity.\", \"The Prefix-Suffix-Middle (PSM) framework, employing the Fill-in-Middle (FIM) strategy at a rate of 0.1, structures data.\", \"A tokenizer with a 128K token vocabulary was used, with combined punctuation/line break tokens split to mitigate token boundary bias.\", \"Hyperparameters include 61 Transformer layers, a hidden dimension of 3768, and 37B activated parameters per token.\", \"Training involved a multi-stage learning rate schedule and a batch size scheduling strategy.\", \"Long context extension used YaRN to progressively expand the context window to 128K.\", \"The MTP strategy consistently enhances model performance on most benchmarks.\", \"The auxiliary-loss-free balancing strategy demonstrates better model performance compared to purely auxiliary-loss-based methods.\", \"Batch-wise balancing enables greater expert specialization patterns and faces efficiency challenges addressed by the training framework.\"]}, {\"chapter_title\": \"5. Post-Training \", \"summaries\": [\"Curated instruction-tuning datasets include 1.5M instances spanning multiple domains, employing domain-specific data creation methods. Expert models generate reasoning data, balancing accuracy with clarity and conciseness. Supervised Fine-Tuning (SFT) uses system prompts to guide responses and incorporates chain-of-thought patterns. Reinforcement Learning (RL) utilizes Group Relative Policy Optimization (GRPO) to optimize policy models. A rule-based Reward Model (RM) is used for verifiable tasks, while a model-based RM is used for free-form responses, incorporating chain-of-thought into the reward model.\", \"The reward model is trained from DeepSeek-V3 SFT checkpoints to enhance reliability.\", \"DeepSeek-V3 demonstrates competitive performance against leading models like GPT-4o and Claude-3.5-Sonnet in code and math benchmarks.\", \"Distillation from DeepSeek-R1 enhances model performance but increases response length, requiring careful setting of distillation parameters.\", \"DeepSeek-V3 uses self-rewarding, leveraging its own voting results as feedback, leading to significant alignment effects.\", \"Multi-Token Prediction enhances decoding speed, achieving a token acceptance rate of 2.\", \"In open-ended evaluation DeepSeek-V3 shows strong performance against leading models\"]}, {\"chapter_title\": \"6. Conclusion, Limitations, and Future Directions\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter MoE model trained on 14.8T tokens, incorporating MLA, DeepSeekMoE, auxiliary-loss-free load balancing, and a multi-token prediction training objective.\", \"Training cost was 2.788M H800 GPU hours.\", \"DeepSeek-V3 demonstrates performance comparable to leading closed-source models (GPT-4o, Claude-3.5-Sonnet) while remaining cost-effective.\", \"A limitation is the relatively large deployment unit required for efficient inference.\", \"Future research directions include architectural improvements, increased data quantity and quality, enhancement of deep thinking capabilities, and development of more comprehensive evaluation methods.\"]}, {\"chapter_title\": \"References\", \"summaries\": []}, {\"chapter_title\": \"Appendix \", \"summaries\": []}, {\"chapter_title\": \"A. Contributions and Acknowledgments\", \"summaries\": [\"A comprehensive list of individuals contributed to the research and engine efforts.\", \"A separate list acknowledges the contributions of those involved in data annotation.\"]}]```\n",
      "* Current Chapter Title: ``Business & Compliance Dongjie Ji `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"Business & Compliance Dongjie Ji \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Jian Liang   \\nJin Chen   \\nLeyi Xia   \\nMiaojun Wang Mingming Li Peng Zhang   \\nShaoqing Wu Shengfeng Ye T. Wang \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"W.L. Xiao Wei An Xianzu Wang Xinxia Shan Ying Tang Yukun Zha Yuting Yan Zhen Zhang \"}}, {\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"Within each role, authors are listed alphabetically by the first name. Names marked with \\\\* denote individuals who have departed from our team. \"}}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 849344994\n",
      "{'chapter_title': 'Business & Compliance Dongjie Ji', 'summaries': ['Lists contributors to the business and compliance aspects of the project, organized by role.', 'Contributors are alphabetized by first name within each role.', 'Individuals who have left the team are indicated with an asterisk (*).']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}, {\"chapter_title\": \"1. Introduction \", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\", \"The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\", \"FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\", \"The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\", \"Training cost was 2.788M H800 GPU hours.\", \"Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\", \"DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\"]}, {\"chapter_title\": \"2. Architecture\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.\", \"An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.\", \"Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.\", \"The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.\", \"During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.\"]}, {\"chapter_title\": \"3. Infrastructures\", \"summaries\": [\"DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.\", \"The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.\", \"DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.\", \"Efficient cross-node all-to-all communication kernels minimize communication overhead.\", \"The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.\", \"FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.\", \"The framework employs tile- and block-wise quantization to mitigate quantization errors.\", \"The inference deployment separates prefilling and decoding stages for improved throughput.\", \"Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.\"]}, {\"chapter_title\": \"4. Pre-Training\", \"summaries\": [\"The pre-training corpus was optimized by enhancing mathematical and programming samples and expanding multilingual coverage.\", \"The data processing pipeline was refined to minimize redundancy while maintaining corpus diversity.\", \"The Prefix-Suffix-Middle (PSM) framework, employing the Fill-in-Middle (FIM) strategy at a rate of 0.1, structures data.\", \"A tokenizer with a 128K token vocabulary was used, with combined punctuation/line break tokens split to mitigate token boundary bias.\", \"Hyperparameters include 61 Transformer layers, a hidden dimension of 3768, and 37B activated parameters per token.\", \"Training involved a multi-stage learning rate schedule and a batch size scheduling strategy.\", \"Long context extension used YaRN to progressively expand the context window to 128K.\", \"The MTP strategy consistently enhances model performance on most benchmarks.\", \"The auxiliary-loss-free balancing strategy demonstrates better model performance compared to purely auxiliary-loss-based methods.\", \"Batch-wise balancing enables greater expert specialization patterns and faces efficiency challenges addressed by the training framework.\"]}, {\"chapter_title\": \"5. Post-Training \", \"summaries\": [\"Curated instruction-tuning datasets include 1.5M instances spanning multiple domains, employing domain-specific data creation methods. Expert models generate reasoning data, balancing accuracy with clarity and conciseness. Supervised Fine-Tuning (SFT) uses system prompts to guide responses and incorporates chain-of-thought patterns. Reinforcement Learning (RL) utilizes Group Relative Policy Optimization (GRPO) to optimize policy models. A rule-based Reward Model (RM) is used for verifiable tasks, while a model-based RM is used for free-form responses, incorporating chain-of-thought into the reward model.\", \"The reward model is trained from DeepSeek-V3 SFT checkpoints to enhance reliability.\", \"DeepSeek-V3 demonstrates competitive performance against leading models like GPT-4o and Claude-3.5-Sonnet in code and math benchmarks.\", \"Distillation from DeepSeek-R1 enhances model performance but increases response length, requiring careful setting of distillation parameters.\", \"DeepSeek-V3 uses self-rewarding, leveraging its own voting results as feedback, leading to significant alignment effects.\", \"Multi-Token Prediction enhances decoding speed, achieving a token acceptance rate of 2.\", \"In open-ended evaluation DeepSeek-V3 shows strong performance against leading models\"]}, {\"chapter_title\": \"6. Conclusion, Limitations, and Future Directions\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter MoE model trained on 14.8T tokens, incorporating MLA, DeepSeekMoE, auxiliary-loss-free load balancing, and a multi-token prediction training objective.\", \"Training cost was 2.788M H800 GPU hours.\", \"DeepSeek-V3 demonstrates performance comparable to leading closed-source models (GPT-4o, Claude-3.5-Sonnet) while remaining cost-effective.\", \"A limitation is the relatively large deployment unit required for efficient inference.\", \"Future research directions include architectural improvements, increased data quantity and quality, enhancement of deep thinking capabilities, and development of more comprehensive evaluation methods.\"]}, {\"chapter_title\": \"References\", \"summaries\": []}, {\"chapter_title\": \"Appendix \", \"summaries\": []}, {\"chapter_title\": \"A. Contributions and Acknowledgments\", \"summaries\": [\"A comprehensive list of individuals contributed to the research and engine efforts.\", \"A separate list acknowledges the contributions of those involved in data annotation.\"]}, {\"chapter_title\": \"Business & Compliance Dongjie Ji\", \"summaries\": [\"Lists contributors to the business and compliance aspects of the project, organized by role.\", \"Contributors are alphabetized by first name within each role.\", \"Individuals who have left the team are indicated with an asterisk (*).\"]}]```\n",
      "* Current Chapter Title: ``B. Ablation Studies for Low-Precision Training `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"B. Ablation Studies for Low-Precision Training \"}, \"children\": [{\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/b0ad5ed91f42c8d54b3c818553783cbd94b8e018e59a93200aea26c539e58a1d.jpg\", \"img_caption\": [\"Figure 10 | Loss curves comparison between BF16 and FP8 training. Results are smoothed by Exponential Moving Average (EMA) with a coefficient of 0.9. \"]}}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"B.1. FP8 v.s. BF16 Training \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"We validate our FP8 mixed precision framework with a comparison to BF16 training on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising approximately 16B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising approximately 230B total parameters on around $0.9\\\\mathrm{T}$ tokens. We show the training curves in Figure 10 and demonstrate that the relative error remains below $0.25\\\\%$ with our high-precision accumulation and fine-grained quantization strategies. \"}}]}, {\"type\": \"chapter\", \"level\": 1, \"data\": {\"text\": \"B.2. Discussion About Block-Wise Quantization \"}, \"children\": [{\"type\": \"text\", \"level\": 2, \"data\": {\"text\": \"Although our tile-wise fine-grained quantization effectively mitigates the error introduced by feature outliers, it requires different groupings for activation quantization, i.e., $\\\\mathtt{1x128}$ in forward pass and $128\\\\mathbf{x}\\\\mathbf{1}$ for backward pass. A similar process is also required for the activation gradient. A straightforward strategy is to apply block-wise quantization per $128\\\\mathrm{x}128$ elements like the way we quantize the model weights. In this way, only transposition is required for backward. Therefore, we conduct an experiment where all tensors associated with Dgrad are quantized on a block-wise basis. The results reveal that the Dgrad operation which computes the activation gradients and back-propagates to shallow layers in a chain-like manner, is highly sensitive to precision. Specifically, block-wise quantization of activation gradients leads to model divergence on an MoE model comprising approximately 16B total parameters, trained for around 300B tokens. We hypothesize that this sensitivity arises because activation gradients are highly imbalanced among tokens, resulting in token-correlated outliers (Xi et al., 2023). These outliers cannot be effectively managed by a block-wise quantization approach. \"}}]}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 462353241\n",
      "{'chapter_title': 'B. Ablation Studies for Low-Precision Training', 'summaries': ['FP8 mixed precision training was validated against BF16 training on baseline models of varying scales (16B and 230B parameters).', 'Relative error remained below 0.25% with high-precision accumulation and fine-grained quantization.', 'Block-wise quantization of activation gradients led to model divergence, likely due to sensitivity to imbalanced activation gradients and token-correlated outliers.']}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "**Role:** Expert Knowledge Distiller.\n",
      "\n",
      "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
      "\n",
      "**CRITICAL Instructions:**\n",
      "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
      "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
      "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
      "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
      "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
      "6.  **Output Format:**\n",
      "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
      "    * Otherwise, return a list of concise strings in `summaries`.\n",
      "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
      "\n",
      "**Context:**\n",
      "* Document Hierarchy: ```[{\"DeepSeek-V3 Technical Report \": []}, {\"Abstract \": []}, {\"Contents \": []}, {\"1 Introduction 4 \": []}, {\"2 Architecture 6 \": []}, {\"3 Infrastructures 11 \": []}, {\"4 Pre-Training 21 \": [{\"4.1 Data Construction . 21 \": []}]}, {\"6 Conclusion, Limitations, and Future Directions 35 \": []}, {\"1. Introduction \": [{\"Architecture: Innovative Load Balancing Strategy and Training Objective \": []}, {\"Pre-Training: Towards Ultimate Training Efficiency \": []}, {\"Post-Training: Knowledge Distillation from DeepSeek-R1 \": []}, {\"Summary of Core Evaluation Results \": []}]}, {\"2. Architecture \": [{\"2.1. Basic Architecture \": [{\"2.1.1. Multi-Head Latent Attention \": []}, {\"2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing \": []}]}, {\"2.2. Multi-Token Prediction \": []}]}, {\"3. Infrastructures \": [{\"3.1. Compute Clusters \": []}, {\"3.2. Training Framework \": [{\"3.2.1. DualPipe and Computation-Communication Overlap \": []}, {\"3.2.2. Efficient Implementation of Cross-Node All-to-All Communication \": []}, {\"3.2.3. Extremely Memory Saving with Minimal Overhead \": []}]}, {\"3.3. FP8 Training \": [{\"3.3.1. Mixed Precision Framework \": []}, {\"3.3.2. Improved Precision from Quantization and Multiplication \": []}, {\"3.3.3. Low-Precision Storage and Communication \": []}]}, {\"3.4. Inference and Deployment \": [{\"3.4.1. Prefilling \": []}, {\"3.4.2. Decoding \": []}]}, {\"3.5. Suggestions on Hardware Design \": [{\"3.5.1. Communication Hardware \": []}, {\"3.5.2. Compute Hardware \": []}]}]}, {\"4. Pre-Training \": [{\"4.1. Data Construction \": []}, {\"4.2. Hyper-Parameters \": []}, {\"4.3. Long Context Extension \": []}, {\"4.4. Evaluations \": [{\"4.4.1. Evaluation Benchmarks \": []}, {\"4.4.2. Evaluation Results \": []}]}, {\"4.5. Discussion \": [{\"4.5.1. Ablation Studies for Multi-Token Prediction \": []}, {\"4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy \": []}, {\"4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance \": []}]}]}, {\"5. Post-Training \": [{\"5.1. Supervised Fine-Tuning \": []}, {\"5.2. Reinforcement Learning \": [{\"5.2.1. Reward Model \": []}, {\"5.2.2. Group Relative Policy Optimization \": []}]}, {\"5.3. Evaluations \": [{\"5.3.1. Evaluation Settings \": []}, {\"5.3.2. Standard Evaluation \": []}, {\"5.3.3. Open-Ended Evaluation \": []}, {\"5.3.4. DeepSeek-V3 as a Generative Reward Model \": []}]}, {\"5.4. Discussion \": [{\"5.4.1. Distillation from DeepSeek-R1 \": []}, {\"5.4.2. Self-Rewarding \": []}, {\"5.4.3. Multi-Token Prediction Evaluation \": []}]}]}, {\"6. Conclusion, Limitations, and Future Directions \": []}, {\"References \": []}, {\"Appendix \": []}, {\"A. Contributions and Acknowledgments \": []}, {\"Business & Compliance Dongjie Ji \": []}, {\"B. Ablation Studies for Low-Precision Training \": [{\"B.1. FP8 v.s. BF16 Training \": []}, {\"B.2. Discussion About Block-Wise Quantization \": []}]}, {\"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \": []}]``` (Provides background context only)\n",
      "* Previous Chapter Summaries: ```[{\"chapter_title\": \"DeepSeek-V3 Technical Report \", \"summaries\": []}, {\"chapter_title\": \"Abstract\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\", \"The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\", \"Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\", \"Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\", \"Model checkpoints are available on GitHub.\"]}, {\"chapter_title\": \"Contents \", \"summaries\": []}, {\"chapter_title\": \"1 Introduction 4 \", \"summaries\": []}, {\"chapter_title\": \"2 Architecture 6\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA).\", \"DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\", \"The training objective includes multi-token prediction.\"]}, {\"chapter_title\": \"3 Infrastructures 11\", \"summaries\": [\"DualPipe enables computation-communication overlap.\", \"An efficient cross-node all-to-all communication implementation was developed.\", \"The training framework prioritizes extreme memory saving with minimal overhead.\", \"A mixed-precision framework (FP8) improves precision through quantization and multiplication.\", \"Low-precision storage and communication are employed.\", \"Inference includes prefilling and decoding steps.\", \"Hardware design suggestions include considerations for communication and compute hardware.\"]}, {\"chapter_title\": \"4 Pre-Training 21\", \"summaries\": [\"The pre-training process involved constructing a dataset and optimizing hyperparameters.\", \"A long context extension technique was employed.\", \"Evaluations were conducted using various benchmarks, revealing performance results.\", \"Ablation studies explored the impact of multi-token prediction.\", \"Ablation studies examined the auxiliary-loss-free balancing strategy.\", \"The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\"]}, {\"chapter_title\": \"6 Conclusion, Limitations, and Future Directions 35\", \"summaries\": []}, {\"chapter_title\": \"1. Introduction \", \"summaries\": [\"DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\", \"The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\", \"FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\", \"The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\", \"DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\", \"Training cost was 2.788M H800 GPU hours.\", \"Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\", \"DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\"]}, {\"chapter_title\": \"2. Architecture\", \"summaries\": [\"The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.\", \"An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.\", \"Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.\", \"The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.\", \"During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.\"]}, {\"chapter_title\": \"3. Infrastructures\", \"summaries\": [\"DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.\", \"The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.\", \"DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.\", \"Efficient cross-node all-to-all communication kernels minimize communication overhead.\", \"The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.\", \"FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.\", \"The framework employs tile- and block-wise quantization to mitigate quantization errors.\", \"The inference deployment separates prefilling and decoding stages for improved throughput.\", \"Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.\"]}, {\"chapter_title\": \"4. Pre-Training\", \"summaries\": [\"The pre-training corpus was optimized by enhancing mathematical and programming samples and expanding multilingual coverage.\", \"The data processing pipeline was refined to minimize redundancy while maintaining corpus diversity.\", \"The Prefix-Suffix-Middle (PSM) framework, employing the Fill-in-Middle (FIM) strategy at a rate of 0.1, structures data.\", \"A tokenizer with a 128K token vocabulary was used, with combined punctuation/line break tokens split to mitigate token boundary bias.\", \"Hyperparameters include 61 Transformer layers, a hidden dimension of 3768, and 37B activated parameters per token.\", \"Training involved a multi-stage learning rate schedule and a batch size scheduling strategy.\", \"Long context extension used YaRN to progressively expand the context window to 128K.\", \"The MTP strategy consistently enhances model performance on most benchmarks.\", \"The auxiliary-loss-free balancing strategy demonstrates better model performance compared to purely auxiliary-loss-based methods.\", \"Batch-wise balancing enables greater expert specialization patterns and faces efficiency challenges addressed by the training framework.\"]}, {\"chapter_title\": \"5. Post-Training \", \"summaries\": [\"Curated instruction-tuning datasets include 1.5M instances spanning multiple domains, employing domain-specific data creation methods. Expert models generate reasoning data, balancing accuracy with clarity and conciseness. Supervised Fine-Tuning (SFT) uses system prompts to guide responses and incorporates chain-of-thought patterns. Reinforcement Learning (RL) utilizes Group Relative Policy Optimization (GRPO) to optimize policy models. A rule-based Reward Model (RM) is used for verifiable tasks, while a model-based RM is used for free-form responses, incorporating chain-of-thought into the reward model.\", \"The reward model is trained from DeepSeek-V3 SFT checkpoints to enhance reliability.\", \"DeepSeek-V3 demonstrates competitive performance against leading models like GPT-4o and Claude-3.5-Sonnet in code and math benchmarks.\", \"Distillation from DeepSeek-R1 enhances model performance but increases response length, requiring careful setting of distillation parameters.\", \"DeepSeek-V3 uses self-rewarding, leveraging its own voting results as feedback, leading to significant alignment effects.\", \"Multi-Token Prediction enhances decoding speed, achieving a token acceptance rate of 2.\", \"In open-ended evaluation DeepSeek-V3 shows strong performance against leading models\"]}, {\"chapter_title\": \"6. Conclusion, Limitations, and Future Directions\", \"summaries\": [\"DeepSeek-V3 is a 671B parameter MoE model trained on 14.8T tokens, incorporating MLA, DeepSeekMoE, auxiliary-loss-free load balancing, and a multi-token prediction training objective.\", \"Training cost was 2.788M H800 GPU hours.\", \"DeepSeek-V3 demonstrates performance comparable to leading closed-source models (GPT-4o, Claude-3.5-Sonnet) while remaining cost-effective.\", \"A limitation is the relatively large deployment unit required for efficient inference.\", \"Future research directions include architectural improvements, increased data quantity and quality, enhancement of deep thinking capabilities, and development of more comprehensive evaluation methods.\"]}, {\"chapter_title\": \"References\", \"summaries\": []}, {\"chapter_title\": \"Appendix \", \"summaries\": []}, {\"chapter_title\": \"A. Contributions and Acknowledgments\", \"summaries\": [\"A comprehensive list of individuals contributed to the research and engine efforts.\", \"A separate list acknowledges the contributions of those involved in data annotation.\"]}, {\"chapter_title\": \"Business & Compliance Dongjie Ji\", \"summaries\": [\"Lists contributors to the business and compliance aspects of the project, organized by role.\", \"Contributors are alphabetized by first name within each role.\", \"Individuals who have left the team are indicated with an asterisk (*).\"]}, {\"chapter_title\": \"B. Ablation Studies for Low-Precision Training\", \"summaries\": [\"FP8 mixed precision training was validated against BF16 training on baseline models of varying scales (16B and 230B parameters).\", \"Relative error remained below 0.25% with high-precision accumulation and fine-grained quantization.\", \"Block-wise quantization of activation gradients led to model divergence, likely due to sensitivity to imbalanced activation gradients and token-correlated outliers.\"]}]```\n",
      "* Current Chapter Title: ``C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models `` (Use this exact title in output)\n",
      "\n",
      "**Current Chapter Content to Distill:**\n",
      "```{\"type\": \"chapter\", \"level\": 0, \"data\": {\"text\": \"C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models \"}, \"children\": [{\"type\": \"text\", \"level\": 1, \"data\": {\"text\": \"We record the expert load of the 16B auxiliary-loss-based baseline and the auxiliary-loss-free model on the Pile test set. The auxiliary-loss-free model tends to have greater expert specialization across all layers, as demonstrated in Figure 10. \"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/df414ba6ec58ecdea8399c32970505fd62815fab4090c286317901838eb29684.jpg\"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/ce6b240feda49244a4a0806c8852a8a67ba9428299564763e225999cf132ff86.jpg\"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/3883c917c521a40a6dc026f2ea4a5be3fcce25df9d142f0ea4eb9ec79aaed32a.jpg\"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/935ea04ae9b644bdc56695b8bee3a4a02c590d8ad7e185c711726649f5a80605.jpg\"}}, {\"type\": \"image\", \"level\": 1, \"data\": {\"img_path\": \"./arxiv_downloads_processed\\\\2412.19437_deepseek-v3_technical_report\\\\imgs/aac7de5cd380235acb5843d45956d36baf8f6c532611c0bee9a72195ee967845.jpg\", \"img_caption\": [\"Figure 10 | Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. \"]}}]}```\n",
      "\n",
      "Generate the JSON output conforming to the ChapterSummary structure.\n",
      "\n",
      "----------------------------------------\n",
      "Context length: 16384 / Seed: 888676778\n",
      "{'chapter_title': 'C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models', 'summaries': ['The auxiliary-loss-free model exhibits greater expert specialization patterns across all layers compared to the auxiliary-loss-based model, as observed on the Pile test set.', 'Relative expert load, representing the ratio between actual and theoretically balanced load, demonstrates this specialization.']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'chapter_title': 'DeepSeek-V3 Technical Report ', 'summaries': []},\n",
       " {'chapter_title': 'Abstract',\n",
       "  'summaries': ['DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.',\n",
       "   'The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.',\n",
       "   'DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.',\n",
       "   'Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.',\n",
       "   'Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.',\n",
       "   'Model checkpoints are available on GitHub.']},\n",
       " {'chapter_title': 'Contents ', 'summaries': []},\n",
       " {'chapter_title': '1 Introduction 4 ', 'summaries': []},\n",
       " {'chapter_title': '2 Architecture 6',\n",
       "  'summaries': ['The architecture incorporates Multi-Head Latent Attention (MLA).',\n",
       "   'DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.',\n",
       "   'The training objective includes multi-token prediction.']},\n",
       " {'chapter_title': '3 Infrastructures 11',\n",
       "  'summaries': ['DualPipe enables computation-communication overlap.',\n",
       "   'An efficient cross-node all-to-all communication implementation was developed.',\n",
       "   'The training framework prioritizes extreme memory saving with minimal overhead.',\n",
       "   'A mixed-precision framework (FP8) improves precision through quantization and multiplication.',\n",
       "   'Low-precision storage and communication are employed.',\n",
       "   'Inference includes prefilling and decoding steps.',\n",
       "   'Hardware design suggestions include considerations for communication and compute hardware.']},\n",
       " {'chapter_title': '4 Pre-Training 21',\n",
       "  'summaries': ['The pre-training process involved constructing a dataset and optimizing hyperparameters.',\n",
       "   'A long context extension technique was employed.',\n",
       "   'Evaluations were conducted using various benchmarks, revealing performance results.',\n",
       "   'Ablation studies explored the impact of multi-token prediction.',\n",
       "   'Ablation studies examined the auxiliary-loss-free balancing strategy.',\n",
       "   'The effectiveness of batch-wise versus sequence-wise load balancing was investigated.']},\n",
       " {'chapter_title': '6 Conclusion, Limitations, and Future Directions 35',\n",
       "  'summaries': []},\n",
       " {'chapter_title': '1. Introduction ',\n",
       "  'summaries': ['DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.',\n",
       "   'The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.',\n",
       "   'FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.',\n",
       "   'The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.',\n",
       "   'DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.',\n",
       "   'Training cost was 2.788M H800 GPU hours.',\n",
       "   'Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.',\n",
       "   'DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.']},\n",
       " {'chapter_title': '2. Architecture',\n",
       "  'summaries': ['The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.',\n",
       "   'An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.',\n",
       "   'Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.',\n",
       "   'The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.',\n",
       "   'During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.']},\n",
       " {'chapter_title': '3. Infrastructures',\n",
       "  'summaries': ['DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.',\n",
       "   'The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.',\n",
       "   'DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.',\n",
       "   'Efficient cross-node all-to-all communication kernels minimize communication overhead.',\n",
       "   'The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.',\n",
       "   'FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.',\n",
       "   'The framework employs tile- and block-wise quantization to mitigate quantization errors.',\n",
       "   'The inference deployment separates prefilling and decoding stages for improved throughput.',\n",
       "   'Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.']},\n",
       " {'chapter_title': '4. Pre-Training',\n",
       "  'summaries': ['The pre-training corpus was optimized by enhancing mathematical and programming samples and expanding multilingual coverage.',\n",
       "   'The data processing pipeline was refined to minimize redundancy while maintaining corpus diversity.',\n",
       "   'The Prefix-Suffix-Middle (PSM) framework, employing the Fill-in-Middle (FIM) strategy at a rate of 0.1, structures data.',\n",
       "   'A tokenizer with a 128K token vocabulary was used, with combined punctuation/line break tokens split to mitigate token boundary bias.',\n",
       "   'Hyperparameters include 61 Transformer layers, a hidden dimension of 3768, and 37B activated parameters per token.',\n",
       "   'Training involved a multi-stage learning rate schedule and a batch size scheduling strategy.',\n",
       "   'Long context extension used YaRN to progressively expand the context window to 128K.',\n",
       "   'The MTP strategy consistently enhances model performance on most benchmarks.',\n",
       "   'The auxiliary-loss-free balancing strategy demonstrates better model performance compared to purely auxiliary-loss-based methods.',\n",
       "   'Batch-wise balancing enables greater expert specialization patterns and faces efficiency challenges addressed by the training framework.']},\n",
       " {'chapter_title': '5. Post-Training ',\n",
       "  'summaries': ['Curated instruction-tuning datasets include 1.5M instances spanning multiple domains, employing domain-specific data creation methods. Expert models generate reasoning data, balancing accuracy with clarity and conciseness. Supervised Fine-Tuning (SFT) uses system prompts to guide responses and incorporates chain-of-thought patterns. Reinforcement Learning (RL) utilizes Group Relative Policy Optimization (GRPO) to optimize policy models. A rule-based Reward Model (RM) is used for verifiable tasks, while a model-based RM is used for free-form responses, incorporating chain-of-thought into the reward model.',\n",
       "   'The reward model is trained from DeepSeek-V3 SFT checkpoints to enhance reliability.',\n",
       "   'DeepSeek-V3 demonstrates competitive performance against leading models like GPT-4o and Claude-3.5-Sonnet in code and math benchmarks.',\n",
       "   'Distillation from DeepSeek-R1 enhances model performance but increases response length, requiring careful setting of distillation parameters.',\n",
       "   'DeepSeek-V3 uses self-rewarding, leveraging its own voting results as feedback, leading to significant alignment effects.',\n",
       "   'Multi-Token Prediction enhances decoding speed, achieving a token acceptance rate of 2.',\n",
       "   'In open-ended evaluation DeepSeek-V3 shows strong performance against leading models']},\n",
       " {'chapter_title': '6. Conclusion, Limitations, and Future Directions',\n",
       "  'summaries': ['DeepSeek-V3 is a 671B parameter MoE model trained on 14.8T tokens, incorporating MLA, DeepSeekMoE, auxiliary-loss-free load balancing, and a multi-token prediction training objective.',\n",
       "   'Training cost was 2.788M H800 GPU hours.',\n",
       "   'DeepSeek-V3 demonstrates performance comparable to leading closed-source models (GPT-4o, Claude-3.5-Sonnet) while remaining cost-effective.',\n",
       "   'A limitation is the relatively large deployment unit required for efficient inference.',\n",
       "   'Future research directions include architectural improvements, increased data quantity and quality, enhancement of deep thinking capabilities, and development of more comprehensive evaluation methods.']},\n",
       " {'chapter_title': 'References', 'summaries': []},\n",
       " {'chapter_title': 'Appendix ', 'summaries': []},\n",
       " {'chapter_title': 'A. Contributions and Acknowledgments',\n",
       "  'summaries': ['A comprehensive list of individuals contributed to the research and engine efforts.',\n",
       "   'A separate list acknowledges the contributions of those involved in data annotation.']},\n",
       " {'chapter_title': 'Business & Compliance Dongjie Ji',\n",
       "  'summaries': ['Lists contributors to the business and compliance aspects of the project, organized by role.',\n",
       "   'Contributors are alphabetized by first name within each role.',\n",
       "   'Individuals who have left the team are indicated with an asterisk (*).']},\n",
       " {'chapter_title': 'B. Ablation Studies for Low-Precision Training',\n",
       "  'summaries': ['FP8 mixed precision training was validated against BF16 training on baseline models of varying scales (16B and 230B parameters).',\n",
       "   'Relative error remained below 0.25% with high-precision accumulation and fine-grained quantization.',\n",
       "   'Block-wise quantization of activation gradients led to model divergence, likely due to sensitivity to imbalanced activation gradients and token-correlated outliers.']},\n",
       " {'chapter_title': 'C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models',\n",
       "  'summaries': ['The auxiliary-loss-free model exhibits greater expert specialization patterns across all layers compared to the auxiliary-loss-based model, as observed on the Pile test set.',\n",
       "   'Relative expert load, representing the ratio between actual and theoretically balanced load, demonstrates this specialization.']}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pydantic\n",
    "import ollama\n",
    "import random\n",
    "\n",
    "class ChapterSummary(pydantic.BaseModel):\n",
    "    chapter_title: str\n",
    "    summaries: list[str]\n",
    "\n",
    "\n",
    "def summarize_doc(doc: Content, llm_model=\"mistral-small3.1\", min_tokens=4096):\n",
    "    hierarchy = get_content_hierarchy(doc)\n",
    "\n",
    "    summaries = []\n",
    "    for chapter in doc.children:\n",
    "        assert chapter.type == \"chapter\", \"Chapter type mismatch\"\n",
    "\n",
    "        if len(chapter.children) == 0:\n",
    "            summaries.append(\n",
    "                ChapterSummary(\n",
    "                    chapter_title=chapter.data[\"text\"],\n",
    "                    summaries=[],\n",
    "                ).__dict__\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "**Role:** Expert Knowledge Distiller.\n",
    "\n",
    "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
    "\n",
    "**CRITICAL Instructions:**\n",
    "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
    "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
    "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
    "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
    "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
    "6.  **Output Format:**\n",
    "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
    "    * Otherwise, return a list of concise strings in `summaries`.\n",
    "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
    "\n",
    "**Context:**\n",
    "* Document Hierarchy: ```{json.dumps(hierarchy)}``` (Provides background context only)\n",
    "* Previous Chapter Summaries: ```{json.dumps(summaries)}```\n",
    "* Current Chapter Title: ``{chapter.data['text']}`` (Use this exact title in output)\n",
    "\n",
    "**Current Chapter Content to Distill:**\n",
    "```{printable_content(chapter, indent=None)}```\n",
    "\n",
    "Generate the JSON output conforming to the ChapterSummary structure.\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            print(\"-\"*80)\n",
    "            print(prompt)\n",
    "            print(\"-\"*40)\n",
    "            context_length = max(min_tokens, len(prompt.split()) * 3)\n",
    "            seed = random.randint(0, 2**30 - 1)\n",
    "            print(f\"Context length: {context_length} / Seed: {seed}\")\n",
    "            format = ChapterSummary.model_json_schema()\n",
    "            response = ollama.generate(\n",
    "                llm_model,\n",
    "                prompt,\n",
    "                format=format,\n",
    "                options={\n",
    "                    \"num_ctx\": context_length,\n",
    "                    \"seed\": seed,\n",
    "                },\n",
    "            )\n",
    "            response = ChapterSummary.model_validate_json(response[\"response\"])\n",
    "            print(response.__dict__)\n",
    "\n",
    "            assert response is not None, \"Response is None\"\n",
    "            assert response.chapter_title.lower().strip() == chapter.data[\"text\"].lower().strip(), \"Chapter title mismatch\"\n",
    "\n",
    "            summaries.append(response.__dict__)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "        \n",
    "    return summaries\n",
    "\n",
    "summary = summarize_doc(doc, llm_model='gemma3:12b', min_tokens=16384)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Abstract\n",
      " - DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) language model with 37B activated per token.\n",
      " - The architecture incorporates Multi-head Latent Attention (MLA) and DeepSeekMoE, and utilizes an auxiliary-loss-free load balancing strategy with a multi-token prediction training objective.\n",
      " - DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned using Supervised Fine-Tuning and Reinforcement Learning.\n",
      " - Evaluations show DeepSeek-V3 outperforms other open-source models and rivals leading closed-source models.\n",
      " - Training required 2.788M H800 GPU hours and exhibited remarkable stability without significant loss spikes or rollbacks.\n",
      " - Model checkpoints are available on GitHub.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Contents \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 Introduction 4 \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 Architecture 6\n",
      " - The architecture incorporates Multi-Head Latent Attention (MLA).\n",
      " - DeepSeekMoE utilizes an auxiliary-loss-free load balancing strategy.\n",
      " - The training objective includes multi-token prediction.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 Infrastructures 11\n",
      " - DualPipe enables computation-communication overlap.\n",
      " - An efficient cross-node all-to-all communication implementation was developed.\n",
      " - The training framework prioritizes extreme memory saving with minimal overhead.\n",
      " - A mixed-precision framework (FP8) improves precision through quantization and multiplication.\n",
      " - Low-precision storage and communication are employed.\n",
      " - Inference includes prefilling and decoding steps.\n",
      " - Hardware design suggestions include considerations for communication and compute hardware.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 Pre-Training 21\n",
      " - The pre-training process involved constructing a dataset and optimizing hyperparameters.\n",
      " - A long context extension technique was employed.\n",
      " - Evaluations were conducted using various benchmarks, revealing performance results.\n",
      " - Ablation studies explored the impact of multi-token prediction.\n",
      " - Ablation studies examined the auxiliary-loss-free balancing strategy.\n",
      " - The effectiveness of batch-wise versus sequence-wise load balancing was investigated.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 Conclusion, Limitations, and Future Directions 35\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1. Introduction \n",
      " - DeepSeek-V3 is a 671B parameter Mixture-of-Experts (MoE) model with 37B parameters activated per token.\n",
      " - The architecture incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE, utilizing an auxiliary-loss-free load balancing strategy and a multi-token prediction training objective.\n",
      " - FP8 mixed precision training is implemented to accelerate training and reduce GPU memory usage.\n",
      " - The training framework includes DualPipe for computation-communication overlap and efficient cross-node all-to-all communication.\n",
      " - DeepSeek-V3 was pre-trained on 14.8 trillion tokens and achieves state-of-the-art performance on various benchmarks, rivaling leading closed-source models.\n",
      " - Training cost was 2.788M H800 GPU hours.\n",
      " - Main contributions include the auxiliary-loss-free load balancing strategy, Multi-Token Prediction training objective, FP8 mixed precision training, and knowledge distillation from DeepSeek-R1.\n",
      " - DeepSeek-V3 achieves strong performance on educational (e.g., MMLU), factual knowledge, coding (LiveCodeBench), and math benchmarks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2. Architecture\n",
      " - The architecture incorporates Multi-Head Latent Attention (MLA) for efficient inference and DeepSeekMoE for economical training.\n",
      " - An auxiliary-loss-free load balancing strategy is utilized for DeepSeekMoE to mitigate performance degradation.\n",
      " - Multi-Token Prediction (MTP) extends the prediction scope to multiple future tokens at each position, densifying training signals and potentially improving data efficiency.\n",
      " - The MTP implementation maintains the causal chain of predictions, leveraging shared embedding and output heads with the main model.\n",
      " - During inference, MTP modules can be discarded, and the main model functions normally or can be repurposed for speculative decoding.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3. Infrastructures\n",
      " - DeepSeek-V3 was trained on a cluster with 2048 NVIDIA H800 GPUs, interconnected via NVLink and InfiniBand.\n",
      " - The training framework uses 16-way Pipeline Parallelism, 64-way Expert Parallelism, and ZeRO-1 Data Parallelism.\n",
      " - DualPipe algorithm overlaps computation and communication phases to reduce pipeline bubbles.\n",
      " - Efficient cross-node all-to-all communication kernels minimize communication overhead.\n",
      " - The training framework prioritizes memory saving techniques, including recomputation of RMSNorm and shared embedding/output heads.\n",
      " - FP8 training utilizes a mixed-precision framework to improve training speed and reduce memory usage.\n",
      " - The framework employs tile- and block-wise quantization to mitigate quantization errors.\n",
      " - The inference deployment separates prefilling and decoding stages for improved throughput.\n",
      " - Suggestions for hardware design include improvements to communication and compute hardware, particularly focusing on enhancing FP8 GEMM accumulation precision and supporting online quantization.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4. Pre-Training\n",
      " - The pre-training corpus was optimized by enhancing mathematical and programming samples and expanding multilingual coverage.\n",
      " - The data processing pipeline was refined to minimize redundancy while maintaining corpus diversity.\n",
      " - The Prefix-Suffix-Middle (PSM) framework, employing the Fill-in-Middle (FIM) strategy at a rate of 0.1, structures data.\n",
      " - A tokenizer with a 128K token vocabulary was used, with combined punctuation/line break tokens split to mitigate token boundary bias.\n",
      " - Hyperparameters include 61 Transformer layers, a hidden dimension of 3768, and 37B activated parameters per token.\n",
      " - Training involved a multi-stage learning rate schedule and a batch size scheduling strategy.\n",
      " - Long context extension used YaRN to progressively expand the context window to 128K.\n",
      " - The MTP strategy consistently enhances model performance on most benchmarks.\n",
      " - The auxiliary-loss-free balancing strategy demonstrates better model performance compared to purely auxiliary-loss-based methods.\n",
      " - Batch-wise balancing enables greater expert specialization patterns and faces efficiency challenges addressed by the training framework.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5. Post-Training \n",
      " - Curated instruction-tuning datasets include 1.5M instances spanning multiple domains, employing domain-specific data creation methods. Expert models generate reasoning data, balancing accuracy with clarity and conciseness. Supervised Fine-Tuning (SFT) uses system prompts to guide responses and incorporates chain-of-thought patterns. Reinforcement Learning (RL) utilizes Group Relative Policy Optimization (GRPO) to optimize policy models. A rule-based Reward Model (RM) is used for verifiable tasks, while a model-based RM is used for free-form responses, incorporating chain-of-thought into the reward model.\n",
      " - The reward model is trained from DeepSeek-V3 SFT checkpoints to enhance reliability.\n",
      " - DeepSeek-V3 demonstrates competitive performance against leading models like GPT-4o and Claude-3.5-Sonnet in code and math benchmarks.\n",
      " - Distillation from DeepSeek-R1 enhances model performance but increases response length, requiring careful setting of distillation parameters.\n",
      " - DeepSeek-V3 uses self-rewarding, leveraging its own voting results as feedback, leading to significant alignment effects.\n",
      " - Multi-Token Prediction enhances decoding speed, achieving a token acceptance rate of 2.\n",
      " - In open-ended evaluation DeepSeek-V3 shows strong performance against leading models\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6. Conclusion, Limitations, and Future Directions\n",
      " - DeepSeek-V3 is a 671B parameter MoE model trained on 14.8T tokens, incorporating MLA, DeepSeekMoE, auxiliary-loss-free load balancing, and a multi-token prediction training objective.\n",
      " - Training cost was 2.788M H800 GPU hours.\n",
      " - DeepSeek-V3 demonstrates performance comparable to leading closed-source models (GPT-4o, Claude-3.5-Sonnet) while remaining cost-effective.\n",
      " - A limitation is the relatively large deployment unit required for efficient inference.\n",
      " - Future research directions include architectural improvements, increased data quantity and quality, enhancement of deep thinking capabilities, and development of more comprehensive evaluation methods.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "References\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Appendix \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "A. Contributions and Acknowledgments\n",
      " - A comprehensive list of individuals contributed to the research and engine efforts.\n",
      " - A separate list acknowledges the contributions of those involved in data annotation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Business & Compliance Dongjie Ji\n",
      " - Lists contributors to the business and compliance aspects of the project, organized by role.\n",
      " - Contributors are alphabetized by first name within each role.\n",
      " - Individuals who have left the team are indicated with an asterisk (*).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "B. Ablation Studies for Low-Precision Training\n",
      " - FP8 mixed precision training was validated against BF16 training on baseline models of varying scales (16B and 230B parameters).\n",
      " - Relative error remained below 0.25% with high-precision accumulation and fine-grained quantization.\n",
      " - Block-wise quantization of activation gradients led to model divergence, likely due to sensitivity to imbalanced activation gradients and token-correlated outliers.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "C. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-LossFree Models\n",
      " - The auxiliary-loss-free model exhibits greater expert specialization patterns across all layers compared to the auxiliary-loss-based model, as observed on the Pile test set.\n",
      " - Relative expert load, representing the ratio between actual and theoretically balanced load, demonstrates this specialization.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for e in summary:\n",
    "    print(e[\"chapter_title\"])\n",
    "    for s in e[\"summaries\"]:\n",
    "        print(f' - {s}')\n",
    "    print()\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 8192 / Seed: 738712360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChapterSummary(chapter_title='Abstract ', summaries=['DeepSeek-V3 is a 671B Mixture-of-Experts (MoE) language model with 37B active parameters per token.', 'DeepSeek-V3 utilizes Multi-head Latent Attention (MLA) and DeepSeekMoE architectures.', 'A novel auxiliary-loss-free load balancing strategy and multi-token prediction training objective are introduced.', 'DeepSeek-V3 was pre-trained on 14.8 trillion tokens and fine-tuned with Supervised Fine-Tuning and Reinforcement Learning.', 'DeepSeek-V3 outperforms other open-source models and performs comparably to leading closed-source models.', 'Training DeepSeek-V3 required 2.788M H800 GPU hours and was remarkably stable.', 'Model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def summarize_content(chapter: Content, llm_model=\"gemma3:12b\", min_tokens=8192):\n",
    "    hierarchy = get_content_hierarchy(doc)\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "**Role:** Expert Knowledge Distiller.\n",
    "\n",
    "**Objective:** Analyze the 'Current Chapter Content'. Your **sole focus** is to extract and synthesize the **absolute core knowledge** (key findings, critical methods/definitions, essential results, crucial context) presented within. Disregard chapter structure and meta-commentary. The goal is a highly condensed summary enabling understanding of the chapter's *contribution* without reading it.\n",
    "\n",
    "**CRITICAL Instructions:**\n",
    "1.  **NO OUTLINES / STRUCTURE:** **Do NOT** list section titles, subsection numbers, or describe the chapter's organization (e.g., AVOID \"This chapter discusses X in section Y\"). Summarize the *information itself*, not the chapter's structure.\n",
    "2.  **SUBSTANCE ONLY:** Generate summary points **only** if the chapter presents significant *new substantive information* (e.g., a new technique, a key result, a core definition) compared to previous summaries. Ignore introductory phrases, forward references, or purely structural sections.\n",
    "3.  **EXTREME CONCISENESS:** Aim for **highly condensed** points (like bullet points). Capture the *essence* only. If a detail isn't critical for understanding the main point, omit it. Think \"executive summary\" level for each key concept.\n",
    "4.  **Accuracy:** Base summaries strictly on the provided 'Current Chapter Content'. No external knowledge or interpretation.\n",
    "5.  **Empty if Necessary:** If a chapter contains primarily structure, table of contents, people, introductions, references, or information already covered, return an empty `summaries` list (`[]`). Do not summarize just for the sake of summarizing.\n",
    "6.  **Output Format:**\n",
    "    * Return `[]` for `summaries` if no new *substantive* core knowledge is found.\n",
    "    * Otherwise, return a list of concise strings in `summaries`.\n",
    "    * The `chapter_title` field *must* exactly match the 'Current Chapter Title'.\n",
    "\n",
    "**Context:**\n",
    "* Current Chapter Title: ``{chapter.data['text']}`` (Use this exact title in output)\n",
    "\n",
    "**Current Chapter Content to Distill:**\n",
    "```{printable_content(chapter, indent=None)}```\n",
    "\n",
    "Generate the JSON output conforming to the ChapterSummary structure.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        context_length = max(min_tokens, len(prompt.split()) * 3)\n",
    "        seed = random.randint(0, 2**30 - 1)\n",
    "        print(f\"Context length: {context_length} / Seed: {seed}\")\n",
    "        format = ChapterSummary.model_json_schema()\n",
    "        response = ollama.generate(\n",
    "            llm_model,\n",
    "            prompt,\n",
    "            format=format,\n",
    "            options={\n",
    "                \"num_ctx\": context_length,\n",
    "                \"seed\": seed,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        response = ChapterSummary.model_validate_json(response[\"response\"])\n",
    "        assert response is not None, \"Response is None\"\n",
    "        assert response.chapter_title.lower().strip() == chapter.data[\"text\"].lower().strip(), \"Chapter title mismatch\"\n",
    "\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "\n",
    "summary = summarize_content(doc.children[1], llm_model='gemma3:12b', min_tokens=8192)\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
