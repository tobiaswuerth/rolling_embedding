{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rolling.embedding import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6985033872705319\n",
      "0.44313470322301296\n"
     ]
    }
   ],
   "source": [
    "# embedding test\n",
    "from scipy.spatial.distance import cosine\n",
    "def get_similarity(embedding1, embedding2):\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "results = [\n",
    "    get_embedding(\"machine learning\"),\n",
    "    get_embedding(\"deep learning\")\n",
    "]\n",
    "\n",
    "similarity = get_similarity(results[0], results[1])\n",
    "print(similarity)\n",
    "\n",
    "results = [\n",
    "    get_embedding(\"machine learning\"),\n",
    "    get_embedding(\"cow\")\n",
    "]\n",
    "\n",
    "similarity = get_similarity(results[0], results[1])\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28052,\n",
       " 'Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load text\n",
    "# files = [\"text1_atari200.txt\", \"text2_shakespear\", \"text3_wikipedia.txt\"]\n",
    "files = [\"text1_atari200.txt\"]\n",
    "text = \"\"\n",
    "for file in files:\n",
    "    with open(f'texts/{file}', \"r\", encoding='UTF-8') as f:\n",
    "        text += f.read()\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "len(text), text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, n=512):\n",
    "    return [text[i:i+n] for i in range(0, len(text), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:00<00:00, 74.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\\x02tant goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environm',\n",
       " (768,),\n",
       " array([ 1.98868051e-01,  1.05415082e+00, -3.55517745e+00,  1.83754802e-01,\n",
       "         8.20932627e-01, -7.63532892e-02,  1.52788270e+00, -4.03974324e-01,\n",
       "        -3.33955258e-01,  5.77719450e-01,  8.22067261e-02,  4.97762978e-01,\n",
       "         1.25620306e+00,  3.80036235e-01, -4.23142880e-01, -4.81210947e-01,\n",
       "        -3.65734659e-02, -1.14032161e+00, -9.09155667e-01, -9.40786302e-03,\n",
       "         2.69297332e-01, -1.43962431e+00,  8.39840174e-01, -1.33535415e-01,\n",
       "         1.03174746e+00,  3.57248336e-01, -1.39295709e+00, -6.14056110e-01,\n",
       "        -5.36601067e-01,  3.74195516e-01,  7.39551663e-01, -7.86419988e-01,\n",
       "        -9.98838425e-01, -9.18183327e-01, -1.29037425e-01, -1.20188728e-01,\n",
       "         7.53489554e-01,  6.79003954e-01,  2.31489599e-01,  8.86743516e-02,\n",
       "        -5.99775434e-01, -2.18609467e-01,  8.77157673e-02, -9.73927736e-01,\n",
       "         9.47699368e-01, -5.96290648e-01,  5.20260632e-01, -4.79560643e-02,\n",
       "         3.08525801e-01, -4.56029534e-01,  2.60743171e-01, -2.34328344e-01,\n",
       "        -3.81477982e-01, -1.80460706e-01,  1.12616026e+00,  8.35857928e-01,\n",
       "        -1.45482159e+00, -2.46865347e-01,  9.55579579e-01, -2.02106404e+00,\n",
       "         1.77727365e+00,  7.91714907e-01, -1.20596552e+00,  8.71810913e-01,\n",
       "         2.51696974e-01, -2.07210764e-01,  7.65763968e-02,  1.50311613e+00,\n",
       "        -3.41971993e-01, -5.28355300e-01, -1.89099126e-02, -7.97456801e-01,\n",
       "        -6.01541519e-01,  6.42763853e-01, -7.60019898e-01, -3.18855703e-01,\n",
       "        -5.66099286e-01,  6.36916816e-01, -5.97838722e-02,  8.08430612e-01,\n",
       "         1.47547483e+00, -4.13740277e-01,  4.84302223e-01, -8.68715644e-02,\n",
       "         6.98821127e-01, -5.58776855e-01, -3.72797996e-01, -1.05462812e-01,\n",
       "        -8.53269100e-01,  1.66458607e+00,  4.19976056e-01,  1.70992330e-01,\n",
       "         7.96762586e-01,  7.39133656e-01, -7.50566304e-01,  4.69877332e-01,\n",
       "        -1.29111397e+00,  1.51125431e-01, -8.45176458e-01,  4.18804944e-01,\n",
       "        -8.15970421e-01,  5.01679741e-02,  3.36812623e-02, -5.29465675e-01,\n",
       "         1.43160582e-01, -6.96506917e-01,  2.98003312e-02, -5.33789396e-04,\n",
       "        -7.04226792e-01,  4.80082721e-01,  6.48254901e-02,  5.01529098e-01,\n",
       "        -4.80170727e-01, -3.36484373e-01, -5.39342687e-02, -3.01303416e-01,\n",
       "         5.43177783e-01, -1.68299630e-01, -1.63511619e-01,  1.74000174e-01,\n",
       "         1.06478810e-01, -1.59819886e-01, -1.73172280e-01, -1.41875118e-01,\n",
       "         1.58549026e-01,  1.21380782e+00, -9.29584324e-01, -1.83797300e-01,\n",
       "        -4.62364778e-02, -7.31995940e-01, -8.27736139e-01, -3.91568542e-01,\n",
       "        -2.32237473e-01, -6.64198160e-01, -9.97941256e-01,  1.35665941e+00,\n",
       "        -1.48871469e+00, -4.61061865e-01,  5.86047351e-01,  5.06905198e-01,\n",
       "         6.32134750e-02,  6.41562641e-01,  1.36173517e-01,  5.78645945e-01,\n",
       "        -1.45302545e-02, -6.92999184e-01,  3.73326570e-01,  5.93989015e-01,\n",
       "        -1.26715541e+00, -7.04182982e-02, -2.99215466e-01,  1.32756937e+00,\n",
       "        -4.85914767e-01,  5.28746545e-01, -5.22036366e-02, -2.31734943e-02,\n",
       "        -5.28823078e-01,  1.79428354e-01,  5.70144892e-01,  1.01887810e+00,\n",
       "         8.05403173e-01,  3.77700388e-01, -7.42758870e-01,  4.88305628e-01,\n",
       "        -7.54338324e-01, -8.04710031e-01,  1.01093340e+00,  6.53961599e-01,\n",
       "         2.84908295e-01,  4.50139850e-01, -1.57726336e+00, -7.18300641e-01,\n",
       "         2.19769895e-01,  1.40459806e-01, -1.55142754e-01, -5.35381436e-01,\n",
       "         5.10602355e-01, -5.84876597e-01, -2.72933573e-01, -2.09781319e-01,\n",
       "         5.81505239e-01, -1.35602331e+00,  3.69153827e-01,  3.67375255e-01,\n",
       "         3.67360950e-01, -2.89436951e-02, -5.28494895e-01,  1.28364339e-01,\n",
       "        -4.89159524e-01, -8.83066714e-01,  3.41351837e-01,  1.44714236e-01,\n",
       "        -6.88415647e-01, -5.61954618e-01, -8.60315681e-01, -6.43607080e-02,\n",
       "         1.51931393e+00, -1.33235550e+00,  5.40166318e-01,  7.10731894e-02,\n",
       "        -5.93232930e-01, -4.26250696e-02, -4.96475756e-01,  2.84042776e-01,\n",
       "         8.78648367e-03, -3.92722964e-01, -4.80575234e-01,  1.16445376e-02,\n",
       "        -1.89737231e-01,  2.66043216e-01,  1.03831375e+00, -7.90539831e-02,\n",
       "         6.70854300e-02,  5.31086564e-01,  6.78615153e-01, -1.02551556e+00,\n",
       "        -6.38073757e-02, -2.70840049e-01,  8.84808362e-01,  3.10759246e-01,\n",
       "        -1.13614038e-01, -3.94573286e-02,  2.92951405e-01, -1.39353082e-01,\n",
       "        -3.47038388e-01, -4.10734236e-01, -2.27985382e-01,  3.17538798e-01,\n",
       "        -4.82853174e-01,  9.08048272e-01,  4.70642507e-01, -2.90908337e-01,\n",
       "         3.66438478e-01,  4.93015014e-02, -9.97482017e-02,  8.73029053e-01,\n",
       "         2.86077619e-01,  1.64490318e+00,  4.38219011e-02,  2.16908813e-01,\n",
       "         2.31738880e-01,  8.13425839e-01, -5.75804949e-01,  2.88557291e-01,\n",
       "        -7.46931314e-01, -5.57522953e-01,  1.85330600e-01, -9.96771097e-01,\n",
       "        -1.38520941e-01,  9.63315129e-01, -2.69473612e-01,  4.85592723e-01,\n",
       "         4.61221129e-01,  9.38829631e-02,  5.20020902e-01, -7.70576894e-01,\n",
       "        -2.63728667e-02,  3.15032423e-01,  9.89461601e-01, -5.43491304e-01,\n",
       "         5.08917451e-01,  9.53127027e-01, -1.06064893e-01, -7.17010498e-01,\n",
       "        -2.79392332e-01, -7.08238900e-01,  5.41981220e-01,  3.59628260e-01,\n",
       "        -5.33555597e-02,  6.18685298e-02, -2.00373843e-01,  6.17880784e-02,\n",
       "         3.84477347e-01, -2.36495167e-01, -6.45894647e-01,  5.15644372e-01,\n",
       "         3.57199162e-02, -9.43666637e-01, -1.18152626e-01,  6.73338950e-01,\n",
       "        -1.19708693e+00, -1.62979341e+00, -3.17160189e-01,  1.21676169e-01,\n",
       "        -3.57016742e-01,  5.90216555e-02,  1.53830647e+00,  5.50868571e-01,\n",
       "        -1.87818091e-02,  4.90006804e-01,  4.85080183e-02,  5.50018311e-01,\n",
       "         1.92496136e-01,  2.76526898e-01, -1.67196259e-01,  1.40223503e-01,\n",
       "         1.13635468e+00, -4.59393591e-01,  6.72552109e-01, -3.97721976e-02,\n",
       "        -4.87219729e-03, -3.47614288e-01,  7.89455473e-01,  7.28242636e-01,\n",
       "        -1.43212974e-01, -3.58007461e-01, -4.67075974e-01,  3.53595972e-01,\n",
       "        -4.86512184e-02, -7.17578828e-01, -6.35122359e-02,  2.39256829e-01,\n",
       "        -8.44573081e-01, -6.43402115e-02, -5.11035204e-01, -3.98207933e-01,\n",
       "         3.53473902e-01,  6.45872235e-01,  7.67252862e-01, -7.58565247e-01,\n",
       "         7.30487049e-01, -8.94482493e-01, -2.95205593e-01, -3.98093015e-01,\n",
       "        -6.67115226e-02,  1.08810031e+00, -1.16563663e-02,  3.04258794e-01,\n",
       "        -4.39826250e-01,  1.16206095e-01,  1.51438844e+00,  1.08340943e+00,\n",
       "         4.89628851e-01, -1.75138354e-01, -8.48695815e-01, -6.93951249e-01,\n",
       "         7.17394203e-02,  1.61308274e-01,  7.48634189e-02, -2.02109337e-01,\n",
       "        -1.59647223e-02, -3.30520928e-01, -5.33623397e-01, -6.29872978e-01,\n",
       "        -1.00438881e+00,  2.01113090e-01, -1.00849509e-01, -6.40622340e-03,\n",
       "         3.09829444e-01,  2.24124476e-01, -3.22848618e-01,  2.75426567e-01,\n",
       "        -1.10706635e-01, -1.09968990e-01,  7.19025373e-01, -4.62499321e-01,\n",
       "         4.42899734e-01,  6.23390436e-01, -6.04537308e-01, -6.02635801e-01,\n",
       "         5.98100245e-01,  5.17473400e-01, -1.22193806e-01, -1.34412616e-01,\n",
       "        -1.83776468e-01,  2.24968135e-01,  1.39253139e+00, -8.52068186e-01,\n",
       "         3.06633830e-01,  3.34716797e-01,  1.81856483e-01,  1.74221262e-01,\n",
       "         4.44148570e-01,  8.26783121e-01, -8.46527293e-02,  1.66577980e-01,\n",
       "        -8.41142654e-01, -4.87185568e-01, -1.57870710e-01,  1.67782798e-01,\n",
       "         8.22985917e-02, -1.26855612e-01, -2.48820722e-01, -4.64532912e-01,\n",
       "         1.81840524e-01,  3.70865583e-01,  7.82014988e-03, -7.80671597e-01,\n",
       "         6.02081239e-01, -2.95317024e-01, -1.22136998e+00, -4.38077211e-01,\n",
       "        -9.10956442e-01,  5.03945410e-01,  2.04799831e-01, -1.54868722e-01,\n",
       "         5.97034037e-01, -4.75563593e-02,  2.66813517e-01,  8.64472508e-01,\n",
       "        -8.01299274e-01, -9.03512061e-01, -5.00609688e-02, -3.08413506e-01,\n",
       "        -4.62127477e-01, -4.61191505e-01, -1.03324497e+00, -3.57073098e-01,\n",
       "         6.31889794e-03, -2.07014397e-01, -4.58560675e-01,  1.10666764e+00,\n",
       "        -5.21718740e-01, -1.40729702e+00, -3.56182188e-01,  3.95769626e-01,\n",
       "         5.52586496e-01,  1.26841688e+00, -9.56656814e-01, -1.24126524e-01,\n",
       "         2.12595537e-01, -1.20449275e-01,  3.82150739e-01, -7.05494463e-01,\n",
       "         9.13298845e-01,  4.57648128e-01,  3.63396496e-01,  1.11756718e+00,\n",
       "         2.79509630e-02, -1.27866101e+00,  6.58666193e-01, -3.44334036e-01,\n",
       "         1.41802564e-01, -1.37825072e-01, -1.71556309e-01,  1.79152250e-01,\n",
       "         4.15705025e-01,  4.11853164e-01,  1.11172688e+00,  1.51680315e+00,\n",
       "         8.85831490e-02, -2.75737584e-01,  3.86477143e-01, -1.40645429e-01,\n",
       "         2.97328353e-01,  1.37392437e+00,  7.98136771e-01,  1.18846998e-01,\n",
       "        -1.61584854e+00,  1.19781949e-01,  6.60009742e-01,  7.06022203e-01,\n",
       "         8.50283027e-01,  4.24421638e-01,  1.63634634e+00, -7.42927015e-01,\n",
       "         3.96563485e-02, -1.58637822e-01,  8.66865039e-01,  7.14684844e-01,\n",
       "         3.15283649e-02, -3.21145087e-01, -3.40846956e-01, -1.65935680e-01,\n",
       "         3.18175286e-01,  3.15798253e-01,  1.19819097e-01, -8.37848857e-02,\n",
       "         7.85157859e-01,  1.96116209e+00, -8.77228737e-01, -4.35097933e-01,\n",
       "         5.41514456e-01,  1.89057827e-01, -7.10378110e-01, -2.43199006e-01,\n",
       "        -9.91295755e-01, -5.24013758e-01,  1.75325632e+00,  1.25074196e+00,\n",
       "        -5.98617494e-01, -3.97547513e-01, -1.17637742e+00, -1.29668176e+00,\n",
       "        -4.88766253e-01,  8.22919011e-01, -2.11975411e-01, -3.46005559e-01,\n",
       "         8.19399238e-01, -3.23946625e-01, -1.29551619e-01,  2.03795582e-01,\n",
       "        -2.43372247e-01, -6.12107456e-01, -1.45179975e+00, -2.41853252e-01,\n",
       "         1.87085554e-01,  3.78353506e-01,  4.84790355e-01, -1.29995167e-01,\n",
       "        -4.12216485e-01,  6.62581742e-01,  3.24637592e-01, -9.13735449e-01,\n",
       "         2.58191109e-01, -3.55667561e-01,  5.66718340e-01, -1.16193604e+00,\n",
       "        -1.09483051e+00,  7.93634892e-01, -7.51797259e-01,  3.08238924e-01,\n",
       "         2.52374232e-01,  1.26907527e-01, -7.65756983e-03, -5.13207614e-01,\n",
       "         4.21413690e-01,  6.36050344e-01, -1.69629180e+00, -7.85952747e-01,\n",
       "         1.40036428e+00, -2.02274710e-01, -8.64009440e-01, -9.32515979e-01,\n",
       "        -2.11413407e+00,  1.05405772e+00,  9.90910009e-02, -5.39084077e-01,\n",
       "         1.54955029e-01,  5.01700602e-02,  1.28677619e+00, -2.84221202e-01,\n",
       "        -1.09461617e+00, -5.04410863e-01, -5.22786438e-01, -3.64136845e-01,\n",
       "         9.03083384e-03, -8.96499306e-02,  6.68974757e-01, -1.25524759e-01,\n",
       "         7.04356968e-01,  4.81492817e-01,  3.09169590e-01, -1.27517432e-01,\n",
       "        -3.64558697e-01,  1.15053408e-01, -1.04893649e+00,  4.03506011e-01,\n",
       "         5.40215135e-01, -1.27942002e+00,  1.20960104e+00, -7.34678626e-01,\n",
       "         3.73035967e-02, -4.84746844e-01,  6.91270947e-01,  9.31447744e-02,\n",
       "        -1.00330186e+00, -8.76555741e-01, -4.01126780e-02,  1.46241888e-01,\n",
       "        -3.80462646e-01, -2.16093808e-01,  1.57434452e+00,  1.32377952e-01,\n",
       "        -2.30852067e-01, -7.87708223e-01, -1.24569751e-01, -8.68231416e-01,\n",
       "         4.71791159e-03,  1.17320263e+00,  6.76509857e-01, -7.26155221e-01,\n",
       "         3.08258444e-01,  9.15838778e-01, -7.26075172e-02, -8.19120109e-01,\n",
       "         3.64061385e-01, -8.85262489e-01, -1.34511292e+00, -4.69722271e-01,\n",
       "         1.47936165e-01, -1.18231738e+00,  1.11009926e-01,  6.11257702e-02,\n",
       "         1.16021466e+00, -1.68472484e-01, -9.28546906e-01,  2.05679163e-02,\n",
       "         8.66566002e-02, -1.48070538e+00,  1.13240905e-01, -4.82930839e-01,\n",
       "         4.98405606e-01, -1.77540433e+00, -3.99646848e-01, -5.21184206e-01,\n",
       "         3.38279545e-01, -7.42470086e-01, -1.03183985e+00, -4.85747188e-01,\n",
       "         2.90232837e-01,  8.91085148e-01,  9.63417530e-01, -1.26504338e+00,\n",
       "         8.94957185e-01,  1.33462298e+00, -3.41636613e-02,  9.00697708e-01,\n",
       "         5.46654582e-01,  2.35621214e-01, -1.69268951e-01,  5.09557962e-01,\n",
       "         3.42907935e-01,  3.72604996e-01,  5.23969889e-01,  3.60325212e-03,\n",
       "         1.22502899e+00, -6.38449132e-01,  1.37423292e-01, -1.05847740e+00,\n",
       "        -3.12423557e-01, -8.62487614e-01,  7.33344793e-01, -4.90958244e-01,\n",
       "         9.16643292e-02, -6.42160714e-01, -4.88975614e-01, -1.08737564e+00,\n",
       "         5.61654449e-01,  1.29699588e+00,  6.06857501e-02,  4.35149856e-02,\n",
       "        -3.59131366e-01, -3.10991257e-01, -1.95584491e-01,  6.08847216e-02,\n",
       "        -6.70787513e-01,  9.43048060e-01, -5.74696243e-01,  1.36310756e+00,\n",
       "         2.97885299e-01, -2.74778157e-01, -1.16386294e+00, -4.50980291e-02,\n",
       "         6.98629379e-01, -9.61966217e-01,  1.35252583e+00,  1.83477724e+00,\n",
       "         6.95676029e-01,  3.62612531e-02,  1.36576962e+00,  6.74681783e-01,\n",
       "         6.09486699e-01, -7.16981411e-01, -3.59486103e-01, -2.49286145e-02,\n",
       "         7.28854001e-01,  7.50291571e-02,  9.13158596e-01, -3.09355497e-01,\n",
       "        -4.91404712e-01,  6.32512152e-01,  1.18053675e-01,  3.62758487e-01,\n",
       "         3.17010432e-02,  2.38172963e-01, -8.47471476e-01, -1.50374234e-01,\n",
       "        -5.32355905e-01, -3.86500955e-01,  1.57087123e+00, -4.03099507e-02,\n",
       "         2.99724519e-01, -7.23349810e-01,  1.94399789e-01,  1.20847523e+00,\n",
       "        -1.44973099e-01,  7.35146224e-01,  3.16223621e-01, -4.79108840e-01,\n",
       "         3.01885545e-01, -3.66956204e-01,  9.89314258e-01, -9.78752151e-02,\n",
       "         1.35622397e-01, -9.95726883e-03,  3.18824500e-01, -3.81013632e-01,\n",
       "        -2.95307994e-01, -1.61243200e+00, -2.91801654e-02, -7.62161255e-01,\n",
       "         2.78931689e-02,  2.66269028e-01,  2.09642112e-01,  3.90555561e-01,\n",
       "        -3.38136822e-01,  4.11387652e-01,  3.13874155e-01,  1.23691213e+00,\n",
       "        -5.56477785e-01,  7.24072874e-01,  1.04996413e-01,  1.33328760e+00,\n",
       "         3.78656127e-02,  2.82812804e-01, -2.90634692e-01, -6.01828039e-01,\n",
       "         2.38984134e-02, -8.87216270e-01, -2.14768857e-01,  4.22836542e-01,\n",
       "        -1.12754357e+00,  6.64880872e-01, -3.02628148e-04, -2.82065421e-02,\n",
       "        -2.22702071e-01, -2.09390402e-01,  2.12961808e-03,  8.82050574e-01,\n",
       "         1.62262094e+00, -6.10361278e-01,  1.49134457e-01,  9.30668414e-02,\n",
       "        -2.95445740e-01, -1.86632946e-01,  1.22727096e+00, -2.19826484e+00,\n",
       "         6.14945650e-01, -5.29762432e-02,  3.17534596e-01, -4.10790652e-01,\n",
       "        -1.52733600e+00,  1.66692936e+00, -2.30391428e-01,  1.60329379e-02,\n",
       "        -8.05338979e-01,  3.13495606e-01, -1.25618970e+00,  4.05266546e-02,\n",
       "        -6.99536204e-01,  6.94488704e-01,  4.23421383e-01,  9.33254719e-01,\n",
       "         3.93139392e-01, -5.40032350e-02, -1.19168445e-01,  4.08357829e-01,\n",
       "         2.44578216e-02, -6.13976717e-01,  4.28181499e-01,  9.64387283e-02,\n",
       "        -1.55986059e+00, -2.69245356e-01, -1.95785984e-03,  1.06078684e+00,\n",
       "         7.30899096e-01,  8.03843498e-01,  2.74891496e+00, -3.59239995e-01,\n",
       "         2.01048285e-01, -6.58589840e-01,  5.62379777e-01, -4.50983524e-01,\n",
       "        -1.86615139e-01,  1.70221701e-01, -6.73357725e-01,  1.09005287e-01]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate embedding for all sentences\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Sentence:\n",
    "    text: str\n",
    "    embedding: np.ndarray\n",
    "\n",
    "results = []\n",
    "parts = split_text(text, 512)\n",
    "for sentence in tqdm(parts):\n",
    "    embedding = get_embedding(sentence)\n",
    "    results.append(Sentence(sentence, embedding))\n",
    "\n",
    "results[0].text, results[0].embedding.shape, results[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Assuming embeddings are extracted as shown\n",
    "embeddings = np.array([sentence.embedding for sentence in results])\n",
    "\n",
    "# Apply PCA to reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Generate colors\n",
    "num_points = reduced_embeddings.shape[0]\n",
    "colors = cm.rainbow(np.linspace(0, 1, num_points))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot the points and the line through them in rainbow colors\n",
    "for i in range(num_points - 1):\n",
    "    ax.plot(reduced_embeddings[i:i+2, 0], reduced_embeddings[i:i+2, 1], color=colors[i], marker='o', markersize=2,linewidth=0.5)\n",
    "\n",
    "plt.title('2D PCA of Embeddings with Rainbow Line Through Points')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception.\n",
      "The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark.\n",
      "Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tobias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Tobias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Split the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Print the split sentences\n",
    "for sentence in sentences[:3]:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rolling.paper import create_paper, print_paper, save_paper, load_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tobias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Tobias\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "paper = create_paper('test', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: test\n",
      "> [ 0.71496713  1.38890755 -2.87749505] Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization lay\u0002ers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy over time. 1. Introduction To develop generally capable agents, the question of how to evaluate them is paramount. The Arcade Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate agents on an diverse set of tasks which are interesting to humans, and developed externally to the Reinforcement Learning (RL) community. As a result, several games exhibit reward structures which are highly adversarial to many popular algorithms. Mean and median human normalized scores (HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating deep RL agents. Recent progress has allowed state-of-the-art algorithms to greatly exceed average human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al., 2020; Van Hasselt et al., 2016). However, it has been argued that mean or median HNS might not be well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al., 2019). Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on most games, but completely fail to learn on a small number of them. Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all 57 Atari games. However, such generality came at the cost of data efficiency; requiring tens of billions of environment interactions to achieve above average-human performance in some games, reaching a figure of 78 billion frames before beating the human benchmark in all games. Data efficiency remains a desirable property for agents to possess, as many real-world challenges are data-limited by time and cost constraints (Dulac-Arnold et al., 2019). In this work, our aim is to develop an agent that is as general as Agent57 but that requires only a fraction of the environment interactions to achieve the same result. © 2022 DeepMind. All rights reserved arXiv:2209.07550v1 [cs.LG] 15 Sep 2022 Human-level Atari 200x faster Skiing Private Eye Pitfall! Montezuma's Revenge Surround Asteroids Ice Hockey Solaris Bowling Gravitar H.E.R.O. Berzerk Frostbite Tennis Alien Tutankham Riverraid Ms. Pac-Man Pong Beam Rider Chopper Command Seaquest Double Dunk Freeway Space Invaders Bank Heist Venture Battle Zone Asterix Q*BERT Amidar Name This Game Yars' Revenge Phoenix Time Pilot Demon Attack Wizard Of Wor Breakout Video Pinball Fishing Derby Enduro Stargunner Assault Zaxxon Jamesbond Centipede Kangaroo Atlantis Up'n Down Defender Boxing Robotank Kung-Fu Master Gopher Road Runner Crazy Climber Krull 1M 10M 100M 1B 10B 100B Frames to > human score 390M Agent57 MEME Figure 1 | Number of environment frames required by agents to outperform the human baseline on each game (in log-scale). Lower is better. On average, MEME achieves above human scores using 63× fewer environment interactions than Agent57. The smallest improvement is 9× (road_runner), the maximum is 721× (skiing), and the median across the suite is 35×. We observe small variance across seeds (c.f. Figure 8). There exist two main trends in the literature when it comes to measuring improvements in the learning capabilities of agents. One approach consists in measuring performance after a limited budget of interactions with the environment. While this type of evaluation has led to important progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019). On the other hand, one can aim to achieve a target end-performance with as few interactions as possible (Schmitt et al., 2020; Silver et al., 2017, 2018). Since our goal is to show that our new agent is as general as Agent57, while being more data efficient, we focus on the latter approach. Our contributions can be summarized as follows. Building off Agent57, we carefully examine bottlenecks which slow down learning and address instabilities that arise when these bottlenecks are removed. We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based Exploration agent, which introduces solutions to enable taking advantage of three approaches that would otherwise lead to instabilities: training the value functions of the whole family of policies from Agent57 in parallel, on all policies’ transitions (instead of just the behaviour policy transitions), bootstrapping from the online network, and using high replay ratios. These solutions include carefully normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016) update target with a soft variant of Watkins’ Q(𝜆) (Watkins and Dayan, 1992) that enables faster signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for value updates. Moreover, we explore several recent advances in deep learning and determine which of them are beneficial for non-stationary problems like the ones considered in this work. Finally, we examine approaches to robustify performance by introducing a policy distillation mechanism that learns a policy head based on the actions obtained from the value network without being sensitive to value magnitudes. Our agent outperforms the human baseline across all 57 Atari games in 390M frames, using two orders of magnitude fewer interactions with the environment than Agent57 as shown in Figure 1. 2. Related work Large scale distributed agents have exhibited compelling results in recent years. Actor-critic (Espeholt et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari 2 Human-level Atari 200x faster 57 benchmark. Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that achieve competitive results on Atari. Finally, search-augmented distributed agents (Hessel et al., 2021; Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely they hold the highest mean and median human normalized scores over the 57 Atari games. However, all these methods show the same failure mode: they perform poorly in hard exploration games, such as Pitfall!, and Montezuma’s Revenge. In contrast, Agent57 (Badia et al., 2020) surpassed the human benchmark on all 57 games, showing better general performance. Go-Explore (Ecoffet et al., 2021) similarly achieved such general performance, by relying on coarse-grained state representations via a downscaling function that is highly specific to Atari. Learning as much as possible from previous experience is key for data-efficiency. Since it is often desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002; Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over the same batch of experience in policy gradient methods to avoid collecting new transitions for every learning step (Schulman et al., 2017). This decoupling between collecting experience and learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al., 2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005). Multiple approaches for making more efficient use of a replay buffer have been proposed, including prioritized sampling of transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al., 2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018), or reanalyzing old trajectories with the most recent version of a learned model to generate new targets in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz et al., 2017). The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps (400k frames). Work on this benchmark has focused on leveraging the use of models (Kaiser et al., 2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel, 2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020). While we consider this to be an important line of research, this tight budget produces an incentive to focus on a subset of games where exploration is easier, and it is unclear some games can be solved from scratch with such a small data budget. Such a setting is likely to prevent any meaningful learning on hard-exploration games, which is in contrast with the goal of our work. 3. Background: Agent57 Our work builds on top of Agent57, which combines three main ideas: (i) a distributed deep RL framework based on Recurrent Replay Distributed DQN (R2D2) (Kapturowski et al., 2018), (ii) ex\u0002ploration with a family of policies and the Never Give Up (NGU) intrinsic reward (Badia et al., 2019), and (iii) a meta-controller that dynamically adjusts the discount factor and balances exploration and exploitation throughout the course of training, by selecting from a family of policies. Below, we give a general introduction to the problem setting and some of the relevant components of Agent57. Problem definition. We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP) (Puterman, 1994). The goal is to find a policy 𝜋 that maximises the expected sum of future discounted rewards, 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡], where 𝛾 ∈ [0, 1) is the discount factor, 𝑟𝑡 = 𝑟(𝑥𝑡 , 𝑎𝑡) is the reward at time t, 𝑥𝑡 is the state at time t, and 𝑎𝑡 ∼ 𝜋(𝑎|𝑥𝑡) is the action generated by following some policy 𝜋. In the off-policy learning setting, data generated by a behavior policy 𝜇 is used to 3 Human-level Atari 200x faster learn about the target policy 𝜋. This can be achieved by employing a variant of Q-learning (Watkins and Dayan, 1992) to estimate the action-value function, 𝑄 𝜋 (𝑥, 𝑎) = 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡 |𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎]. The estimated action-value function can then be used to derive a new policy 𝜋(𝑎|𝑥) using the 𝜖-greedy operator G𝜖 (Sutton and Barto, 2018). 1 This new policy can then be used as target policy for another iteration, repeating the process. Agent57 uses a deep neural network with parameters 𝜃 to estimate action-value functions, 𝑄 𝜋 (𝑥, 𝑎; 𝜃) 2 , trained on return estimates 𝐺𝑡 derived with Retrace from sequences of off-policy data (Munos et al., 2016). In order to stabilize learning, a target network is used for bootstrapping the return estimates using double Q-learning (Van Hasselt et al., 2016); the parameters of this target network, 𝜃𝑇 , are periodically copied from the online network parameters 𝜃 (Mnih et al., 2015). Distributed RL framework. Agent57 is a distributed deep RL agent based on R2D2 that decouples acting from learning. Multiple actors interact with independent copies of the environment and feed trajectories to a central replay buffer. A separate learning process obtains trajectories from this buffer using prioritized sampling and updates the neural network parameters to predict the action-value function at each state. Actors obtain parameters from the learner periodically. See Appendix D for more details. Exploration with NGU. Agent57 uses the Never Give Up (NGU) intrinsic reward to encourage exploration. It aims at learning a family of 𝑁 = 32 policies which maximize different weightings of the extrinsic reward given by the environment (𝑟 𝑒 𝑡 ) and the intrinsic reward (𝑟 𝑖 𝑡 ), 𝑟𝑗,𝑡 = 𝑟 𝑒 𝑡 + 𝛽𝑗𝑟 𝑖 𝑡 (𝛽𝑗 ∈ ℝ+ , 𝑗 ∈ {0, . . . , 𝑁 −1}). The value of 𝛽𝑗 controls the degree of exploration, with higher values encouraging more exploratory behaviors, and each policy in the family is optimized with a different discount factor 𝛾𝑗 . The Universal Value Function Approximators (UVFA) framework (Schaul et al., 2015a) is employed to efficiently learn 𝑄 𝜋 𝑗 (𝑥, 𝑎; 𝜃) = 𝔼𝜋𝑗[ Í 𝑡≥0 𝛾 𝑡 𝑗 𝑟𝑗,𝑡|𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎] (we use a shorthand notation 𝑄 𝑗 (𝑥, 𝑎; 𝜃)) for 𝑗 ∈ {0, . . . , 𝑁 − 1} using a single set of shared parameters 𝜃. The policy 𝜋 𝑗 (𝑎|𝑥) can then be derived using the 𝜖-greedy operator as G𝜖𝑄 𝑗 (𝑥, 𝑎; 𝜃). We refer the reader to Appendix G for more details. Meta-controller. Agent57 introduces an adaptive meta-controller that decides which policies from the family of N policies to use for collecting experience based on their episodic returns. This naturally creates a curriculum over 𝛽𝑗 and 𝛾𝑗 by adapting their value throughout training. This optimization process is formulated as a non-stationary bandit problem. A detailed description about the meta\u0002controller implementation is provided in Appendix E. Q-function separation. The architecture of the Q-function in Agent57 is implemented as two separate networks in order to split the intrinsic and extrinsic components. The network parameters of 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑒) and 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑖) are separate and independently optimized with 𝑟 𝑒 𝑗 and 𝑟 𝑖 𝑗 , respectively. The main motivation behind this decomposition is to prevent the gradients of the decomposed intrinsic and extrinsic value function heads from interfering with each other. This can be beneficial in environments where the intrinsic reward is poorly aligned with the task’s extrinsic reward. 4. MEME: Improving the data efficiency of Agent57 This section describes the main algorithmic contributions of the MEME agent, aimed at improving the data-efficiency of Agent57. These contributions aim to achieve faster propagation of learning signals related to rare events ( A ), stabilize learning under differing value scales ( B ), improve the neural network architecture ( C ), and make updates more robust under a rapidly-changing policy ( D ). For 1We also use G B G0 to denote the pure greedy operator. 2For convenience, we occasionally omit (𝑥, 𝑎) or 𝜃 from 𝑄(𝑥, 𝑎; 𝜃), 𝜋(𝑥, 𝑎; 𝜃) when there is no danger of confusion. 4 Human-level Atari 200x faster Torso LSTM Q-value Head Policy Head Qj (at , xt ) 𝛑j (at | xt ) N-mixtures Torso LSTM Q-value Head Policy Head Q1 (at , x t ) 𝛑1 (at | xt ) Member policy 1 Q-value Head Policy Head QN (at , xt ) 𝛑N (at | xt ) Member policy 1 … … Dueling Head Dueling Head Qi (at , xt ) Qe (at , xt ) + Qj (at , xt ) Q-value Head N policy family members Torso LSTM Q-value Head Policy Head Q1 (at , xt ) 𝛑1 (at | xt ) N policy family members … QN (at , x t ) 𝛑N (at | xt ) Torso LSTM Q1 (xt , at ; θ) 𝛑1 (at | xt ) N policy family members … QN (xt , at ; θ) 𝛑N (at | xt ) Policy Head Q-value Head Figure 2 | MEME agent network architecture. The output of the LSTM block is passed to each of the N members of the family of policies, depicted as a light-grey box. Each policy consists of an Q-value and policy heads. The Q-value head is similar as in Agent57 paper, while the policy head is introduced for acting and target computation, and trained via policy distillation. clarity of exposition, we label methods according to the type of limitation they address. A1 Bootstrapping with online network. Target networks are frequently used in conjunction with value-based agents due to their stabilizing effect when learning from off-policy data (Mnih et al., 2015; Van Hasselt et al., 2016). This design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate. This issue can be mitigated to some extent by simply updating the target network more frequently, but the result is typically a less stable agent. To accelerate signal propagation while maintaining stability, we use online network bootstrapping, and we stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute to the loss. The trust region masks out the loss at any timestep for which both of the following conditions hold: |𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )| > 𝛼𝜎𝑗 (1) sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )) ≠ sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝐺𝑡) (2) where 𝛼 is a fixed hyperparameter, 𝐺𝑡 denotes the return estimate, 𝜃 and 𝜃𝑇 denote the online and target parameters respectively, and 𝜎𝑗 is the standard deviation of the TD-errors (a more precise description of which we defer until B1). Intuitively, we only mask if the current value of the online network is outside of the trust region (Equation 1) and the sign of the TD-error points away from the trust region (Equation 2), as depicted in Figure 3 in red. We note that a very similar trust region scheme is used for the value-function in most Proximal Policy Optimization (PPO) implementations (Schulman et al., 2017), though not described in the original paper. In contrast, the PPO version instead uses a constant threshold, and thus is not able to adapt to differing scales of value functions. Qj(xt ,at ;θT ) Qj(xt ,at ;θT ) + ασ Qj j (xt ,at ;θT ) - ασj Qj(xt ,at ;θ) Figure 3 | Trust region. The position of dots is given by the relationship between the values predicted by the online network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃), and the values predicted by the target network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 ) (Equation 1 and left hand side of Equation 2), the box represents the trust region bounds defined in Equation 1, and the direction of the arrow is given by the right hand side of Equation 2. Green-colored transitions are used in the loss computation, whereas red ones are masked out. A2 Target computation with tolerance. Agent57 uses Retrace (Munos et al., 2016) to compute return estimates from off-policy data, but we observed that it tends to cut traces too aggressively when 5 Human-level Atari 200x faster using 𝜖-greedy policies thus slowing down the propagation of information into the value function. Preliminary experiments showed that data-efficiency was improved in many dense-reward tasks when replacing Retrace with Peng’s Q(𝜆) (Peng and Williams, 1994), but its lack of off-policy corrections tends to result in degraded performance as data becomes more off-policy (e.g. by increasing the expected number of times that a sequence is sampled from replay, or by sharing data across a family of policies). This motivates us to propose an alternative return estimator, which we derive from Q(𝜆) (Watkins and Dayan, 1992): 𝐺𝑡 = max 𝑎 𝑄(𝑥𝑡 , 𝑎) + ∑︁ 𝑘≥0 Ö 𝑘−1 𝑖=0 𝜆𝑖 ! 𝛾 𝑘 (𝑟𝑡+𝑘 + 𝛾 max 𝑎 𝑄(𝑥𝑡+𝑘+1, 𝑎) − max 𝑎 𝑄(𝑥𝑡+𝑘, 𝑎)) (3) where Î𝑘−1 𝑖=0 𝜆𝑖 ∈ [0, 1] effectively controls how much information from the future is used in the return estimation and is generally used as a trace cutting coefficient to perform off-policy correction. Peng’s Q(𝜆) does not perform any kind of off-policy correction and sets 𝜆𝑖 = 𝜆, whereas Watkins’ Q(𝜆) (Watkins and Dayan, 1992) aggressively cuts traces whenever it encounters an off-policy action by using 𝜆𝑖 = 𝜆𝟙𝑎𝑖 ∈argmax𝑎𝑄(𝑥𝑖 ,𝑎) , where 𝟙 denotes the indicator function. We propose to use a softer trace cutting mechanism by adding a fixed tolerance parameter 𝜅 and taking the expectation of trace coefficients under 𝜋: 𝜆𝑖 = 𝜆𝔼𝑎∼𝜋(𝑎|𝑥𝑡) \u0002 𝟙[𝑄(𝑥𝑡 ,𝑎𝑡;𝜃) ≥𝑄(𝑥𝑡 ,𝑎;𝜃)−𝜅|𝑄(𝑥𝑖 ,𝑎;𝜃) |] \u0003 (4) Finally, we replace all occurrences of the max operator in Equation 3 with the expectation under 𝜋. The resulting return estimator, which we denote Soft Watkins Q(𝜆), leads to more transitions being used and increased sample efficiency. Note that Watkins Q(𝜆) is recovered when setting 𝜅 = 0 and 𝜋 = G (𝑄). B1 Loss and priority normalization. As we learn a family of Q-functions which vary over a wide range of discount factors and intrinsic reward scales, we expect that the Q-functions will vary considerably in scale. This may cause the larger-scale Q-values to dominate learning and destabilize learning of smaller Q-values. This is a particular concern in environments with very small extrinsic reward scales. To counteract this effect we introduce a normalization scheme on the TD-errors similar to that used in Schaul et al. (2021). Specifically, we compute a running estimate of the standard deviation of TD-errors of the online network 𝜎 running 𝑗 as well as a batch standard deviation 𝜎 batch 𝑗 , and compute 𝜎𝑗 = max(𝜎 running 𝑗 , 𝜎batch 𝑗 , 𝜖), where 𝜖 acts as small threshold to avoid amplification of noise past a specified scale, which we fix to 0.01 in all our experiments. We then divide the TD-errors by 𝜎𝑗 when computing both the loss and priorities. As opposed to Schaul et al. (2021) we compute the running statistics on the learner, and make use of importance sampling to correct the sampling distribution. B2 Cross-mixture training. Agent57 only trains the policy 𝑗 which was used to collect a given trajectory, but it is natural to ask whether data-efficiency and robustness may be improved by training all policies at once. We propose a training loss 𝐿 according to the following weighting scheme between the behavior policy loss and the mean over all losses: 𝐿 = 𝜂𝐿𝑗𝜇 + 1 − 𝜂 𝑁 𝑁 ∑︁−1 𝑗=0 𝐿𝑗 (5) where 𝐿𝑗 denotes the Q-learning loss for policy 𝑗, and 𝑗𝜇 denotes the index for the behavior policy selected by the meta-controller for the sampled trajectory. We find that an intermediate value for the mixture parameter of 𝜂 = 0.5 tends to work well. To achieve better compute-efficiency we choose to deviate from the original UVFA architecture which fed a 1-hot encoding of the policy index to the LSTM, aand instead modify the Q-value heads to output N sets of Q-values, one for each of the 6 Human-level Atari 200x faster members in the family of policies introduced in Section 3. Therefore, in the end we output values for all combinations of actions and policies (see Figure 2). We note that in this setting, there is also less deviation in the recurrent states when learning across different mixture policies. C1 Normalizer-free torso network. Normalization layers are a common feature of ResNet architec\u0002tures, and which are known to aid in training of very deep networks, but preliminary investigation revealed that several commonly used normalization layers are in fact detrimental to performance in our setting. Instead, we employ a variation of the NFNet architecture (Brock et al., 2021) for our policy torso network, which combines a variance-scaling strategy with scaled weight standardization and careful initialization to achieve state-of-the-art performance on ImageNet without the need for normalization layers. We adopt their use of stochastic depth (Huang et al., 2016) at training-time but omit the application of ordinary dropout to fully-connected layers as we did not observe any benefit from this form of regularization. Some care is required when using stochastic depth in conjunction with multi-step returns, as resampling of the stochastic depth mask at each timestep injects additional noise into the bootstrap values, resulting in a higher-variance return estimator. As such, we employ a temporally-consistent stochastic depth mask which remains fixed over the length of each training trajectory. C2 Shared torso with combined loss. Agent57 decomposes the combined Q-function into intrinsic and extrinsic components, 𝑄𝑒 and 𝑄𝑖 , which are represented by separate networks. Such a decomposi\u0002tion prevents the gradients of the decomposed value functions from interfering with each other. This interference may occur in environments where the intrinsic reward is poorly aligned with the task objective, as defined by the extrinsic reward. However, the choice to use separate separate networks comes at an expensive computational cost, and potentially limits sample-efficiency since generic low-level features cannot be shared. To alleviate these issues, we introduce a shared torso for the two Q-functions while retaining separate heads. While the form of the decomposition in Agent57 was chosen so as to ensure convergence to the optimal value-function 𝑄 ★ in the tabular setting, this does not generally hold under function approximation. Comparing the combined and decomposed losses we observe a mismatch in the gradients due to the absence of cross-terms 𝑄𝑖(𝜃) 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 and 𝑄𝑒 (𝜃) 𝜕𝑄𝑖(𝜃) 𝜕𝜃 in the decomposed loss: 𝜕 𝜕𝜃 h 1 2 (𝑄(𝜃) − 𝐺) 2 i | {z } combined loss ≠ 𝜕 𝜕𝜃 h 1 2 (𝑄𝑒 (𝜃) − 𝐺𝑒) 2 + 1 2 (𝛽𝑄𝑖(𝜃) − 𝛽𝐺𝑖) 2 i | {z } decomposed loss (6) \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) − 𝐺 \u0003 𝜕 𝜕𝜃 \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) \u0003 ≠ \u0002 𝑄𝑒 (𝜃) − 𝐺𝑒 \u0003 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 + 𝛽 2 \u0002 𝑄𝑖(𝜃) − 𝐺𝑖 \u0003 𝜕𝑄𝑖(𝜃) 𝜕𝜃 (7) Since we use a behavior policy induced by the total Q-function 𝑄 = 𝑄𝑒 + 𝛽𝑄𝑖 rather than the individual components, theory would suggest to use the combined loss instead. In addition, from a practical implementation perspective, this switch to the combined loss greatly simplifies the design choices involved in our proposed trust region method described in A1. The penalty paid for this choice is that the decomposition of the value function into extrinsic and intrinsic components no longer carries a strict semantic meaning. Nevertheless we do still retain an implicit inductive bias induced by multiplication of 𝑄𝑖 with the intrinsic reward weight 𝛽 𝑗 . D Robustifying behavior via policy distillation. Schaul et al. (2022) describe the effect of policy churn, whereby the greedy action of value-based RL algorithms may change frequently over consecutive parameter updates. This can have a deleterious effect on off-policy correction methods: traces will be cut more aggressively than with a stochastic policy, and bootstrap values will change frequently which can result in a higher variance return estimator. In addition, our choice of training with temporally-consistent stochastic depth masks can be interpreted as learning an implicit ensemble 7 Human-level Atari 200x faster of Q-functions; thus it is natural to ask whether we may see additional benefit from leveraging the policy induced by this ensemble. We propose to train an explicit policy head 𝜋dist (see Figure 2) via policy distillation to match the 𝜖-greedy policy induced by the Q-function. In expectation over multiple gradient steps this should help to smooth out the policy over time, as well as over the ensemble, while being much faster to evaluate than the individual members of the ensemble. Similarly to the trust-region described in A1, we mask the policy distillation loss at any timestep where a KL constraint 𝐶KL is violated: 𝐿𝜋 = − ∑︁ 𝑎,𝑡 G𝜖\n",
      "  > [ 0.68259829  1.19762492 -3.00449371] Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization lay\u0002ers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy over time. 1. Introduction To develop generally capable agents, the question of how to evaluate them is paramount. The Arcade Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate agents on an diverse set of tasks which are interesting to humans, and developed externally to the Reinforcement Learning (RL) community. As a result, several games exhibit reward structures which are highly adversarial to many popular algorithms. Mean and median human normalized scores (HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating deep RL agents. Recent progress has allowed state-of-the-art algorithms to greatly exceed average human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al., 2020; Van Hasselt et al., 2016). However, it has been argued that mean or median HNS might not be well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al., 2019). Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on most games, but completely fail to learn on a small number of them. Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all 57 Atari games. However, such generality came at the cost of data efficiency; requiring tens of billions of environment interactions to achieve above average-human performance in some games, reaching a figure of 78 billion frames before beating the human benchmark in all games. Data efficiency remains a desirable property for agents to possess, as many real-world challenges are data-limited by time and cost constraints (Dulac-Arnold et al., 2019). In this work, our aim is to develop an agent that is as general as Agent57 but that requires only a fraction of the environment interactions to achieve the same result. © 2022 DeepMind. All rights reserved arXiv:2209.07550v1 [cs.LG] 15 Sep 2022 Human-level Atari 200x faster Skiing Private Eye Pitfall! Montezuma's Revenge Surround Asteroids Ice Hockey Solaris Bowling Gravitar H.E.R.O. Berzerk Frostbite Tennis Alien Tutankham Riverraid Ms. Pac-Man Pong Beam Rider Chopper Command Seaquest Double Dunk Freeway Space Invaders Bank Heist Venture Battle Zone Asterix Q*BERT Amidar Name This Game Yars' Revenge Phoenix Time Pilot Demon Attack Wizard Of Wor Breakout Video Pinball Fishing Derby Enduro Stargunner Assault Zaxxon Jamesbond Centipede Kangaroo Atlantis Up'n Down Defender Boxing Robotank Kung-Fu Master Gopher Road Runner Crazy Climber Krull 1M 10M 100M 1B 10B 100B Frames to > human score 390M Agent57 MEME Figure 1 | Number of environment frames required by agents to outperform the human baseline on each game (in log-scale). Lower is better. On average, MEME achieves above human scores using 63× fewer environment interactions than Agent57. The smallest improvement is 9× (road_runner), the maximum is 721× (skiing), and the median across the suite is 35×. We observe small variance across seeds (c.f. Figure 8). There exist two main trends in the literature when it comes to measuring improvements in the learning capabilities of agents. One approach consists in measuring performance after a limited budget of interactions with the environment. While this type of evaluation has led to important progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019). On the other hand, one can aim to achieve a target end-performance with as few interactions as possible (Schmitt et al., 2020; Silver et al., 2017, 2018). Since our goal is to show that our new agent is as general as Agent57, while being more data efficient, we focus on the latter approach. Our contributions can be summarized as follows. Building off Agent57, we carefully examine bottlenecks which slow down learning and address instabilities that arise when these bottlenecks are removed. We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based Exploration agent, which introduces solutions to enable taking advantage of three approaches that would otherwise lead to instabilities: training the value functions of the whole family of policies from Agent57 in parallel, on all policies’ transitions (instead of just the behaviour policy transitions), bootstrapping from the online network, and using high replay ratios. These solutions include carefully normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016) update target with a soft variant of Watkins’ Q(𝜆) (Watkins and Dayan, 1992) that enables faster signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for value updates. Moreover, we explore several recent advances in deep learning and determine which of them are beneficial for non-stationary problems like the ones considered in this work. Finally, we examine approaches to robustify performance by introducing a policy distillation mechanism that learns a policy head based on the actions obtained from the value network without being sensitive to value magnitudes. Our agent outperforms the human baseline across all 57 Atari games in 390M frames, using two orders of magnitude fewer interactions with the environment than Agent57 as shown in Figure 1. 2. Related work Large scale distributed agents have exhibited compelling results in recent years. Actor-critic (Espeholt et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari 2 Human-level Atari 200x faster 57 benchmark. Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that achieve competitive results on Atari. Finally, search-augmented distributed agents (Hessel et al., 2021; Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely they hold the highest mean and median human normalized scores over the 57 Atari games. However, all these methods show the same failure mode: they perform poorly in hard exploration games, such as Pitfall!, and Montezuma’s Revenge. In contrast, Agent57 (Badia et al., 2020) surpassed the human benchmark on all 57 games, showing better general performance. Go-Explore (Ecoffet et al., 2021) similarly achieved such general performance, by relying on coarse-grained state representations via a downscaling function that is highly specific to Atari. Learning as much as possible from previous experience is key for data-efficiency. Since it is often desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002; Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over the same batch of experience in policy gradient methods to avoid collecting new transitions for every learning step (Schulman et al., 2017). This decoupling between collecting experience and learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al., 2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005). Multiple approaches for making more efficient use of a replay buffer have been proposed, including prioritized sampling of transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al., 2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018), or reanalyzing old trajectories with the most recent version of a learned model to generate new targets in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz et al., 2017). The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps (400k frames). Work on this benchmark has focused on leveraging the use of models (Kaiser et al., 2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel, 2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020). While we consider this to be an important line of research, this tight budget produces an incentive to focus on a subset of games where exploration is easier, and it is unclear some games can be solved from scratch with such a small data budget. Such a setting is likely to prevent any meaningful learning on hard-exploration games, which is in contrast with the goal of our work. 3. Background: Agent57 Our work builds on top of Agent57, which combines three main ideas: (i) a distributed deep RL framework based on Recurrent Replay Distributed DQN (R2D2) (Kapturowski et al., 2018), (ii) ex\u0002ploration with a family of policies and the Never Give Up (NGU) intrinsic reward (Badia et al., 2019), and (iii) a meta-controller that dynamically adjusts the discount factor and balances exploration and exploitation throughout the course of training, by selecting from a family of policies. Below, we give a general introduction to the problem setting and some of the relevant components of Agent57. Problem definition. We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP) (Puterman, 1994). The goal is to find a policy 𝜋 that maximises the expected sum of future discounted rewards, 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡], where 𝛾 ∈ [0, 1) is the discount factor, 𝑟𝑡 = 𝑟(𝑥𝑡 , 𝑎𝑡) is the reward at time t, 𝑥𝑡 is the state at time t, and 𝑎𝑡 ∼ 𝜋(𝑎|𝑥𝑡) is the action generated by following some policy 𝜋. In the off-policy learning setting, data generated by a behavior policy 𝜇 is used to 3 Human-level Atari 200x faster learn about the target policy 𝜋. This can be achieved by employing a variant of Q-learning (Watkins and Dayan, 1992) to estimate the action-value function, 𝑄 𝜋 (𝑥, 𝑎) = 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡 |𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎]. The estimated action-value function can then be used to derive a new policy 𝜋(𝑎|𝑥) using the 𝜖-greedy operator G𝜖 (Sutton and Barto, 2018). 1 This new policy can then be used as target policy for another iteration, repeating the process. Agent57 uses a deep neural network with parameters 𝜃 to estimate action-value functions, 𝑄 𝜋 (𝑥, 𝑎; 𝜃) 2 , trained on return estimates 𝐺𝑡 derived with Retrace from sequences of off-policy data (Munos et al., 2016).\n",
      "    > [ 0.73764092  1.50381505 -3.48627043] Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization lay\u0002ers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy over time. 1. Introduction To develop generally capable agents, the question of how to evaluate them is paramount. The Arcade Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate agents on an diverse set of tasks which are interesting to humans, and developed externally to the Reinforcement Learning (RL) community. As a result, several games exhibit reward structures which are highly adversarial to many popular algorithms. Mean and median human normalized scores (HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating deep RL agents. Recent progress has allowed state-of-the-art algorithms to greatly exceed average human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al., 2020; Van Hasselt et al., 2016). However, it has been argued that mean or median HNS might not be well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al., 2019). Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on most games, but completely fail to learn on a small number of them. Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all 57 Atari games. However, such generality came at the cost of data efficiency; requiring tens of billions of environment interactions to achieve above average-human performance in some games, reaching a figure of 78 billion frames before beating the human benchmark in all games.\n",
      "      > [ 0.54513168  1.30529583 -3.62534451] Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline.\n",
      "        > [ 0.24919949  1.10780084 -3.36006045] Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception.\n",
      "        > [ 0.97025383  1.59567904 -3.85882521] The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark.\n",
      "        > [ 0.91469681  1.53874981 -4.08522415] Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve.\n",
      "        > [-0.30261958  0.52889246 -3.80358195] Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline.\n",
      "      > [ 0.0083265   1.45206392 -3.69237518] We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization lay\u0002ers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy over time. 1. Introduction To develop generally capable agents, the question of how to evaluate them is paramount.\n",
      "        > [ 0.0091347   1.04603553 -3.89195371] We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent.\n",
      "        > [-0.01612254  1.34338641 -4.03994083] We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero.\n",
      "        > [ 0.51877671  1.45161867 -3.44339466] The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization lay\u0002ers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy over time.\n",
      "        > [-0.39243293 -0.37381107 -3.73424196] 1. Introduction To develop generally capable agents, the question of how to evaluate them is paramount.\n",
      "      > [ 0.21551606  1.74004591 -3.36210585] The Arcade Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate agents on an diverse set of tasks which are interesting to humans, and developed externally to the Reinforcement Learning (RL) community. As a result, several games exhibit reward structures which are highly adversarial to many popular algorithms. Mean and median human normalized scores (HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating deep RL agents. Recent progress has allowed state-of-the-art algorithms to greatly exceed average human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al., 2020; Van Hasselt et al., 2016).\n",
      "        > [-0.21065013  1.36229897 -3.04802752] The Arcade Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate agents on an diverse set of tasks which are interesting to humans, and developed externally to the Reinforcement Learning (RL) community.\n",
      "        > [ 0.77098304  3.18415594 -3.73751974] As a result, several games exhibit reward structures which are highly adversarial to many popular algorithms.\n",
      "        > [ 0.33797631  0.72383374 -3.8279717 ] Mean and median human normalized scores (HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating deep RL agents.\n",
      "        > [ 0.99851942  1.93932235 -4.06023741] Recent progress has allowed state-of-the-art algorithms to greatly exceed average human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al., 2020; Van Hasselt et al., 2016).\n",
      "      > [ 0.53349113  1.5636363  -3.62031221] However, it has been argued that mean or median HNS might not be well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al., 2019). Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on most games, but completely fail to learn on a small number of them. Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all 57 Atari games. However, such generality came at the cost of data efficiency; requiring tens of billions of environment interactions to achieve above average-human performance in some games, reaching a figure of 78 billion frames before beating the human benchmark in all games.\n",
      "        > [ 0.27158213  0.87324095 -3.21215916] However, it has been argued that mean or median HNS might not be well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al., 2019).\n",
      "        > [ 0.56424135  1.94155169 -3.5372982 ] Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on most games, but completely fail to learn on a small number of them.\n",
      "        > [ 1.05856872  0.78969014 -3.97198629] Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all 57 Atari games.\n",
      "        > [ 0.66503614  2.52805543 -3.57940173] However, such generality came at the cost of data efficiency; requiring tens of billions of environment interactions to achieve above average-human performance in some games, reaching a figure of 78 billion frames before beating the human benchmark in all games.\n",
      "    > [-0.54222685 -0.21036945 -3.53933764] Data efficiency remains a desirable property for agents to possess, as many real-world challenges are data-limited by time and cost constraints (Dulac-Arnold et al., 2019). In this work, our aim is to develop an agent that is as general as Agent57 but that requires only a fraction of the environment interactions to achieve the same result. © 2022 DeepMind. All rights reserved arXiv:2209.07550v1 [cs.LG] 15 Sep 2022 Human-level Atari 200x faster Skiing Private Eye Pitfall! Montezuma's Revenge Surround Asteroids Ice Hockey Solaris Bowling Gravitar H.E.R.O. Berzerk Frostbite Tennis Alien Tutankham Riverraid Ms. Pac-Man Pong Beam Rider Chopper Command Seaquest Double Dunk Freeway Space Invaders Bank Heist Venture Battle Zone Asterix Q*BERT Amidar Name This Game Yars' Revenge Phoenix Time Pilot Demon Attack Wizard Of Wor Breakout Video Pinball Fishing Derby Enduro Stargunner Assault Zaxxon Jamesbond Centipede Kangaroo Atlantis Up'n Down Defender Boxing Robotank Kung-Fu Master Gopher Road Runner Crazy Climber Krull 1M 10M 100M 1B 10B 100B Frames to > human score 390M Agent57 MEME Figure 1 | Number of environment frames required by agents to outperform the human baseline on each game (in log-scale). Lower is better. On average, MEME achieves above human scores using 63× fewer environment interactions than Agent57. The smallest improvement is 9× (road_runner), the maximum is 721× (skiing), and the median across the suite is 35×. We observe small variance across seeds (c.f. Figure 8). There exist two main trends in the literature when it comes to measuring improvements in the learning capabilities of agents. One approach consists in measuring performance after a limited budget of interactions with the environment. While this type of evaluation has led to important progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019). On the other hand, one can aim to achieve a target end-performance with as few interactions as possible (Schmitt et al., 2020; Silver et al., 2017, 2018). Since our goal is to show that our new agent is as general as Agent57, while being more data efficient, we focus on the latter approach.\n",
      "      > [ 0.13913843  0.73815775 -3.60093856] Data efficiency remains a desirable property for agents to possess, as many real-world challenges are data-limited by time and cost constraints (Dulac-Arnold et al., 2019). In this work, our aim is to develop an agent that is as general as Agent57 but that requires only a fraction of the environment interactions to achieve the same result. © 2022 DeepMind. All rights reserved arXiv:2209.07550v1 [cs.LG] 15 Sep 2022 Human-level Atari 200x faster Skiing Private Eye Pitfall!\n",
      "        > [ 0.43127725  1.40037048 -3.55078578] Data efficiency remains a desirable property for agents to possess, as many real-world challenges are data-limited by time and cost constraints (Dulac-Arnold et al., 2019).\n",
      "        > [-0.45719379  0.14279269 -3.62644744] In this work, our aim is to develop an agent that is as general as Agent57 but that requires only a fraction of the environment interactions to achieve the same result.\n",
      "        > [ 0.73377609 -0.19811249 -2.69165039] © 2022 DeepMind.\n",
      "        > [ 0.28785694  0.82403886 -3.36685371] All rights reserved arXiv:2209.07550v1 [cs.LG] 15 Sep 2022 Human-level Atari 200x faster Skiing Private Eye Pitfall!\n",
      "      > [ 0.18977275  1.46257591 -4.27421808] Montezuma's Revenge Surround Asteroids Ice Hockey Solaris Bowling Gravitar H.E.R.O. Berzerk Frostbite Tennis Alien Tutankham Riverraid Ms. Pac-Man Pong Beam Rider Chopper Command Seaquest Double Dunk Freeway Space Invaders Bank Heist Venture Battle Zone Asterix Q*BERT Amidar Name This Game Yars' Revenge Phoenix Time Pilot Demon Attack Wizard Of Wor Breakout Video Pinball Fishing Derby Enduro Stargunner Assault Zaxxon Jamesbond Centipede Kangaroo Atlantis Up'n Down Defender Boxing Robotank Kung-Fu Master Gopher Road Runner Crazy Climber Krull 1M 10M 100M 1B 10B 100B Frames to > human score 390M Agent57 MEME Figure 1 | Number of environment frames required by agents to outperform the human baseline on each game (in log-scale). Lower is better. On average, MEME achieves above human scores using 63× fewer environment interactions than Agent57.\n",
      "        > [ 0.74667752  2.50994587 -3.57808161] Montezuma's Revenge Surround Asteroids Ice Hockey Solaris Bowling Gravitar H.E.R.O.\n",
      "        > [-0.14634939  1.33877432 -4.00952482] Berzerk Frostbite Tennis Alien Tutankham Riverraid Ms. Pac-Man Pong Beam Rider Chopper Command Seaquest Double Dunk Freeway Space Invaders Bank Heist Venture Battle Zone Asterix Q*BERT Amidar Name This Game Yars' Revenge Phoenix Time Pilot Demon Attack Wizard Of Wor Breakout Video Pinball Fishing Derby Enduro Stargunner Assault Zaxxon Jamesbond Centipede Kangaroo Atlantis Up'n Down Defender Boxing Robotank Kung-Fu Master Gopher Road Runner Crazy Climber Krull 1M 10M 100M 1B 10B 100B Frames to > human score 390M Agent57 MEME Figure 1 | Number of environment frames required by agents to outperform the human baseline on each game (in log-scale).\n",
      "        > [ 1.14254117  0.6736536  -3.83900976] Lower is better.\n",
      "        > [-0.83767539  0.15573472 -3.90117121] On average, MEME achieves above human scores using 63× fewer environment interactions than Agent57.\n",
      "      > [-0.10683829  0.90246546 -3.72813535] The smallest improvement is 9× (road_runner), the maximum is 721× (skiing), and the median across the suite is 35×. We observe small variance across seeds (c.f. Figure 8). There exist two main trends in the literature when it comes to measuring improvements in the learning capabilities of agents.\n",
      "        > [-0.10170384  1.69497347 -3.27114916] The smallest improvement is 9× (road_runner), the maximum is 721× (skiing), and the median across the suite is 35×.\n",
      "        > [ 1.30813992  0.44627652 -4.222229  ] We observe small variance across seeds (c.f.\n",
      "        > [-0.10386238  0.59339112 -3.27320719] Figure 8).\n",
      "        > [-0.35651377  0.11115725 -3.81634045] There exist two main trends in the literature when it comes to measuring improvements in the learning capabilities of agents.\n",
      "      > [ 0.31954104  0.76051486 -3.56013656] One approach consists in measuring performance after a limited budget of interactions with the environment. While this type of evaluation has led to important progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019). On the other hand, one can aim to achieve a target end-performance with as few interactions as possible (Schmitt et al., 2020; Silver et al., 2017, 2018). Since our goal is to show that our new agent is as general as Agent57, while being more data efficient, we focus on the latter approach.\n",
      "        > [ 0.5768609   0.62879241 -3.87962294] One approach consists in measuring performance after a limited budget of interactions with the environment.\n",
      "        > [ 1.14975643  1.62443447 -2.55217934] While this type of evaluation has led to important progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019).\n",
      "        > [-0.20560002  1.14403808 -3.24420881] On the other hand, one can aim to achieve a target end-performance with as few interactions as possible (Schmitt et al., 2020; Silver et al., 2017, 2018).\n",
      "        > [-0.39901352 -0.00746727 -3.81951904] Since our goal is to show that our new agent is as general as Agent57, while being more data efficient, we focus on the latter approach.\n",
      "    > [ 0.45883456  1.01980472 -3.251858  ] Our contributions can be summarized as follows. Building off Agent57, we carefully examine bottlenecks which slow down learning and address instabilities that arise when these bottlenecks are removed. We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based Exploration agent, which introduces solutions to enable taking advantage of three approaches that would otherwise lead to instabilities: training the value functions of the whole family of policies from Agent57 in parallel, on all policies’ transitions (instead of just the behaviour policy transitions), bootstrapping from the online network, and using high replay ratios. These solutions include carefully normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016) update target with a soft variant of Watkins’ Q(𝜆) (Watkins and Dayan, 1992) that enables faster signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for value updates. Moreover, we explore several recent advances in deep learning and determine which of them are beneficial for non-stationary problems like the ones considered in this work. Finally, we examine approaches to robustify performance by introducing a policy distillation mechanism that learns a policy head based on the actions obtained from the value network without being sensitive to value magnitudes. Our agent outperforms the human baseline across all 57 Atari games in 390M frames, using two orders of magnitude fewer interactions with the environment than Agent57 as shown in Figure 1. 2. Related work Large scale distributed agents have exhibited compelling results in recent years. Actor-critic (Espeholt et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari 2 Human-level Atari 200x faster 57 benchmark. Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that achieve competitive results on Atari. Finally, search-augmented distributed agents (Hessel et al., 2021; Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely they hold the highest mean and median human normalized scores over the 57 Atari games. However, all these methods show the same failure mode: they perform poorly in hard exploration games, such as Pitfall!, and Montezuma’s Revenge. In contrast, Agent57 (Badia et al., 2020) surpassed the human benchmark on all 57 games, showing better general performance. Go-Explore (Ecoffet et al., 2021) similarly achieved such general performance, by relying on coarse-grained state representations via a downscaling function that is highly specific to Atari. Learning as much as possible from previous experience is key for data-efficiency. Since it is often desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002; Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over the same batch of experience in policy gradient methods to avoid collecting new transitions for every learning step (Schulman et al., 2017).\n",
      "      > [-0.2872633   0.74760199 -3.33247542] Our contributions can be summarized as follows. Building off Agent57, we carefully examine bottlenecks which slow down learning and address instabilities that arise when these bottlenecks are removed. We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based Exploration agent, which introduces solutions to enable taking advantage of three approaches that would otherwise lead to instabilities: training the value functions of the whole family of policies from Agent57 in parallel, on all policies’ transitions (instead of just the behaviour policy transitions), bootstrapping from the online network, and using high replay ratios. These solutions include carefully normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016) update target with a soft variant of Watkins’ Q(𝜆) (Watkins and Dayan, 1992) that enables faster signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for value updates.\n",
      "        > [ 0.7928825   1.13360858 -3.90872192] Our contributions can be summarized as follows.\n",
      "        > [ 0.56057423  0.54478264 -3.40270996] Building off Agent57, we carefully examine bottlenecks which slow down learning and address instabilities that arise when these bottlenecks are removed.\n",
      "        > [-0.65775555  0.99744618 -3.62086987] We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based Exploration agent, which introduces solutions to enable taking advantage of three approaches that would otherwise lead to instabilities: training the value functions of the whole family of policies from Agent57 in parallel, on all policies’ transitions (instead of just the behaviour policy transitions), bootstrapping from the online network, and using high replay ratios.\n",
      "        > [ 0.38410351  0.58741522 -3.43422174] These solutions include carefully normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016) update target with a soft variant of Watkins’ Q(𝜆) (Watkins and Dayan, 1992) that enables faster signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for value updates.\n",
      "      > [ 0.79653192  1.25508618 -3.48983192] Moreover, we explore several recent advances in deep learning and determine which of them are beneficial for non-stationary problems like the ones considered in this work. Finally, we examine approaches to robustify performance by introducing a policy distillation mechanism that learns a policy head based on the actions obtained from the value network without being sensitive to value magnitudes. Our agent outperforms the human baseline across all 57 Atari games in 390M frames, using two orders of magnitude fewer interactions with the environment than Agent57 as shown in Figure 1. 2. Related work Large scale distributed agents have exhibited compelling results in recent years.\n",
      "        > [ 0.30601859  0.92427826 -3.0291748 ] Moreover, we explore several recent advances in deep learning and determine which of them are beneficial for non-stationary problems like the ones considered in this work.\n",
      "        > [ 0.90404004  1.54152775 -3.88966799] Finally, we examine approaches to robustify performance by introducing a policy distillation mechanism that learns a policy head based on the actions obtained from the value network without being sensitive to value magnitudes.\n",
      "        > [ 1.24271429  0.76500911 -3.66317892] Our agent outperforms the human baseline across all 57 Atari games in 390M frames, using two orders of magnitude fewer interactions with the environment than Agent57 as shown in Figure 1.\n",
      "        > [-0.05279379  1.21547258 -4.11268568] 2. Related work Large scale distributed agents have exhibited compelling results in recent years.\n",
      "      > [ 0.08963294  0.91520596 -3.45079494] Actor-critic (Espeholt et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari 2 Human-level Atari 200x faster 57 benchmark. Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that achieve competitive results on Atari. Finally, search-augmented distributed agents (Hessel et al., 2021; Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely they hold the highest mean and median human normalized scores over the 57 Atari games. However, all these methods show the same failure mode: they perform poorly in hard exploration games, such as Pitfall!, and Montezuma’s Revenge.\n",
      "        > [-0.1840429   0.6762746  -3.69677806] Actor-critic (Espeholt et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari 2 Human-level Atari 200x faster 57 benchmark.\n",
      "        > [ 0.1209147   1.23092747 -3.5815444 ] Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that achieve competitive results on Atari.\n",
      "        > [ 0.73156685  1.02851462 -3.73471665] Finally, search-augmented distributed agents (Hessel et al., 2021; Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely they hold the highest mean and median human normalized scores over the 57 Atari games.\n",
      "        > [ 0.50331199  0.80987477 -3.50011086] However, all these methods show the same failure mode: they perform poorly in hard exploration games, such as Pitfall!, and Montezuma’s Revenge.\n",
      "      > [ 0.39745802  1.17727816 -3.40199876] In contrast, Agent57 (Badia et al., 2020) surpassed the human benchmark on all 57 games, showing better general performance. Go-Explore (Ecoffet et al., 2021) similarly achieved such general performance, by relying on coarse-grained state representations via a downscaling function that is highly specific to Atari. Learning as much as possible from previous experience is key for data-efficiency. Since it is often desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002; Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over the same batch of experience in policy gradient methods to avoid collecting new transitions for every learning step (Schulman et al., 2017).\n",
      "        > [ 0.93458122  0.94471395 -3.85838032] In contrast, Agent57 (Badia et al., 2020) surpassed the human benchmark on all 57 games, showing better general performance.\n",
      "        > [ 0.48246258  1.68996477 -3.45937347] Go-Explore (Ecoffet et al., 2021) similarly achieved such general performance, by relying on coarse-grained state representations via a downscaling function that is highly specific to Atari.\n",
      "        > [ 0.2355969   1.03137898 -3.71926713] Learning as much as possible from previous experience is key for data-efficiency.\n",
      "        > [-0.22155008  0.60037041 -3.54233456] Since it is often desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002; Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over the same batch of experience in policy gradient methods to avoid collecting new transitions for every learning step (Schulman et al., 2017).\n",
      "    > [ 0.51335281  1.02100384 -2.83701015] This decoupling between collecting experience and learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al., 2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005). Multiple approaches for making more efficient use of a replay buffer have been proposed, including prioritized sampling of transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al., 2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018), or reanalyzing old trajectories with the most recent version of a learned model to generate new targets in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz et al., 2017). The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps (400k frames). Work on this benchmark has focused on leveraging the use of models (Kaiser et al., 2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel, 2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020). While we consider this to be an important line of research, this tight budget produces an incentive to focus on a subset of games where exploration is easier, and it is unclear some games can be solved from scratch with such a small data budget. Such a setting is likely to prevent any meaningful learning on hard-exploration games, which is in contrast with the goal of our work. 3. Background: Agent57 Our work builds on top of Agent57, which combines three main ideas: (i) a distributed deep RL framework based on Recurrent Replay Distributed DQN (R2D2) (Kapturowski et al., 2018), (ii) ex\u0002ploration with a family of policies and the Never Give Up (NGU) intrinsic reward (Badia et al., 2019), and (iii) a meta-controller that dynamically adjusts the discount factor and balances exploration and exploitation throughout the course of training, by selecting from a family of policies. Below, we give a general introduction to the problem setting and some of the relevant components of Agent57. Problem definition. We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP) (Puterman, 1994). The goal is to find a policy 𝜋 that maximises the expected sum of future discounted rewards, 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡], where 𝛾 ∈ [0, 1) is the discount factor, 𝑟𝑡 = 𝑟(𝑥𝑡 , 𝑎𝑡) is the reward at time t, 𝑥𝑡 is the state at time t, and 𝑎𝑡 ∼ 𝜋(𝑎|𝑥𝑡) is the action generated by following some policy 𝜋. In the off-policy learning setting, data generated by a behavior policy 𝜇 is used to 3 Human-level Atari 200x faster learn about the target policy 𝜋. This can be achieved by employing a variant of Q-learning (Watkins and Dayan, 1992) to estimate the action-value function, 𝑄 𝜋 (𝑥, 𝑎) = 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡 |𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎]. The estimated action-value function can then be used to derive a new policy 𝜋(𝑎|𝑥) using the 𝜖-greedy operator G𝜖 (Sutton and Barto, 2018). 1 This new policy can then be used as target policy for another iteration, repeating the process. Agent57 uses a deep neural network with parameters 𝜃 to estimate action-value functions, 𝑄 𝜋 (𝑥, 𝑎; 𝜃) 2 , trained on return estimates 𝐺𝑡 derived with Retrace from sequences of off-policy data (Munos et al., 2016).\n",
      "      > [ 0.38228187  0.5783661  -3.69414115] This decoupling between collecting experience and learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al., 2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005). Multiple approaches for making more efficient use of a replay buffer have been proposed, including prioritized sampling of transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al., 2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018), or reanalyzing old trajectories with the most recent version of a learned model to generate new targets in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz et al., 2017). The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps (400k frames). Work on this benchmark has focused on leveraging the use of models (Kaiser et al., 2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel, 2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020).\n",
      "        > [ 0.72470093  0.15437357 -3.72896433] This decoupling between collecting experience and learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al., 2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005).\n",
      "        > [ 0.15986696  0.17476125 -3.74717593] Multiple approaches for making more efficient use of a replay buffer have been proposed, including prioritized sampling of transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al., 2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018), or reanalyzing old trajectories with the most recent version of a learned model to generate new targets in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz et al., 2017).\n",
      "        > [ 0.93368649  1.350155   -3.55754185] The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps (400k frames).\n",
      "        > [ 0.34625167  0.85442144 -3.50115228] Work on this benchmark has focused on leveraging the use of models (Kaiser et al., 2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel, 2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020).\n",
      "      > [ 0.31430274  1.5839783  -3.24490714] While we consider this to be an important line of research, this tight budget produces an incentive to focus on a subset of games where exploration is easier, and it is unclear some games can be solved from scratch with such a small data budget. Such a setting is likely to prevent any meaningful learning on hard-exploration games, which is in contrast with the goal of our work. 3. Background: Agent57 Our work builds on top of Agent57, which combines three main ideas: (i) a distributed deep RL framework based on Recurrent Replay Distributed DQN (R2D2) (Kapturowski et al., 2018), (ii) ex\u0002ploration with a family of policies and the Never Give Up (NGU) intrinsic reward (Badia et al., 2019), and (iii) a meta-controller that dynamically adjusts the discount factor and balances exploration and exploitation throughout the course of training, by selecting from a family of policies. Below, we give a general introduction to the problem setting and some of the relevant components of Agent57.\n",
      "        > [ 0.81355953  2.50255799 -3.26226234] While we consider this to be an important line of research, this tight budget produces an incentive to focus on a subset of games where exploration is easier, and it is unclear some games can be solved from scratch with such a small data budget.\n",
      "        > [ 0.66747922  1.9412992  -3.04060674] Such a setting is likely to prevent any meaningful learning on hard-exploration games, which is in contrast with the goal of our work.\n",
      "        > [-0.01680447  0.75847727 -3.33639812] 3. Background: Agent57 Our work builds on top of Agent57, which combines three main ideas: (i) a distributed deep RL framework based on Recurrent Replay Distributed DQN (R2D2) (Kapturowski et al., 2018), (ii) ex\u0002ploration with a family of policies and the Never Give Up (NGU) intrinsic reward (Badia et al., 2019), and (iii) a meta-controller that dynamically adjusts the discount factor and balances exploration and exploitation throughout the course of training, by selecting from a family of policies.\n",
      "        > [ 0.59555101  0.67579812 -3.67723036] Below, we give a general introduction to the problem setting and some of the relevant components of Agent57.\n",
      "      > [ 1.13682389  1.23473656 -3.4669404 ] Problem definition. We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP) (Puterman, 1994). The goal is to find a policy 𝜋 that maximises the expected sum of future discounted rewards, 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡], where 𝛾 ∈ [0, 1) is the discount factor, 𝑟𝑡 = 𝑟(𝑥𝑡 , 𝑎𝑡) is the reward at time t, 𝑥𝑡 is the state at time t, and 𝑎𝑡 ∼ 𝜋(𝑎|𝑥𝑡) is the action generated by following some policy 𝜋. In the off-policy learning setting, data generated by a behavior policy 𝜇 is used to 3 Human-level Atari 200x faster learn about the target policy 𝜋.\n",
      "        > [ 1.4677856   1.7406131  -3.49562263] Problem definition.\n",
      "        > [ 1.5230782   0.3142308  -3.37746096] We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP) (Puterman, 1994).\n",
      "        > [ 0.4208498  1.8430717 -3.5412631] The goal is to find a policy 𝜋 that maximises the expected sum of future discounted rewards, 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡], where 𝛾 ∈ [0, 1) is the discount factor, 𝑟𝑡 = 𝑟(𝑥𝑡 , 𝑎𝑡) is the reward at time t, 𝑥𝑡 is the state at time t, and 𝑎𝑡 ∼ 𝜋(𝑎|𝑥𝑡) is the action generated by following some policy 𝜋.\n",
      "        > [ 0.72746301  1.36823213 -3.78030634] In the off-policy learning setting, data generated by a behavior policy 𝜇 is used to 3 Human-level Atari 200x faster learn about the target policy 𝜋.\n",
      "      > [ 0.1502223   1.03220332 -3.15926433] This can be achieved by employing a variant of Q-learning (Watkins and Dayan, 1992) to estimate the action-value function, 𝑄 𝜋 (𝑥, 𝑎) = 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡 |𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎]. The estimated action-value function can then be used to derive a new policy 𝜋(𝑎|𝑥) using the 𝜖-greedy operator G𝜖 (Sutton and Barto, 2018). 1 This new policy can then be used as target policy for another iteration, repeating the process. Agent57 uses a deep neural network with parameters 𝜃 to estimate action-value functions, 𝑄 𝜋 (𝑥, 𝑎; 𝜃) 2 , trained on return estimates 𝐺𝑡 derived with Retrace from sequences of off-policy data (Munos et al., 2016).\n",
      "        > [-0.34489465  0.68378484 -3.60048103] This can be achieved by employing a variant of Q-learning (Watkins and Dayan, 1992) to estimate the action-value function, 𝑄 𝜋 (𝑥, 𝑎) = 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡 |𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎].\n",
      "        > [ 0.38787207  1.4623189  -3.32228661] The estimated action-value function can then be used to derive a new policy 𝜋(𝑎|𝑥) using the 𝜖-greedy operator G𝜖 (Sutton and Barto, 2018).\n",
      "        > [ 0.04103762  0.47426555 -3.46172857] 1 This new policy can then be used as target policy for another iteration, repeating the process.\n",
      "        > [ 0.20426732  0.21348107 -3.31607556] Agent57 uses a deep neural network with parameters 𝜃 to estimate action-value functions, 𝑄 𝜋 (𝑥, 𝑎; 𝜃) 2 , trained on return estimates 𝐺𝑡 derived with Retrace from sequences of off-policy data (Munos et al., 2016).\n",
      "  > [ 1.05901265  1.22528863 -3.12542081] In order to stabilize learning, a target network is used for bootstrapping the return estimates using double Q-learning (Van Hasselt et al., 2016); the parameters of this target network, 𝜃𝑇 , are periodically copied from the online network parameters 𝜃 (Mnih et al., 2015). Distributed RL framework. Agent57 is a distributed deep RL agent based on R2D2 that decouples acting from learning. Multiple actors interact with independent copies of the environment and feed trajectories to a central replay buffer. A separate learning process obtains trajectories from this buffer using prioritized sampling and updates the neural network parameters to predict the action-value function at each state. Actors obtain parameters from the learner periodically. See Appendix D for more details. Exploration with NGU. Agent57 uses the Never Give Up (NGU) intrinsic reward to encourage exploration. It aims at learning a family of 𝑁 = 32 policies which maximize different weightings of the extrinsic reward given by the environment (𝑟 𝑒 𝑡 ) and the intrinsic reward (𝑟 𝑖 𝑡 ), 𝑟𝑗,𝑡 = 𝑟 𝑒 𝑡 + 𝛽𝑗𝑟 𝑖 𝑡 (𝛽𝑗 ∈ ℝ+ , 𝑗 ∈ {0, . . . , 𝑁 −1}). The value of 𝛽𝑗 controls the degree of exploration, with higher values encouraging more exploratory behaviors, and each policy in the family is optimized with a different discount factor 𝛾𝑗 . The Universal Value Function Approximators (UVFA) framework (Schaul et al., 2015a) is employed to efficiently learn 𝑄 𝜋 𝑗 (𝑥, 𝑎; 𝜃) = 𝔼𝜋𝑗[ Í 𝑡≥0 𝛾 𝑡 𝑗 𝑟𝑗,𝑡|𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎] (we use a shorthand notation 𝑄 𝑗 (𝑥, 𝑎; 𝜃)) for 𝑗 ∈ {0, . . . , 𝑁 − 1} using a single set of shared parameters 𝜃. The policy 𝜋 𝑗 (𝑎|𝑥) can then be derived using the 𝜖-greedy operator as G𝜖𝑄 𝑗 (𝑥, 𝑎; 𝜃). We refer the reader to Appendix G for more details. Meta-controller. Agent57 introduces an adaptive meta-controller that decides which policies from the family of N policies to use for collecting experience based on their episodic returns. This naturally creates a curriculum over 𝛽𝑗 and 𝛾𝑗 by adapting their value throughout training. This optimization process is formulated as a non-stationary bandit problem. A detailed description about the meta\u0002controller implementation is provided in Appendix E. Q-function separation. The architecture of the Q-function in Agent57 is implemented as two separate networks in order to split the intrinsic and extrinsic components. The network parameters of 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑒) and 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑖) are separate and independently optimized with 𝑟 𝑒 𝑗 and 𝑟 𝑖 𝑗 , respectively. The main motivation behind this decomposition is to prevent the gradients of the decomposed intrinsic and extrinsic value function heads from interfering with each other. This can be beneficial in environments where the intrinsic reward is poorly aligned with the task’s extrinsic reward. 4. MEME: Improving the data efficiency of Agent57 This section describes the main algorithmic contributions of the MEME agent, aimed at improving the data-efficiency of Agent57. These contributions aim to achieve faster propagation of learning signals related to rare events ( A ), stabilize learning under differing value scales ( B ), improve the neural network architecture ( C ), and make updates more robust under a rapidly-changing policy ( D ). For 1We also use G B G0 to denote the pure greedy operator. 2For convenience, we occasionally omit (𝑥, 𝑎) or 𝜃 from 𝑄(𝑥, 𝑎; 𝜃), 𝜋(𝑥, 𝑎; 𝜃) when there is no danger of confusion. 4 Human-level Atari 200x faster Torso LSTM Q-value Head Policy Head Qj (at , xt ) 𝛑j (at | xt ) N-mixtures Torso LSTM Q-value Head Policy Head Q1 (at , x t ) 𝛑1 (at | xt ) Member policy 1 Q-value Head Policy Head QN (at , xt ) 𝛑N (at | xt ) Member policy 1 … … Dueling Head Dueling Head Qi (at , xt ) Qe (at , xt ) + Qj (at , xt ) Q-value Head N policy family members Torso LSTM Q-value Head Policy Head Q1 (at , xt ) 𝛑1 (at | xt ) N policy family members … QN (at , x t ) 𝛑N (at | xt ) Torso LSTM Q1 (xt , at ; θ) 𝛑1 (at | xt ) N policy family members … QN (xt , at ; θ) 𝛑N (at | xt ) Policy Head Q-value Head Figure 2 | MEME agent network architecture. The output of the LSTM block is passed to each of the N members of the family of policies, depicted as a light-grey box. Each policy consists of an Q-value and policy heads. The Q-value head is similar as in Agent57 paper, while the policy head is introduced for acting and target computation, and trained via policy distillation. clarity of exposition, we label methods according to the type of limitation they address. A1 Bootstrapping with online network. Target networks are frequently used in conjunction with value-based agents due to their stabilizing effect when learning from off-policy data (Mnih et al., 2015; Van Hasselt et al., 2016). This design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate. This issue can be mitigated to some extent by simply updating the target network more frequently, but the result is typically a less stable agent. To accelerate signal propagation while maintaining stability, we use online network bootstrapping, and we stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute to the loss. The trust region masks out the loss at any timestep for which both of the following conditions hold: |𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )| > 𝛼𝜎𝑗 (1) sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )) ≠ sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝐺𝑡) (2) where 𝛼 is a fixed hyperparameter, 𝐺𝑡 denotes the return estimate, 𝜃 and 𝜃𝑇 denote the online and target parameters respectively, and 𝜎𝑗 is the standard deviation of the TD-errors (a more precise description of which we defer until B1). Intuitively, we only mask if the current value of the online network is outside of the trust region (Equation 1) and the sign of the TD-error points away from the trust region (Equation 2), as depicted in Figure 3 in red. We note that a very similar trust region scheme is used for the value-function in most Proximal Policy Optimization (PPO) implementations (Schulman et al., 2017), though not described in the original paper. In contrast, the PPO version instead uses a constant threshold, and thus is not able to adapt to differing scales of value functions. Qj(xt ,at ;θT ) Qj(xt ,at ;θT ) + ασ Qj j (xt ,at ;θT ) - ασj Qj(xt ,at ;θ) Figure 3 | Trust region. The position of dots is given by the relationship between the values predicted by the online network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃), and the values predicted by the target network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 ) (Equation 1 and left hand side of Equation 2), the box represents the trust region bounds defined in Equation 1, and the direction of the arrow is given by the right hand side of Equation 2. Green-colored transitions are used in the loss computation, whereas red ones are masked out. A2 Target computation with tolerance. Agent57 uses Retrace (Munos et al., 2016) to compute return estimates from off-policy data, but we observed that it tends to cut traces too aggressively when 5 Human-level Atari 200x faster using 𝜖-greedy policies thus slowing down the propagation of information into the value function. Preliminary experiments showed that data-efficiency was improved in many dense-reward tasks when replacing Retrace with Peng’s Q(𝜆) (Peng and Williams, 1994), but its lack of off-policy corrections tends to result in degraded performance as data becomes more off-policy (e.g. by increasing the expected number of times that a sequence is sampled from replay, or by sharing data across a family of policies). This motivates us to propose an alternative return estimator, which we derive from Q(𝜆) (Watkins and Dayan, 1992): 𝐺𝑡 = max 𝑎 𝑄(𝑥𝑡 , 𝑎) + ∑︁ 𝑘≥0 Ö 𝑘−1 𝑖=0 𝜆𝑖 ! 𝛾 𝑘 (𝑟𝑡+𝑘 + 𝛾 max 𝑎 𝑄(𝑥𝑡+𝑘+1, 𝑎) − max 𝑎 𝑄(𝑥𝑡+𝑘, 𝑎)) (3) where Î𝑘−1 𝑖=0 𝜆𝑖 ∈ [0, 1] effectively controls how much information from the future is used in the return estimation and is generally used as a trace cutting coefficient to perform off-policy correction. Peng’s Q(𝜆) does not perform any kind of off-policy correction and sets 𝜆𝑖 = 𝜆, whereas Watkins’ Q(𝜆) (Watkins and Dayan, 1992) aggressively cuts traces whenever it encounters an off-policy action by using 𝜆𝑖 = 𝜆𝟙𝑎𝑖 ∈argmax𝑎𝑄(𝑥𝑖 ,𝑎) , where 𝟙 denotes the indicator function. We propose to use a softer trace cutting mechanism by adding a fixed tolerance parameter 𝜅 and taking the expectation of trace coefficients under 𝜋: 𝜆𝑖 = 𝜆𝔼𝑎∼𝜋(𝑎|𝑥𝑡) \u0002 𝟙[𝑄(𝑥𝑡 ,𝑎𝑡;𝜃) ≥𝑄(𝑥𝑡 ,𝑎;𝜃)−𝜅|𝑄(𝑥𝑖 ,𝑎;𝜃) |] \u0003 (4) Finally, we replace all occurrences of the max operator in Equation 3 with the expectation under 𝜋. The resulting return estimator, which we denote Soft Watkins Q(𝜆), leads to more transitions being used and increased sample efficiency. Note that Watkins Q(𝜆) is recovered when setting 𝜅 = 0 and 𝜋 = G (𝑄). B1 Loss and priority normalization. As we learn a family of Q-functions which vary over a wide range of discount factors and intrinsic reward scales, we expect that the Q-functions will vary considerably in scale. This may cause the larger-scale Q-values to dominate learning and destabilize learning of smaller Q-values. This is a particular concern in environments with very small extrinsic reward scales. To counteract this effect we introduce a normalization scheme on the TD-errors similar to that used in Schaul et al. (2021). Specifically, we compute a running estimate of the standard deviation of TD-errors of the online network 𝜎 running 𝑗 as well as a batch standard deviation 𝜎 batch 𝑗 , and compute 𝜎𝑗 = max(𝜎 running 𝑗 , 𝜎batch 𝑗 , 𝜖), where 𝜖 acts as small threshold to avoid amplification of noise past a specified scale, which we fix to 0.01 in all our experiments. We then divide the TD-errors by 𝜎𝑗 when computing both the loss and priorities. As opposed to Schaul et al.\n",
      "    > [ 0.24724634  0.8579281  -3.099334  ] In order to stabilize learning, a target network is used for bootstrapping the return estimates using double Q-learning (Van Hasselt et al., 2016); the parameters of this target network, 𝜃𝑇 , are periodically copied from the online network parameters 𝜃 (Mnih et al., 2015). Distributed RL framework. Agent57 is a distributed deep RL agent based on R2D2 that decouples acting from learning. Multiple actors interact with independent copies of the environment and feed trajectories to a central replay buffer. A separate learning process obtains trajectories from this buffer using prioritized sampling and updates the neural network parameters to predict the action-value function at each state. Actors obtain parameters from the learner periodically. See Appendix D for more details. Exploration with NGU. Agent57 uses the Never Give Up (NGU) intrinsic reward to encourage exploration. It aims at learning a family of 𝑁 = 32 policies which maximize different weightings of the extrinsic reward given by the environment (𝑟 𝑒 𝑡 ) and the intrinsic reward (𝑟 𝑖 𝑡 ), 𝑟𝑗,𝑡 = 𝑟 𝑒 𝑡 + 𝛽𝑗𝑟 𝑖 𝑡 (𝛽𝑗 ∈ ℝ+ , 𝑗 ∈ {0, . . . , 𝑁 −1}). The value of 𝛽𝑗 controls the degree of exploration, with higher values encouraging more exploratory behaviors, and each policy in the family is optimized with a different discount factor 𝛾𝑗 . The Universal Value Function Approximators (UVFA) framework (Schaul et al., 2015a) is employed to efficiently learn 𝑄 𝜋 𝑗 (𝑥, 𝑎; 𝜃) = 𝔼𝜋𝑗[ Í 𝑡≥0 𝛾 𝑡 𝑗 𝑟𝑗,𝑡|𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎] (we use a shorthand notation 𝑄 𝑗 (𝑥, 𝑎; 𝜃)) for 𝑗 ∈ {0, . . . , 𝑁 − 1} using a single set of shared parameters 𝜃. The policy 𝜋 𝑗 (𝑎|𝑥) can then be derived using the 𝜖-greedy operator as G𝜖𝑄 𝑗 (𝑥, 𝑎; 𝜃). We refer the reader to Appendix G for more details.\n",
      "      > [ 0.08309031 -0.13692975 -3.5322237 ] In order to stabilize learning, a target network is used for bootstrapping the return estimates using double Q-learning (Van Hasselt et al., 2016); the parameters of this target network, 𝜃𝑇 , are periodically copied from the online network parameters 𝜃 (Mnih et al., 2015). Distributed RL framework. Agent57 is a distributed deep RL agent based on R2D2 that decouples acting from learning. Multiple actors interact with independent copies of the environment and feed trajectories to a central replay buffer.\n",
      "        > [-0.15476449  0.46693391 -3.76116085] In order to stabilize learning, a target network is used for bootstrapping the return estimates using double Q-learning (Van Hasselt et al., 2016); the parameters of this target network, 𝜃𝑇 , are periodically copied from the online network parameters 𝜃 (Mnih et al., 2015).\n",
      "        > [ 0.37259459  0.32757211 -2.83744431] Distributed RL framework.\n",
      "        > [ 0.13165946 -0.40877861 -3.5158534 ] Agent57 is a distributed deep RL agent based on R2D2 that decouples acting from learning.\n",
      "        > [-0.29429686 -0.19072579 -3.78009701] Multiple actors interact with independent copies of the environment and feed trajectories to a central replay buffer.\n",
      "      > [ 0.21494794  0.12586567 -2.7468915 ] A separate learning process obtains trajectories from this buffer using prioritized sampling and updates the neural network parameters to predict the action-value function at each state. Actors obtain parameters from the learner periodically. See Appendix D for more details. Exploration with NGU.\n",
      "        > [ 0.79911631 -0.12134305 -2.649405  ] A separate learning process obtains trajectories from this buffer using prioritized sampling and updates the neural network parameters to predict the action-value function at each state.\n",
      "        > [-0.93681723 -0.44184145 -3.68372393] Actors obtain parameters from the learner periodically.\n",
      "        > [ 1.0714072   1.22560942 -3.40878224] See Appendix D for more details.\n",
      "        > [-0.8182739   1.13558567 -3.7908659 ] Exploration with NGU.\n",
      "      > [ 0.72578365  1.86540353 -3.84573817] Agent57 uses the Never Give Up (NGU) intrinsic reward to encourage exploration. It aims at learning a family of 𝑁 = 32 policies which maximize different weightings of the extrinsic reward given by the environment (𝑟 𝑒 𝑡 ) and the intrinsic reward (𝑟 𝑖 𝑡 ), 𝑟𝑗,𝑡 = 𝑟 𝑒 𝑡 + 𝛽𝑗𝑟 𝑖 𝑡 (𝛽𝑗 ∈ ℝ+ , 𝑗 ∈ {0, . . . , 𝑁 −1}). The value of 𝛽𝑗 controls the degree of exploration, with higher values encouraging more exploratory behaviors, and each policy in the family is optimized with a different discount factor 𝛾𝑗 .\n",
      "        > [ 0.21285588  0.80043602 -3.80259418] Agent57 uses the Never Give Up (NGU) intrinsic reward to encourage exploration.\n",
      "        > [ 1.25303948  2.02638626 -3.65778971] It aims at learning a family of 𝑁 = 32 policies which maximize different weightings of the extrinsic reward given by the environment (𝑟 𝑒 𝑡 ) and the intrinsic reward (𝑟 𝑖 𝑡 ), 𝑟𝑗,𝑡 = 𝑟 𝑒 𝑡 + 𝛽𝑗𝑟 𝑖 𝑡 (𝛽𝑗 ∈ ℝ+ , 𝑗 ∈ {0, .\n",
      "        > [ 0.27843422 -0.21084921 -3.39189291] . . , 𝑁 −1}).\n",
      "        > [ 0.6180498   1.87620211 -3.3709054 ] The value of 𝛽𝑗 controls the degree of exploration, with higher values encouraging more exploratory behaviors, and each policy in the family is optimized with a different discount factor 𝛾𝑗 .\n",
      "      > [-0.01581046  1.38225031 -3.18354225] The Universal Value Function Approximators (UVFA) framework (Schaul et al., 2015a) is employed to efficiently learn 𝑄 𝜋 𝑗 (𝑥, 𝑎; 𝜃) = 𝔼𝜋𝑗[ Í 𝑡≥0 𝛾 𝑡 𝑗 𝑟𝑗,𝑡|𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎] (we use a shorthand notation 𝑄 𝑗 (𝑥, 𝑎; 𝜃)) for 𝑗 ∈ {0, . . . , 𝑁 − 1} using a single set of shared parameters 𝜃. The policy 𝜋 𝑗 (𝑎|𝑥) can then be derived using the 𝜖-greedy operator as G𝜖𝑄 𝑗 (𝑥, 𝑎; 𝜃). We refer the reader to Appendix G for more details.\n",
      "        > [-0.0540204   0.56094921 -3.2413137 ] The Universal Value Function Approximators (UVFA) framework (Schaul et al., 2015a) is employed to efficiently learn 𝑄 𝜋 𝑗 (𝑥, 𝑎; 𝜃) = 𝔼𝜋𝑗[ Í 𝑡≥0 𝛾 𝑡 𝑗 𝑟𝑗,𝑡|𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎] (we use a shorthand notation 𝑄 𝑗 (𝑥, 𝑎; 𝜃)) for 𝑗 ∈ {0, .\n",
      "        > [ 0.06725101 -0.2912232  -3.28725815] . . , 𝑁 − 1} using a single set of shared parameters 𝜃.\n",
      "        > [ 0.67780876  1.80688477 -3.43768311] The policy 𝜋 𝑗 (𝑎|𝑥) can then be derived using the 𝜖-greedy operator as G𝜖𝑄 𝑗 (𝑥, 𝑎; 𝜃).\n",
      "        > [ 0.28800443  1.1277746  -2.97133589] We refer the reader to Appendix G for more details.\n",
      "    > [-1.19668436  1.79886687 -3.34555531] Meta-controller. Agent57 introduces an adaptive meta-controller that decides which policies from the family of N policies to use for collecting experience based on their episodic returns. This naturally creates a curriculum over 𝛽𝑗 and 𝛾𝑗 by adapting their value throughout training. This optimization process is formulated as a non-stationary bandit problem. A detailed description about the meta\u0002controller implementation is provided in Appendix E. Q-function separation. The architecture of the Q-function in Agent57 is implemented as two separate networks in order to split the intrinsic and extrinsic components. The network parameters of 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑒) and 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑖) are separate and independently optimized with 𝑟 𝑒 𝑗 and 𝑟 𝑖 𝑗 , respectively. The main motivation behind this decomposition is to prevent the gradients of the decomposed intrinsic and extrinsic value function heads from interfering with each other. This can be beneficial in environments where the intrinsic reward is poorly aligned with the task’s extrinsic reward. 4. MEME: Improving the data efficiency of Agent57 This section describes the main algorithmic contributions of the MEME agent, aimed at improving the data-efficiency of Agent57. These contributions aim to achieve faster propagation of learning signals related to rare events ( A ), stabilize learning under differing value scales ( B ), improve the neural network architecture ( C ), and make updates more robust under a rapidly-changing policy ( D ). For 1We also use G B G0 to denote the pure greedy operator. 2For convenience, we occasionally omit (𝑥, 𝑎) or 𝜃 from 𝑄(𝑥, 𝑎; 𝜃), 𝜋(𝑥, 𝑎; 𝜃) when there is no danger of confusion. 4 Human-level Atari 200x faster Torso LSTM Q-value Head Policy Head Qj (at , xt ) 𝛑j (at | xt ) N-mixtures Torso LSTM Q-value Head Policy Head Q1 (at , x t ) 𝛑1 (at | xt ) Member policy 1 Q-value Head Policy Head QN (at , xt ) 𝛑N (at | xt ) Member policy 1 … … Dueling Head Dueling Head Qi (at , xt ) Qe (at , xt ) + Qj (at , xt ) Q-value Head N policy family members Torso LSTM Q-value Head Policy Head Q1 (at , xt ) 𝛑1 (at | xt ) N policy family members … QN (at , x t ) 𝛑N (at | xt ) Torso LSTM Q1 (xt , at ; θ) 𝛑1 (at | xt ) N policy family members … QN (xt , at ; θ) 𝛑N (at | xt ) Policy Head Q-value Head Figure 2 | MEME agent network architecture. The output of the LSTM block is passed to each of the N members of the family of policies, depicted as a light-grey box. Each policy consists of an Q-value and policy heads.\n",
      "      > [-0.5080815   0.98721302 -3.97170997] Meta-controller. Agent57 introduces an adaptive meta-controller that decides which policies from the family of N policies to use for collecting experience based on their episodic returns. This naturally creates a curriculum over 𝛽𝑗 and 𝛾𝑗 by adapting their value throughout training. This optimization process is formulated as a non-stationary bandit problem.\n",
      "        > [-0.68465096  1.35226727 -3.84204578] Meta-controller.\n",
      "        > [-0.34199688  0.72575986 -3.99025822] Agent57 introduces an adaptive meta-controller that decides which policies from the family of N policies to use for collecting experience based on their episodic returns.\n",
      "        > [-0.68158084  0.62796396 -3.2854557 ] This naturally creates a curriculum over 𝛽𝑗 and 𝛾𝑗 by adapting their value throughout training.\n",
      "        > [ 0.98666334  0.95215321 -3.62662101] This optimization process is formulated as a non-stationary bandit problem.\n",
      "      > [-0.01363323  0.91310734 -3.21258307] A detailed description about the meta\u0002controller implementation is provided in Appendix E. Q-function separation. The architecture of the Q-function in Agent57 is implemented as two separate networks in order to split the intrinsic and extrinsic components. The network parameters of 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑒) and 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑖) are separate and independently optimized with 𝑟 𝑒 𝑗 and 𝑟 𝑖 𝑗 , respectively. The main motivation behind this decomposition is to prevent the gradients of the decomposed intrinsic and extrinsic value function heads from interfering with each other.\n",
      "        > [-0.22610778  1.30528152 -2.72823405] A detailed description about the meta\u0002controller implementation is provided in Appendix E. Q-function separation.\n",
      "        > [ 0.13348165  0.41536289 -3.3214035 ] The architecture of the Q-function in Agent57 is implemented as two separate networks in order to split the intrinsic and extrinsic components.\n",
      "        > [-0.34686908  0.54866487 -3.62301326] The network parameters of 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑒) and 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑖) are separate and independently optimized with 𝑟 𝑒 𝑗 and 𝑟 𝑖 𝑗 , respectively.\n",
      "        > [ 1.47760391  0.90644562 -3.18847942] The main motivation behind this decomposition is to prevent the gradients of the decomposed intrinsic and extrinsic value function heads from interfering with each other.\n",
      "      > [-0.26473135  0.68173391 -3.26266956] This can be beneficial in environments where the intrinsic reward is poorly aligned with the task’s extrinsic reward. 4. MEME: Improving the data efficiency of Agent57 This section describes the main algorithmic contributions of the MEME agent, aimed at improving the data-efficiency of Agent57. These contributions aim to achieve faster propagation of learning signals related to rare events ( A ), stabilize learning under differing value scales ( B ), improve the neural network architecture ( C ), and make updates more robust under a rapidly-changing policy ( D ). For 1We also use G B G0 to denote the pure greedy operator.\n",
      "        > [ 1.06217217  1.43763685 -3.2647953 ] This can be beneficial in environments where the intrinsic reward is poorly aligned with the task’s extrinsic reward.\n",
      "        > [-0.58437777  0.26673824 -3.96780777] 4. MEME: Improving the data efficiency of Agent57 This section describes the main algorithmic contributions of the MEME agent, aimed at improving the data-efficiency of Agent57.\n",
      "        > [ 0.31886378  0.58455175 -3.36847496] These contributions aim to achieve faster propagation of learning signals related to rare events ( A ), stabilize learning under differing value scales ( B ), improve the neural network architecture ( C ), and make updates more robust under a rapidly-changing policy ( D ).\n",
      "        > [ 0.36550134  0.98825377 -2.99861431] For 1We also use G B G0 to denote the pure greedy operator.\n",
      "      > [-0.21424887  1.14142001 -3.0414958 ] 2For convenience, we occasionally omit (𝑥, 𝑎) or 𝜃 from 𝑄(𝑥, 𝑎; 𝜃), 𝜋(𝑥, 𝑎; 𝜃) when there is no danger of confusion. 4 Human-level Atari 200x faster Torso LSTM Q-value Head Policy Head Qj (at , xt ) 𝛑j (at | xt ) N-mixtures Torso LSTM Q-value Head Policy Head Q1 (at , x t ) 𝛑1 (at | xt ) Member policy 1 Q-value Head Policy Head QN (at , xt ) 𝛑N (at | xt ) Member policy 1 … … Dueling Head Dueling Head Qi (at , xt ) Qe (at , xt ) + Qj (at , xt ) Q-value Head N policy family members Torso LSTM Q-value Head Policy Head Q1 (at , xt ) 𝛑1 (at | xt ) N policy family members … QN (at , x t ) 𝛑N (at | xt ) Torso LSTM Q1 (xt , at ; θ) 𝛑1 (at | xt ) N policy family members … QN (xt , at ; θ) 𝛑N (at | xt ) Policy Head Q-value Head Figure 2 | MEME agent network architecture. The output of the LSTM block is passed to each of the N members of the family of policies, depicted as a light-grey box. Each policy consists of an Q-value and policy heads.\n",
      "        > [ 0.48799074  0.32544705 -2.07424784] 2For convenience, we occasionally omit (𝑥, 𝑎) or 𝜃 from 𝑄(𝑥, 𝑎; 𝜃), 𝜋(𝑥, 𝑎; 𝜃) when there is no danger of confusion.\n",
      "        > [-0.7775619   1.47879338 -3.85448718] 4 Human-level Atari 200x faster Torso LSTM Q-value Head Policy Head Qj (at , xt ) 𝛑j (at | xt ) N-mixtures Torso LSTM Q-value Head Policy Head Q1 (at , x t ) 𝛑1 (at | xt ) Member policy 1 Q-value Head Policy Head QN (at , xt ) 𝛑N (at | xt ) Member policy 1 … … Dueling Head Dueling Head Qi (at , xt ) Qe (at , xt ) + Qj (at , xt ) Q-value Head N policy family members Torso LSTM Q-value Head Policy Head Q1 (at , xt ) 𝛑1 (at | xt ) N policy family members … QN (at , x t ) 𝛑N (at | xt ) Torso LSTM Q1 (xt , at ; θ) 𝛑1 (at | xt ) N policy family members … QN (xt , at ; θ) 𝛑N (at | xt ) Policy Head Q-value Head Figure 2 | MEME agent network architecture.\n",
      "        > [-0.12754738  1.6769886  -2.79473877] The output of the LSTM block is passed to each of the N members of the family of policies, depicted as a light-grey box.\n",
      "        > [-0.05673028  1.51221895 -3.78680134] Each policy consists of an Q-value and policy heads.\n",
      "    > [ 0.99172509  0.93755758 -2.91803646] The Q-value head is similar as in Agent57 paper, while the policy head is introduced for acting and target computation, and trained via policy distillation. clarity of exposition, we label methods according to the type of limitation they address. A1 Bootstrapping with online network. Target networks are frequently used in conjunction with value-based agents due to their stabilizing effect when learning from off-policy data (Mnih et al., 2015; Van Hasselt et al., 2016). This design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate. This issue can be mitigated to some extent by simply updating the target network more frequently, but the result is typically a less stable agent. To accelerate signal propagation while maintaining stability, we use online network bootstrapping, and we stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute to the loss. The trust region masks out the loss at any timestep for which both of the following conditions hold: |𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )| > 𝛼𝜎𝑗 (1) sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )) ≠ sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝐺𝑡) (2) where 𝛼 is a fixed hyperparameter, 𝐺𝑡 denotes the return estimate, 𝜃 and 𝜃𝑇 denote the online and target parameters respectively, and 𝜎𝑗 is the standard deviation of the TD-errors (a more precise description of which we defer until B1). Intuitively, we only mask if the current value of the online network is outside of the trust region (Equation 1) and the sign of the TD-error points away from the trust region (Equation 2), as depicted in Figure 3 in red. We note that a very similar trust region scheme is used for the value-function in most Proximal Policy Optimization (PPO) implementations (Schulman et al., 2017), though not described in the original paper. In contrast, the PPO version instead uses a constant threshold, and thus is not able to adapt to differing scales of value functions. Qj(xt ,at ;θT ) Qj(xt ,at ;θT ) + ασ Qj j (xt ,at ;θT ) - ασj Qj(xt ,at ;θ) Figure 3 | Trust region. The position of dots is given by the relationship between the values predicted by the online network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃), and the values predicted by the target network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 ) (Equation 1 and left hand side of Equation 2), the box represents the trust region bounds defined in Equation 1, and the direction of the arrow is given by the right hand side of Equation 2. Green-colored transitions are used in the loss computation, whereas red ones are masked out. A2 Target computation with tolerance. Agent57 uses Retrace (Munos et al., 2016) to compute return estimates from off-policy data, but we observed that it tends to cut traces too aggressively when 5 Human-level Atari 200x faster using 𝜖-greedy policies thus slowing down the propagation of information into the value function.\n",
      "      > [-0.34800288  1.28586924 -3.40999317] The Q-value head is similar as in Agent57 paper, while the policy head is introduced for acting and target computation, and trained via policy distillation. clarity of exposition, we label methods according to the type of limitation they address. A1 Bootstrapping with online network. Target networks are frequently used in conjunction with value-based agents due to their stabilizing effect when learning from off-policy data (Mnih et al., 2015; Van Hasselt et al., 2016).\n",
      "        > [-0.09732585  0.47564903 -3.55112338] The Q-value head is similar as in Agent57 paper, while the policy head is introduced for acting and target computation, and trained via policy distillation.\n",
      "        > [ 1.29600346  1.64040256 -2.43304157] clarity of exposition, we label methods according to the type of limitation they address.\n",
      "        > [-0.87263453 -0.17507124 -3.20998621] A1 Bootstrapping with online network.\n",
      "        > [-0.43167883  1.03378606 -3.69129825] Target networks are frequently used in conjunction with value-based agents due to their stabilizing effect when learning from off-policy data (Mnih et al., 2015; Van Hasselt et al., 2016).\n",
      "      > [ 0.38971573  0.79064453 -3.8769455 ] This design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate. This issue can be mitigated to some extent by simply updating the target network more frequently, but the result is typically a less stable agent. To accelerate signal propagation while maintaining stability, we use online network bootstrapping, and we stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute to the loss. The trust region masks out the loss at any timestep for which both of the following conditions hold: |𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )| > 𝛼𝜎𝑗 (1) sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )) ≠ sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝐺𝑡) (2) where 𝛼 is a fixed hyperparameter, 𝐺𝑡 denotes the return estimate, 𝜃 and 𝜃𝑇 denote the online and target parameters respectively, and 𝜎𝑗 is the standard deviation of the TD-errors (a more precise description of which we defer until B1).\n",
      "        > [ 0.95291829  1.39314985 -3.50739574] This design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate.\n",
      "        > [-0.19577125  0.38423347 -3.76263428] This issue can be mitigated to some extent by simply updating the target network more frequently, but the result is typically a less stable agent.\n",
      "        > [ 0.26482978  0.46542245 -3.69558001] To accelerate signal propagation while maintaining stability, we use online network bootstrapping, and we stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute to the loss.\n",
      "        > [ 1.47615647  0.77640563 -3.70765305] The trust region masks out the loss at any timestep for which both of the following conditions hold: |𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )| > 𝛼𝜎𝑗 (1) sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )) ≠ sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝐺𝑡) (2) where 𝛼 is a fixed hyperparameter, 𝐺𝑡 denotes the return estimate, 𝜃 and 𝜃𝑇 denote the online and target parameters respectively, and 𝜎𝑗 is the standard deviation of the TD-errors (a more precise description of which we defer until B1).\n",
      "      > [ 0.51332843  1.49229527 -3.55079436] Intuitively, we only mask if the current value of the online network is outside of the trust region (Equation 1) and the sign of the TD-error points away from the trust region (Equation 2), as depicted in Figure 3 in red. We note that a very similar trust region scheme is used for the value-function in most Proximal Policy Optimization (PPO) implementations (Schulman et al., 2017), though not described in the original paper. In contrast, the PPO version instead uses a constant threshold, and thus is not able to adapt to differing scales of value functions. Qj(xt ,at ;θT ) Qj(xt ,at ;θT ) + ασ Qj j (xt ,at ;θT ) - ασj Qj(xt ,at ;θ) Figure 3 | Trust region.\n",
      "        > [ 0.80471706  0.8094579  -3.84065557] Intuitively, we only mask if the current value of the online network is outside of the trust region (Equation 1) and the sign of the TD-error points away from the trust region (Equation 2), as depicted in Figure 3 in red.\n",
      "        > [ 0.73307902  1.75650012 -2.85370636] We note that a very similar trust region scheme is used for the value-function in most Proximal Policy Optimization (PPO) implementations (Schulman et al., 2017), though not described in the original paper.\n",
      "        > [ 1.21313715  1.59962547 -3.06056643] In contrast, the PPO version instead uses a constant threshold, and thus is not able to adapt to differing scales of value functions.\n",
      "        > [ 0.24790382  0.81160963 -4.01327419] Qj(xt ,at ;θT ) Qj(xt ,at ;θT ) + ασ Qj j (xt ,at ;θT ) - ασj Qj(xt ,at ;θ) Figure 3 | Trust region.\n",
      "      > [ 0.93357581  0.73363888 -3.43133783] The position of dots is given by the relationship between the values predicted by the online network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃), and the values predicted by the target network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 ) (Equation 1 and left hand side of Equation 2), the box represents the trust region bounds defined in Equation 1, and the direction of the arrow is given by the right hand side of Equation 2. Green-colored transitions are used in the loss computation, whereas red ones are masked out. A2 Target computation with tolerance. Agent57 uses Retrace (Munos et al., 2016) to compute return estimates from off-policy data, but we observed that it tends to cut traces too aggressively when 5 Human-level Atari 200x faster using 𝜖-greedy policies thus slowing down the propagation of information into the value function.\n",
      "        > [ 0.28240347  0.97284162 -3.98949909] The position of dots is given by the relationship between the values predicted by the online network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃), and the values predicted by the target network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 ) (Equation 1 and left hand side of Equation 2), the box represents the trust region bounds defined in Equation 1, and the direction of the arrow is given by the right hand side of Equation 2.\n",
      "        > [ 1.03729093  0.51261896 -3.13966298] Green-colored transitions are used in the loss computation, whereas red ones are masked out.\n",
      "        > [ 1.29681396 -0.59389591 -2.89599609] A2 Target computation with tolerance.\n",
      "        > [ 1.29901314  0.44542834 -3.63269043] Agent57 uses Retrace (Munos et al., 2016) to compute return estimates from off-policy data, but we observed that it tends to cut traces too aggressively when 5 Human-level Atari 200x faster using 𝜖-greedy policies thus slowing down the propagation of information into the value function.\n",
      "    > [ 0.97227257  0.05652307 -3.43045068] Preliminary experiments showed that data-efficiency was improved in many dense-reward tasks when replacing Retrace with Peng’s Q(𝜆) (Peng and Williams, 1994), but its lack of off-policy corrections tends to result in degraded performance as data becomes more off-policy (e.g. by increasing the expected number of times that a sequence is sampled from replay, or by sharing data across a family of policies). This motivates us to propose an alternative return estimator, which we derive from Q(𝜆) (Watkins and Dayan, 1992): 𝐺𝑡 = max 𝑎 𝑄(𝑥𝑡 , 𝑎) + ∑︁ 𝑘≥0 Ö 𝑘−1 𝑖=0 𝜆𝑖 ! 𝛾 𝑘 (𝑟𝑡+𝑘 + 𝛾 max 𝑎 𝑄(𝑥𝑡+𝑘+1, 𝑎) − max 𝑎 𝑄(𝑥𝑡+𝑘, 𝑎)) (3) where Î𝑘−1 𝑖=0 𝜆𝑖 ∈ [0, 1] effectively controls how much information from the future is used in the return estimation and is generally used as a trace cutting coefficient to perform off-policy correction. Peng’s Q(𝜆) does not perform any kind of off-policy correction and sets 𝜆𝑖 = 𝜆, whereas Watkins’ Q(𝜆) (Watkins and Dayan, 1992) aggressively cuts traces whenever it encounters an off-policy action by using 𝜆𝑖 = 𝜆𝟙𝑎𝑖 ∈argmax𝑎𝑄(𝑥𝑖 ,𝑎) , where 𝟙 denotes the indicator function. We propose to use a softer trace cutting mechanism by adding a fixed tolerance parameter 𝜅 and taking the expectation of trace coefficients under 𝜋: 𝜆𝑖 = 𝜆𝔼𝑎∼𝜋(𝑎|𝑥𝑡) \u0002 𝟙[𝑄(𝑥𝑡 ,𝑎𝑡;𝜃) ≥𝑄(𝑥𝑡 ,𝑎;𝜃)−𝜅|𝑄(𝑥𝑖 ,𝑎;𝜃) |] \u0003 (4) Finally, we replace all occurrences of the max operator in Equation 3 with the expectation under 𝜋. The resulting return estimator, which we denote Soft Watkins Q(𝜆), leads to more transitions being used and increased sample efficiency. Note that Watkins Q(𝜆) is recovered when setting 𝜅 = 0 and 𝜋 = G (𝑄). B1 Loss and priority normalization. As we learn a family of Q-functions which vary over a wide range of discount factors and intrinsic reward scales, we expect that the Q-functions will vary considerably in scale. This may cause the larger-scale Q-values to dominate learning and destabilize learning of smaller Q-values. This is a particular concern in environments with very small extrinsic reward scales. To counteract this effect we introduce a normalization scheme on the TD-errors similar to that used in Schaul et al. (2021). Specifically, we compute a running estimate of the standard deviation of TD-errors of the online network 𝜎 running 𝑗 as well as a batch standard deviation 𝜎 batch 𝑗 , and compute 𝜎𝑗 = max(𝜎 running 𝑗 , 𝜎batch 𝑗 , 𝜖), where 𝜖 acts as small threshold to avoid amplification of noise past a specified scale, which we fix to 0.01 in all our experiments. We then divide the TD-errors by 𝜎𝑗 when computing both the loss and priorities. As opposed to Schaul et al.\n",
      "      > [ 0.6810028   0.83167225 -3.52396965] Preliminary experiments showed that data-efficiency was improved in many dense-reward tasks when replacing Retrace with Peng’s Q(𝜆) (Peng and Williams, 1994), but its lack of off-policy corrections tends to result in degraded performance as data becomes more off-policy (e.g. by increasing the expected number of times that a sequence is sampled from replay, or by sharing data across a family of policies). This motivates us to propose an alternative return estimator, which we derive from Q(𝜆) (Watkins and Dayan, 1992): 𝐺𝑡 = max 𝑎 𝑄(𝑥𝑡 , 𝑎) + ∑︁ 𝑘≥0 Ö 𝑘−1 𝑖=0 𝜆𝑖 ! 𝛾 𝑘 (𝑟𝑡+𝑘 + 𝛾 max 𝑎 𝑄(𝑥𝑡+𝑘+1, 𝑎) − max 𝑎 𝑄(𝑥𝑡+𝑘, 𝑎)) (3) where Î𝑘−1 𝑖=0 𝜆𝑖 ∈ [0, 1] effectively controls how much information from the future is used in the return estimation and is generally used as a trace cutting coefficient to perform off-policy correction.\n",
      "        > [ 0.69633192  1.14167058 -3.64535522] Preliminary experiments showed that data-efficiency was improved in many dense-reward tasks when replacing Retrace with Peng’s Q(𝜆) (Peng and Williams, 1994), but its lack of off-policy corrections tends to result in degraded performance as data becomes more off-policy (e.g.\n",
      "        > [ 0.82488048  1.29294729 -3.73939109] by increasing the expected number of times that a sequence is sampled from replay, or by sharing data across a family of policies).\n",
      "        > [ 0.65325707  0.86920488 -3.34789658] This motivates us to propose an alternative return estimator, which we derive from Q(𝜆) (Watkins and Dayan, 1992): 𝐺𝑡 = max 𝑎 𝑄(𝑥𝑡 , 𝑎) + ∑︁ 𝑘≥0 Ö 𝑘−1 𝑖=0 𝜆𝑖 !\n",
      "        > [ 0.58513927  0.44938999 -3.23289561] 𝛾 𝑘 (𝑟𝑡+𝑘 + 𝛾 max 𝑎 𝑄(𝑥𝑡+𝑘+1, 𝑎) − max 𝑎 𝑄(𝑥𝑡+𝑘, 𝑎)) (3) where Î𝑘−1 𝑖=0 𝜆𝑖 ∈ [0, 1] effectively controls how much information from the future is used in the return estimation and is generally used as a trace cutting coefficient to perform off-policy correction.\n",
      "      > [ 0.90944296  0.83923215 -3.52400494] Peng’s Q(𝜆) does not perform any kind of off-policy correction and sets 𝜆𝑖 = 𝜆, whereas Watkins’ Q(𝜆) (Watkins and Dayan, 1992) aggressively cuts traces whenever it encounters an off-policy action by using 𝜆𝑖 = 𝜆𝟙𝑎𝑖 ∈argmax𝑎𝑄(𝑥𝑖 ,𝑎) , where 𝟙 denotes the indicator function. We propose to use a softer trace cutting mechanism by adding a fixed tolerance parameter 𝜅 and taking the expectation of trace coefficients under 𝜋: 𝜆𝑖 = 𝜆𝔼𝑎∼𝜋(𝑎|𝑥𝑡) \u0002 𝟙[𝑄(𝑥𝑡 ,𝑎𝑡;𝜃) ≥𝑄(𝑥𝑡 ,𝑎;𝜃)−𝜅|𝑄(𝑥𝑖 ,𝑎;𝜃) |] \u0003 (4) Finally, we replace all occurrences of the max operator in Equation 3 with the expectation under 𝜋. The resulting return estimator, which we denote Soft Watkins Q(𝜆), leads to more transitions being used and increased sample efficiency. Note that Watkins Q(𝜆) is recovered when setting 𝜅 = 0 and 𝜋 = G (𝑄).\n",
      "        > [ 0.87851852  0.98650485 -3.49044228] Peng’s Q(𝜆) does not perform any kind of off-policy correction and sets 𝜆𝑖 = 𝜆, whereas Watkins’ Q(𝜆) (Watkins and Dayan, 1992) aggressively cuts traces whenever it encounters an off-policy action by using 𝜆𝑖 = 𝜆𝟙𝑎𝑖 ∈argmax𝑎𝑄(𝑥𝑖 ,𝑎) , where 𝟙 denotes the indicator function.\n",
      "        > [ 1.40856004  0.29988095 -3.56746721] We propose to use a softer trace cutting mechanism by adding a fixed tolerance parameter 𝜅 and taking the expectation of trace coefficients under 𝜋: 𝜆𝑖 = 𝜆𝔼𝑎∼𝜋(𝑎|𝑥𝑡) \u0002 𝟙[𝑄(𝑥𝑡 ,𝑎𝑡;𝜃) ≥𝑄(𝑥𝑡 ,𝑎;𝜃)−𝜅|𝑄(𝑥𝑖 ,𝑎;𝜃) |] \u0003 (4) Finally, we replace all occurrences of the max operator in Equation 3 with the expectation under 𝜋.\n",
      "        > [ 0.00974596  1.24414492 -3.75455189] The resulting return estimator, which we denote Soft Watkins Q(𝜆), leads to more transitions being used and increased sample efficiency.\n",
      "        > [ 0.69401896  0.6114139  -3.45207858] Note that Watkins Q(𝜆) is recovered when setting 𝜅 = 0 and 𝜋 = G (𝑄).\n",
      "      > [ 1.16900659  1.49616468 -3.29879069] B1 Loss and priority normalization. As we learn a family of Q-functions which vary over a wide range of discount factors and intrinsic reward scales, we expect that the Q-functions will vary considerably in scale. This may cause the larger-scale Q-values to dominate learning and destabilize learning of smaller Q-values. This is a particular concern in environments with very small extrinsic reward scales.\n",
      "        > [ 1.44510925  0.03123915 -2.75557685] B1 Loss and priority normalization.\n",
      "        > [ 0.67401195  1.95890057 -3.837605  ] As we learn a family of Q-functions which vary over a wide range of discount factors and intrinsic reward scales, we expect that the Q-functions will vary considerably in scale.\n",
      "        > [ 0.79077947  1.56390524 -3.57856941] This may cause the larger-scale Q-values to dominate learning and destabilize learning of smaller Q-values.\n",
      "        > [ 1.36908174  1.86421275 -3.63627315] This is a particular concern in environments with very small extrinsic reward scales.\n",
      "      > [ 1.07992792  0.2428886  -3.71096134] To counteract this effect we introduce a normalization scheme on the TD-errors similar to that used in Schaul et al. (2021). Specifically, we compute a running estimate of the standard deviation of TD-errors of the online network 𝜎 running 𝑗 as well as a batch standard deviation 𝜎 batch 𝑗 , and compute 𝜎𝑗 = max(𝜎 running 𝑗 , 𝜎batch 𝑗 , 𝜖), where 𝜖 acts as small threshold to avoid amplification of noise past a specified scale, which we fix to 0.01 in all our experiments. We then divide the TD-errors by 𝜎𝑗 when computing both the loss and priorities. As opposed to Schaul et al.\n",
      "        > [ 1.58753896  0.36092821 -3.77511406] To counteract this effect we introduce a normalization scheme on the TD-errors similar to that used in Schaul et al.\n",
      "        > [ 0.88827169  0.2900148  -3.99661636] (2021). Specifically, we compute a running estimate of the standard deviation of TD-errors of the online network 𝜎 running 𝑗 as well as a batch standard deviation 𝜎 batch 𝑗 , and compute 𝜎𝑗 = max(𝜎 running 𝑗 , 𝜎batch 𝑗 , 𝜖), where 𝜖 acts as small threshold to avoid amplification of noise past a specified scale, which we fix to 0.01 in all our experiments.\n",
      "        > [ 1.05458367 -0.92874116 -3.26949358] We then divide the TD-errors by 𝜎𝑗 when computing both the loss and priorities.\n",
      "        > [ 0.55720961  0.27255166 -3.39318848] As opposed to Schaul et al.\n",
      "  > [ 0.65917641  1.28343213 -3.24010944] (2021) we compute the running statistics on the learner, and make use of importance sampling to correct the sampling distribution. B2 Cross-mixture training. Agent57 only trains the policy 𝑗 which was used to collect a given trajectory, but it is natural to ask whether data-efficiency and robustness may be improved by training all policies at once. We propose a training loss 𝐿 according to the following weighting scheme between the behavior policy loss and the mean over all losses: 𝐿 = 𝜂𝐿𝑗𝜇 + 1 − 𝜂 𝑁 𝑁 ∑︁−1 𝑗=0 𝐿𝑗 (5) where 𝐿𝑗 denotes the Q-learning loss for policy 𝑗, and 𝑗𝜇 denotes the index for the behavior policy selected by the meta-controller for the sampled trajectory. We find that an intermediate value for the mixture parameter of 𝜂 = 0.5 tends to work well. To achieve better compute-efficiency we choose to deviate from the original UVFA architecture which fed a 1-hot encoding of the policy index to the LSTM, aand instead modify the Q-value heads to output N sets of Q-values, one for each of the 6 Human-level Atari 200x faster members in the family of policies introduced in Section 3. Therefore, in the end we output values for all combinations of actions and policies (see Figure 2). We note that in this setting, there is also less deviation in the recurrent states when learning across different mixture policies. C1 Normalizer-free torso network. Normalization layers are a common feature of ResNet architec\u0002tures, and which are known to aid in training of very deep networks, but preliminary investigation revealed that several commonly used normalization layers are in fact detrimental to performance in our setting. Instead, we employ a variation of the NFNet architecture (Brock et al., 2021) for our policy torso network, which combines a variance-scaling strategy with scaled weight standardization and careful initialization to achieve state-of-the-art performance on ImageNet without the need for normalization layers. We adopt their use of stochastic depth (Huang et al., 2016) at training-time but omit the application of ordinary dropout to fully-connected layers as we did not observe any benefit from this form of regularization. Some care is required when using stochastic depth in conjunction with multi-step returns, as resampling of the stochastic depth mask at each timestep injects additional noise into the bootstrap values, resulting in a higher-variance return estimator. As such, we employ a temporally-consistent stochastic depth mask which remains fixed over the length of each training trajectory. C2 Shared torso with combined loss. Agent57 decomposes the combined Q-function into intrinsic and extrinsic components, 𝑄𝑒 and 𝑄𝑖 , which are represented by separate networks. Such a decomposi\u0002tion prevents the gradients of the decomposed value functions from interfering with each other. This interference may occur in environments where the intrinsic reward is poorly aligned with the task objective, as defined by the extrinsic reward. However, the choice to use separate separate networks comes at an expensive computational cost, and potentially limits sample-efficiency since generic low-level features cannot be shared. To alleviate these issues, we introduce a shared torso for the two Q-functions while retaining separate heads. While the form of the decomposition in Agent57 was chosen so as to ensure convergence to the optimal value-function 𝑄 ★ in the tabular setting, this does not generally hold under function approximation. Comparing the combined and decomposed losses we observe a mismatch in the gradients due to the absence of cross-terms 𝑄𝑖(𝜃) 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 and 𝑄𝑒 (𝜃) 𝜕𝑄𝑖(𝜃) 𝜕𝜃 in the decomposed loss: 𝜕 𝜕𝜃 h 1 2 (𝑄(𝜃) − 𝐺) 2 i | {z } combined loss ≠ 𝜕 𝜕𝜃 h 1 2 (𝑄𝑒 (𝜃) − 𝐺𝑒) 2 + 1 2 (𝛽𝑄𝑖(𝜃) − 𝛽𝐺𝑖) 2 i | {z } decomposed loss (6) \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) − 𝐺 \u0003 𝜕 𝜕𝜃 \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) \u0003 ≠ \u0002 𝑄𝑒 (𝜃) − 𝐺𝑒 \u0003 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 + 𝛽 2 \u0002 𝑄𝑖(𝜃) − 𝐺𝑖 \u0003 𝜕𝑄𝑖(𝜃) 𝜕𝜃 (7) Since we use a behavior policy induced by the total Q-function 𝑄 = 𝑄𝑒 + 𝛽𝑄𝑖 rather than the individual components, theory would suggest to use the combined loss instead. In addition, from a practical implementation perspective, this switch to the combined loss greatly simplifies the design choices involved in our proposed trust region method described in A1. The penalty paid for this choice is that the decomposition of the value function into extrinsic and intrinsic components no longer carries a strict semantic meaning. Nevertheless we do still retain an implicit inductive bias induced by multiplication of 𝑄𝑖 with the intrinsic reward weight 𝛽 𝑗 . D Robustifying behavior via policy distillation. Schaul et al. (2022) describe the effect of policy churn, whereby the greedy action of value-based RL algorithms may change frequently over consecutive parameter updates. This can have a deleterious effect on off-policy correction methods: traces will be cut more aggressively than with a stochastic policy, and bootstrap values will change frequently which can result in a higher variance return estimator. In addition, our choice of training with temporally-consistent stochastic depth masks can be interpreted as learning an implicit ensemble 7 Human-level Atari 200x faster of Q-functions; thus it is natural to ask whether we may see additional benefit from leveraging the policy induced by this ensemble. We propose to train an explicit policy head 𝜋dist (see Figure 2) via policy distillation to match the 𝜖-greedy policy induced by the Q-function. In expectation over multiple gradient steps this should help to smooth out the policy over time, as well as over the ensemble, while being much faster to evaluate than the individual members of the ensemble. Similarly to the trust-region described in A1, we mask the policy distillation loss at any timestep where a KL constraint 𝐶KL is violated: 𝐿𝜋 = − ∑︁ 𝑎,𝑡 G𝜖\n",
      "    > [ 0.36495072  0.21816242 -3.19323063] (2021) we compute the running statistics on the learner, and make use of importance sampling to correct the sampling distribution. B2 Cross-mixture training. Agent57 only trains the policy 𝑗 which was used to collect a given trajectory, but it is natural to ask whether data-efficiency and robustness may be improved by training all policies at once. We propose a training loss 𝐿 according to the following weighting scheme between the behavior policy loss and the mean over all losses: 𝐿 = 𝜂𝐿𝑗𝜇 + 1 − 𝜂 𝑁 𝑁 ∑︁−1 𝑗=0 𝐿𝑗 (5) where 𝐿𝑗 denotes the Q-learning loss for policy 𝑗, and 𝑗𝜇 denotes the index for the behavior policy selected by the meta-controller for the sampled trajectory. We find that an intermediate value for the mixture parameter of 𝜂 = 0.5 tends to work well. To achieve better compute-efficiency we choose to deviate from the original UVFA architecture which fed a 1-hot encoding of the policy index to the LSTM, aand instead modify the Q-value heads to output N sets of Q-values, one for each of the 6 Human-level Atari 200x faster members in the family of policies introduced in Section 3. Therefore, in the end we output values for all combinations of actions and policies (see Figure 2). We note that in this setting, there is also less deviation in the recurrent states when learning across different mixture policies. C1 Normalizer-free torso network. Normalization layers are a common feature of ResNet architec\u0002tures, and which are known to aid in training of very deep networks, but preliminary investigation revealed that several commonly used normalization layers are in fact detrimental to performance in our setting. Instead, we employ a variation of the NFNet architecture (Brock et al., 2021) for our policy torso network, which combines a variance-scaling strategy with scaled weight standardization and careful initialization to achieve state-of-the-art performance on ImageNet without the need for normalization layers. We adopt their use of stochastic depth (Huang et al., 2016) at training-time but omit the application of ordinary dropout to fully-connected layers as we did not observe any benefit from this form of regularization. Some care is required when using stochastic depth in conjunction with multi-step returns, as resampling of the stochastic depth mask at each timestep injects additional noise into the bootstrap values, resulting in a higher-variance return estimator. As such, we employ a temporally-consistent stochastic depth mask which remains fixed over the length of each training trajectory. C2 Shared torso with combined loss. Agent57 decomposes the combined Q-function into intrinsic and extrinsic components, 𝑄𝑒 and 𝑄𝑖 , which are represented by separate networks.\n",
      "      > [-2.76797079e-03  2.87285030e-01 -3.57096291e+00] (2021) we compute the running statistics on the learner, and make use of importance sampling to correct the sampling distribution. B2 Cross-mixture training. Agent57 only trains the policy 𝑗 which was used to collect a given trajectory, but it is natural to ask whether data-efficiency and robustness may be improved by training all policies at once. We propose a training loss 𝐿 according to the following weighting scheme between the behavior policy loss and the mean over all losses: 𝐿 = 𝜂𝐿𝑗𝜇 + 1 − 𝜂 𝑁 𝑁 ∑︁−1 𝑗=0 𝐿𝑗 (5) where 𝐿𝑗 denotes the Q-learning loss for policy 𝑗, and 𝑗𝜇 denotes the index for the behavior policy selected by the meta-controller for the sampled trajectory.\n",
      "        > [-0.06755495  0.22459108 -3.64804626] (2021) we compute the running statistics on the learner, and make use of importance sampling to correct the sampling distribution.\n",
      "        > [-2.72913605e-01 -4.36373055e-04 -3.53351283e+00] B2 Cross-mixture training.\n",
      "        > [ 0.02375893  0.48241761 -3.72854352] Agent57 only trains the policy 𝑗 which was used to collect a given trajectory, but it is natural to ask whether data-efficiency and robustness may be improved by training all policies at once.\n",
      "        > [ 0.58207226  0.60941178 -3.30605888] We propose a training loss 𝐿 according to the following weighting scheme between the behavior policy loss and the mean over all losses: 𝐿 = 𝜂𝐿𝑗𝜇 + 1 − 𝜂 𝑁 𝑁 ∑︁−1 𝑗=0 𝐿𝑗 (5) where 𝐿𝑗 denotes the Q-learning loss for policy 𝑗, and 𝑗𝜇 denotes the index for the behavior policy selected by the meta-controller for the sampled trajectory.\n",
      "      > [ 0.08266668  0.78475523 -3.70290852] We find that an intermediate value for the mixture parameter of 𝜂 = 0.5 tends to work well. To achieve better compute-efficiency we choose to deviate from the original UVFA architecture which fed a 1-hot encoding of the policy index to the LSTM, aand instead modify the Q-value heads to output N sets of Q-values, one for each of the 6 Human-level Atari 200x faster members in the family of policies introduced in Section 3. Therefore, in the end we output values for all combinations of actions and policies (see Figure 2). We note that in this setting, there is also less deviation in the recurrent states when learning across different mixture policies.\n",
      "        > [ 0.20755544 -0.1121033  -3.41112566] We find that an intermediate value for the mixture parameter of 𝜂 = 0.5 tends to work well.\n",
      "        > [ 0.19441366  1.51141047 -3.60674143] To achieve better compute-efficiency we choose to deviate from the original UVFA architecture which fed a 1-hot encoding of the policy index to the LSTM, aand instead modify the Q-value heads to output N sets of Q-values, one for each of the 6 Human-level Atari 200x faster members in the family of policies introduced in Section 3.\n",
      "        > [ 1.16599965  0.9770022  -3.27355862] Therefore, in the end we output values for all combinations of actions and policies (see Figure 2).\n",
      "        > [ 0.53739607  0.5306623  -3.87489414] We note that in this setting, there is also less deviation in the recurrent states when learning across different mixture policies.\n",
      "      > [ 0.2060919   1.79318166 -3.3093462 ] C1 Normalizer-free torso network. Normalization layers are a common feature of ResNet architec\u0002tures, and which are known to aid in training of very deep networks, but preliminary investigation revealed that several commonly used normalization layers are in fact detrimental to performance in our setting. Instead, we employ a variation of the NFNet architecture (Brock et al., 2021) for our policy torso network, which combines a variance-scaling strategy with scaled weight standardization and careful initialization to achieve state-of-the-art performance on ImageNet without the need for normalization layers. We adopt their use of stochastic depth (Huang et al., 2016) at training-time but omit the application of ordinary dropout to fully-connected layers as we did not observe any benefit from this form of regularization.\n",
      "        > [ 0.80805182  0.47794044 -2.81690907] C1 Normalizer-free torso network.\n",
      "        > [ 0.48574927  1.94301355 -3.2280426 ] Normalization layers are a common feature of ResNet architec\u0002tures, and which are known to aid in training of very deep networks, but preliminary investigation revealed that several commonly used normalization layers are in fact detrimental to performance in our setting.\n",
      "        > [-0.01615191  2.14500427 -3.45329285] Instead, we employ a variation of the NFNet architecture (Brock et al., 2021) for our policy torso network, which combines a variance-scaling strategy with scaled weight standardization and careful initialization to achieve state-of-the-art performance on ImageNet without the need for normalization layers.\n",
      "        > [ 0.51484334  1.1185298  -3.3471384 ] We adopt their use of stochastic depth (Huang et al., 2016) at training-time but omit the application of ordinary dropout to fully-connected layers as we did not observe any benefit from this form of regularization.\n",
      "      > [ 0.19317929  0.67340052 -3.50411034] Some care is required when using stochastic depth in conjunction with multi-step returns, as resampling of the stochastic depth mask at each timestep injects additional noise into the bootstrap values, resulting in a higher-variance return estimator. As such, we employ a temporally-consistent stochastic depth mask which remains fixed over the length of each training trajectory. C2 Shared torso with combined loss. Agent57 decomposes the combined Q-function into intrinsic and extrinsic components, 𝑄𝑒 and 𝑄𝑖 , which are represented by separate networks.\n",
      "        > [ 0.33363619  1.07369614 -3.44311333] Some care is required when using stochastic depth in conjunction with multi-step returns, as resampling of the stochastic depth mask at each timestep injects additional noise into the bootstrap values, resulting in a higher-variance return estimator.\n",
      "        > [ 0.47342023  0.33535099 -3.85501575] As such, we employ a temporally-consistent stochastic depth mask which remains fixed over the length of each training trajectory.\n",
      "        > [ 1.44942534 -0.98459154 -2.653234  ] C2 Shared torso with combined loss.\n",
      "        > [ 0.50425529  0.18513048 -3.62561035] Agent57 decomposes the combined Q-function into intrinsic and extrinsic components, 𝑄𝑒 and 𝑄𝑖 , which are represented by separate networks.\n",
      "    > [ 0.24527615  1.54273427 -3.47636652] Such a decomposi\u0002tion prevents the gradients of the decomposed value functions from interfering with each other. This interference may occur in environments where the intrinsic reward is poorly aligned with the task objective, as defined by the extrinsic reward. However, the choice to use separate separate networks comes at an expensive computational cost, and potentially limits sample-efficiency since generic low-level features cannot be shared. To alleviate these issues, we introduce a shared torso for the two Q-functions while retaining separate heads. While the form of the decomposition in Agent57 was chosen so as to ensure convergence to the optimal value-function 𝑄 ★ in the tabular setting, this does not generally hold under function approximation. Comparing the combined and decomposed losses we observe a mismatch in the gradients due to the absence of cross-terms 𝑄𝑖(𝜃) 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 and 𝑄𝑒 (𝜃) 𝜕𝑄𝑖(𝜃) 𝜕𝜃 in the decomposed loss: 𝜕 𝜕𝜃 h 1 2 (𝑄(𝜃) − 𝐺) 2 i | {z } combined loss ≠ 𝜕 𝜕𝜃 h 1 2 (𝑄𝑒 (𝜃) − 𝐺𝑒) 2 + 1 2 (𝛽𝑄𝑖(𝜃) − 𝛽𝐺𝑖) 2 i | {z } decomposed loss (6) \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) − 𝐺 \u0003 𝜕 𝜕𝜃 \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) \u0003 ≠ \u0002 𝑄𝑒 (𝜃) − 𝐺𝑒 \u0003 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 + 𝛽 2 \u0002 𝑄𝑖(𝜃) − 𝐺𝑖 \u0003 𝜕𝑄𝑖(𝜃) 𝜕𝜃 (7) Since we use a behavior policy induced by the total Q-function 𝑄 = 𝑄𝑒 + 𝛽𝑄𝑖 rather than the individual components, theory would suggest to use the combined loss instead. In addition, from a practical implementation perspective, this switch to the combined loss greatly simplifies the design choices involved in our proposed trust region method described in A1. The penalty paid for this choice is that the decomposition of the value function into extrinsic and intrinsic components no longer carries a strict semantic meaning. Nevertheless we do still retain an implicit inductive bias induced by multiplication of 𝑄𝑖 with the intrinsic reward weight 𝛽 𝑗 . D Robustifying behavior via policy distillation. Schaul et al. (2022) describe the effect of policy churn, whereby the greedy action of value-based RL algorithms may change frequently over consecutive parameter updates. This can have a deleterious effect on off-policy correction methods: traces will be cut more aggressively than with a stochastic policy, and bootstrap values will change frequently which can result in a higher variance return estimator. In addition, our choice of training with temporally-consistent stochastic depth masks can be interpreted as learning an implicit ensemble 7 Human-level Atari 200x faster of Q-functions; thus it is natural to ask whether we may see additional benefit from leveraging the policy induced by this ensemble. We propose to train an explicit policy head 𝜋dist (see Figure 2) via policy distillation to match the 𝜖-greedy policy induced by the Q-function. In expectation over multiple gradient steps this should help to smooth out the policy over time, as well as over the ensemble, while being much faster to evaluate than the individual members of the ensemble. Similarly to the trust-region described in A1, we mask the policy distillation loss at any timestep where a KL constraint 𝐶KL is violated: 𝐿𝜋 = − ∑︁ 𝑎,𝑡 G𝜖\n",
      "      > [ 0.49344268  1.3351928  -3.00036573] Such a decomposi\u0002tion prevents the gradients of the decomposed value functions from interfering with each other. This interference may occur in environments where the intrinsic reward is poorly aligned with the task objective, as defined by the extrinsic reward. However, the choice to use separate separate networks comes at an expensive computational cost, and potentially limits sample-efficiency since generic low-level features cannot be shared. To alleviate these issues, we introduce a shared torso for the two Q-functions while retaining separate heads.\n",
      "        > [ 1.52261376  0.60501063 -2.9164629 ] Such a decomposi\u0002tion prevents the gradients of the decomposed value functions from interfering with each other.\n",
      "        > [ 1.38678873  1.38382113 -2.9831109 ] This interference may occur in environments where the intrinsic reward is poorly aligned with the task objective, as defined by the extrinsic reward.\n",
      "        > [-0.27675688  1.90623522 -2.86672354] However, the choice to use separate separate networks comes at an expensive computational cost, and potentially limits sample-efficiency since generic low-level features cannot be shared.\n",
      "        > [-0.02876019  0.8821938  -3.25152779] To alleviate these issues, we introduce a shared torso for the two Q-functions while retaining separate heads.\n",
      "      > [ 1.25661087  0.11879988 -2.90302515] While the form of the decomposition in Agent57 was chosen so as to ensure convergence to the optimal value-function 𝑄 ★ in the tabular setting, this does not generally hold under function approximation. Comparing the combined and decomposed losses we observe a mismatch in the gradients due to the absence of cross-terms 𝑄𝑖(𝜃) 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 and 𝑄𝑒 (𝜃) 𝜕𝑄𝑖(𝜃) 𝜕𝜃 in the decomposed loss: 𝜕 𝜕𝜃 h 1 2 (𝑄(𝜃) − 𝐺) 2 i | {z } combined loss ≠ 𝜕 𝜕𝜃 h 1 2 (𝑄𝑒 (𝜃) − 𝐺𝑒) 2 + 1 2 (𝛽𝑄𝑖(𝜃) − 𝛽𝐺𝑖) 2 i | {z } decomposed loss (6) \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) − 𝐺 \u0003 𝜕 𝜕𝜃 \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) \u0003 ≠ \u0002 𝑄𝑒 (𝜃) − 𝐺𝑒 \u0003 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 + 𝛽 2 \u0002 𝑄𝑖(𝜃) − 𝐺𝑖 \u0003 𝜕𝑄𝑖(𝜃) 𝜕𝜃 (7) Since we use a behavior policy induced by the total Q-function 𝑄 = 𝑄𝑒 + 𝛽𝑄𝑖 rather than the individual components, theory would suggest to use the combined loss instead. In addition, from a practical implementation perspective, this switch to the combined loss greatly simplifies the design choices involved in our proposed trust region method described in A1. The penalty paid for this choice is that the decomposition of the value function into extrinsic and intrinsic components no longer carries a strict semantic meaning.\n",
      "        > [ 0.96272266  0.26060796 -3.06219196] While the form of the decomposition in Agent57 was chosen so as to ensure convergence to the optimal value-function 𝑄 ★ in the tabular setting, this does not generally hold under function approximation.\n",
      "        > [ 1.25702715  0.01215496 -3.1916163 ] Comparing the combined and decomposed losses we observe a mismatch in the gradients due to the absence of cross-terms 𝑄𝑖(𝜃) 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 and 𝑄𝑒 (𝜃) 𝜕𝑄𝑖(𝜃) 𝜕𝜃 in the decomposed loss: 𝜕 𝜕𝜃 h 1 2 (𝑄(𝜃) − 𝐺) 2 i | {z } combined loss ≠ 𝜕 𝜕𝜃 h 1 2 (𝑄𝑒 (𝜃) − 𝐺𝑒) 2 + 1 2 (𝛽𝑄𝑖(𝜃) − 𝛽𝐺𝑖) 2 i | {z } decomposed loss (6) \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) − 𝐺 \u0003 𝜕 𝜕𝜃 \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) \u0003 ≠ \u0002 𝑄𝑒 (𝜃) − 𝐺𝑒 \u0003 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 + 𝛽 2 \u0002 𝑄𝑖(𝜃) − 𝐺𝑖 \u0003 𝜕𝑄𝑖(𝜃) 𝜕𝜃 (7) Since we use a behavior policy induced by the total Q-function 𝑄 = 𝑄𝑒 + 𝛽𝑄𝑖 rather than the individual components, theory would suggest to use the combined loss instead.\n",
      "        > [ 0.91605347  0.19196057 -2.9565506 ] In addition, from a practical implementation perspective, this switch to the combined loss greatly simplifies the design choices involved in our proposed trust region method described in A1.\n",
      "        > [ 1.43501282  0.92157841 -2.21875   ] The penalty paid for this choice is that the decomposition of the value function into extrinsic and intrinsic components no longer carries a strict semantic meaning.\n",
      "      > [ 1.16069651  1.13236344 -2.70242095] Nevertheless we do still retain an implicit inductive bias induced by multiplication of 𝑄𝑖 with the intrinsic reward weight 𝛽 𝑗 . D Robustifying behavior via policy distillation. Schaul et al. (2022) describe the effect of policy churn, whereby the greedy action of value-based RL algorithms may change frequently over consecutive parameter updates.\n",
      "        > [ 1.3592844   1.22498441 -2.45735216] Nevertheless we do still retain an implicit inductive bias induced by multiplication of 𝑄𝑖 with the intrinsic reward weight 𝛽 𝑗 .\n",
      "        > [ 1.64958096  0.83598793 -4.10088634] D Robustifying behavior via policy distillation.\n",
      "        > [ 0.19819736 -0.26570129 -3.79846191] Schaul et al.\n",
      "        > [ 0.76732945  1.2479223  -3.37842059] (2022) describe the effect of policy churn, whereby the greedy action of value-based RL algorithms may change frequently over consecutive parameter updates.\n",
      "      > [ 0.50970864  1.54597712 -3.47720861] This can have a deleterious effect on off-policy correction methods: traces will be cut more aggressively than with a stochastic policy, and bootstrap values will change frequently which can result in a higher variance return estimator. In addition, our choice of training with temporally-consistent stochastic depth masks can be interpreted as learning an implicit ensemble 7 Human-level Atari 200x faster of Q-functions; thus it is natural to ask whether we may see additional benefit from leveraging the policy induced by this ensemble. We propose to train an explicit policy head 𝜋dist (see Figure 2) via policy distillation to match the 𝜖-greedy policy induced by the Q-function. In expectation over multiple gradient steps this should help to smooth out the policy over time, as well as over the ensemble, while being much faster to evaluate than the individual members of the ensemble. Similarly to the trust-region described in A1, we mask the policy distillation loss at any timestep where a KL constraint 𝐶KL is violated: 𝐿𝜋 = − ∑︁ 𝑎,𝑡 G𝜖\n",
      "        > [ 1.58264601  0.93122804 -3.47120285] This can have a deleterious effect on off-policy correction methods: traces will be cut more aggressively than with a stochastic policy, and bootstrap values will change frequently which can result in a higher variance return estimator.\n",
      "        > [-0.12844233  1.63852859 -3.37387109] In addition, our choice of training with temporally-consistent stochastic depth masks can be interpreted as learning an implicit ensemble 7 Human-level Atari 200x faster of Q-functions; thus it is natural to ask whether we may see additional benefit from leveraging the policy induced by this ensemble.\n",
      "        > [-0.20357643  1.50329614 -3.59765458] We propose to train an explicit policy head 𝜋dist (see Figure 2) via policy distillation to match the 𝜖-greedy policy induced by the Q-function.\n",
      "        > [-0.26338631  1.34101772 -4.13024139] In expectation over multiple gradient steps this should help to smooth out the policy over time, as well as over the ensemble, while being much faster to evaluate than the individual members of the ensemble.\n",
      "        > [ 1.06615913  0.9088676  -3.12861252] Similarly to the trust-region described in A1, we mask the policy distillation loss at any timestep where a KL constraint 𝐶KL is violated: 𝐿𝜋 = − ∑︁ 𝑎,𝑡 G𝜖\n"
     ]
    }
   ],
   "source": [
    "print_paper(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_paper(paper, 'test_paper.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = load_paper('test_paper.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: test\n",
      "> [ 0.71496713  1.38890755 -2.87749505] Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization lay\u0002ers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy over time. 1. Introduction To develop generally capable agents, the question of how to evaluate them is paramount. The Arcade Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate agents on an diverse set of tasks which are interesting to humans, and developed externally to the Reinforcement Learning (RL) community. As a result, several games exhibit reward structures which are highly adversarial to many popular algorithms. Mean and median human normalized scores (HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating deep RL agents. Recent progress has allowed state-of-the-art algorithms to greatly exceed average human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al., 2020; Van Hasselt et al., 2016). However, it has been argued that mean or median HNS might not be well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al., 2019). Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on most games, but completely fail to learn on a small number of them. Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all 57 Atari games. However, such generality came at the cost of data efficiency; requiring tens of billions of environment interactions to achieve above average-human performance in some games, reaching a figure of 78 billion frames before beating the human benchmark in all games. Data efficiency remains a desirable property for agents to possess, as many real-world challenges are data-limited by time and cost constraints (Dulac-Arnold et al., 2019). In this work, our aim is to develop an agent that is as general as Agent57 but that requires only a fraction of the environment interactions to achieve the same result. © 2022 DeepMind. All rights reserved arXiv:2209.07550v1 [cs.LG] 15 Sep 2022 Human-level Atari 200x faster Skiing Private Eye Pitfall! Montezuma's Revenge Surround Asteroids Ice Hockey Solaris Bowling Gravitar H.E.R.O. Berzerk Frostbite Tennis Alien Tutankham Riverraid Ms. Pac-Man Pong Beam Rider Chopper Command Seaquest Double Dunk Freeway Space Invaders Bank Heist Venture Battle Zone Asterix Q*BERT Amidar Name This Game Yars' Revenge Phoenix Time Pilot Demon Attack Wizard Of Wor Breakout Video Pinball Fishing Derby Enduro Stargunner Assault Zaxxon Jamesbond Centipede Kangaroo Atlantis Up'n Down Defender Boxing Robotank Kung-Fu Master Gopher Road Runner Crazy Climber Krull 1M 10M 100M 1B 10B 100B Frames to > human score 390M Agent57 MEME Figure 1 | Number of environment frames required by agents to outperform the human baseline on each game (in log-scale). Lower is better. On average, MEME achieves above human scores using 63× fewer environment interactions than Agent57. The smallest improvement is 9× (road_runner), the maximum is 721× (skiing), and the median across the suite is 35×. We observe small variance across seeds (c.f. Figure 8). There exist two main trends in the literature when it comes to measuring improvements in the learning capabilities of agents. One approach consists in measuring performance after a limited budget of interactions with the environment. While this type of evaluation has led to important progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019). On the other hand, one can aim to achieve a target end-performance with as few interactions as possible (Schmitt et al., 2020; Silver et al., 2017, 2018). Since our goal is to show that our new agent is as general as Agent57, while being more data efficient, we focus on the latter approach. Our contributions can be summarized as follows. Building off Agent57, we carefully examine bottlenecks which slow down learning and address instabilities that arise when these bottlenecks are removed. We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based Exploration agent, which introduces solutions to enable taking advantage of three approaches that would otherwise lead to instabilities: training the value functions of the whole family of policies from Agent57 in parallel, on all policies’ transitions (instead of just the behaviour policy transitions), bootstrapping from the online network, and using high replay ratios. These solutions include carefully normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016) update target with a soft variant of Watkins’ Q(𝜆) (Watkins and Dayan, 1992) that enables faster signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for value updates. Moreover, we explore several recent advances in deep learning and determine which of them are beneficial for non-stationary problems like the ones considered in this work. Finally, we examine approaches to robustify performance by introducing a policy distillation mechanism that learns a policy head based on the actions obtained from the value network without being sensitive to value magnitudes. Our agent outperforms the human baseline across all 57 Atari games in 390M frames, using two orders of magnitude fewer interactions with the environment than Agent57 as shown in Figure 1. 2. Related work Large scale distributed agents have exhibited compelling results in recent years. Actor-critic (Espeholt et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari 2 Human-level Atari 200x faster 57 benchmark. Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that achieve competitive results on Atari. Finally, search-augmented distributed agents (Hessel et al., 2021; Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely they hold the highest mean and median human normalized scores over the 57 Atari games. However, all these methods show the same failure mode: they perform poorly in hard exploration games, such as Pitfall!, and Montezuma’s Revenge. In contrast, Agent57 (Badia et al., 2020) surpassed the human benchmark on all 57 games, showing better general performance. Go-Explore (Ecoffet et al., 2021) similarly achieved such general performance, by relying on coarse-grained state representations via a downscaling function that is highly specific to Atari. Learning as much as possible from previous experience is key for data-efficiency. Since it is often desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002; Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over the same batch of experience in policy gradient methods to avoid collecting new transitions for every learning step (Schulman et al., 2017). This decoupling between collecting experience and learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al., 2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005). Multiple approaches for making more efficient use of a replay buffer have been proposed, including prioritized sampling of transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al., 2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018), or reanalyzing old trajectories with the most recent version of a learned model to generate new targets in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz et al., 2017). The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps (400k frames). Work on this benchmark has focused on leveraging the use of models (Kaiser et al., 2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel, 2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020). While we consider this to be an important line of research, this tight budget produces an incentive to focus on a subset of games where exploration is easier, and it is unclear some games can be solved from scratch with such a small data budget. Such a setting is likely to prevent any meaningful learning on hard-exploration games, which is in contrast with the goal of our work. 3. Background: Agent57 Our work builds on top of Agent57, which combines three main ideas: (i) a distributed deep RL framework based on Recurrent Replay Distributed DQN (R2D2) (Kapturowski et al., 2018), (ii) ex\u0002ploration with a family of policies and the Never Give Up (NGU) intrinsic reward (Badia et al., 2019), and (iii) a meta-controller that dynamically adjusts the discount factor and balances exploration and exploitation throughout the course of training, by selecting from a family of policies. Below, we give a general introduction to the problem setting and some of the relevant components of Agent57. Problem definition. We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP) (Puterman, 1994). The goal is to find a policy 𝜋 that maximises the expected sum of future discounted rewards, 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡], where 𝛾 ∈ [0, 1) is the discount factor, 𝑟𝑡 = 𝑟(𝑥𝑡 , 𝑎𝑡) is the reward at time t, 𝑥𝑡 is the state at time t, and 𝑎𝑡 ∼ 𝜋(𝑎|𝑥𝑡) is the action generated by following some policy 𝜋. In the off-policy learning setting, data generated by a behavior policy 𝜇 is used to 3 Human-level Atari 200x faster learn about the target policy 𝜋. This can be achieved by employing a variant of Q-learning (Watkins and Dayan, 1992) to estimate the action-value function, 𝑄 𝜋 (𝑥, 𝑎) = 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡 |𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎]. The estimated action-value function can then be used to derive a new policy 𝜋(𝑎|𝑥) using the 𝜖-greedy operator G𝜖 (Sutton and Barto, 2018). 1 This new policy can then be used as target policy for another iteration, repeating the process. Agent57 uses a deep neural network with parameters 𝜃 to estimate action-value functions, 𝑄 𝜋 (𝑥, 𝑎; 𝜃) 2 , trained on return estimates 𝐺𝑡 derived with Retrace from sequences of off-policy data (Munos et al., 2016). In order to stabilize learning, a target network is used for bootstrapping the return estimates using double Q-learning (Van Hasselt et al., 2016); the parameters of this target network, 𝜃𝑇 , are periodically copied from the online network parameters 𝜃 (Mnih et al., 2015). Distributed RL framework. Agent57 is a distributed deep RL agent based on R2D2 that decouples acting from learning. Multiple actors interact with independent copies of the environment and feed trajectories to a central replay buffer. A separate learning process obtains trajectories from this buffer using prioritized sampling and updates the neural network parameters to predict the action-value function at each state. Actors obtain parameters from the learner periodically. See Appendix D for more details. Exploration with NGU. Agent57 uses the Never Give Up (NGU) intrinsic reward to encourage exploration. It aims at learning a family of 𝑁 = 32 policies which maximize different weightings of the extrinsic reward given by the environment (𝑟 𝑒 𝑡 ) and the intrinsic reward (𝑟 𝑖 𝑡 ), 𝑟𝑗,𝑡 = 𝑟 𝑒 𝑡 + 𝛽𝑗𝑟 𝑖 𝑡 (𝛽𝑗 ∈ ℝ+ , 𝑗 ∈ {0, . . . , 𝑁 −1}). The value of 𝛽𝑗 controls the degree of exploration, with higher values encouraging more exploratory behaviors, and each policy in the family is optimized with a different discount factor 𝛾𝑗 . The Universal Value Function Approximators (UVFA) framework (Schaul et al., 2015a) is employed to efficiently learn 𝑄 𝜋 𝑗 (𝑥, 𝑎; 𝜃) = 𝔼𝜋𝑗[ Í 𝑡≥0 𝛾 𝑡 𝑗 𝑟𝑗,𝑡|𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎] (we use a shorthand notation 𝑄 𝑗 (𝑥, 𝑎; 𝜃)) for 𝑗 ∈ {0, . . . , 𝑁 − 1} using a single set of shared parameters 𝜃. The policy 𝜋 𝑗 (𝑎|𝑥) can then be derived using the 𝜖-greedy operator as G𝜖𝑄 𝑗 (𝑥, 𝑎; 𝜃). We refer the reader to Appendix G for more details. Meta-controller. Agent57 introduces an adaptive meta-controller that decides which policies from the family of N policies to use for collecting experience based on their episodic returns. This naturally creates a curriculum over 𝛽𝑗 and 𝛾𝑗 by adapting their value throughout training. This optimization process is formulated as a non-stationary bandit problem. A detailed description about the meta\u0002controller implementation is provided in Appendix E. Q-function separation. The architecture of the Q-function in Agent57 is implemented as two separate networks in order to split the intrinsic and extrinsic components. The network parameters of 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑒) and 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑖) are separate and independently optimized with 𝑟 𝑒 𝑗 and 𝑟 𝑖 𝑗 , respectively. The main motivation behind this decomposition is to prevent the gradients of the decomposed intrinsic and extrinsic value function heads from interfering with each other. This can be beneficial in environments where the intrinsic reward is poorly aligned with the task’s extrinsic reward. 4. MEME: Improving the data efficiency of Agent57 This section describes the main algorithmic contributions of the MEME agent, aimed at improving the data-efficiency of Agent57. These contributions aim to achieve faster propagation of learning signals related to rare events ( A ), stabilize learning under differing value scales ( B ), improve the neural network architecture ( C ), and make updates more robust under a rapidly-changing policy ( D ). For 1We also use G B G0 to denote the pure greedy operator. 2For convenience, we occasionally omit (𝑥, 𝑎) or 𝜃 from 𝑄(𝑥, 𝑎; 𝜃), 𝜋(𝑥, 𝑎; 𝜃) when there is no danger of confusion. 4 Human-level Atari 200x faster Torso LSTM Q-value Head Policy Head Qj (at , xt ) 𝛑j (at | xt ) N-mixtures Torso LSTM Q-value Head Policy Head Q1 (at , x t ) 𝛑1 (at | xt ) Member policy 1 Q-value Head Policy Head QN (at , xt ) 𝛑N (at | xt ) Member policy 1 … … Dueling Head Dueling Head Qi (at , xt ) Qe (at , xt ) + Qj (at , xt ) Q-value Head N policy family members Torso LSTM Q-value Head Policy Head Q1 (at , xt ) 𝛑1 (at | xt ) N policy family members … QN (at , x t ) 𝛑N (at | xt ) Torso LSTM Q1 (xt , at ; θ) 𝛑1 (at | xt ) N policy family members … QN (xt , at ; θ) 𝛑N (at | xt ) Policy Head Q-value Head Figure 2 | MEME agent network architecture. The output of the LSTM block is passed to each of the N members of the family of policies, depicted as a light-grey box. Each policy consists of an Q-value and policy heads. The Q-value head is similar as in Agent57 paper, while the policy head is introduced for acting and target computation, and trained via policy distillation. clarity of exposition, we label methods according to the type of limitation they address. A1 Bootstrapping with online network. Target networks are frequently used in conjunction with value-based agents due to their stabilizing effect when learning from off-policy data (Mnih et al., 2015; Van Hasselt et al., 2016). This design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate. This issue can be mitigated to some extent by simply updating the target network more frequently, but the result is typically a less stable agent. To accelerate signal propagation while maintaining stability, we use online network bootstrapping, and we stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute to the loss. The trust region masks out the loss at any timestep for which both of the following conditions hold: |𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )| > 𝛼𝜎𝑗 (1) sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )) ≠ sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝐺𝑡) (2) where 𝛼 is a fixed hyperparameter, 𝐺𝑡 denotes the return estimate, 𝜃 and 𝜃𝑇 denote the online and target parameters respectively, and 𝜎𝑗 is the standard deviation of the TD-errors (a more precise description of which we defer until B1). Intuitively, we only mask if the current value of the online network is outside of the trust region (Equation 1) and the sign of the TD-error points away from the trust region (Equation 2), as depicted in Figure 3 in red. We note that a very similar trust region scheme is used for the value-function in most Proximal Policy Optimization (PPO) implementations (Schulman et al., 2017), though not described in the original paper. In contrast, the PPO version instead uses a constant threshold, and thus is not able to adapt to differing scales of value functions. Qj(xt ,at ;θT ) Qj(xt ,at ;θT ) + ασ Qj j (xt ,at ;θT ) - ασj Qj(xt ,at ;θ) Figure 3 | Trust region. The position of dots is given by the relationship between the values predicted by the online network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃), and the values predicted by the target network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 ) (Equation 1 and left hand side of Equation 2), the box represents the trust region bounds defined in Equation 1, and the direction of the arrow is given by the right hand side of Equation 2. Green-colored transitions are used in the loss computation, whereas red ones are masked out. A2 Target computation with tolerance. Agent57 uses Retrace (Munos et al., 2016) to compute return estimates from off-policy data, but we observed that it tends to cut traces too aggressively when 5 Human-level Atari 200x faster using 𝜖-greedy policies thus slowing down the propagation of information into the value function. Preliminary experiments showed that data-efficiency was improved in many dense-reward tasks when replacing Retrace with Peng’s Q(𝜆) (Peng and Williams, 1994), but its lack of off-policy corrections tends to result in degraded performance as data becomes more off-policy (e.g. by increasing the expected number of times that a sequence is sampled from replay, or by sharing data across a family of policies). This motivates us to propose an alternative return estimator, which we derive from Q(𝜆) (Watkins and Dayan, 1992): 𝐺𝑡 = max 𝑎 𝑄(𝑥𝑡 , 𝑎) + ∑︁ 𝑘≥0 Ö 𝑘−1 𝑖=0 𝜆𝑖 ! 𝛾 𝑘 (𝑟𝑡+𝑘 + 𝛾 max 𝑎 𝑄(𝑥𝑡+𝑘+1, 𝑎) − max 𝑎 𝑄(𝑥𝑡+𝑘, 𝑎)) (3) where Î𝑘−1 𝑖=0 𝜆𝑖 ∈ [0, 1] effectively controls how much information from the future is used in the return estimation and is generally used as a trace cutting coefficient to perform off-policy correction. Peng’s Q(𝜆) does not perform any kind of off-policy correction and sets 𝜆𝑖 = 𝜆, whereas Watkins’ Q(𝜆) (Watkins and Dayan, 1992) aggressively cuts traces whenever it encounters an off-policy action by using 𝜆𝑖 = 𝜆𝟙𝑎𝑖 ∈argmax𝑎𝑄(𝑥𝑖 ,𝑎) , where 𝟙 denotes the indicator function. We propose to use a softer trace cutting mechanism by adding a fixed tolerance parameter 𝜅 and taking the expectation of trace coefficients under 𝜋: 𝜆𝑖 = 𝜆𝔼𝑎∼𝜋(𝑎|𝑥𝑡) \u0002 𝟙[𝑄(𝑥𝑡 ,𝑎𝑡;𝜃) ≥𝑄(𝑥𝑡 ,𝑎;𝜃)−𝜅|𝑄(𝑥𝑖 ,𝑎;𝜃) |] \u0003 (4) Finally, we replace all occurrences of the max operator in Equation 3 with the expectation under 𝜋. The resulting return estimator, which we denote Soft Watkins Q(𝜆), leads to more transitions being used and increased sample efficiency. Note that Watkins Q(𝜆) is recovered when setting 𝜅 = 0 and 𝜋 = G (𝑄). B1 Loss and priority normalization. As we learn a family of Q-functions which vary over a wide range of discount factors and intrinsic reward scales, we expect that the Q-functions will vary considerably in scale. This may cause the larger-scale Q-values to dominate learning and destabilize learning of smaller Q-values. This is a particular concern in environments with very small extrinsic reward scales. To counteract this effect we introduce a normalization scheme on the TD-errors similar to that used in Schaul et al. (2021). Specifically, we compute a running estimate of the standard deviation of TD-errors of the online network 𝜎 running 𝑗 as well as a batch standard deviation 𝜎 batch 𝑗 , and compute 𝜎𝑗 = max(𝜎 running 𝑗 , 𝜎batch 𝑗 , 𝜖), where 𝜖 acts as small threshold to avoid amplification of noise past a specified scale, which we fix to 0.01 in all our experiments. We then divide the TD-errors by 𝜎𝑗 when computing both the loss and priorities. As opposed to Schaul et al. (2021) we compute the running statistics on the learner, and make use of importance sampling to correct the sampling distribution. B2 Cross-mixture training. Agent57 only trains the policy 𝑗 which was used to collect a given trajectory, but it is natural to ask whether data-efficiency and robustness may be improved by training all policies at once. We propose a training loss 𝐿 according to the following weighting scheme between the behavior policy loss and the mean over all losses: 𝐿 = 𝜂𝐿𝑗𝜇 + 1 − 𝜂 𝑁 𝑁 ∑︁−1 𝑗=0 𝐿𝑗 (5) where 𝐿𝑗 denotes the Q-learning loss for policy 𝑗, and 𝑗𝜇 denotes the index for the behavior policy selected by the meta-controller for the sampled trajectory. We find that an intermediate value for the mixture parameter of 𝜂 = 0.5 tends to work well. To achieve better compute-efficiency we choose to deviate from the original UVFA architecture which fed a 1-hot encoding of the policy index to the LSTM, aand instead modify the Q-value heads to output N sets of Q-values, one for each of the 6 Human-level Atari 200x faster members in the family of policies introduced in Section 3. Therefore, in the end we output values for all combinations of actions and policies (see Figure 2). We note that in this setting, there is also less deviation in the recurrent states when learning across different mixture policies. C1 Normalizer-free torso network. Normalization layers are a common feature of ResNet architec\u0002tures, and which are known to aid in training of very deep networks, but preliminary investigation revealed that several commonly used normalization layers are in fact detrimental to performance in our setting. Instead, we employ a variation of the NFNet architecture (Brock et al., 2021) for our policy torso network, which combines a variance-scaling strategy with scaled weight standardization and careful initialization to achieve state-of-the-art performance on ImageNet without the need for normalization layers. We adopt their use of stochastic depth (Huang et al., 2016) at training-time but omit the application of ordinary dropout to fully-connected layers as we did not observe any benefit from this form of regularization. Some care is required when using stochastic depth in conjunction with multi-step returns, as resampling of the stochastic depth mask at each timestep injects additional noise into the bootstrap values, resulting in a higher-variance return estimator. As such, we employ a temporally-consistent stochastic depth mask which remains fixed over the length of each training trajectory. C2 Shared torso with combined loss. Agent57 decomposes the combined Q-function into intrinsic and extrinsic components, 𝑄𝑒 and 𝑄𝑖 , which are represented by separate networks. Such a decomposi\u0002tion prevents the gradients of the decomposed value functions from interfering with each other. This interference may occur in environments where the intrinsic reward is poorly aligned with the task objective, as defined by the extrinsic reward. However, the choice to use separate separate networks comes at an expensive computational cost, and potentially limits sample-efficiency since generic low-level features cannot be shared. To alleviate these issues, we introduce a shared torso for the two Q-functions while retaining separate heads. While the form of the decomposition in Agent57 was chosen so as to ensure convergence to the optimal value-function 𝑄 ★ in the tabular setting, this does not generally hold under function approximation. Comparing the combined and decomposed losses we observe a mismatch in the gradients due to the absence of cross-terms 𝑄𝑖(𝜃) 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 and 𝑄𝑒 (𝜃) 𝜕𝑄𝑖(𝜃) 𝜕𝜃 in the decomposed loss: 𝜕 𝜕𝜃 h 1 2 (𝑄(𝜃) − 𝐺) 2 i | {z } combined loss ≠ 𝜕 𝜕𝜃 h 1 2 (𝑄𝑒 (𝜃) − 𝐺𝑒) 2 + 1 2 (𝛽𝑄𝑖(𝜃) − 𝛽𝐺𝑖) 2 i | {z } decomposed loss (6) \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) − 𝐺 \u0003 𝜕 𝜕𝜃 \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) \u0003 ≠ \u0002 𝑄𝑒 (𝜃) − 𝐺𝑒 \u0003 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 + 𝛽 2 \u0002 𝑄𝑖(𝜃) − 𝐺𝑖 \u0003 𝜕𝑄𝑖(𝜃) 𝜕𝜃 (7) Since we use a behavior policy induced by the total Q-function 𝑄 = 𝑄𝑒 + 𝛽𝑄𝑖 rather than the individual components, theory would suggest to use the combined loss instead. In addition, from a practical implementation perspective, this switch to the combined loss greatly simplifies the design choices involved in our proposed trust region method described in A1. The penalty paid for this choice is that the decomposition of the value function into extrinsic and intrinsic components no longer carries a strict semantic meaning. Nevertheless we do still retain an implicit inductive bias induced by multiplication of 𝑄𝑖 with the intrinsic reward weight 𝛽 𝑗 . D Robustifying behavior via policy distillation. Schaul et al. (2022) describe the effect of policy churn, whereby the greedy action of value-based RL algorithms may change frequently over consecutive parameter updates. This can have a deleterious effect on off-policy correction methods: traces will be cut more aggressively than with a stochastic policy, and bootstrap values will change frequently which can result in a higher variance return estimator. In addition, our choice of training with temporally-consistent stochastic depth masks can be interpreted as learning an implicit ensemble 7 Human-level Atari 200x faster of Q-functions; thus it is natural to ask whether we may see additional benefit from leveraging the policy induced by this ensemble. We propose to train an explicit policy head 𝜋dist (see Figure 2) via policy distillation to match the 𝜖-greedy policy induced by the Q-function. In expectation over multiple gradient steps this should help to smooth out the policy over time, as well as over the ensemble, while being much faster to evaluate than the individual members of the ensemble. Similarly to the trust-region described in A1, we mask the policy distillation loss at any timestep where a KL constraint 𝐶KL is violated: 𝐿𝜋 = − ∑︁ 𝑎,𝑡 G𝜖\n",
      "  > [ 0.68259829  1.19762492 -3.00449371] Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization lay\u0002ers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy over time. 1. Introduction To develop generally capable agents, the question of how to evaluate them is paramount. The Arcade Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate agents on an diverse set of tasks which are interesting to humans, and developed externally to the Reinforcement Learning (RL) community. As a result, several games exhibit reward structures which are highly adversarial to many popular algorithms. Mean and median human normalized scores (HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating deep RL agents. Recent progress has allowed state-of-the-art algorithms to greatly exceed average human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al., 2020; Van Hasselt et al., 2016). However, it has been argued that mean or median HNS might not be well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al., 2019). Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on most games, but completely fail to learn on a small number of them. Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all 57 Atari games. However, such generality came at the cost of data efficiency; requiring tens of billions of environment interactions to achieve above average-human performance in some games, reaching a figure of 78 billion frames before beating the human benchmark in all games. Data efficiency remains a desirable property for agents to possess, as many real-world challenges are data-limited by time and cost constraints (Dulac-Arnold et al., 2019). In this work, our aim is to develop an agent that is as general as Agent57 but that requires only a fraction of the environment interactions to achieve the same result. © 2022 DeepMind. All rights reserved arXiv:2209.07550v1 [cs.LG] 15 Sep 2022 Human-level Atari 200x faster Skiing Private Eye Pitfall! Montezuma's Revenge Surround Asteroids Ice Hockey Solaris Bowling Gravitar H.E.R.O. Berzerk Frostbite Tennis Alien Tutankham Riverraid Ms. Pac-Man Pong Beam Rider Chopper Command Seaquest Double Dunk Freeway Space Invaders Bank Heist Venture Battle Zone Asterix Q*BERT Amidar Name This Game Yars' Revenge Phoenix Time Pilot Demon Attack Wizard Of Wor Breakout Video Pinball Fishing Derby Enduro Stargunner Assault Zaxxon Jamesbond Centipede Kangaroo Atlantis Up'n Down Defender Boxing Robotank Kung-Fu Master Gopher Road Runner Crazy Climber Krull 1M 10M 100M 1B 10B 100B Frames to > human score 390M Agent57 MEME Figure 1 | Number of environment frames required by agents to outperform the human baseline on each game (in log-scale). Lower is better. On average, MEME achieves above human scores using 63× fewer environment interactions than Agent57. The smallest improvement is 9× (road_runner), the maximum is 721× (skiing), and the median across the suite is 35×. We observe small variance across seeds (c.f. Figure 8). There exist two main trends in the literature when it comes to measuring improvements in the learning capabilities of agents. One approach consists in measuring performance after a limited budget of interactions with the environment. While this type of evaluation has led to important progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019). On the other hand, one can aim to achieve a target end-performance with as few interactions as possible (Schmitt et al., 2020; Silver et al., 2017, 2018). Since our goal is to show that our new agent is as general as Agent57, while being more data efficient, we focus on the latter approach. Our contributions can be summarized as follows. Building off Agent57, we carefully examine bottlenecks which slow down learning and address instabilities that arise when these bottlenecks are removed. We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based Exploration agent, which introduces solutions to enable taking advantage of three approaches that would otherwise lead to instabilities: training the value functions of the whole family of policies from Agent57 in parallel, on all policies’ transitions (instead of just the behaviour policy transitions), bootstrapping from the online network, and using high replay ratios. These solutions include carefully normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016) update target with a soft variant of Watkins’ Q(𝜆) (Watkins and Dayan, 1992) that enables faster signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for value updates. Moreover, we explore several recent advances in deep learning and determine which of them are beneficial for non-stationary problems like the ones considered in this work. Finally, we examine approaches to robustify performance by introducing a policy distillation mechanism that learns a policy head based on the actions obtained from the value network without being sensitive to value magnitudes. Our agent outperforms the human baseline across all 57 Atari games in 390M frames, using two orders of magnitude fewer interactions with the environment than Agent57 as shown in Figure 1. 2. Related work Large scale distributed agents have exhibited compelling results in recent years. Actor-critic (Espeholt et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari 2 Human-level Atari 200x faster 57 benchmark. Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that achieve competitive results on Atari. Finally, search-augmented distributed agents (Hessel et al., 2021; Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely they hold the highest mean and median human normalized scores over the 57 Atari games. However, all these methods show the same failure mode: they perform poorly in hard exploration games, such as Pitfall!, and Montezuma’s Revenge. In contrast, Agent57 (Badia et al., 2020) surpassed the human benchmark on all 57 games, showing better general performance. Go-Explore (Ecoffet et al., 2021) similarly achieved such general performance, by relying on coarse-grained state representations via a downscaling function that is highly specific to Atari. Learning as much as possible from previous experience is key for data-efficiency. Since it is often desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002; Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over the same batch of experience in policy gradient methods to avoid collecting new transitions for every learning step (Schulman et al., 2017). This decoupling between collecting experience and learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al., 2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005). Multiple approaches for making more efficient use of a replay buffer have been proposed, including prioritized sampling of transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al., 2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018), or reanalyzing old trajectories with the most recent version of a learned model to generate new targets in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz et al., 2017). The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps (400k frames). Work on this benchmark has focused on leveraging the use of models (Kaiser et al., 2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel, 2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020). While we consider this to be an important line of research, this tight budget produces an incentive to focus on a subset of games where exploration is easier, and it is unclear some games can be solved from scratch with such a small data budget. Such a setting is likely to prevent any meaningful learning on hard-exploration games, which is in contrast with the goal of our work. 3. Background: Agent57 Our work builds on top of Agent57, which combines three main ideas: (i) a distributed deep RL framework based on Recurrent Replay Distributed DQN (R2D2) (Kapturowski et al., 2018), (ii) ex\u0002ploration with a family of policies and the Never Give Up (NGU) intrinsic reward (Badia et al., 2019), and (iii) a meta-controller that dynamically adjusts the discount factor and balances exploration and exploitation throughout the course of training, by selecting from a family of policies. Below, we give a general introduction to the problem setting and some of the relevant components of Agent57. Problem definition. We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP) (Puterman, 1994). The goal is to find a policy 𝜋 that maximises the expected sum of future discounted rewards, 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡], where 𝛾 ∈ [0, 1) is the discount factor, 𝑟𝑡 = 𝑟(𝑥𝑡 , 𝑎𝑡) is the reward at time t, 𝑥𝑡 is the state at time t, and 𝑎𝑡 ∼ 𝜋(𝑎|𝑥𝑡) is the action generated by following some policy 𝜋. In the off-policy learning setting, data generated by a behavior policy 𝜇 is used to 3 Human-level Atari 200x faster learn about the target policy 𝜋. This can be achieved by employing a variant of Q-learning (Watkins and Dayan, 1992) to estimate the action-value function, 𝑄 𝜋 (𝑥, 𝑎) = 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡 |𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎]. The estimated action-value function can then be used to derive a new policy 𝜋(𝑎|𝑥) using the 𝜖-greedy operator G𝜖 (Sutton and Barto, 2018). 1 This new policy can then be used as target policy for another iteration, repeating the process. Agent57 uses a deep neural network with parameters 𝜃 to estimate action-value functions, 𝑄 𝜋 (𝑥, 𝑎; 𝜃) 2 , trained on return estimates 𝐺𝑡 derived with Retrace from sequences of off-policy data (Munos et al., 2016).\n",
      "    > [ 0.73764092  1.50381505 -3.48627043] Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization lay\u0002ers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy over time. 1. Introduction To develop generally capable agents, the question of how to evaluate them is paramount. The Arcade Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate agents on an diverse set of tasks which are interesting to humans, and developed externally to the Reinforcement Learning (RL) community. As a result, several games exhibit reward structures which are highly adversarial to many popular algorithms. Mean and median human normalized scores (HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating deep RL agents. Recent progress has allowed state-of-the-art algorithms to greatly exceed average human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al., 2020; Van Hasselt et al., 2016). However, it has been argued that mean or median HNS might not be well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al., 2019). Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on most games, but completely fail to learn on a small number of them. Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all 57 Atari games. However, such generality came at the cost of data efficiency; requiring tens of billions of environment interactions to achieve above average-human performance in some games, reaching a figure of 78 billion frames before beating the human benchmark in all games.\n",
      "      > [ 0.54513168  1.30529583 -3.62534451] Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline.\n",
      "        > [ 0.24919949  1.10780084 -3.36006045] Human-level Atari 200x faster Steven Kapturowski1 , Víctor Campos*1, Ray Jiang*1, Nemanja Rakićević1 , Hado van Hasselt1 , Charles Blundell1 and Adrià Puigdomènech Badia1 1DeepMind, *Equal contribution The task of building general agents that perform well over a wide range of tasks has been an impor\u0002tant goal in reinforcement learning since its inception.\n",
      "        > [ 0.97025383  1.59567904 -3.85882521] The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark.\n",
      "        > [ 0.91469681  1.53874981 -4.08522415] Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 bil\u0002lion frames of experience to achieve.\n",
      "        > [-0.30261958  0.52889246 -3.80358195] Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline.\n",
      "      > [ 0.0083265   1.45206392 -3.69237518] We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization lay\u0002ers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy over time. 1. Introduction To develop generally capable agents, the question of how to evaluate them is paramount.\n",
      "        > [ 0.0091347   1.04603553 -3.89195371] We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent.\n",
      "        > [-0.01612254  1.34338641 -4.03994083] We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero.\n",
      "        > [ 0.51877671  1.45161867 -3.44339466] The four key components to our approach are (1) an approximate trust region method which enables stable bootstrapping from the online network, (2) a normalisation scheme for the loss and priorities which improves robustness when learning a set of value functions with a wide range of scales, (3) an improved architecture employing techniques from NFNets in order to leverage deeper networks without the need for normalization lay\u0002ers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy over time.\n",
      "        > [-0.39243293 -0.37381107 -3.73424196] 1. Introduction To develop generally capable agents, the question of how to evaluate them is paramount.\n",
      "      > [ 0.21551606  1.74004591 -3.36210585] The Arcade Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate agents on an diverse set of tasks which are interesting to humans, and developed externally to the Reinforcement Learning (RL) community. As a result, several games exhibit reward structures which are highly adversarial to many popular algorithms. Mean and median human normalized scores (HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating deep RL agents. Recent progress has allowed state-of-the-art algorithms to greatly exceed average human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al., 2020; Van Hasselt et al., 2016).\n",
      "        > [-0.21065013  1.36229897 -3.04802752] The Arcade Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate agents on an diverse set of tasks which are interesting to humans, and developed externally to the Reinforcement Learning (RL) community.\n",
      "        > [ 0.77098304  3.18415594 -3.73751974] As a result, several games exhibit reward structures which are highly adversarial to many popular algorithms.\n",
      "        > [ 0.33797631  0.72383374 -3.8279717 ] Mean and median human normalized scores (HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating deep RL agents.\n",
      "        > [ 0.99851942  1.93932235 -4.06023741] Recent progress has allowed state-of-the-art algorithms to greatly exceed average human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al., 2020; Van Hasselt et al., 2016).\n",
      "      > [ 0.53349113  1.5636363  -3.62031221] However, it has been argued that mean or median HNS might not be well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al., 2019). Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on most games, but completely fail to learn on a small number of them. Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all 57 Atari games. However, such generality came at the cost of data efficiency; requiring tens of billions of environment interactions to achieve above average-human performance in some games, reaching a figure of 78 billion frames before beating the human benchmark in all games.\n",
      "        > [ 0.27158213  0.87324095 -3.21215916] However, it has been argued that mean or median HNS might not be well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al., 2019).\n",
      "        > [ 0.56424135  1.94155169 -3.5372982 ] Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on most games, but completely fail to learn on a small number of them.\n",
      "        > [ 1.05856872  0.78969014 -3.97198629] Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all 57 Atari games.\n",
      "        > [ 0.66503614  2.52805543 -3.57940173] However, such generality came at the cost of data efficiency; requiring tens of billions of environment interactions to achieve above average-human performance in some games, reaching a figure of 78 billion frames before beating the human benchmark in all games.\n",
      "    > [-0.54222685 -0.21036945 -3.53933764] Data efficiency remains a desirable property for agents to possess, as many real-world challenges are data-limited by time and cost constraints (Dulac-Arnold et al., 2019). In this work, our aim is to develop an agent that is as general as Agent57 but that requires only a fraction of the environment interactions to achieve the same result. © 2022 DeepMind. All rights reserved arXiv:2209.07550v1 [cs.LG] 15 Sep 2022 Human-level Atari 200x faster Skiing Private Eye Pitfall! Montezuma's Revenge Surround Asteroids Ice Hockey Solaris Bowling Gravitar H.E.R.O. Berzerk Frostbite Tennis Alien Tutankham Riverraid Ms. Pac-Man Pong Beam Rider Chopper Command Seaquest Double Dunk Freeway Space Invaders Bank Heist Venture Battle Zone Asterix Q*BERT Amidar Name This Game Yars' Revenge Phoenix Time Pilot Demon Attack Wizard Of Wor Breakout Video Pinball Fishing Derby Enduro Stargunner Assault Zaxxon Jamesbond Centipede Kangaroo Atlantis Up'n Down Defender Boxing Robotank Kung-Fu Master Gopher Road Runner Crazy Climber Krull 1M 10M 100M 1B 10B 100B Frames to > human score 390M Agent57 MEME Figure 1 | Number of environment frames required by agents to outperform the human baseline on each game (in log-scale). Lower is better. On average, MEME achieves above human scores using 63× fewer environment interactions than Agent57. The smallest improvement is 9× (road_runner), the maximum is 721× (skiing), and the median across the suite is 35×. We observe small variance across seeds (c.f. Figure 8). There exist two main trends in the literature when it comes to measuring improvements in the learning capabilities of agents. One approach consists in measuring performance after a limited budget of interactions with the environment. While this type of evaluation has led to important progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019). On the other hand, one can aim to achieve a target end-performance with as few interactions as possible (Schmitt et al., 2020; Silver et al., 2017, 2018). Since our goal is to show that our new agent is as general as Agent57, while being more data efficient, we focus on the latter approach.\n",
      "      > [ 0.13913843  0.73815775 -3.60093856] Data efficiency remains a desirable property for agents to possess, as many real-world challenges are data-limited by time and cost constraints (Dulac-Arnold et al., 2019). In this work, our aim is to develop an agent that is as general as Agent57 but that requires only a fraction of the environment interactions to achieve the same result. © 2022 DeepMind. All rights reserved arXiv:2209.07550v1 [cs.LG] 15 Sep 2022 Human-level Atari 200x faster Skiing Private Eye Pitfall!\n",
      "        > [ 0.43127725  1.40037048 -3.55078578] Data efficiency remains a desirable property for agents to possess, as many real-world challenges are data-limited by time and cost constraints (Dulac-Arnold et al., 2019).\n",
      "        > [-0.45719379  0.14279269 -3.62644744] In this work, our aim is to develop an agent that is as general as Agent57 but that requires only a fraction of the environment interactions to achieve the same result.\n",
      "        > [ 0.73377609 -0.19811249 -2.69165039] © 2022 DeepMind.\n",
      "        > [ 0.28785694  0.82403886 -3.36685371] All rights reserved arXiv:2209.07550v1 [cs.LG] 15 Sep 2022 Human-level Atari 200x faster Skiing Private Eye Pitfall!\n",
      "      > [ 0.18977275  1.46257591 -4.27421808] Montezuma's Revenge Surround Asteroids Ice Hockey Solaris Bowling Gravitar H.E.R.O. Berzerk Frostbite Tennis Alien Tutankham Riverraid Ms. Pac-Man Pong Beam Rider Chopper Command Seaquest Double Dunk Freeway Space Invaders Bank Heist Venture Battle Zone Asterix Q*BERT Amidar Name This Game Yars' Revenge Phoenix Time Pilot Demon Attack Wizard Of Wor Breakout Video Pinball Fishing Derby Enduro Stargunner Assault Zaxxon Jamesbond Centipede Kangaroo Atlantis Up'n Down Defender Boxing Robotank Kung-Fu Master Gopher Road Runner Crazy Climber Krull 1M 10M 100M 1B 10B 100B Frames to > human score 390M Agent57 MEME Figure 1 | Number of environment frames required by agents to outperform the human baseline on each game (in log-scale). Lower is better. On average, MEME achieves above human scores using 63× fewer environment interactions than Agent57.\n",
      "        > [ 0.74667752  2.50994587 -3.57808161] Montezuma's Revenge Surround Asteroids Ice Hockey Solaris Bowling Gravitar H.E.R.O.\n",
      "        > [-0.14634939  1.33877432 -4.00952482] Berzerk Frostbite Tennis Alien Tutankham Riverraid Ms. Pac-Man Pong Beam Rider Chopper Command Seaquest Double Dunk Freeway Space Invaders Bank Heist Venture Battle Zone Asterix Q*BERT Amidar Name This Game Yars' Revenge Phoenix Time Pilot Demon Attack Wizard Of Wor Breakout Video Pinball Fishing Derby Enduro Stargunner Assault Zaxxon Jamesbond Centipede Kangaroo Atlantis Up'n Down Defender Boxing Robotank Kung-Fu Master Gopher Road Runner Crazy Climber Krull 1M 10M 100M 1B 10B 100B Frames to > human score 390M Agent57 MEME Figure 1 | Number of environment frames required by agents to outperform the human baseline on each game (in log-scale).\n",
      "        > [ 1.14254117  0.6736536  -3.83900976] Lower is better.\n",
      "        > [-0.83767539  0.15573472 -3.90117121] On average, MEME achieves above human scores using 63× fewer environment interactions than Agent57.\n",
      "      > [-0.10683829  0.90246546 -3.72813535] The smallest improvement is 9× (road_runner), the maximum is 721× (skiing), and the median across the suite is 35×. We observe small variance across seeds (c.f. Figure 8). There exist two main trends in the literature when it comes to measuring improvements in the learning capabilities of agents.\n",
      "        > [-0.10170384  1.69497347 -3.27114916] The smallest improvement is 9× (road_runner), the maximum is 721× (skiing), and the median across the suite is 35×.\n",
      "        > [ 1.30813992  0.44627652 -4.222229  ] We observe small variance across seeds (c.f.\n",
      "        > [-0.10386238  0.59339112 -3.27320719] Figure 8).\n",
      "        > [-0.35651377  0.11115725 -3.81634045] There exist two main trends in the literature when it comes to measuring improvements in the learning capabilities of agents.\n",
      "      > [ 0.31954104  0.76051486 -3.56013656] One approach consists in measuring performance after a limited budget of interactions with the environment. While this type of evaluation has led to important progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019). On the other hand, one can aim to achieve a target end-performance with as few interactions as possible (Schmitt et al., 2020; Silver et al., 2017, 2018). Since our goal is to show that our new agent is as general as Agent57, while being more data efficient, we focus on the latter approach.\n",
      "        > [ 0.5768609   0.62879241 -3.87962294] One approach consists in measuring performance after a limited budget of interactions with the environment.\n",
      "        > [ 1.14975643  1.62443447 -2.55217934] While this type of evaluation has led to important progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019).\n",
      "        > [-0.20560002  1.14403808 -3.24420881] On the other hand, one can aim to achieve a target end-performance with as few interactions as possible (Schmitt et al., 2020; Silver et al., 2017, 2018).\n",
      "        > [-0.39901352 -0.00746727 -3.81951904] Since our goal is to show that our new agent is as general as Agent57, while being more data efficient, we focus on the latter approach.\n",
      "    > [ 0.45883456  1.01980472 -3.251858  ] Our contributions can be summarized as follows. Building off Agent57, we carefully examine bottlenecks which slow down learning and address instabilities that arise when these bottlenecks are removed. We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based Exploration agent, which introduces solutions to enable taking advantage of three approaches that would otherwise lead to instabilities: training the value functions of the whole family of policies from Agent57 in parallel, on all policies’ transitions (instead of just the behaviour policy transitions), bootstrapping from the online network, and using high replay ratios. These solutions include carefully normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016) update target with a soft variant of Watkins’ Q(𝜆) (Watkins and Dayan, 1992) that enables faster signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for value updates. Moreover, we explore several recent advances in deep learning and determine which of them are beneficial for non-stationary problems like the ones considered in this work. Finally, we examine approaches to robustify performance by introducing a policy distillation mechanism that learns a policy head based on the actions obtained from the value network without being sensitive to value magnitudes. Our agent outperforms the human baseline across all 57 Atari games in 390M frames, using two orders of magnitude fewer interactions with the environment than Agent57 as shown in Figure 1. 2. Related work Large scale distributed agents have exhibited compelling results in recent years. Actor-critic (Espeholt et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari 2 Human-level Atari 200x faster 57 benchmark. Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that achieve competitive results on Atari. Finally, search-augmented distributed agents (Hessel et al., 2021; Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely they hold the highest mean and median human normalized scores over the 57 Atari games. However, all these methods show the same failure mode: they perform poorly in hard exploration games, such as Pitfall!, and Montezuma’s Revenge. In contrast, Agent57 (Badia et al., 2020) surpassed the human benchmark on all 57 games, showing better general performance. Go-Explore (Ecoffet et al., 2021) similarly achieved such general performance, by relying on coarse-grained state representations via a downscaling function that is highly specific to Atari. Learning as much as possible from previous experience is key for data-efficiency. Since it is often desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002; Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over the same batch of experience in policy gradient methods to avoid collecting new transitions for every learning step (Schulman et al., 2017).\n",
      "      > [-0.2872633   0.74760199 -3.33247542] Our contributions can be summarized as follows. Building off Agent57, we carefully examine bottlenecks which slow down learning and address instabilities that arise when these bottlenecks are removed. We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based Exploration agent, which introduces solutions to enable taking advantage of three approaches that would otherwise lead to instabilities: training the value functions of the whole family of policies from Agent57 in parallel, on all policies’ transitions (instead of just the behaviour policy transitions), bootstrapping from the online network, and using high replay ratios. These solutions include carefully normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016) update target with a soft variant of Watkins’ Q(𝜆) (Watkins and Dayan, 1992) that enables faster signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for value updates.\n",
      "        > [ 0.7928825   1.13360858 -3.90872192] Our contributions can be summarized as follows.\n",
      "        > [ 0.56057423  0.54478264 -3.40270996] Building off Agent57, we carefully examine bottlenecks which slow down learning and address instabilities that arise when these bottlenecks are removed.\n",
      "        > [-0.65775555  0.99744618 -3.62086987] We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based Exploration agent, which introduces solutions to enable taking advantage of three approaches that would otherwise lead to instabilities: training the value functions of the whole family of policies from Agent57 in parallel, on all policies’ transitions (instead of just the behaviour policy transitions), bootstrapping from the online network, and using high replay ratios.\n",
      "        > [ 0.38410351  0.58741522 -3.43422174] These solutions include carefully normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016) update target with a soft variant of Watkins’ Q(𝜆) (Watkins and Dayan, 1992) that enables faster signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for value updates.\n",
      "      > [ 0.79653192  1.25508618 -3.48983192] Moreover, we explore several recent advances in deep learning and determine which of them are beneficial for non-stationary problems like the ones considered in this work. Finally, we examine approaches to robustify performance by introducing a policy distillation mechanism that learns a policy head based on the actions obtained from the value network without being sensitive to value magnitudes. Our agent outperforms the human baseline across all 57 Atari games in 390M frames, using two orders of magnitude fewer interactions with the environment than Agent57 as shown in Figure 1. 2. Related work Large scale distributed agents have exhibited compelling results in recent years.\n",
      "        > [ 0.30601859  0.92427826 -3.0291748 ] Moreover, we explore several recent advances in deep learning and determine which of them are beneficial for non-stationary problems like the ones considered in this work.\n",
      "        > [ 0.90404004  1.54152775 -3.88966799] Finally, we examine approaches to robustify performance by introducing a policy distillation mechanism that learns a policy head based on the actions obtained from the value network without being sensitive to value magnitudes.\n",
      "        > [ 1.24271429  0.76500911 -3.66317892] Our agent outperforms the human baseline across all 57 Atari games in 390M frames, using two orders of magnitude fewer interactions with the environment than Agent57 as shown in Figure 1.\n",
      "        > [-0.05279379  1.21547258 -4.11268568] 2. Related work Large scale distributed agents have exhibited compelling results in recent years.\n",
      "      > [ 0.08963294  0.91520596 -3.45079494] Actor-critic (Espeholt et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari 2 Human-level Atari 200x faster 57 benchmark. Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that achieve competitive results on Atari. Finally, search-augmented distributed agents (Hessel et al., 2021; Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely they hold the highest mean and median human normalized scores over the 57 Atari games. However, all these methods show the same failure mode: they perform poorly in hard exploration games, such as Pitfall!, and Montezuma’s Revenge.\n",
      "        > [-0.1840429   0.6762746  -3.69677806] Actor-critic (Espeholt et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari 2 Human-level Atari 200x faster 57 benchmark.\n",
      "        > [ 0.1209147   1.23092747 -3.5815444 ] Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that achieve competitive results on Atari.\n",
      "        > [ 0.73156685  1.02851462 -3.73471665] Finally, search-augmented distributed agents (Hessel et al., 2021; Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely they hold the highest mean and median human normalized scores over the 57 Atari games.\n",
      "        > [ 0.50331199  0.80987477 -3.50011086] However, all these methods show the same failure mode: they perform poorly in hard exploration games, such as Pitfall!, and Montezuma’s Revenge.\n",
      "      > [ 0.39745802  1.17727816 -3.40199876] In contrast, Agent57 (Badia et al., 2020) surpassed the human benchmark on all 57 games, showing better general performance. Go-Explore (Ecoffet et al., 2021) similarly achieved such general performance, by relying on coarse-grained state representations via a downscaling function that is highly specific to Atari. Learning as much as possible from previous experience is key for data-efficiency. Since it is often desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002; Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over the same batch of experience in policy gradient methods to avoid collecting new transitions for every learning step (Schulman et al., 2017).\n",
      "        > [ 0.93458122  0.94471395 -3.85838032] In contrast, Agent57 (Badia et al., 2020) surpassed the human benchmark on all 57 games, showing better general performance.\n",
      "        > [ 0.48246258  1.68996477 -3.45937347] Go-Explore (Ecoffet et al., 2021) similarly achieved such general performance, by relying on coarse-grained state representations via a downscaling function that is highly specific to Atari.\n",
      "        > [ 0.2355969   1.03137898 -3.71926713] Learning as much as possible from previous experience is key for data-efficiency.\n",
      "        > [-0.22155008  0.60037041 -3.54233456] Since it is often desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002; Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over the same batch of experience in policy gradient methods to avoid collecting new transitions for every learning step (Schulman et al., 2017).\n",
      "    > [ 0.51335281  1.02100384 -2.83701015] This decoupling between collecting experience and learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al., 2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005). Multiple approaches for making more efficient use of a replay buffer have been proposed, including prioritized sampling of transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al., 2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018), or reanalyzing old trajectories with the most recent version of a learned model to generate new targets in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz et al., 2017). The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps (400k frames). Work on this benchmark has focused on leveraging the use of models (Kaiser et al., 2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel, 2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020). While we consider this to be an important line of research, this tight budget produces an incentive to focus on a subset of games where exploration is easier, and it is unclear some games can be solved from scratch with such a small data budget. Such a setting is likely to prevent any meaningful learning on hard-exploration games, which is in contrast with the goal of our work. 3. Background: Agent57 Our work builds on top of Agent57, which combines three main ideas: (i) a distributed deep RL framework based on Recurrent Replay Distributed DQN (R2D2) (Kapturowski et al., 2018), (ii) ex\u0002ploration with a family of policies and the Never Give Up (NGU) intrinsic reward (Badia et al., 2019), and (iii) a meta-controller that dynamically adjusts the discount factor and balances exploration and exploitation throughout the course of training, by selecting from a family of policies. Below, we give a general introduction to the problem setting and some of the relevant components of Agent57. Problem definition. We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP) (Puterman, 1994). The goal is to find a policy 𝜋 that maximises the expected sum of future discounted rewards, 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡], where 𝛾 ∈ [0, 1) is the discount factor, 𝑟𝑡 = 𝑟(𝑥𝑡 , 𝑎𝑡) is the reward at time t, 𝑥𝑡 is the state at time t, and 𝑎𝑡 ∼ 𝜋(𝑎|𝑥𝑡) is the action generated by following some policy 𝜋. In the off-policy learning setting, data generated by a behavior policy 𝜇 is used to 3 Human-level Atari 200x faster learn about the target policy 𝜋. This can be achieved by employing a variant of Q-learning (Watkins and Dayan, 1992) to estimate the action-value function, 𝑄 𝜋 (𝑥, 𝑎) = 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡 |𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎]. The estimated action-value function can then be used to derive a new policy 𝜋(𝑎|𝑥) using the 𝜖-greedy operator G𝜖 (Sutton and Barto, 2018). 1 This new policy can then be used as target policy for another iteration, repeating the process. Agent57 uses a deep neural network with parameters 𝜃 to estimate action-value functions, 𝑄 𝜋 (𝑥, 𝑎; 𝜃) 2 , trained on return estimates 𝐺𝑡 derived with Retrace from sequences of off-policy data (Munos et al., 2016).\n",
      "      > [ 0.38228187  0.5783661  -3.69414115] This decoupling between collecting experience and learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al., 2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005). Multiple approaches for making more efficient use of a replay buffer have been proposed, including prioritized sampling of transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al., 2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018), or reanalyzing old trajectories with the most recent version of a learned model to generate new targets in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz et al., 2017). The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps (400k frames). Work on this benchmark has focused on leveraging the use of models (Kaiser et al., 2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel, 2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020).\n",
      "        > [ 0.72470093  0.15437357 -3.72896433] This decoupling between collecting experience and learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al., 2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005).\n",
      "        > [ 0.15986696  0.17476125 -3.74717593] Multiple approaches for making more efficient use of a replay buffer have been proposed, including prioritized sampling of transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al., 2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018), or reanalyzing old trajectories with the most recent version of a learned model to generate new targets in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz et al., 2017).\n",
      "        > [ 0.93368649  1.350155   -3.55754185] The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps (400k frames).\n",
      "        > [ 0.34625167  0.85442144 -3.50115228] Work on this benchmark has focused on leveraging the use of models (Kaiser et al., 2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel, 2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020).\n",
      "      > [ 0.31430274  1.5839783  -3.24490714] While we consider this to be an important line of research, this tight budget produces an incentive to focus on a subset of games where exploration is easier, and it is unclear some games can be solved from scratch with such a small data budget. Such a setting is likely to prevent any meaningful learning on hard-exploration games, which is in contrast with the goal of our work. 3. Background: Agent57 Our work builds on top of Agent57, which combines three main ideas: (i) a distributed deep RL framework based on Recurrent Replay Distributed DQN (R2D2) (Kapturowski et al., 2018), (ii) ex\u0002ploration with a family of policies and the Never Give Up (NGU) intrinsic reward (Badia et al., 2019), and (iii) a meta-controller that dynamically adjusts the discount factor and balances exploration and exploitation throughout the course of training, by selecting from a family of policies. Below, we give a general introduction to the problem setting and some of the relevant components of Agent57.\n",
      "        > [ 0.81355953  2.50255799 -3.26226234] While we consider this to be an important line of research, this tight budget produces an incentive to focus on a subset of games where exploration is easier, and it is unclear some games can be solved from scratch with such a small data budget.\n",
      "        > [ 0.66747922  1.9412992  -3.04060674] Such a setting is likely to prevent any meaningful learning on hard-exploration games, which is in contrast with the goal of our work.\n",
      "        > [-0.01680447  0.75847727 -3.33639812] 3. Background: Agent57 Our work builds on top of Agent57, which combines three main ideas: (i) a distributed deep RL framework based on Recurrent Replay Distributed DQN (R2D2) (Kapturowski et al., 2018), (ii) ex\u0002ploration with a family of policies and the Never Give Up (NGU) intrinsic reward (Badia et al., 2019), and (iii) a meta-controller that dynamically adjusts the discount factor and balances exploration and exploitation throughout the course of training, by selecting from a family of policies.\n",
      "        > [ 0.59555101  0.67579812 -3.67723036] Below, we give a general introduction to the problem setting and some of the relevant components of Agent57.\n",
      "      > [ 1.13682389  1.23473656 -3.4669404 ] Problem definition. We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP) (Puterman, 1994). The goal is to find a policy 𝜋 that maximises the expected sum of future discounted rewards, 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡], where 𝛾 ∈ [0, 1) is the discount factor, 𝑟𝑡 = 𝑟(𝑥𝑡 , 𝑎𝑡) is the reward at time t, 𝑥𝑡 is the state at time t, and 𝑎𝑡 ∼ 𝜋(𝑎|𝑥𝑡) is the action generated by following some policy 𝜋. In the off-policy learning setting, data generated by a behavior policy 𝜇 is used to 3 Human-level Atari 200x faster learn about the target policy 𝜋.\n",
      "        > [ 1.4677856   1.7406131  -3.49562263] Problem definition.\n",
      "        > [ 1.5230782   0.3142308  -3.37746096] We consider the problem of discounted infinite-horizon RL in Markov Decision Processes (MDP) (Puterman, 1994).\n",
      "        > [ 0.4208498  1.8430717 -3.5412631] The goal is to find a policy 𝜋 that maximises the expected sum of future discounted rewards, 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡], where 𝛾 ∈ [0, 1) is the discount factor, 𝑟𝑡 = 𝑟(𝑥𝑡 , 𝑎𝑡) is the reward at time t, 𝑥𝑡 is the state at time t, and 𝑎𝑡 ∼ 𝜋(𝑎|𝑥𝑡) is the action generated by following some policy 𝜋.\n",
      "        > [ 0.72746301  1.36823213 -3.78030634] In the off-policy learning setting, data generated by a behavior policy 𝜇 is used to 3 Human-level Atari 200x faster learn about the target policy 𝜋.\n",
      "      > [ 0.1502223   1.03220332 -3.15926433] This can be achieved by employing a variant of Q-learning (Watkins and Dayan, 1992) to estimate the action-value function, 𝑄 𝜋 (𝑥, 𝑎) = 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡 |𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎]. The estimated action-value function can then be used to derive a new policy 𝜋(𝑎|𝑥) using the 𝜖-greedy operator G𝜖 (Sutton and Barto, 2018). 1 This new policy can then be used as target policy for another iteration, repeating the process. Agent57 uses a deep neural network with parameters 𝜃 to estimate action-value functions, 𝑄 𝜋 (𝑥, 𝑎; 𝜃) 2 , trained on return estimates 𝐺𝑡 derived with Retrace from sequences of off-policy data (Munos et al., 2016).\n",
      "        > [-0.34489465  0.68378484 -3.60048103] This can be achieved by employing a variant of Q-learning (Watkins and Dayan, 1992) to estimate the action-value function, 𝑄 𝜋 (𝑥, 𝑎) = 𝔼𝜋[ Í 𝑡≥0 𝛾 𝑡 𝑟𝑡 |𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎].\n",
      "        > [ 0.38787207  1.4623189  -3.32228661] The estimated action-value function can then be used to derive a new policy 𝜋(𝑎|𝑥) using the 𝜖-greedy operator G𝜖 (Sutton and Barto, 2018).\n",
      "        > [ 0.04103762  0.47426555 -3.46172857] 1 This new policy can then be used as target policy for another iteration, repeating the process.\n",
      "        > [ 0.20426732  0.21348107 -3.31607556] Agent57 uses a deep neural network with parameters 𝜃 to estimate action-value functions, 𝑄 𝜋 (𝑥, 𝑎; 𝜃) 2 , trained on return estimates 𝐺𝑡 derived with Retrace from sequences of off-policy data (Munos et al., 2016).\n",
      "  > [ 1.05901265  1.22528863 -3.12542081] In order to stabilize learning, a target network is used for bootstrapping the return estimates using double Q-learning (Van Hasselt et al., 2016); the parameters of this target network, 𝜃𝑇 , are periodically copied from the online network parameters 𝜃 (Mnih et al., 2015). Distributed RL framework. Agent57 is a distributed deep RL agent based on R2D2 that decouples acting from learning. Multiple actors interact with independent copies of the environment and feed trajectories to a central replay buffer. A separate learning process obtains trajectories from this buffer using prioritized sampling and updates the neural network parameters to predict the action-value function at each state. Actors obtain parameters from the learner periodically. See Appendix D for more details. Exploration with NGU. Agent57 uses the Never Give Up (NGU) intrinsic reward to encourage exploration. It aims at learning a family of 𝑁 = 32 policies which maximize different weightings of the extrinsic reward given by the environment (𝑟 𝑒 𝑡 ) and the intrinsic reward (𝑟 𝑖 𝑡 ), 𝑟𝑗,𝑡 = 𝑟 𝑒 𝑡 + 𝛽𝑗𝑟 𝑖 𝑡 (𝛽𝑗 ∈ ℝ+ , 𝑗 ∈ {0, . . . , 𝑁 −1}). The value of 𝛽𝑗 controls the degree of exploration, with higher values encouraging more exploratory behaviors, and each policy in the family is optimized with a different discount factor 𝛾𝑗 . The Universal Value Function Approximators (UVFA) framework (Schaul et al., 2015a) is employed to efficiently learn 𝑄 𝜋 𝑗 (𝑥, 𝑎; 𝜃) = 𝔼𝜋𝑗[ Í 𝑡≥0 𝛾 𝑡 𝑗 𝑟𝑗,𝑡|𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎] (we use a shorthand notation 𝑄 𝑗 (𝑥, 𝑎; 𝜃)) for 𝑗 ∈ {0, . . . , 𝑁 − 1} using a single set of shared parameters 𝜃. The policy 𝜋 𝑗 (𝑎|𝑥) can then be derived using the 𝜖-greedy operator as G𝜖𝑄 𝑗 (𝑥, 𝑎; 𝜃). We refer the reader to Appendix G for more details. Meta-controller. Agent57 introduces an adaptive meta-controller that decides which policies from the family of N policies to use for collecting experience based on their episodic returns. This naturally creates a curriculum over 𝛽𝑗 and 𝛾𝑗 by adapting their value throughout training. This optimization process is formulated as a non-stationary bandit problem. A detailed description about the meta\u0002controller implementation is provided in Appendix E. Q-function separation. The architecture of the Q-function in Agent57 is implemented as two separate networks in order to split the intrinsic and extrinsic components. The network parameters of 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑒) and 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑖) are separate and independently optimized with 𝑟 𝑒 𝑗 and 𝑟 𝑖 𝑗 , respectively. The main motivation behind this decomposition is to prevent the gradients of the decomposed intrinsic and extrinsic value function heads from interfering with each other. This can be beneficial in environments where the intrinsic reward is poorly aligned with the task’s extrinsic reward. 4. MEME: Improving the data efficiency of Agent57 This section describes the main algorithmic contributions of the MEME agent, aimed at improving the data-efficiency of Agent57. These contributions aim to achieve faster propagation of learning signals related to rare events ( A ), stabilize learning under differing value scales ( B ), improve the neural network architecture ( C ), and make updates more robust under a rapidly-changing policy ( D ). For 1We also use G B G0 to denote the pure greedy operator. 2For convenience, we occasionally omit (𝑥, 𝑎) or 𝜃 from 𝑄(𝑥, 𝑎; 𝜃), 𝜋(𝑥, 𝑎; 𝜃) when there is no danger of confusion. 4 Human-level Atari 200x faster Torso LSTM Q-value Head Policy Head Qj (at , xt ) 𝛑j (at | xt ) N-mixtures Torso LSTM Q-value Head Policy Head Q1 (at , x t ) 𝛑1 (at | xt ) Member policy 1 Q-value Head Policy Head QN (at , xt ) 𝛑N (at | xt ) Member policy 1 … … Dueling Head Dueling Head Qi (at , xt ) Qe (at , xt ) + Qj (at , xt ) Q-value Head N policy family members Torso LSTM Q-value Head Policy Head Q1 (at , xt ) 𝛑1 (at | xt ) N policy family members … QN (at , x t ) 𝛑N (at | xt ) Torso LSTM Q1 (xt , at ; θ) 𝛑1 (at | xt ) N policy family members … QN (xt , at ; θ) 𝛑N (at | xt ) Policy Head Q-value Head Figure 2 | MEME agent network architecture. The output of the LSTM block is passed to each of the N members of the family of policies, depicted as a light-grey box. Each policy consists of an Q-value and policy heads. The Q-value head is similar as in Agent57 paper, while the policy head is introduced for acting and target computation, and trained via policy distillation. clarity of exposition, we label methods according to the type of limitation they address. A1 Bootstrapping with online network. Target networks are frequently used in conjunction with value-based agents due to their stabilizing effect when learning from off-policy data (Mnih et al., 2015; Van Hasselt et al., 2016). This design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate. This issue can be mitigated to some extent by simply updating the target network more frequently, but the result is typically a less stable agent. To accelerate signal propagation while maintaining stability, we use online network bootstrapping, and we stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute to the loss. The trust region masks out the loss at any timestep for which both of the following conditions hold: |𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )| > 𝛼𝜎𝑗 (1) sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )) ≠ sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝐺𝑡) (2) where 𝛼 is a fixed hyperparameter, 𝐺𝑡 denotes the return estimate, 𝜃 and 𝜃𝑇 denote the online and target parameters respectively, and 𝜎𝑗 is the standard deviation of the TD-errors (a more precise description of which we defer until B1). Intuitively, we only mask if the current value of the online network is outside of the trust region (Equation 1) and the sign of the TD-error points away from the trust region (Equation 2), as depicted in Figure 3 in red. We note that a very similar trust region scheme is used for the value-function in most Proximal Policy Optimization (PPO) implementations (Schulman et al., 2017), though not described in the original paper. In contrast, the PPO version instead uses a constant threshold, and thus is not able to adapt to differing scales of value functions. Qj(xt ,at ;θT ) Qj(xt ,at ;θT ) + ασ Qj j (xt ,at ;θT ) - ασj Qj(xt ,at ;θ) Figure 3 | Trust region. The position of dots is given by the relationship between the values predicted by the online network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃), and the values predicted by the target network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 ) (Equation 1 and left hand side of Equation 2), the box represents the trust region bounds defined in Equation 1, and the direction of the arrow is given by the right hand side of Equation 2. Green-colored transitions are used in the loss computation, whereas red ones are masked out. A2 Target computation with tolerance. Agent57 uses Retrace (Munos et al., 2016) to compute return estimates from off-policy data, but we observed that it tends to cut traces too aggressively when 5 Human-level Atari 200x faster using 𝜖-greedy policies thus slowing down the propagation of information into the value function. Preliminary experiments showed that data-efficiency was improved in many dense-reward tasks when replacing Retrace with Peng’s Q(𝜆) (Peng and Williams, 1994), but its lack of off-policy corrections tends to result in degraded performance as data becomes more off-policy (e.g. by increasing the expected number of times that a sequence is sampled from replay, or by sharing data across a family of policies). This motivates us to propose an alternative return estimator, which we derive from Q(𝜆) (Watkins and Dayan, 1992): 𝐺𝑡 = max 𝑎 𝑄(𝑥𝑡 , 𝑎) + ∑︁ 𝑘≥0 Ö 𝑘−1 𝑖=0 𝜆𝑖 ! 𝛾 𝑘 (𝑟𝑡+𝑘 + 𝛾 max 𝑎 𝑄(𝑥𝑡+𝑘+1, 𝑎) − max 𝑎 𝑄(𝑥𝑡+𝑘, 𝑎)) (3) where Î𝑘−1 𝑖=0 𝜆𝑖 ∈ [0, 1] effectively controls how much information from the future is used in the return estimation and is generally used as a trace cutting coefficient to perform off-policy correction. Peng’s Q(𝜆) does not perform any kind of off-policy correction and sets 𝜆𝑖 = 𝜆, whereas Watkins’ Q(𝜆) (Watkins and Dayan, 1992) aggressively cuts traces whenever it encounters an off-policy action by using 𝜆𝑖 = 𝜆𝟙𝑎𝑖 ∈argmax𝑎𝑄(𝑥𝑖 ,𝑎) , where 𝟙 denotes the indicator function. We propose to use a softer trace cutting mechanism by adding a fixed tolerance parameter 𝜅 and taking the expectation of trace coefficients under 𝜋: 𝜆𝑖 = 𝜆𝔼𝑎∼𝜋(𝑎|𝑥𝑡) \u0002 𝟙[𝑄(𝑥𝑡 ,𝑎𝑡;𝜃) ≥𝑄(𝑥𝑡 ,𝑎;𝜃)−𝜅|𝑄(𝑥𝑖 ,𝑎;𝜃) |] \u0003 (4) Finally, we replace all occurrences of the max operator in Equation 3 with the expectation under 𝜋. The resulting return estimator, which we denote Soft Watkins Q(𝜆), leads to more transitions being used and increased sample efficiency. Note that Watkins Q(𝜆) is recovered when setting 𝜅 = 0 and 𝜋 = G (𝑄). B1 Loss and priority normalization. As we learn a family of Q-functions which vary over a wide range of discount factors and intrinsic reward scales, we expect that the Q-functions will vary considerably in scale. This may cause the larger-scale Q-values to dominate learning and destabilize learning of smaller Q-values. This is a particular concern in environments with very small extrinsic reward scales. To counteract this effect we introduce a normalization scheme on the TD-errors similar to that used in Schaul et al. (2021). Specifically, we compute a running estimate of the standard deviation of TD-errors of the online network 𝜎 running 𝑗 as well as a batch standard deviation 𝜎 batch 𝑗 , and compute 𝜎𝑗 = max(𝜎 running 𝑗 , 𝜎batch 𝑗 , 𝜖), where 𝜖 acts as small threshold to avoid amplification of noise past a specified scale, which we fix to 0.01 in all our experiments. We then divide the TD-errors by 𝜎𝑗 when computing both the loss and priorities. As opposed to Schaul et al.\n",
      "    > [ 0.24724634  0.8579281  -3.099334  ] In order to stabilize learning, a target network is used for bootstrapping the return estimates using double Q-learning (Van Hasselt et al., 2016); the parameters of this target network, 𝜃𝑇 , are periodically copied from the online network parameters 𝜃 (Mnih et al., 2015). Distributed RL framework. Agent57 is a distributed deep RL agent based on R2D2 that decouples acting from learning. Multiple actors interact with independent copies of the environment and feed trajectories to a central replay buffer. A separate learning process obtains trajectories from this buffer using prioritized sampling and updates the neural network parameters to predict the action-value function at each state. Actors obtain parameters from the learner periodically. See Appendix D for more details. Exploration with NGU. Agent57 uses the Never Give Up (NGU) intrinsic reward to encourage exploration. It aims at learning a family of 𝑁 = 32 policies which maximize different weightings of the extrinsic reward given by the environment (𝑟 𝑒 𝑡 ) and the intrinsic reward (𝑟 𝑖 𝑡 ), 𝑟𝑗,𝑡 = 𝑟 𝑒 𝑡 + 𝛽𝑗𝑟 𝑖 𝑡 (𝛽𝑗 ∈ ℝ+ , 𝑗 ∈ {0, . . . , 𝑁 −1}). The value of 𝛽𝑗 controls the degree of exploration, with higher values encouraging more exploratory behaviors, and each policy in the family is optimized with a different discount factor 𝛾𝑗 . The Universal Value Function Approximators (UVFA) framework (Schaul et al., 2015a) is employed to efficiently learn 𝑄 𝜋 𝑗 (𝑥, 𝑎; 𝜃) = 𝔼𝜋𝑗[ Í 𝑡≥0 𝛾 𝑡 𝑗 𝑟𝑗,𝑡|𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎] (we use a shorthand notation 𝑄 𝑗 (𝑥, 𝑎; 𝜃)) for 𝑗 ∈ {0, . . . , 𝑁 − 1} using a single set of shared parameters 𝜃. The policy 𝜋 𝑗 (𝑎|𝑥) can then be derived using the 𝜖-greedy operator as G𝜖𝑄 𝑗 (𝑥, 𝑎; 𝜃). We refer the reader to Appendix G for more details.\n",
      "      > [ 0.08309031 -0.13692975 -3.5322237 ] In order to stabilize learning, a target network is used for bootstrapping the return estimates using double Q-learning (Van Hasselt et al., 2016); the parameters of this target network, 𝜃𝑇 , are periodically copied from the online network parameters 𝜃 (Mnih et al., 2015). Distributed RL framework. Agent57 is a distributed deep RL agent based on R2D2 that decouples acting from learning. Multiple actors interact with independent copies of the environment and feed trajectories to a central replay buffer.\n",
      "        > [-0.15476449  0.46693391 -3.76116085] In order to stabilize learning, a target network is used for bootstrapping the return estimates using double Q-learning (Van Hasselt et al., 2016); the parameters of this target network, 𝜃𝑇 , are periodically copied from the online network parameters 𝜃 (Mnih et al., 2015).\n",
      "        > [ 0.37259459  0.32757211 -2.83744431] Distributed RL framework.\n",
      "        > [ 0.13165946 -0.40877861 -3.5158534 ] Agent57 is a distributed deep RL agent based on R2D2 that decouples acting from learning.\n",
      "        > [-0.29429686 -0.19072579 -3.78009701] Multiple actors interact with independent copies of the environment and feed trajectories to a central replay buffer.\n",
      "      > [ 0.21494794  0.12586567 -2.7468915 ] A separate learning process obtains trajectories from this buffer using prioritized sampling and updates the neural network parameters to predict the action-value function at each state. Actors obtain parameters from the learner periodically. See Appendix D for more details. Exploration with NGU.\n",
      "        > [ 0.79911631 -0.12134305 -2.649405  ] A separate learning process obtains trajectories from this buffer using prioritized sampling and updates the neural network parameters to predict the action-value function at each state.\n",
      "        > [-0.93681723 -0.44184145 -3.68372393] Actors obtain parameters from the learner periodically.\n",
      "        > [ 1.0714072   1.22560942 -3.40878224] See Appendix D for more details.\n",
      "        > [-0.8182739   1.13558567 -3.7908659 ] Exploration with NGU.\n",
      "      > [ 0.72578365  1.86540353 -3.84573817] Agent57 uses the Never Give Up (NGU) intrinsic reward to encourage exploration. It aims at learning a family of 𝑁 = 32 policies which maximize different weightings of the extrinsic reward given by the environment (𝑟 𝑒 𝑡 ) and the intrinsic reward (𝑟 𝑖 𝑡 ), 𝑟𝑗,𝑡 = 𝑟 𝑒 𝑡 + 𝛽𝑗𝑟 𝑖 𝑡 (𝛽𝑗 ∈ ℝ+ , 𝑗 ∈ {0, . . . , 𝑁 −1}). The value of 𝛽𝑗 controls the degree of exploration, with higher values encouraging more exploratory behaviors, and each policy in the family is optimized with a different discount factor 𝛾𝑗 .\n",
      "        > [ 0.21285588  0.80043602 -3.80259418] Agent57 uses the Never Give Up (NGU) intrinsic reward to encourage exploration.\n",
      "        > [ 1.25303948  2.02638626 -3.65778971] It aims at learning a family of 𝑁 = 32 policies which maximize different weightings of the extrinsic reward given by the environment (𝑟 𝑒 𝑡 ) and the intrinsic reward (𝑟 𝑖 𝑡 ), 𝑟𝑗,𝑡 = 𝑟 𝑒 𝑡 + 𝛽𝑗𝑟 𝑖 𝑡 (𝛽𝑗 ∈ ℝ+ , 𝑗 ∈ {0, .\n",
      "        > [ 0.27843422 -0.21084921 -3.39189291] . . , 𝑁 −1}).\n",
      "        > [ 0.6180498   1.87620211 -3.3709054 ] The value of 𝛽𝑗 controls the degree of exploration, with higher values encouraging more exploratory behaviors, and each policy in the family is optimized with a different discount factor 𝛾𝑗 .\n",
      "      > [-0.01581046  1.38225031 -3.18354225] The Universal Value Function Approximators (UVFA) framework (Schaul et al., 2015a) is employed to efficiently learn 𝑄 𝜋 𝑗 (𝑥, 𝑎; 𝜃) = 𝔼𝜋𝑗[ Í 𝑡≥0 𝛾 𝑡 𝑗 𝑟𝑗,𝑡|𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎] (we use a shorthand notation 𝑄 𝑗 (𝑥, 𝑎; 𝜃)) for 𝑗 ∈ {0, . . . , 𝑁 − 1} using a single set of shared parameters 𝜃. The policy 𝜋 𝑗 (𝑎|𝑥) can then be derived using the 𝜖-greedy operator as G𝜖𝑄 𝑗 (𝑥, 𝑎; 𝜃). We refer the reader to Appendix G for more details.\n",
      "        > [-0.0540204   0.56094921 -3.2413137 ] The Universal Value Function Approximators (UVFA) framework (Schaul et al., 2015a) is employed to efficiently learn 𝑄 𝜋 𝑗 (𝑥, 𝑎; 𝜃) = 𝔼𝜋𝑗[ Í 𝑡≥0 𝛾 𝑡 𝑗 𝑟𝑗,𝑡|𝑥𝑡 = 𝑥, 𝑎𝑡 = 𝑎] (we use a shorthand notation 𝑄 𝑗 (𝑥, 𝑎; 𝜃)) for 𝑗 ∈ {0, .\n",
      "        > [ 0.06725101 -0.2912232  -3.28725815] . . , 𝑁 − 1} using a single set of shared parameters 𝜃.\n",
      "        > [ 0.67780876  1.80688477 -3.43768311] The policy 𝜋 𝑗 (𝑎|𝑥) can then be derived using the 𝜖-greedy operator as G𝜖𝑄 𝑗 (𝑥, 𝑎; 𝜃).\n",
      "        > [ 0.28800443  1.1277746  -2.97133589] We refer the reader to Appendix G for more details.\n",
      "    > [-1.19668436  1.79886687 -3.34555531] Meta-controller. Agent57 introduces an adaptive meta-controller that decides which policies from the family of N policies to use for collecting experience based on their episodic returns. This naturally creates a curriculum over 𝛽𝑗 and 𝛾𝑗 by adapting their value throughout training. This optimization process is formulated as a non-stationary bandit problem. A detailed description about the meta\u0002controller implementation is provided in Appendix E. Q-function separation. The architecture of the Q-function in Agent57 is implemented as two separate networks in order to split the intrinsic and extrinsic components. The network parameters of 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑒) and 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑖) are separate and independently optimized with 𝑟 𝑒 𝑗 and 𝑟 𝑖 𝑗 , respectively. The main motivation behind this decomposition is to prevent the gradients of the decomposed intrinsic and extrinsic value function heads from interfering with each other. This can be beneficial in environments where the intrinsic reward is poorly aligned with the task’s extrinsic reward. 4. MEME: Improving the data efficiency of Agent57 This section describes the main algorithmic contributions of the MEME agent, aimed at improving the data-efficiency of Agent57. These contributions aim to achieve faster propagation of learning signals related to rare events ( A ), stabilize learning under differing value scales ( B ), improve the neural network architecture ( C ), and make updates more robust under a rapidly-changing policy ( D ). For 1We also use G B G0 to denote the pure greedy operator. 2For convenience, we occasionally omit (𝑥, 𝑎) or 𝜃 from 𝑄(𝑥, 𝑎; 𝜃), 𝜋(𝑥, 𝑎; 𝜃) when there is no danger of confusion. 4 Human-level Atari 200x faster Torso LSTM Q-value Head Policy Head Qj (at , xt ) 𝛑j (at | xt ) N-mixtures Torso LSTM Q-value Head Policy Head Q1 (at , x t ) 𝛑1 (at | xt ) Member policy 1 Q-value Head Policy Head QN (at , xt ) 𝛑N (at | xt ) Member policy 1 … … Dueling Head Dueling Head Qi (at , xt ) Qe (at , xt ) + Qj (at , xt ) Q-value Head N policy family members Torso LSTM Q-value Head Policy Head Q1 (at , xt ) 𝛑1 (at | xt ) N policy family members … QN (at , x t ) 𝛑N (at | xt ) Torso LSTM Q1 (xt , at ; θ) 𝛑1 (at | xt ) N policy family members … QN (xt , at ; θ) 𝛑N (at | xt ) Policy Head Q-value Head Figure 2 | MEME agent network architecture. The output of the LSTM block is passed to each of the N members of the family of policies, depicted as a light-grey box. Each policy consists of an Q-value and policy heads.\n",
      "      > [-0.5080815   0.98721302 -3.97170997] Meta-controller. Agent57 introduces an adaptive meta-controller that decides which policies from the family of N policies to use for collecting experience based on their episodic returns. This naturally creates a curriculum over 𝛽𝑗 and 𝛾𝑗 by adapting their value throughout training. This optimization process is formulated as a non-stationary bandit problem.\n",
      "        > [-0.68465096  1.35226727 -3.84204578] Meta-controller.\n",
      "        > [-0.34199688  0.72575986 -3.99025822] Agent57 introduces an adaptive meta-controller that decides which policies from the family of N policies to use for collecting experience based on their episodic returns.\n",
      "        > [-0.68158084  0.62796396 -3.2854557 ] This naturally creates a curriculum over 𝛽𝑗 and 𝛾𝑗 by adapting their value throughout training.\n",
      "        > [ 0.98666334  0.95215321 -3.62662101] This optimization process is formulated as a non-stationary bandit problem.\n",
      "      > [-0.01363323  0.91310734 -3.21258307] A detailed description about the meta\u0002controller implementation is provided in Appendix E. Q-function separation. The architecture of the Q-function in Agent57 is implemented as two separate networks in order to split the intrinsic and extrinsic components. The network parameters of 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑒) and 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑖) are separate and independently optimized with 𝑟 𝑒 𝑗 and 𝑟 𝑖 𝑗 , respectively. The main motivation behind this decomposition is to prevent the gradients of the decomposed intrinsic and extrinsic value function heads from interfering with each other.\n",
      "        > [-0.22610778  1.30528152 -2.72823405] A detailed description about the meta\u0002controller implementation is provided in Appendix E. Q-function separation.\n",
      "        > [ 0.13348165  0.41536289 -3.3214035 ] The architecture of the Q-function in Agent57 is implemented as two separate networks in order to split the intrinsic and extrinsic components.\n",
      "        > [-0.34686908  0.54866487 -3.62301326] The network parameters of 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑒) and 𝑄 𝑗 (𝑥, 𝑎; 𝜃𝑖) are separate and independently optimized with 𝑟 𝑒 𝑗 and 𝑟 𝑖 𝑗 , respectively.\n",
      "        > [ 1.47760391  0.90644562 -3.18847942] The main motivation behind this decomposition is to prevent the gradients of the decomposed intrinsic and extrinsic value function heads from interfering with each other.\n",
      "      > [-0.26473135  0.68173391 -3.26266956] This can be beneficial in environments where the intrinsic reward is poorly aligned with the task’s extrinsic reward. 4. MEME: Improving the data efficiency of Agent57 This section describes the main algorithmic contributions of the MEME agent, aimed at improving the data-efficiency of Agent57. These contributions aim to achieve faster propagation of learning signals related to rare events ( A ), stabilize learning under differing value scales ( B ), improve the neural network architecture ( C ), and make updates more robust under a rapidly-changing policy ( D ). For 1We also use G B G0 to denote the pure greedy operator.\n",
      "        > [ 1.06217217  1.43763685 -3.2647953 ] This can be beneficial in environments where the intrinsic reward is poorly aligned with the task’s extrinsic reward.\n",
      "        > [-0.58437777  0.26673824 -3.96780777] 4. MEME: Improving the data efficiency of Agent57 This section describes the main algorithmic contributions of the MEME agent, aimed at improving the data-efficiency of Agent57.\n",
      "        > [ 0.31886378  0.58455175 -3.36847496] These contributions aim to achieve faster propagation of learning signals related to rare events ( A ), stabilize learning under differing value scales ( B ), improve the neural network architecture ( C ), and make updates more robust under a rapidly-changing policy ( D ).\n",
      "        > [ 0.36550134  0.98825377 -2.99861431] For 1We also use G B G0 to denote the pure greedy operator.\n",
      "      > [-0.21424887  1.14142001 -3.0414958 ] 2For convenience, we occasionally omit (𝑥, 𝑎) or 𝜃 from 𝑄(𝑥, 𝑎; 𝜃), 𝜋(𝑥, 𝑎; 𝜃) when there is no danger of confusion. 4 Human-level Atari 200x faster Torso LSTM Q-value Head Policy Head Qj (at , xt ) 𝛑j (at | xt ) N-mixtures Torso LSTM Q-value Head Policy Head Q1 (at , x t ) 𝛑1 (at | xt ) Member policy 1 Q-value Head Policy Head QN (at , xt ) 𝛑N (at | xt ) Member policy 1 … … Dueling Head Dueling Head Qi (at , xt ) Qe (at , xt ) + Qj (at , xt ) Q-value Head N policy family members Torso LSTM Q-value Head Policy Head Q1 (at , xt ) 𝛑1 (at | xt ) N policy family members … QN (at , x t ) 𝛑N (at | xt ) Torso LSTM Q1 (xt , at ; θ) 𝛑1 (at | xt ) N policy family members … QN (xt , at ; θ) 𝛑N (at | xt ) Policy Head Q-value Head Figure 2 | MEME agent network architecture. The output of the LSTM block is passed to each of the N members of the family of policies, depicted as a light-grey box. Each policy consists of an Q-value and policy heads.\n",
      "        > [ 0.48799074  0.32544705 -2.07424784] 2For convenience, we occasionally omit (𝑥, 𝑎) or 𝜃 from 𝑄(𝑥, 𝑎; 𝜃), 𝜋(𝑥, 𝑎; 𝜃) when there is no danger of confusion.\n",
      "        > [-0.7775619   1.47879338 -3.85448718] 4 Human-level Atari 200x faster Torso LSTM Q-value Head Policy Head Qj (at , xt ) 𝛑j (at | xt ) N-mixtures Torso LSTM Q-value Head Policy Head Q1 (at , x t ) 𝛑1 (at | xt ) Member policy 1 Q-value Head Policy Head QN (at , xt ) 𝛑N (at | xt ) Member policy 1 … … Dueling Head Dueling Head Qi (at , xt ) Qe (at , xt ) + Qj (at , xt ) Q-value Head N policy family members Torso LSTM Q-value Head Policy Head Q1 (at , xt ) 𝛑1 (at | xt ) N policy family members … QN (at , x t ) 𝛑N (at | xt ) Torso LSTM Q1 (xt , at ; θ) 𝛑1 (at | xt ) N policy family members … QN (xt , at ; θ) 𝛑N (at | xt ) Policy Head Q-value Head Figure 2 | MEME agent network architecture.\n",
      "        > [-0.12754738  1.6769886  -2.79473877] The output of the LSTM block is passed to each of the N members of the family of policies, depicted as a light-grey box.\n",
      "        > [-0.05673028  1.51221895 -3.78680134] Each policy consists of an Q-value and policy heads.\n",
      "    > [ 0.99172509  0.93755758 -2.91803646] The Q-value head is similar as in Agent57 paper, while the policy head is introduced for acting and target computation, and trained via policy distillation. clarity of exposition, we label methods according to the type of limitation they address. A1 Bootstrapping with online network. Target networks are frequently used in conjunction with value-based agents due to their stabilizing effect when learning from off-policy data (Mnih et al., 2015; Van Hasselt et al., 2016). This design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate. This issue can be mitigated to some extent by simply updating the target network more frequently, but the result is typically a less stable agent. To accelerate signal propagation while maintaining stability, we use online network bootstrapping, and we stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute to the loss. The trust region masks out the loss at any timestep for which both of the following conditions hold: |𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )| > 𝛼𝜎𝑗 (1) sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )) ≠ sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝐺𝑡) (2) where 𝛼 is a fixed hyperparameter, 𝐺𝑡 denotes the return estimate, 𝜃 and 𝜃𝑇 denote the online and target parameters respectively, and 𝜎𝑗 is the standard deviation of the TD-errors (a more precise description of which we defer until B1). Intuitively, we only mask if the current value of the online network is outside of the trust region (Equation 1) and the sign of the TD-error points away from the trust region (Equation 2), as depicted in Figure 3 in red. We note that a very similar trust region scheme is used for the value-function in most Proximal Policy Optimization (PPO) implementations (Schulman et al., 2017), though not described in the original paper. In contrast, the PPO version instead uses a constant threshold, and thus is not able to adapt to differing scales of value functions. Qj(xt ,at ;θT ) Qj(xt ,at ;θT ) + ασ Qj j (xt ,at ;θT ) - ασj Qj(xt ,at ;θ) Figure 3 | Trust region. The position of dots is given by the relationship between the values predicted by the online network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃), and the values predicted by the target network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 ) (Equation 1 and left hand side of Equation 2), the box represents the trust region bounds defined in Equation 1, and the direction of the arrow is given by the right hand side of Equation 2. Green-colored transitions are used in the loss computation, whereas red ones are masked out. A2 Target computation with tolerance. Agent57 uses Retrace (Munos et al., 2016) to compute return estimates from off-policy data, but we observed that it tends to cut traces too aggressively when 5 Human-level Atari 200x faster using 𝜖-greedy policies thus slowing down the propagation of information into the value function.\n",
      "      > [-0.34800288  1.28586924 -3.40999317] The Q-value head is similar as in Agent57 paper, while the policy head is introduced for acting and target computation, and trained via policy distillation. clarity of exposition, we label methods according to the type of limitation they address. A1 Bootstrapping with online network. Target networks are frequently used in conjunction with value-based agents due to their stabilizing effect when learning from off-policy data (Mnih et al., 2015; Van Hasselt et al., 2016).\n",
      "        > [-0.09732585  0.47564903 -3.55112338] The Q-value head is similar as in Agent57 paper, while the policy head is introduced for acting and target computation, and trained via policy distillation.\n",
      "        > [ 1.29600346  1.64040256 -2.43304157] clarity of exposition, we label methods according to the type of limitation they address.\n",
      "        > [-0.87263453 -0.17507124 -3.20998621] A1 Bootstrapping with online network.\n",
      "        > [-0.43167883  1.03378606 -3.69129825] Target networks are frequently used in conjunction with value-based agents due to their stabilizing effect when learning from off-policy data (Mnih et al., 2015; Van Hasselt et al., 2016).\n",
      "      > [ 0.38971573  0.79064453 -3.8769455 ] This design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate. This issue can be mitigated to some extent by simply updating the target network more frequently, but the result is typically a less stable agent. To accelerate signal propagation while maintaining stability, we use online network bootstrapping, and we stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute to the loss. The trust region masks out the loss at any timestep for which both of the following conditions hold: |𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )| > 𝛼𝜎𝑗 (1) sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )) ≠ sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝐺𝑡) (2) where 𝛼 is a fixed hyperparameter, 𝐺𝑡 denotes the return estimate, 𝜃 and 𝜃𝑇 denote the online and target parameters respectively, and 𝜎𝑗 is the standard deviation of the TD-errors (a more precise description of which we defer until B1).\n",
      "        > [ 0.95291829  1.39314985 -3.50739574] This design choice places a fundamental restriction on how quickly changes in the Q-function are able to propagate.\n",
      "        > [-0.19577125  0.38423347 -3.76263428] This issue can be mitigated to some extent by simply updating the target network more frequently, but the result is typically a less stable agent.\n",
      "        > [ 0.26482978  0.46542245 -3.69558001] To accelerate signal propagation while maintaining stability, we use online network bootstrapping, and we stabilise the learning by introducing an approximate trust region for value updates that allows us to filter which samples contribute to the loss.\n",
      "        > [ 1.47615647  0.77640563 -3.70765305] The trust region masks out the loss at any timestep for which both of the following conditions hold: |𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )| > 𝛼𝜎𝑗 (1) sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 )) ≠ sgn(𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃) − 𝐺𝑡) (2) where 𝛼 is a fixed hyperparameter, 𝐺𝑡 denotes the return estimate, 𝜃 and 𝜃𝑇 denote the online and target parameters respectively, and 𝜎𝑗 is the standard deviation of the TD-errors (a more precise description of which we defer until B1).\n",
      "      > [ 0.51332843  1.49229527 -3.55079436] Intuitively, we only mask if the current value of the online network is outside of the trust region (Equation 1) and the sign of the TD-error points away from the trust region (Equation 2), as depicted in Figure 3 in red. We note that a very similar trust region scheme is used for the value-function in most Proximal Policy Optimization (PPO) implementations (Schulman et al., 2017), though not described in the original paper. In contrast, the PPO version instead uses a constant threshold, and thus is not able to adapt to differing scales of value functions. Qj(xt ,at ;θT ) Qj(xt ,at ;θT ) + ασ Qj j (xt ,at ;θT ) - ασj Qj(xt ,at ;θ) Figure 3 | Trust region.\n",
      "        > [ 0.80471706  0.8094579  -3.84065557] Intuitively, we only mask if the current value of the online network is outside of the trust region (Equation 1) and the sign of the TD-error points away from the trust region (Equation 2), as depicted in Figure 3 in red.\n",
      "        > [ 0.73307902  1.75650012 -2.85370636] We note that a very similar trust region scheme is used for the value-function in most Proximal Policy Optimization (PPO) implementations (Schulman et al., 2017), though not described in the original paper.\n",
      "        > [ 1.21313715  1.59962547 -3.06056643] In contrast, the PPO version instead uses a constant threshold, and thus is not able to adapt to differing scales of value functions.\n",
      "        > [ 0.24790382  0.81160963 -4.01327419] Qj(xt ,at ;θT ) Qj(xt ,at ;θT ) + ασ Qj j (xt ,at ;θT ) - ασj Qj(xt ,at ;θ) Figure 3 | Trust region.\n",
      "      > [ 0.93357581  0.73363888 -3.43133783] The position of dots is given by the relationship between the values predicted by the online network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃), and the values predicted by the target network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 ) (Equation 1 and left hand side of Equation 2), the box represents the trust region bounds defined in Equation 1, and the direction of the arrow is given by the right hand side of Equation 2. Green-colored transitions are used in the loss computation, whereas red ones are masked out. A2 Target computation with tolerance. Agent57 uses Retrace (Munos et al., 2016) to compute return estimates from off-policy data, but we observed that it tends to cut traces too aggressively when 5 Human-level Atari 200x faster using 𝜖-greedy policies thus slowing down the propagation of information into the value function.\n",
      "        > [ 0.28240347  0.97284162 -3.98949909] The position of dots is given by the relationship between the values predicted by the online network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃), and the values predicted by the target network, 𝑄 𝑗 (𝑥𝑡 , 𝑎𝑡; 𝜃𝑇 ) (Equation 1 and left hand side of Equation 2), the box represents the trust region bounds defined in Equation 1, and the direction of the arrow is given by the right hand side of Equation 2.\n",
      "        > [ 1.03729093  0.51261896 -3.13966298] Green-colored transitions are used in the loss computation, whereas red ones are masked out.\n",
      "        > [ 1.29681396 -0.59389591 -2.89599609] A2 Target computation with tolerance.\n",
      "        > [ 1.29901314  0.44542834 -3.63269043] Agent57 uses Retrace (Munos et al., 2016) to compute return estimates from off-policy data, but we observed that it tends to cut traces too aggressively when 5 Human-level Atari 200x faster using 𝜖-greedy policies thus slowing down the propagation of information into the value function.\n",
      "    > [ 0.97227257  0.05652307 -3.43045068] Preliminary experiments showed that data-efficiency was improved in many dense-reward tasks when replacing Retrace with Peng’s Q(𝜆) (Peng and Williams, 1994), but its lack of off-policy corrections tends to result in degraded performance as data becomes more off-policy (e.g. by increasing the expected number of times that a sequence is sampled from replay, or by sharing data across a family of policies). This motivates us to propose an alternative return estimator, which we derive from Q(𝜆) (Watkins and Dayan, 1992): 𝐺𝑡 = max 𝑎 𝑄(𝑥𝑡 , 𝑎) + ∑︁ 𝑘≥0 Ö 𝑘−1 𝑖=0 𝜆𝑖 ! 𝛾 𝑘 (𝑟𝑡+𝑘 + 𝛾 max 𝑎 𝑄(𝑥𝑡+𝑘+1, 𝑎) − max 𝑎 𝑄(𝑥𝑡+𝑘, 𝑎)) (3) where Î𝑘−1 𝑖=0 𝜆𝑖 ∈ [0, 1] effectively controls how much information from the future is used in the return estimation and is generally used as a trace cutting coefficient to perform off-policy correction. Peng’s Q(𝜆) does not perform any kind of off-policy correction and sets 𝜆𝑖 = 𝜆, whereas Watkins’ Q(𝜆) (Watkins and Dayan, 1992) aggressively cuts traces whenever it encounters an off-policy action by using 𝜆𝑖 = 𝜆𝟙𝑎𝑖 ∈argmax𝑎𝑄(𝑥𝑖 ,𝑎) , where 𝟙 denotes the indicator function. We propose to use a softer trace cutting mechanism by adding a fixed tolerance parameter 𝜅 and taking the expectation of trace coefficients under 𝜋: 𝜆𝑖 = 𝜆𝔼𝑎∼𝜋(𝑎|𝑥𝑡) \u0002 𝟙[𝑄(𝑥𝑡 ,𝑎𝑡;𝜃) ≥𝑄(𝑥𝑡 ,𝑎;𝜃)−𝜅|𝑄(𝑥𝑖 ,𝑎;𝜃) |] \u0003 (4) Finally, we replace all occurrences of the max operator in Equation 3 with the expectation under 𝜋. The resulting return estimator, which we denote Soft Watkins Q(𝜆), leads to more transitions being used and increased sample efficiency. Note that Watkins Q(𝜆) is recovered when setting 𝜅 = 0 and 𝜋 = G (𝑄). B1 Loss and priority normalization. As we learn a family of Q-functions which vary over a wide range of discount factors and intrinsic reward scales, we expect that the Q-functions will vary considerably in scale. This may cause the larger-scale Q-values to dominate learning and destabilize learning of smaller Q-values. This is a particular concern in environments with very small extrinsic reward scales. To counteract this effect we introduce a normalization scheme on the TD-errors similar to that used in Schaul et al. (2021). Specifically, we compute a running estimate of the standard deviation of TD-errors of the online network 𝜎 running 𝑗 as well as a batch standard deviation 𝜎 batch 𝑗 , and compute 𝜎𝑗 = max(𝜎 running 𝑗 , 𝜎batch 𝑗 , 𝜖), where 𝜖 acts as small threshold to avoid amplification of noise past a specified scale, which we fix to 0.01 in all our experiments. We then divide the TD-errors by 𝜎𝑗 when computing both the loss and priorities. As opposed to Schaul et al.\n",
      "      > [ 0.6810028   0.83167225 -3.52396965] Preliminary experiments showed that data-efficiency was improved in many dense-reward tasks when replacing Retrace with Peng’s Q(𝜆) (Peng and Williams, 1994), but its lack of off-policy corrections tends to result in degraded performance as data becomes more off-policy (e.g. by increasing the expected number of times that a sequence is sampled from replay, or by sharing data across a family of policies). This motivates us to propose an alternative return estimator, which we derive from Q(𝜆) (Watkins and Dayan, 1992): 𝐺𝑡 = max 𝑎 𝑄(𝑥𝑡 , 𝑎) + ∑︁ 𝑘≥0 Ö 𝑘−1 𝑖=0 𝜆𝑖 ! 𝛾 𝑘 (𝑟𝑡+𝑘 + 𝛾 max 𝑎 𝑄(𝑥𝑡+𝑘+1, 𝑎) − max 𝑎 𝑄(𝑥𝑡+𝑘, 𝑎)) (3) where Î𝑘−1 𝑖=0 𝜆𝑖 ∈ [0, 1] effectively controls how much information from the future is used in the return estimation and is generally used as a trace cutting coefficient to perform off-policy correction.\n",
      "        > [ 0.69633192  1.14167058 -3.64535522] Preliminary experiments showed that data-efficiency was improved in many dense-reward tasks when replacing Retrace with Peng’s Q(𝜆) (Peng and Williams, 1994), but its lack of off-policy corrections tends to result in degraded performance as data becomes more off-policy (e.g.\n",
      "        > [ 0.82488048  1.29294729 -3.73939109] by increasing the expected number of times that a sequence is sampled from replay, or by sharing data across a family of policies).\n",
      "        > [ 0.65325707  0.86920488 -3.34789658] This motivates us to propose an alternative return estimator, which we derive from Q(𝜆) (Watkins and Dayan, 1992): 𝐺𝑡 = max 𝑎 𝑄(𝑥𝑡 , 𝑎) + ∑︁ 𝑘≥0 Ö 𝑘−1 𝑖=0 𝜆𝑖 !\n",
      "        > [ 0.58513927  0.44938999 -3.23289561] 𝛾 𝑘 (𝑟𝑡+𝑘 + 𝛾 max 𝑎 𝑄(𝑥𝑡+𝑘+1, 𝑎) − max 𝑎 𝑄(𝑥𝑡+𝑘, 𝑎)) (3) where Î𝑘−1 𝑖=0 𝜆𝑖 ∈ [0, 1] effectively controls how much information from the future is used in the return estimation and is generally used as a trace cutting coefficient to perform off-policy correction.\n",
      "      > [ 0.90944296  0.83923215 -3.52400494] Peng’s Q(𝜆) does not perform any kind of off-policy correction and sets 𝜆𝑖 = 𝜆, whereas Watkins’ Q(𝜆) (Watkins and Dayan, 1992) aggressively cuts traces whenever it encounters an off-policy action by using 𝜆𝑖 = 𝜆𝟙𝑎𝑖 ∈argmax𝑎𝑄(𝑥𝑖 ,𝑎) , where 𝟙 denotes the indicator function. We propose to use a softer trace cutting mechanism by adding a fixed tolerance parameter 𝜅 and taking the expectation of trace coefficients under 𝜋: 𝜆𝑖 = 𝜆𝔼𝑎∼𝜋(𝑎|𝑥𝑡) \u0002 𝟙[𝑄(𝑥𝑡 ,𝑎𝑡;𝜃) ≥𝑄(𝑥𝑡 ,𝑎;𝜃)−𝜅|𝑄(𝑥𝑖 ,𝑎;𝜃) |] \u0003 (4) Finally, we replace all occurrences of the max operator in Equation 3 with the expectation under 𝜋. The resulting return estimator, which we denote Soft Watkins Q(𝜆), leads to more transitions being used and increased sample efficiency. Note that Watkins Q(𝜆) is recovered when setting 𝜅 = 0 and 𝜋 = G (𝑄).\n",
      "        > [ 0.87851852  0.98650485 -3.49044228] Peng’s Q(𝜆) does not perform any kind of off-policy correction and sets 𝜆𝑖 = 𝜆, whereas Watkins’ Q(𝜆) (Watkins and Dayan, 1992) aggressively cuts traces whenever it encounters an off-policy action by using 𝜆𝑖 = 𝜆𝟙𝑎𝑖 ∈argmax𝑎𝑄(𝑥𝑖 ,𝑎) , where 𝟙 denotes the indicator function.\n",
      "        > [ 1.40856004  0.29988095 -3.56746721] We propose to use a softer trace cutting mechanism by adding a fixed tolerance parameter 𝜅 and taking the expectation of trace coefficients under 𝜋: 𝜆𝑖 = 𝜆𝔼𝑎∼𝜋(𝑎|𝑥𝑡) \u0002 𝟙[𝑄(𝑥𝑡 ,𝑎𝑡;𝜃) ≥𝑄(𝑥𝑡 ,𝑎;𝜃)−𝜅|𝑄(𝑥𝑖 ,𝑎;𝜃) |] \u0003 (4) Finally, we replace all occurrences of the max operator in Equation 3 with the expectation under 𝜋.\n",
      "        > [ 0.00974596  1.24414492 -3.75455189] The resulting return estimator, which we denote Soft Watkins Q(𝜆), leads to more transitions being used and increased sample efficiency.\n",
      "        > [ 0.69401896  0.6114139  -3.45207858] Note that Watkins Q(𝜆) is recovered when setting 𝜅 = 0 and 𝜋 = G (𝑄).\n",
      "      > [ 1.16900659  1.49616468 -3.29879069] B1 Loss and priority normalization. As we learn a family of Q-functions which vary over a wide range of discount factors and intrinsic reward scales, we expect that the Q-functions will vary considerably in scale. This may cause the larger-scale Q-values to dominate learning and destabilize learning of smaller Q-values. This is a particular concern in environments with very small extrinsic reward scales.\n",
      "        > [ 1.44510925  0.03123915 -2.75557685] B1 Loss and priority normalization.\n",
      "        > [ 0.67401195  1.95890057 -3.837605  ] As we learn a family of Q-functions which vary over a wide range of discount factors and intrinsic reward scales, we expect that the Q-functions will vary considerably in scale.\n",
      "        > [ 0.79077947  1.56390524 -3.57856941] This may cause the larger-scale Q-values to dominate learning and destabilize learning of smaller Q-values.\n",
      "        > [ 1.36908174  1.86421275 -3.63627315] This is a particular concern in environments with very small extrinsic reward scales.\n",
      "      > [ 1.07992792  0.2428886  -3.71096134] To counteract this effect we introduce a normalization scheme on the TD-errors similar to that used in Schaul et al. (2021). Specifically, we compute a running estimate of the standard deviation of TD-errors of the online network 𝜎 running 𝑗 as well as a batch standard deviation 𝜎 batch 𝑗 , and compute 𝜎𝑗 = max(𝜎 running 𝑗 , 𝜎batch 𝑗 , 𝜖), where 𝜖 acts as small threshold to avoid amplification of noise past a specified scale, which we fix to 0.01 in all our experiments. We then divide the TD-errors by 𝜎𝑗 when computing both the loss and priorities. As opposed to Schaul et al.\n",
      "        > [ 1.58753896  0.36092821 -3.77511406] To counteract this effect we introduce a normalization scheme on the TD-errors similar to that used in Schaul et al.\n",
      "        > [ 0.88827169  0.2900148  -3.99661636] (2021). Specifically, we compute a running estimate of the standard deviation of TD-errors of the online network 𝜎 running 𝑗 as well as a batch standard deviation 𝜎 batch 𝑗 , and compute 𝜎𝑗 = max(𝜎 running 𝑗 , 𝜎batch 𝑗 , 𝜖), where 𝜖 acts as small threshold to avoid amplification of noise past a specified scale, which we fix to 0.01 in all our experiments.\n",
      "        > [ 1.05458367 -0.92874116 -3.26949358] We then divide the TD-errors by 𝜎𝑗 when computing both the loss and priorities.\n",
      "        > [ 0.55720961  0.27255166 -3.39318848] As opposed to Schaul et al.\n",
      "  > [ 0.65917641  1.28343213 -3.24010944] (2021) we compute the running statistics on the learner, and make use of importance sampling to correct the sampling distribution. B2 Cross-mixture training. Agent57 only trains the policy 𝑗 which was used to collect a given trajectory, but it is natural to ask whether data-efficiency and robustness may be improved by training all policies at once. We propose a training loss 𝐿 according to the following weighting scheme between the behavior policy loss and the mean over all losses: 𝐿 = 𝜂𝐿𝑗𝜇 + 1 − 𝜂 𝑁 𝑁 ∑︁−1 𝑗=0 𝐿𝑗 (5) where 𝐿𝑗 denotes the Q-learning loss for policy 𝑗, and 𝑗𝜇 denotes the index for the behavior policy selected by the meta-controller for the sampled trajectory. We find that an intermediate value for the mixture parameter of 𝜂 = 0.5 tends to work well. To achieve better compute-efficiency we choose to deviate from the original UVFA architecture which fed a 1-hot encoding of the policy index to the LSTM, aand instead modify the Q-value heads to output N sets of Q-values, one for each of the 6 Human-level Atari 200x faster members in the family of policies introduced in Section 3. Therefore, in the end we output values for all combinations of actions and policies (see Figure 2). We note that in this setting, there is also less deviation in the recurrent states when learning across different mixture policies. C1 Normalizer-free torso network. Normalization layers are a common feature of ResNet architec\u0002tures, and which are known to aid in training of very deep networks, but preliminary investigation revealed that several commonly used normalization layers are in fact detrimental to performance in our setting. Instead, we employ a variation of the NFNet architecture (Brock et al., 2021) for our policy torso network, which combines a variance-scaling strategy with scaled weight standardization and careful initialization to achieve state-of-the-art performance on ImageNet without the need for normalization layers. We adopt their use of stochastic depth (Huang et al., 2016) at training-time but omit the application of ordinary dropout to fully-connected layers as we did not observe any benefit from this form of regularization. Some care is required when using stochastic depth in conjunction with multi-step returns, as resampling of the stochastic depth mask at each timestep injects additional noise into the bootstrap values, resulting in a higher-variance return estimator. As such, we employ a temporally-consistent stochastic depth mask which remains fixed over the length of each training trajectory. C2 Shared torso with combined loss. Agent57 decomposes the combined Q-function into intrinsic and extrinsic components, 𝑄𝑒 and 𝑄𝑖 , which are represented by separate networks. Such a decomposi\u0002tion prevents the gradients of the decomposed value functions from interfering with each other. This interference may occur in environments where the intrinsic reward is poorly aligned with the task objective, as defined by the extrinsic reward. However, the choice to use separate separate networks comes at an expensive computational cost, and potentially limits sample-efficiency since generic low-level features cannot be shared. To alleviate these issues, we introduce a shared torso for the two Q-functions while retaining separate heads. While the form of the decomposition in Agent57 was chosen so as to ensure convergence to the optimal value-function 𝑄 ★ in the tabular setting, this does not generally hold under function approximation. Comparing the combined and decomposed losses we observe a mismatch in the gradients due to the absence of cross-terms 𝑄𝑖(𝜃) 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 and 𝑄𝑒 (𝜃) 𝜕𝑄𝑖(𝜃) 𝜕𝜃 in the decomposed loss: 𝜕 𝜕𝜃 h 1 2 (𝑄(𝜃) − 𝐺) 2 i | {z } combined loss ≠ 𝜕 𝜕𝜃 h 1 2 (𝑄𝑒 (𝜃) − 𝐺𝑒) 2 + 1 2 (𝛽𝑄𝑖(𝜃) − 𝛽𝐺𝑖) 2 i | {z } decomposed loss (6) \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) − 𝐺 \u0003 𝜕 𝜕𝜃 \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) \u0003 ≠ \u0002 𝑄𝑒 (𝜃) − 𝐺𝑒 \u0003 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 + 𝛽 2 \u0002 𝑄𝑖(𝜃) − 𝐺𝑖 \u0003 𝜕𝑄𝑖(𝜃) 𝜕𝜃 (7) Since we use a behavior policy induced by the total Q-function 𝑄 = 𝑄𝑒 + 𝛽𝑄𝑖 rather than the individual components, theory would suggest to use the combined loss instead. In addition, from a practical implementation perspective, this switch to the combined loss greatly simplifies the design choices involved in our proposed trust region method described in A1. The penalty paid for this choice is that the decomposition of the value function into extrinsic and intrinsic components no longer carries a strict semantic meaning. Nevertheless we do still retain an implicit inductive bias induced by multiplication of 𝑄𝑖 with the intrinsic reward weight 𝛽 𝑗 . D Robustifying behavior via policy distillation. Schaul et al. (2022) describe the effect of policy churn, whereby the greedy action of value-based RL algorithms may change frequently over consecutive parameter updates. This can have a deleterious effect on off-policy correction methods: traces will be cut more aggressively than with a stochastic policy, and bootstrap values will change frequently which can result in a higher variance return estimator. In addition, our choice of training with temporally-consistent stochastic depth masks can be interpreted as learning an implicit ensemble 7 Human-level Atari 200x faster of Q-functions; thus it is natural to ask whether we may see additional benefit from leveraging the policy induced by this ensemble. We propose to train an explicit policy head 𝜋dist (see Figure 2) via policy distillation to match the 𝜖-greedy policy induced by the Q-function. In expectation over multiple gradient steps this should help to smooth out the policy over time, as well as over the ensemble, while being much faster to evaluate than the individual members of the ensemble. Similarly to the trust-region described in A1, we mask the policy distillation loss at any timestep where a KL constraint 𝐶KL is violated: 𝐿𝜋 = − ∑︁ 𝑎,𝑡 G𝜖\n",
      "    > [ 0.36495072  0.21816242 -3.19323063] (2021) we compute the running statistics on the learner, and make use of importance sampling to correct the sampling distribution. B2 Cross-mixture training. Agent57 only trains the policy 𝑗 which was used to collect a given trajectory, but it is natural to ask whether data-efficiency and robustness may be improved by training all policies at once. We propose a training loss 𝐿 according to the following weighting scheme between the behavior policy loss and the mean over all losses: 𝐿 = 𝜂𝐿𝑗𝜇 + 1 − 𝜂 𝑁 𝑁 ∑︁−1 𝑗=0 𝐿𝑗 (5) where 𝐿𝑗 denotes the Q-learning loss for policy 𝑗, and 𝑗𝜇 denotes the index for the behavior policy selected by the meta-controller for the sampled trajectory. We find that an intermediate value for the mixture parameter of 𝜂 = 0.5 tends to work well. To achieve better compute-efficiency we choose to deviate from the original UVFA architecture which fed a 1-hot encoding of the policy index to the LSTM, aand instead modify the Q-value heads to output N sets of Q-values, one for each of the 6 Human-level Atari 200x faster members in the family of policies introduced in Section 3. Therefore, in the end we output values for all combinations of actions and policies (see Figure 2). We note that in this setting, there is also less deviation in the recurrent states when learning across different mixture policies. C1 Normalizer-free torso network. Normalization layers are a common feature of ResNet architec\u0002tures, and which are known to aid in training of very deep networks, but preliminary investigation revealed that several commonly used normalization layers are in fact detrimental to performance in our setting. Instead, we employ a variation of the NFNet architecture (Brock et al., 2021) for our policy torso network, which combines a variance-scaling strategy with scaled weight standardization and careful initialization to achieve state-of-the-art performance on ImageNet without the need for normalization layers. We adopt their use of stochastic depth (Huang et al., 2016) at training-time but omit the application of ordinary dropout to fully-connected layers as we did not observe any benefit from this form of regularization. Some care is required when using stochastic depth in conjunction with multi-step returns, as resampling of the stochastic depth mask at each timestep injects additional noise into the bootstrap values, resulting in a higher-variance return estimator. As such, we employ a temporally-consistent stochastic depth mask which remains fixed over the length of each training trajectory. C2 Shared torso with combined loss. Agent57 decomposes the combined Q-function into intrinsic and extrinsic components, 𝑄𝑒 and 𝑄𝑖 , which are represented by separate networks.\n",
      "      > [-2.76797079e-03  2.87285030e-01 -3.57096291e+00] (2021) we compute the running statistics on the learner, and make use of importance sampling to correct the sampling distribution. B2 Cross-mixture training. Agent57 only trains the policy 𝑗 which was used to collect a given trajectory, but it is natural to ask whether data-efficiency and robustness may be improved by training all policies at once. We propose a training loss 𝐿 according to the following weighting scheme between the behavior policy loss and the mean over all losses: 𝐿 = 𝜂𝐿𝑗𝜇 + 1 − 𝜂 𝑁 𝑁 ∑︁−1 𝑗=0 𝐿𝑗 (5) where 𝐿𝑗 denotes the Q-learning loss for policy 𝑗, and 𝑗𝜇 denotes the index for the behavior policy selected by the meta-controller for the sampled trajectory.\n",
      "        > [-0.06755495  0.22459108 -3.64804626] (2021) we compute the running statistics on the learner, and make use of importance sampling to correct the sampling distribution.\n",
      "        > [-2.72913605e-01 -4.36373055e-04 -3.53351283e+00] B2 Cross-mixture training.\n",
      "        > [ 0.02375893  0.48241761 -3.72854352] Agent57 only trains the policy 𝑗 which was used to collect a given trajectory, but it is natural to ask whether data-efficiency and robustness may be improved by training all policies at once.\n",
      "        > [ 0.58207226  0.60941178 -3.30605888] We propose a training loss 𝐿 according to the following weighting scheme between the behavior policy loss and the mean over all losses: 𝐿 = 𝜂𝐿𝑗𝜇 + 1 − 𝜂 𝑁 𝑁 ∑︁−1 𝑗=0 𝐿𝑗 (5) where 𝐿𝑗 denotes the Q-learning loss for policy 𝑗, and 𝑗𝜇 denotes the index for the behavior policy selected by the meta-controller for the sampled trajectory.\n",
      "      > [ 0.08266668  0.78475523 -3.70290852] We find that an intermediate value for the mixture parameter of 𝜂 = 0.5 tends to work well. To achieve better compute-efficiency we choose to deviate from the original UVFA architecture which fed a 1-hot encoding of the policy index to the LSTM, aand instead modify the Q-value heads to output N sets of Q-values, one for each of the 6 Human-level Atari 200x faster members in the family of policies introduced in Section 3. Therefore, in the end we output values for all combinations of actions and policies (see Figure 2). We note that in this setting, there is also less deviation in the recurrent states when learning across different mixture policies.\n",
      "        > [ 0.20755544 -0.1121033  -3.41112566] We find that an intermediate value for the mixture parameter of 𝜂 = 0.5 tends to work well.\n",
      "        > [ 0.19441366  1.51141047 -3.60674143] To achieve better compute-efficiency we choose to deviate from the original UVFA architecture which fed a 1-hot encoding of the policy index to the LSTM, aand instead modify the Q-value heads to output N sets of Q-values, one for each of the 6 Human-level Atari 200x faster members in the family of policies introduced in Section 3.\n",
      "        > [ 1.16599965  0.9770022  -3.27355862] Therefore, in the end we output values for all combinations of actions and policies (see Figure 2).\n",
      "        > [ 0.53739607  0.5306623  -3.87489414] We note that in this setting, there is also less deviation in the recurrent states when learning across different mixture policies.\n",
      "      > [ 0.2060919   1.79318166 -3.3093462 ] C1 Normalizer-free torso network. Normalization layers are a common feature of ResNet architec\u0002tures, and which are known to aid in training of very deep networks, but preliminary investigation revealed that several commonly used normalization layers are in fact detrimental to performance in our setting. Instead, we employ a variation of the NFNet architecture (Brock et al., 2021) for our policy torso network, which combines a variance-scaling strategy with scaled weight standardization and careful initialization to achieve state-of-the-art performance on ImageNet without the need for normalization layers. We adopt their use of stochastic depth (Huang et al., 2016) at training-time but omit the application of ordinary dropout to fully-connected layers as we did not observe any benefit from this form of regularization.\n",
      "        > [ 0.80805182  0.47794044 -2.81690907] C1 Normalizer-free torso network.\n",
      "        > [ 0.48574927  1.94301355 -3.2280426 ] Normalization layers are a common feature of ResNet architec\u0002tures, and which are known to aid in training of very deep networks, but preliminary investigation revealed that several commonly used normalization layers are in fact detrimental to performance in our setting.\n",
      "        > [-0.01615191  2.14500427 -3.45329285] Instead, we employ a variation of the NFNet architecture (Brock et al., 2021) for our policy torso network, which combines a variance-scaling strategy with scaled weight standardization and careful initialization to achieve state-of-the-art performance on ImageNet without the need for normalization layers.\n",
      "        > [ 0.51484334  1.1185298  -3.3471384 ] We adopt their use of stochastic depth (Huang et al., 2016) at training-time but omit the application of ordinary dropout to fully-connected layers as we did not observe any benefit from this form of regularization.\n",
      "      > [ 0.19317929  0.67340052 -3.50411034] Some care is required when using stochastic depth in conjunction with multi-step returns, as resampling of the stochastic depth mask at each timestep injects additional noise into the bootstrap values, resulting in a higher-variance return estimator. As such, we employ a temporally-consistent stochastic depth mask which remains fixed over the length of each training trajectory. C2 Shared torso with combined loss. Agent57 decomposes the combined Q-function into intrinsic and extrinsic components, 𝑄𝑒 and 𝑄𝑖 , which are represented by separate networks.\n",
      "        > [ 0.33363619  1.07369614 -3.44311333] Some care is required when using stochastic depth in conjunction with multi-step returns, as resampling of the stochastic depth mask at each timestep injects additional noise into the bootstrap values, resulting in a higher-variance return estimator.\n",
      "        > [ 0.47342023  0.33535099 -3.85501575] As such, we employ a temporally-consistent stochastic depth mask which remains fixed over the length of each training trajectory.\n",
      "        > [ 1.44942534 -0.98459154 -2.653234  ] C2 Shared torso with combined loss.\n",
      "        > [ 0.50425529  0.18513048 -3.62561035] Agent57 decomposes the combined Q-function into intrinsic and extrinsic components, 𝑄𝑒 and 𝑄𝑖 , which are represented by separate networks.\n",
      "    > [ 0.24527615  1.54273427 -3.47636652] Such a decomposi\u0002tion prevents the gradients of the decomposed value functions from interfering with each other. This interference may occur in environments where the intrinsic reward is poorly aligned with the task objective, as defined by the extrinsic reward. However, the choice to use separate separate networks comes at an expensive computational cost, and potentially limits sample-efficiency since generic low-level features cannot be shared. To alleviate these issues, we introduce a shared torso for the two Q-functions while retaining separate heads. While the form of the decomposition in Agent57 was chosen so as to ensure convergence to the optimal value-function 𝑄 ★ in the tabular setting, this does not generally hold under function approximation. Comparing the combined and decomposed losses we observe a mismatch in the gradients due to the absence of cross-terms 𝑄𝑖(𝜃) 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 and 𝑄𝑒 (𝜃) 𝜕𝑄𝑖(𝜃) 𝜕𝜃 in the decomposed loss: 𝜕 𝜕𝜃 h 1 2 (𝑄(𝜃) − 𝐺) 2 i | {z } combined loss ≠ 𝜕 𝜕𝜃 h 1 2 (𝑄𝑒 (𝜃) − 𝐺𝑒) 2 + 1 2 (𝛽𝑄𝑖(𝜃) − 𝛽𝐺𝑖) 2 i | {z } decomposed loss (6) \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) − 𝐺 \u0003 𝜕 𝜕𝜃 \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) \u0003 ≠ \u0002 𝑄𝑒 (𝜃) − 𝐺𝑒 \u0003 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 + 𝛽 2 \u0002 𝑄𝑖(𝜃) − 𝐺𝑖 \u0003 𝜕𝑄𝑖(𝜃) 𝜕𝜃 (7) Since we use a behavior policy induced by the total Q-function 𝑄 = 𝑄𝑒 + 𝛽𝑄𝑖 rather than the individual components, theory would suggest to use the combined loss instead. In addition, from a practical implementation perspective, this switch to the combined loss greatly simplifies the design choices involved in our proposed trust region method described in A1. The penalty paid for this choice is that the decomposition of the value function into extrinsic and intrinsic components no longer carries a strict semantic meaning. Nevertheless we do still retain an implicit inductive bias induced by multiplication of 𝑄𝑖 with the intrinsic reward weight 𝛽 𝑗 . D Robustifying behavior via policy distillation. Schaul et al. (2022) describe the effect of policy churn, whereby the greedy action of value-based RL algorithms may change frequently over consecutive parameter updates. This can have a deleterious effect on off-policy correction methods: traces will be cut more aggressively than with a stochastic policy, and bootstrap values will change frequently which can result in a higher variance return estimator. In addition, our choice of training with temporally-consistent stochastic depth masks can be interpreted as learning an implicit ensemble 7 Human-level Atari 200x faster of Q-functions; thus it is natural to ask whether we may see additional benefit from leveraging the policy induced by this ensemble. We propose to train an explicit policy head 𝜋dist (see Figure 2) via policy distillation to match the 𝜖-greedy policy induced by the Q-function. In expectation over multiple gradient steps this should help to smooth out the policy over time, as well as over the ensemble, while being much faster to evaluate than the individual members of the ensemble. Similarly to the trust-region described in A1, we mask the policy distillation loss at any timestep where a KL constraint 𝐶KL is violated: 𝐿𝜋 = − ∑︁ 𝑎,𝑡 G𝜖\n",
      "      > [ 0.49344268  1.3351928  -3.00036573] Such a decomposi\u0002tion prevents the gradients of the decomposed value functions from interfering with each other. This interference may occur in environments where the intrinsic reward is poorly aligned with the task objective, as defined by the extrinsic reward. However, the choice to use separate separate networks comes at an expensive computational cost, and potentially limits sample-efficiency since generic low-level features cannot be shared. To alleviate these issues, we introduce a shared torso for the two Q-functions while retaining separate heads.\n",
      "        > [ 1.52261376  0.60501063 -2.9164629 ] Such a decomposi\u0002tion prevents the gradients of the decomposed value functions from interfering with each other.\n",
      "        > [ 1.38678873  1.38382113 -2.9831109 ] This interference may occur in environments where the intrinsic reward is poorly aligned with the task objective, as defined by the extrinsic reward.\n",
      "        > [-0.27675688  1.90623522 -2.86672354] However, the choice to use separate separate networks comes at an expensive computational cost, and potentially limits sample-efficiency since generic low-level features cannot be shared.\n",
      "        > [-0.02876019  0.8821938  -3.25152779] To alleviate these issues, we introduce a shared torso for the two Q-functions while retaining separate heads.\n",
      "      > [ 1.25661087  0.11879988 -2.90302515] While the form of the decomposition in Agent57 was chosen so as to ensure convergence to the optimal value-function 𝑄 ★ in the tabular setting, this does not generally hold under function approximation. Comparing the combined and decomposed losses we observe a mismatch in the gradients due to the absence of cross-terms 𝑄𝑖(𝜃) 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 and 𝑄𝑒 (𝜃) 𝜕𝑄𝑖(𝜃) 𝜕𝜃 in the decomposed loss: 𝜕 𝜕𝜃 h 1 2 (𝑄(𝜃) − 𝐺) 2 i | {z } combined loss ≠ 𝜕 𝜕𝜃 h 1 2 (𝑄𝑒 (𝜃) − 𝐺𝑒) 2 + 1 2 (𝛽𝑄𝑖(𝜃) − 𝛽𝐺𝑖) 2 i | {z } decomposed loss (6) \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) − 𝐺 \u0003 𝜕 𝜕𝜃 \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) \u0003 ≠ \u0002 𝑄𝑒 (𝜃) − 𝐺𝑒 \u0003 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 + 𝛽 2 \u0002 𝑄𝑖(𝜃) − 𝐺𝑖 \u0003 𝜕𝑄𝑖(𝜃) 𝜕𝜃 (7) Since we use a behavior policy induced by the total Q-function 𝑄 = 𝑄𝑒 + 𝛽𝑄𝑖 rather than the individual components, theory would suggest to use the combined loss instead. In addition, from a practical implementation perspective, this switch to the combined loss greatly simplifies the design choices involved in our proposed trust region method described in A1. The penalty paid for this choice is that the decomposition of the value function into extrinsic and intrinsic components no longer carries a strict semantic meaning.\n",
      "        > [ 0.96272266  0.26060796 -3.06219196] While the form of the decomposition in Agent57 was chosen so as to ensure convergence to the optimal value-function 𝑄 ★ in the tabular setting, this does not generally hold under function approximation.\n",
      "        > [ 1.25702715  0.01215496 -3.1916163 ] Comparing the combined and decomposed losses we observe a mismatch in the gradients due to the absence of cross-terms 𝑄𝑖(𝜃) 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 and 𝑄𝑒 (𝜃) 𝜕𝑄𝑖(𝜃) 𝜕𝜃 in the decomposed loss: 𝜕 𝜕𝜃 h 1 2 (𝑄(𝜃) − 𝐺) 2 i | {z } combined loss ≠ 𝜕 𝜕𝜃 h 1 2 (𝑄𝑒 (𝜃) − 𝐺𝑒) 2 + 1 2 (𝛽𝑄𝑖(𝜃) − 𝛽𝐺𝑖) 2 i | {z } decomposed loss (6) \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) − 𝐺 \u0003 𝜕 𝜕𝜃 \u0002 𝑄𝑒 (𝜃) + 𝛽𝑄𝑖(𝜃) \u0003 ≠ \u0002 𝑄𝑒 (𝜃) − 𝐺𝑒 \u0003 𝜕𝑄𝑒 (𝜃) 𝜕𝜃 + 𝛽 2 \u0002 𝑄𝑖(𝜃) − 𝐺𝑖 \u0003 𝜕𝑄𝑖(𝜃) 𝜕𝜃 (7) Since we use a behavior policy induced by the total Q-function 𝑄 = 𝑄𝑒 + 𝛽𝑄𝑖 rather than the individual components, theory would suggest to use the combined loss instead.\n",
      "        > [ 0.91605347  0.19196057 -2.9565506 ] In addition, from a practical implementation perspective, this switch to the combined loss greatly simplifies the design choices involved in our proposed trust region method described in A1.\n",
      "        > [ 1.43501282  0.92157841 -2.21875   ] The penalty paid for this choice is that the decomposition of the value function into extrinsic and intrinsic components no longer carries a strict semantic meaning.\n",
      "      > [ 1.16069651  1.13236344 -2.70242095] Nevertheless we do still retain an implicit inductive bias induced by multiplication of 𝑄𝑖 with the intrinsic reward weight 𝛽 𝑗 . D Robustifying behavior via policy distillation. Schaul et al. (2022) describe the effect of policy churn, whereby the greedy action of value-based RL algorithms may change frequently over consecutive parameter updates.\n",
      "        > [ 1.3592844   1.22498441 -2.45735216] Nevertheless we do still retain an implicit inductive bias induced by multiplication of 𝑄𝑖 with the intrinsic reward weight 𝛽 𝑗 .\n",
      "        > [ 1.64958096  0.83598793 -4.10088634] D Robustifying behavior via policy distillation.\n",
      "        > [ 0.19819736 -0.26570129 -3.79846191] Schaul et al.\n",
      "        > [ 0.76732945  1.2479223  -3.37842059] (2022) describe the effect of policy churn, whereby the greedy action of value-based RL algorithms may change frequently over consecutive parameter updates.\n",
      "      > [ 0.50970864  1.54597712 -3.47720861] This can have a deleterious effect on off-policy correction methods: traces will be cut more aggressively than with a stochastic policy, and bootstrap values will change frequently which can result in a higher variance return estimator. In addition, our choice of training with temporally-consistent stochastic depth masks can be interpreted as learning an implicit ensemble 7 Human-level Atari 200x faster of Q-functions; thus it is natural to ask whether we may see additional benefit from leveraging the policy induced by this ensemble. We propose to train an explicit policy head 𝜋dist (see Figure 2) via policy distillation to match the 𝜖-greedy policy induced by the Q-function. In expectation over multiple gradient steps this should help to smooth out the policy over time, as well as over the ensemble, while being much faster to evaluate than the individual members of the ensemble. Similarly to the trust-region described in A1, we mask the policy distillation loss at any timestep where a KL constraint 𝐶KL is violated: 𝐿𝜋 = − ∑︁ 𝑎,𝑡 G𝜖\n",
      "        > [ 1.58264601  0.93122804 -3.47120285] This can have a deleterious effect on off-policy correction methods: traces will be cut more aggressively than with a stochastic policy, and bootstrap values will change frequently which can result in a higher variance return estimator.\n",
      "        > [-0.12844233  1.63852859 -3.37387109] In addition, our choice of training with temporally-consistent stochastic depth masks can be interpreted as learning an implicit ensemble 7 Human-level Atari 200x faster of Q-functions; thus it is natural to ask whether we may see additional benefit from leveraging the policy induced by this ensemble.\n",
      "        > [-0.20357643  1.50329614 -3.59765458] We propose to train an explicit policy head 𝜋dist (see Figure 2) via policy distillation to match the 𝜖-greedy policy induced by the Q-function.\n",
      "        > [-0.26338631  1.34101772 -4.13024139] In expectation over multiple gradient steps this should help to smooth out the policy over time, as well as over the ensemble, while being much faster to evaluate than the individual members of the ensemble.\n",
      "        > [ 1.06615913  0.9088676  -3.12861252] Similarly to the trust-region described in A1, we mask the policy distillation loss at any timestep where a KL constraint 𝐶KL is violated: 𝐿𝜋 = − ∑︁ 𝑎,𝑡 G𝜖\n"
     ]
    }
   ],
   "source": [
    "print_paper(paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
