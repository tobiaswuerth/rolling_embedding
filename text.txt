The task of building general agents that perform well over a wide range of tasks has been an important goal in reinforcement learning since its inception. The problem has been subject of research of a
large body of work, with performance frequently measured by observing scores over the wide range of
environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human
benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 billion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of
strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline. We
investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and
propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive
performance with high-performing methods such as Muesli and MuZero. The four key components to
our approach are (1) an approximate trust region method which enables stable bootstrapping from the
online network, (2) a normalisation scheme for the loss and priorities which improves robustness when
learning a set of value functions with a wide range of scales, (3) an improved architecture employing
techniques from NFNets in order to leverage deeper networks without the need for normalization layers, and (4) a policy distillation method which serves to smooth out the instantaneous greedy policy
over time.

To develop generally capable agents, the question of how to evaluate them is paramount. The Arcade
Learning Environment (ALE) (Bellemare et al., 2013) was introduced as a benchmark to evaluate
agents on an diverse set of tasks which are interesting to humans, and developed externally to the
Reinforcement Learning (RL) community. As a result, several games exhibit reward structures which
are highly adversarial to many popular algorithms. Mean and median human normalized scores
(HNS) (Mnih et al., 2015) over all games in the ALE have become standard metrics for evaluating
deep RL agents. Recent progress has allowed state-of-the-art algorithms to greatly exceed average
human-level performance on a large fraction of the games (Espeholt et al., 2018; Schrittwieser et al.,
2020; Van Hasselt et al., 2016). However, it has been argued that mean or median HNS might not be
well suited to assess generality because they tend to ignore the tails of the distribution (Badia et al.,
2019). Indeed, most state-of-the-art algorithms achieve very high scores by performing very well on
most games, but completely fail to learn on a small number of them.
Agent57 (Badia et al., 2020) was the first algorithm to obtain above human-average scores on all
57 Atari games. However, such generality came at the cost of data efficiency; requiring tens of billions
of environment interactions to achieve above average-human performance in some games, reaching a
figure of 78 billion frames before beating the human benchmark in all games. Data efficiency remains
a desirable property for agents to possess, as many real-world challenges are data-limited by time
and cost constraints (Dulac-Arnold et al., 2019). In this work, our aim is to develop an agent that is
as general as Agent57 but that requires only a fraction of the environment interactions to achieve the
same result.

There exist two main trends in the literature when it comes to measuring improvements in the
learning capabilities of agents. One approach consists in measuring performance after a limited
budget of interactions with the environment. While this type of evaluation has led to important
progress (Espeholt et al., 2018; Hessel et al., 2021; van Hasselt et al., 2019), it tends to disregard
problems which are considered too hard to be solved within the allowed budget (Kaiser et al., 2019).
On the other hand, one can aim to achieve a target end-performance with as few interactions as
possible (Schmitt et al., 2020; Silver et al., 2017, 2018). Since our goal is to show that our new agent
is as general as Agent57, while being more data efficient, we focus on the latter approach.
Our contributions can be summarized as follows. Building off Agent57, we carefully examine
bottlenecks which slow down learning and address instabilities that arise when these bottlenecks
are removed. We propose a novel agent that we call MEME, for MEME is an Efficient Memory-based
Exploration agent, which introduces solutions to enable taking advantage of three approaches that
would otherwise lead to instabilities: training the value functions of the whole family of policies
from Agent57 in parallel, on all policies‚Äô transitions (instead of just the behaviour policy transitions),
bootstrapping from the online network, and using high replay ratios. These solutions include carefully
normalising value functions with differing scales, as well as replacing the Retrace (Munos et al., 2016)
update target with a soft variant of Watkins‚Äô Q(ùúÜ) (Watkins and Dayan, 1992) that enables faster
signal propagation by performing less aggressive trace-cutting, and introducing a trust-region for
value updates. Moreover, we explore several recent advances in deep learning and determine which
of them are beneficial for non-stationary problems like the ones considered in this work. Finally, we
examine approaches to robustify performance by introducing a policy distillation mechanism that
learns a policy head based on the actions obtained from the value network without being sensitive to
value magnitudes. Our agent outperforms the human baseline across all 57 Atari games in 390M
frames, using two orders of magnitude fewer interactions with the environment than Agent57 as
shown in Figure 1.

Large scale distributed agents have exhibited compelling results in recent years. Actor-critic (Espeholt
et al., 2018; Song et al., 2019) as well as value-based agents (Horgan et al., 2018; Kapturowski
et al., 2018) demonstrated strong performance in a wide-range of environments, including the Atari
57 benchmark. Moreover, approaches such as evolutionary strategies (Salimans et al., 2017) and
large scale genetic algorithms (Such et al., 2017) presented alternative learning algorithms that
achieve competitive results on Atari. Finally, search-augmented distributed agents (Hessel et al., 2021;
Schrittwieser et al., 2020) also hold high performance across many different tasks, and concretely
they hold the highest mean and median human normalized scores over the 57 Atari games. However,
all these methods show the same failure mode: they perform poorly in hard exploration games, such
as Pitfall!, and Montezuma‚Äôs Revenge. In contrast, Agent57 (Badia et al., 2020) surpassed the human
benchmark on all 57 games, showing better general performance. Go-Explore (Ecoffet et al., 2021)
similarly achieved such general performance, by relying on coarse-grained state representations via a
downscaling function that is highly specific to Atari.
Learning as much as possible from previous experience is key for data-efficiency. Since it is often
desirable for approximate methods to make small updates to the policy (Kakade and Langford, 2002;
Schulman et al., 2015), approaches have been proposed for enabling multiple learning steps over
the same batch of experience in policy gradient methods to avoid collecting new transitions for
every learning step (Schulman et al., 2017). This decoupling between collecting experience and
learning occurs naturally in off-policy learning agents with experience replay (Lin, 1992; Mnih et al.,
2015) and Fitted Q Iteration methods (Ernst et al., 2005; Riedmiller, 2005). Multiple approaches for
making more efficient use of a replay buffer have been proposed, including prioritized sampling of
transitions (Schaul et al., 2015b), sharing experience across populations of agents (Schmitt et al.,
2020), learning multiple policies in parallel from a single stream of experience (Riedmiller et al., 2018),
or reanalyzing old trajectories with the most recent version of a learned model to generate new targets
in model-based settings (Schrittwieser et al., 2020, 2021) or to re-evaluate goals (Andrychowicz
et al., 2017).
The ATARI100k benchmark (Kaiser et al., 2019) was introduced to observe progress in improving
the data efficiency of reinforcement learning agents, by evaluating game scores after 100k agent steps
(400k frames). Work on this benchmark has focused on leveraging the use of models (Kaiser et al.,
2019; Long et al., 2022; Ye et al., 2021), unsupervised learning (Hansen et al., 2019; Liu and Abbeel,
2021; Schwarzer et al., 2020; Srinivas et al., 2020), or greater use of replay data (Kielak, 2020; van
Hasselt et al., 2019) or augmentations (Kostrikov et al., 2020; Schwarzer et al., 2020). While we
consider this to be an important line of research, this tight budget produces an incentive to focus
on a subset of games where exploration is easier, and it is unclear some games can be solved from
scratch with such a small data budget. Such a setting is likely to prevent any meaningful learning on
hard-exploration games, which is in contrast with the goal of our work.
