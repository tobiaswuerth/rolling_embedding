Agent57: Outperforming the Atari Human Benchmark
Adri√† Puigdom√®nech Badia * 1 Bilal Piot * 1 Steven Kapturowski * 1 Pablo Sprechmann * 1 Alex Vitvitskyi 1
Daniel Guo 1 Charles Blundell 1
Abstract
Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was
proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well
on many games of the set, but very poorly in
several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all
57 Atari games. To achieve this result, we train a
neural network which parameterizes a family of
policies ranging from very exploratory to purely
exploitative. We propose an adaptive mechanism
to choose which policy to prioritize throughout
the training process. Additionally, we utilize a
novel parameterization of the architecture that allows for more consistent and stable learning.
1. Introduction
The Arcade Learning Environment (ALE; Bellemare et al.,
2013) was proposed as a platform for empirically assessing agents designed for general competency across a wide
range of games. ALE offers an interface to a diverse set
of Atari 2600 game environments designed to be engaging
and challenging for human players. As Bellemare et al.
(2013) put it, the Atari 2600 games are well suited for evaluating general competency in AI agents for three main reasons: (i) varied enough to claim generality, (ii) each interesting enough to be representative of settings that might be
faced in practice, and (iii) each created by an independent
party to be free of experimenter‚Äôs bias.
Agents are expected to perform well in as many games as
possible making minimal assumptions about the domain
at hand and without the use of game-specific information.
Deep Q-Networks (DQN ; Mnih et al., 2015) was the first
algorithm to achieve human-level control in a large num-
*Equal contribution 1DeepMind. Correspondence to: Adri√†
Puigdom√®nech Badia <adriap@google.com>.
Figure 1. Number of games where algorithms are better than the
human benchmark throughout training for Agent57 and state-ofthe-art baselines on the 57 Atari games.
ber of the Atari 2600 games, measured by human normalized scores (HNS). Subsequently, using HNS to assess
performance on Atari games has become one of the most
widely used benchmarks in deep reinforcement learning
(RL), despite the human baseline scores potentially underestimating human performance relative to what is possible (Toromanoff et al., 2019). Nonetheless, human benchmark performance remains an oracle for ‚Äúreasonable performance‚Äù across the 57 Atari games. Despite all efforts,
no single RL algorithm has been able to achieve over 100%
HNS on all 57 Atari games with one set of hyperparameters. Indeed, state of the art algorithms in model-based
RL, MuZero (Schrittwieser et al., 2019), and in model-free
RL, R2D2 (Kapturowski et al., 2018) surpass 100% HNS
on 51 and 52 games, respectively. While these algorithms
achieve well above average human-level performance on
a large fraction of the games (e.g. achieving more than
1000% HNS), in the games they fail to do so, they often
fail to learn completely. These games showcase particularly important issues that a general RL algorithm should be
able to tackle. Firstly, long-term credit assignment: which
decisions are most deserving of credit for the positive (or
negative) outcomes that follow? This problem is particularly hard when rewards are delayed and credit needs to
be assigned over long sequences of actions, such as in the
games of Skiing or Solaris. The game of Skiing is a canonical example due to its peculiar reward structure. The goal
arXiv:2003.13350v1 [cs.LG] 30 Mar 2020
Agent57: Outperforming the Atari Human Benchmark
of the game is to run downhill through all gates as fast as
possible. A penalty of five seconds is given for each missed
gate. The reward, given only at the end, is proportional to
the time elapsed. Therefore long-term credit assignment
is needed to understand why an action taken early in the
game (e.g. missing a gate) has a negative impact in the
obtained reward. Secondly, exploration: efficient exploration can be critical to effective learning in RL. Games
like Private Eye, Montezuma‚Äôs Revenge, Pitfall! or Venture
are widely considered hard exploration games (Bellemare
et al., 2016; Ostrovski et al., 2017) as hundreds of actions
may be required before a first positive reward is seen. In order to succeed, the agents need to keep exploring the environment despite the apparent impossibility of finding positive rewards. These problems are particularly challenging
in large high dimensional state spaces where function approximation is required.
Exploration algorithms in deep RL generally fall into three
categories: randomized value functions (Osband et al.,
2016; Fortunato et al., 2017; Salimans et al., 2017; Plappert et al., 2017; Osband et al., 2018), unsupervised policy
learning (Gregor et al., 2016; Achiam et al., 2018; Eysenbach et al., 2018) and intrinsic motivation (Schmidhuber,
1991; Oudeyer et al., 2007; Barto, 2013; Bellemare et al.,
2016; Ostrovski et al., 2017; Fu et al., 2017; Tang et al.,
2017; Burda et al., 2018; Choi et al., 2018; Savinov et al.,
2018; Puigdom√®nech Badia et al., 2020). Other work combines handcrafted features, domain-specific knowledge or
privileged pre-training to side-step the exploration problem, sometimes only evaluating on a few Atari games (Aytar et al., 2018; Ecoffet et al., 2019). Despite the encouraging results, no algorithm has been able to significantly
improve performance on challenging games without deteriorating performance on the remaining games without relying on human demonstrations (Pohlen et al., 2018). Notably, amongst all this work, intrinsic motivation, and in
particular, Never Give Up (NGU; Puigdom√®nech Badia
et al., 2020) has shown significant recent promise in improving performance on hard exploration games. NGU
achieves this by augmenting the reward signal with an internally generated intrinsic reward that is sensitive to novelty at two levels: short-term novelty within an episode and
long-term novelty across episodes. It then learns a family
of policies for exploring and exploiting (sharing the same
parameters), with the end goal of obtain the highest score
under the exploitative policy. However, NGU is not the
most general agent: much like R2D2 and MuZero are able
to perform strongly on all but few games, so too NGU suffers in that it performs strongly on a smaller, different set
of games to agents such as MuZero and R2D2 (despite being based on R2D2). For example, in the game Surround
R2D2 achieves the optimal score while NGU performs similar to a random policy. One shortcoming of NGU is that it
collects the same amount of experience following each of
its policies, regardless of their contribution to the learning
progress. Some games require a significantly different degree of exploration to others. Intuitively, one would want
to allocate the shared resources (both network capacity and
data collection) such that end performance is maximized.
We propose allowing NGU to adapt its exploration strategy
over the course of an agent‚Äôs lifetime, enabling specialization to the particular game it is learning. This is the first
significant improvement we make to NGU to allow it to be
a more general agent.
Recent work on long-term credit assignment can be categorized into roughly two types: ensuring that gradients
correctly assign credit (Ke et al., 2017; Weber et al., 2019;
Ferret et al., 2019; Fortunato et al., 2019) and using values or targets to ensure correct credit is assigned (ArjonaMedina et al., 2019; Hung et al., 2019; Liu et al., 2019;
Harutyunyan et al., 2019). NGU is also unable to cope
with long-term credit assignment problems such as Skiing
or Solaris where it fails to reach 100% HNS. Advances in
credit assignment in RL often involve a mixture of both approaches, as values and rewards form the loss whilst the
flow of gradients through a model directs learning.
In this work, we propose tackling the long-term credit assignment problem by improving the overall training stability, dynamically adjusting the discount factor, and increasing the backprop through time window. These are
relatively simple changes compared to the approaches proposed in previous work, but we find them to be effective.
Much recent work has explored this problem of how to dynamically adjust hyperparameters of a deep RL agent, e.g.,
approaches based upon evolution (Jaderberg et al., 2017),
gradients (Xu et al., 2018) or multi-armed bandits (Schaul
et al., 2019). Inspired by Schaul et al. (2019), we propose
using a simple non-stationary multi-armed bandit (Garivier
& Moulines, 2008) to directly control the exploration rate
and discount factor to maximize the episode return, and
then provide this information to the value network of the
agent as an input. Unlike Schaul et al. (2019), 1) it controls
the exploration rate and discount factor (helping with longterm credit assignment), and 2) the bandit controls a family
of state-action value functions that back up the effects of
exploration and longer discounts, rather than linearly tilting a common value function by a fixed functional form.
In summary, our contributions are as follows:
1. A new parameterization of the state-action value function that decomposes the contributions of the intrinsic
and extrinsic rewards. As a result, we significantly increase the training stability over a large range of intrinsic
reward scales.
2. A meta-controller: an adaptive mechanism to select
Agent57: Outperforming the Atari Human Benchmark
which of the policies (parameterized by exploration rate
and discount factors) to prioritize throughout the training process. This allows the agent to control the exploration/exploitation trade-off by dedicating more resources to one or the other.
3. Finally, we demonstrate for the first time performance
that is above the human baseline across all Atari 57
games. As part of these experiments, we also find that
simply re-tuning the backprop through time window to
be twice the previously published window for R2D2
led to superior long-term credit assignment (e.g., in Solaris) while still maintaining or improving overall performance on the remaining games.
These improvements to NGU collectively transform it into
the most general Atari 57 agent, enabling it to outperform
the human baseline uniformly over all Atari 57 games.
Thus, we call this agent: Agent57.
2. Background: Never Give Up (NGU)
Our work builds on top of the NGU agent, which combines
two ideas: first, the curiosity-driven exploration, and second, distributed deep RL agents, in particular R2D2.
NGU computes an intrinsic reward in order to encourage
exploration. This reward is defined by combining perepisode and life-long novelty. The per-episode novelty,
r
episodic
t
, rapidly vanishes over the course of an episode, and
it is computed by comparing observations to the contents
of an episodic memory. The life-long novelty, Œ±t, slowly
vanishes throughout training, and it is computed by using a
parametric model (in NGU and in this work Random Network Distillation (Burda et al., 2018) is used to this end).
With this, the intrinsic reward r
i
t
is defined as follows:
r
i
t = r
episodic
t
¬∑ min {max {Œ±t, 1} , L} ,
where L = 5 is a chosen maximum reward scaling. This
leverages the long-term novelty provided by Œ±t, while
r
episodic
t
continues to encourage the agent to explore within
an episode. For a detailed description of the computation of
r
episodic
t
and Œ±t, see (Puigdom√®nech Badia et al., 2020). At
time t, NGU adds N different scales of the same intrinsic
reward Œ≤j r
i
t
(Œ≤j ‚àà R
+, j ‚àà 0, . . . N ‚àí1) to the extrinsic reward provided by the environment, r
e
t
, to form N potential
total rewards rj,t = r
e
t +Œ≤j r
i
t
. Consequently, NGU aims to
learn the N different associated optimal state-action value
functions Q‚àó
rj
associated with each reward function rj,t.
The exploration rates Œ≤j are parameters that control the
degree of exploration. Higher values will encourage exploratory policies and smaller values will encourage exploitative policies. Additionally, for purposes of learning
long-term credit assignment, each Q‚àó
rj
has its own associated discount factor Œ≥j (for background and notations on
Markov Decision Processes (MDP) see App. A). Since the
intrinsic reward is typically much more dense than the extrinsic reward, {(Œ≤j , Œ≥j )}
N‚àí1
j=0 are chosen so as to allow for
long term horizons (high values of Œ≥j ) for exploitative policies (small values of Œ≤j ) and small term horizons (low values of Œ≥j ) for exploratory policies (high values of Œ≤j ).
To learn the state-action value function Q‚àó
rj
, NGU trains
a recurrent neural network Q(x, a, j; Œ∏), where j is a onehot vector indexing one of N implied MDPs (in particular
(Œ≤j , Œ≥j )), x is the current observation, a is an action, and Œ∏
are the parameters of the network (including the recurrent
state). In practice, NGU can be unstable and fail to learn
an appropriate approximation of Q‚àó
rj
for all the state-action
value functions in the family, even in simple environments.
This is especially the case when the scale and sparseness
of r
e
t
and r
i
t
are both different, or when one reward is more
noisy than the other. We conjecture that learning a common
state-action value function for a mix of rewards is difficult
when the rewards are very different in nature. Therefore, in
Sec. 3.1, we propose an architectural modification to tackle
this issue.
Our agent is a deep distributed RL agent, in the lineage
of R2D2 and NGU. As such, it decouples the data collection and the learning processes by having many actors
feed data to a central prioritized replay buffer. A learner
can then sample training data from this buffer, as shown
in Fig. 2 (for implementation details and hyperparameters
refer to App. E). More precisely, the replay buffer conFigure 2. A schematic depiction of a distributed deep RL agent.
tains sequences of transitions that are removed regularly
in a FIFO-manner. These sequences come from actor processes that interact with independent copies of the environment, and they are prioritized based on temporal differences errors (Kapturowski et al., 2018). The priorities
are initialized by the actors and updated by the learner with
the updated state-action value function Q(x, a, j; Œ∏). According to those priorities, the learner samples sequences
of transitions from the replay buffer to construct an RL
Agent57: Outperforming the Atari Human Benchmark
loss. Then, it updates the parameters of the neural network Q(x, a, j; Œ∏) by minimizing the RL loss to approximate the optimal state-action value function. Finally, each
actor shares the same network architecture as the learner
but with different weights. We refer as Œ∏l
to the parameters
of the l‚àíth actor. The learner weights Œ∏ are sent to the actor frequently, which allows it to update its own weights Œ∏l
.
Each actor uses different values l
, which are employed to
follow an l-greedy policy based on the current estimate of
the state-action value function Q(x, a, j; Œ∏l). In particular,
at the beginning of each episode and in each actor, NGU
uniformly selects a pair (Œ≤j , Œ≥j ). We hypothesize that this
process is sub-optimal and propose to improve it in Sec. 3.2
by introducing a meta-controller for each actor that adapts
the data collection process.
3. Improvements to NGU
3.1. State-Action Value Function Parameterization
The proposed architectural improvement consists in splitting the state-action value function in the following way:
Q(x, a, j; Œ∏) = Q(x, a, j; Œ∏
e
) + Œ≤jQ(x, a, j; Œ∏
i
),
where Q(x, a, j; Œ∏
e
) and Q(x, a, j; Œ∏
i
) are the extrinsic and
intrinsic components of Q(x, a, j; Œ∏) respectively. The sets
of weights Œ∏
e
and Œ∏
i
separately parameterize two neural
networks with identical architecture and Œ∏ = Œ∏
i ‚à™ Œ∏
e
. Both
Q(x, a, j; Œ∏
e
) and Q(x, a, j; Œ∏
i
) are optimized separately in
the learner with rewards r
e
and r
i
respectively, but with
the same target policy œÄ(x) = arg maxa‚ààA Q(x, a, j; Œ∏).
More precisely, to train the weights Œ∏
e
and Œ∏
i
, we use
the same sequence of transitions sampled from the replay, but with two different transformed Retrace loss functions (Munos et al., 2016). For Q(x, a, j; Œ∏
e
) we compute
an extrinsic transformed Retrace loss on the sequence transitions with rewards r
e
and target policy œÄ, whereas for
Q(x, a, j; Œ∏
i
) we compute an intrinsic transformed Retrace
loss on the same sequence of transitions but with rewards
r
i
and target policy œÄ. A reminder of how to compute a
transformed Retrace loss on a sequence of transitions with
rewards r and target policy œÄ is provided in App. C.
In addition, in App. B, we show that this optimization of
separate state-action values is equivalent to the optimization of the original single state-action value function with
reward r
e + Œ≤j r
i
(under a simple gradient descent optimizer). Even though the theoretical objective being optimized is the same, the parameterization is different: we use
two different neural networks to approximate each one of
these state-action values (a schematic and detailed figures
of the architectures used can be found in App. F). By doing
this, we allow each network to adapt to the scale and variance associated with their corresponding reward, and we
also allow for the associated optimizer state to be separated
for intrinsic and extrinsic state-action value functions.
Moreover, when a transformed Bellman operator (Pohlen
et al., 2018) with function h is used (see App. A), we can
split the state-action value function in the following way:
Q(x, a, j; Œ∏) =
h
