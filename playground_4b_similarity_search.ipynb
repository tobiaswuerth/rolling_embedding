{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['./arxiv_downloads_processed\\\\0812.0743v2.A_Novel_Clustering_Algorithm_Based_on_Quantum_Games.pkl',\n",
       "  './arxiv_downloads_processed\\\\1103.4487v1.Handwritten_Digit_Recognition_with_a_Committee_of_Deep_Neural_Nets_on_GPUs.pkl',\n",
       "  './arxiv_downloads_processed\\\\1106.4509v1.Machine_Learning_Markets.pkl'],\n",
       " 999)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all papers stored\n",
    "from rolling.pdf import list_pdfs_processed\n",
    "files = list_pdfs_processed()\n",
    "files[:3], len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rolling.paper import Paper, load_paper\n",
    "papers:list[Paper] = [load_paper(file) for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# match papers by query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"autonomous agents achieving a common goal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08496, -0.00838, -0.0841 ,  0.08167,  0.0863 ], dtype=float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rolling.embedding import GTEEmbeddingModel\n",
    "model = GTEEmbeddingModel()\n",
    "query_embedding = model.encode(query)[0]\n",
    "query_embedding[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find paper which most closly matches query embedding\n",
    "from rolling.embedding import get_similarity\n",
    "similarities = [get_similarity(query_embedding, paper.embedding) for paper in papers]\n",
    "results = list(zip(similarities, papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float16(0.7734), Paper([-0.0388  -0.02464 -0.0824   0.04892  0.0464 ] ./arxiv_downloads\\2011.05605v2.Decentralized_Motion_Planning_for_Multi_Robot_Navigation_using_Deep_Reinforcement_Learning.pdf))\n",
      "(np.float16(0.7646), Paper([-0.04507 -0.02788 -0.05954  0.05148  0.01959] ./arxiv_downloads\\2208.13626v1.CH_MARL__A_Multimodal_Benchmark_for_Cooperative__Heterogeneous_Multi_Agent_Reinforcement_Learning.pdf))\n",
      "(np.float16(0.758), Paper([-0.04904 -0.01817 -0.08276  0.0552   0.05768] ./arxiv_downloads\\1812.09755v1.Learning_when_to_Communicate_at_Scale_in_Multiagent_Cooperative_and_Competitive_Tasks.pdf))\n",
      "(np.float16(0.758), Paper([-0.03525  -0.006313 -0.0814    0.0571    0.03363 ] ./arxiv_downloads\\1911.03743v1.A_perspective_on_multi_agent_communication_for_information_fusion.pdf))\n",
      "(np.float16(0.7573), Paper([-0.04626  -0.003174 -0.0893    0.069     0.03845 ] ./arxiv_downloads\\1709.06620v1.Learning_of_Coordination_Policies_for_Robotic_Swarms.pdf))\n"
     ]
    }
   ],
   "source": [
    "best_matches = sorted(results, key=lambda x:x[0], reverse=True)\n",
    "for match in best_matches[:5]:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# find best matching text segments within paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_paper:Paper = best_matches[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = [get_similarity(query_embedding, emb) for emb in best_paper.text_segment_embeddings]\n",
    "results = list(zip(similarities, best_paper.text_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float16(0.7563), 'in a centralized [6] or decentralized [7] manner. While the prior approach employs a central server for state estimation, prediction and motion planning of all the robots, the later one employs independent on-board resources such that each robot plans its own motion considering the states of others (either through exteroceptive sensing for non-cooperative agents or inter-robot communica- tion for cooperative ones). Although centralized implementa- tions are optimal and safe, their performance heavily degrades when')\n",
      "(np.float16(0.745), '(G2GCARI). This work shall be pursued further to investigate the navigation problem of multiple non-cooperative agents. REFERENCES [1] J. P. Queralta, J. Taipalmaa, B. C. Pullinen, V . K. Sarker, T. N. Gia, H. Tenhunen, M. Gabbouj, J. Raitoharju, and T. Westerlund, “Collaborative multi-robot systems for search and rescue: Coordination and perception,” 2020. [2] F. Augugliaro, S. Lupashin, M. Hamer, C. Male, M. Hehn, M. W. Mueller, J. S. Willmann, F. Gramazio, M. Kohler, and R. D’Andrea, “The ﬂight assembled architecture installation: Cooperative construction with ﬂying machines,” IEEE Control Systems, vol. 34, no. 4, pp. 46–64, 2014. [3] H. Godwin, “Step inside Ocado’s next generation warehouses,” 2018. [4] S. W. Loke, “Cooperative automated vehicles: A review of opportunities and challenges in socially intelligent vehicles beyond networking,” IEEE Transactions on Intelligent Vehicles, vol. 4, no. 4, pp. 509–518, 2019. [5] F. Augugliaro, A. P. Schoellig, and R. D’Andrea, “Dance of the ﬂying machines: Methods for designing and executing an aerial')\n",
      "(np.float16(0.738), 'having a different distance- to-goal owing to the fact that all agents were spawned at random locations with random orientations. As a result, the motion planned by each agent was uniquely subject to its own state, observable states of its peer agents and its relative goal location. A particularly impressive observation here is that since the red and green agents were spawned quite close, the red agent proceeded quickly with high velocity while the green agent accelerated gradually (cautiously with lesser velocity) to')\n",
      "(np.float16(0.731), 'task of multi-robot navigation using deep reinforcement learning. A custom simulator was developed in order to experimentally investigate the navigation problem of 4 cooperative non-holonomic robots sharing limited state information with each other in 3 different settings. The notion of decentralized motion planning with common and shared policy learning was adopted, which allowed robust training and testing of this approach in a stochastic environment since the agents were mutually independent and exhibited asynchronous')\n",
      "(np.float16(0.7305), 'missions [1], collaborative material transfer and construction [2], warehouse management [3], connected autonomous vehicles [4] and even aesthetic performances [5]. In the context of multi-robot motion planning, the algo- rithms can be essentially implemented in a centralized [6] or decentralized [7] manner. While the prior approach employs a central server for state estimation, prediction and motion planning of all the robots, the later one employs independent on-board resources such that each robot plans its own')\n"
     ]
    }
   ],
   "source": [
    "best_matches = sorted(results, key=lambda x:x[0], reverse=True)\n",
    "for match in best_matches[:5]:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# find best paper by segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddins = [emb for paper in papers for emb in paper.text_segment_embeddings]\n",
    "papers_ext = [paper for paper in papers for _ in paper.text_segment_embeddings]\n",
    "texts = [text for paper in papers for text in paper.text_segments]\n",
    "similarities = [get_similarity(query_embedding, emb) for emb in all_embeddins]\n",
    "results = list(zip(similarities, papers_ext, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float16(0.799), Paper([-0.05334 -0.02155 -0.07294  0.05246  0.05124] ./arxiv_downloads\\1908.03963v4.A_Review_of_Cooperative_Multi_Agent_Deep_Reinforcement_Learning.pdf))\n",
      "On the other hand, in the dece ntralized system, each agent takes a decision for itself. Also, the agents might cooperat e to achieve a common goal, e.g. a group of robots who want to identify a source or they might com pete with each other to max- imize their own reward, e.g. the players in different teams o f a game. In each of these cases, the agent might be able to access the whole information and th e sensory observation (if any) of the other agents, or on the other hand, each agent might be abl e to observe\n",
      "\n",
      "(np.float16(0.779), Paper([-0.02124 -0.02347 -0.0475   0.0295   0.05072] ./arxiv_downloads\\2103.01636v1.Sparse_Training_Theory_for_Scalable_and_Efficient_Agents.pdf))\n",
      "INTRODUCTION In recent years, deep learning has become an alternative name for artificial intelligence (AI), while many other fields have remained behind. One one side, this is due to the success of deep learning in solving some real-world open problems in computer vision and natural language processing [22, 42], and on the other side due to the slower pace of advancement made by the other AI fields, including here the world of autonomous agents [9]. Nevertheless, one of the main goals of autonomous agents is to behave intelligently\n",
      "\n",
      "(np.float16(0.768), Paper([-0.04193 -0.01782 -0.06366  0.04486  0.02957] ./arxiv_downloads\\1905.04835v2.Multi_Agent_Image_Classification_via_Reinforcement_Learning.pdf))\n",
      "design of autonomous agents that are capable of cooperation is increasing. The interconnected robots will be major players in the future, accomplishing many duties in industrial automation [1], military support [2], and health- care [3]. In many of these applications, a major issue is that every agent has limited sensing capabilities, and therefore, may not have sufﬁcient information for accomplishing a complex task. One way to mitigate this shortcoming is to let the task to be solved collectively by multiple agents.\n",
      "\n",
      "(np.float16(0.768), Paper([-0.05334 -0.02155 -0.07294  0.05246  0.05124] ./arxiv_downloads\\1908.03963v4.A_Review_of_Cooperative_Multi_Agent_Deep_Reinforcement_Learning.pdf))\n",
      "personal goals . Each agent observes a local ob- servation, local reward corresponding to the goal, and its o wn history of actions, while the ac- tions are executed jointly in the environment. This is a comm on situation in problems like au- tonomous car driving. For example, each car has to reach a giv en destination and all cars need to avoid the accident and cooperate in intersections. The au thors propose an algorithm (central- ized training, decentralized execution) called CM3, with t wo phases. In the ﬁrst\n",
      "\n",
      "(np.float16(0.764), Paper([-0.0466    0.001242 -0.0699    0.0618    0.05014 ] ./arxiv_downloads\\2006.03553v2.Logical_Team_Q_learning__An_approach_towards_factored_policies_in_cooperative_MARL.pdf))\n",
      "and with each other. This is the object of study of Multi-agent RL (MARL), which goes back to the early work of [Tan, 1993] and has seen renewed interest of late (for an updated survey see [Zhang et al., 2019]). In this paper we consider the particular case of cooper- ative MARL in which the agents form ateam and have a shared unique goal. We are interested in tasks where collaboration is fundamental and a high degree of co- ordination is necessary to achieve good performance. In particular, we consider two scenarios.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_matches = sorted(results, key=lambda x:x[0], reverse=True)\n",
    "for match in best_matches[:5]:\n",
    "    print(match[:2])\n",
    "    print(match[2])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
