{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./arxiv_downloads\\\\2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf',\n",
       " './arxiv_downloads\\\\2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rolling.pdf import list_pdfs, read_pdf\n",
    "pdfs = list_pdfs()\n",
    "pdfs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adaptive Local Clustering over Attributed Graphs\\nTechnical Report\\nHaoran Zheng\\nHong Kong Baptist University\\nHong Kong SAR, China\\ncshrzheng@comp.hkbu.edu.hk\\nRenchi Yang\\nHong Kong Baptist University\\nHong Kong SAR, China\\nrenchi@hkbu.edu.hk\\nJianliang Xu\\nHong Kong Baptist University\\nHong Kong SAR, China\\nxujl@hkbu.edu.hk\\nAbstractâ€”Given a graph G and a seed node vs, the objective\\nof local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G\\n(a.k.a. local cluster) surrounding vs in time roughly linear with\\nthe size of Cs. This approach yields personalized clusters without\\nneeding to access the entire graph, which makes it highly suitable\\nfor numerous applications involving large graphs. However, most\\nexisting solutions merely rely on the topological connectivity\\nbetween nodes in G, rendering them vulnerable to missing or\\nnoisy links that are commonly present in real-world graphs.\\nTo address this issue, this paper resorts to leveraging the\\ncomplementary nature of graph topology and node attr'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = read_pdf(pdfs[0])\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rolling.embedding import GTEEmbeddingModel\n",
    "model = GTEEmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: [-0.02779  0.01419 -0.0361   0.04477  0.0311 ] ./arxiv_downloads\\2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf\n",
      " > 000 [-0.06433   0.008675 -0.04153   0.07434   0.05066 ] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size\n",
      " > 001 [-0.06604   0.012856 -0.0344    0.0818    0.05963 ]  Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between\n",
      " > 002 [-0.0586   0.02893 -0.03262  0.0936   0.02443]  size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering\n",
      " > 003 [-0.0486   0.03452 -0.0625   0.112    0.04013]  between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes.\n",
      " > 004 [-0.02919  0.03049 -0.07434  0.08276  0.05405]  clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded\n",
      " > 005 [-0.012665  0.01805  -0.05014   0.06256   0.0642  ]  attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments,\n",
      " > 006 [-0.0347   0.01418 -0.04953  0.0645   0.0605 ]  theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph\n",
      " > 007 [-0.01552  0.0166  -0.03864  0.0898   0.02663]  experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts,\n",
      " > 008 [-1.958e-02 -6.139e-05 -5.530e-02  8.838e-02  2.081e-02]  graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks,\n",
      " > 009 [-0.02066    0.0002027 -0.04794    0.0787     0.02551  ]  counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce\n",
      " > 010 [-0.03787 -0.02257 -0.04324  0.0827   0.02315]  networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology\n",
      " > 011 [-0.0534  -0.02104 -0.0234   0.0877   0.02707]  e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this\n",
      " > 012 [-0.0622  -0.01028 -0.061    0.0935   0.0793 ]  methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly\n",
      " > 013 [-0.0661  -0.01639 -0.05038  0.05884  0.0684 ]  this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed\n",
      " > 014 [-0.05035  0.01588 -0.02812  0.03934  0.0248 ]  mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal\n",
      " > 015 [-0.02182  0.0269  -0.03308  0.0486   0.03607]  constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely\n",
      " > 016 [-0.02625  0.0405  -0.0504   0.07434  0.04755]  sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are\n",
      " > 017 [-0.046     0.02943  -0.01125   0.05872  -0.002268]  solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively\n",
      " > 018 [-0.044     0.00848  -0.004364  0.04434   0.01152 ]  often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33]\n",
      " > 019 [-0.0542   0.02528 -0.02402  0.06323  0.05698]  effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections.\n",
      " > 020 [-0.05542  0.02766 -0.02625  0.079    0.04785]  [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1\n",
      " > 021 [-0.03592  0.00904 -0.05545  0.0829   0.0441 ]  connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to\n",
      " > 022 [-0.0172  -0.0161  -0.0395   0.05472  0.0664 ]  arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs\n",
      " > 023 [-0.05017   0.006844 -0.04892   0.09454   0.07416 ]  node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the\n",
      " > 024 [-0.03592  0.01484 -0.05627  0.0989   0.069  ]  vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large\n",
      " > 025 [-0.02629 -0.00294 -0.0444   0.01976  0.05457]  the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into\n",
      " > 026 [-0.0126   -0.01727  -0.0543    0.010925  0.04813 ]  large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse,\n",
      " > 027 [-0.000406 -0.01469  -0.0621    0.04138   0.015175]  low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also\n",
      " > 028 [-0.01529 -0.02103 -0.05463  0.02989  0.02673]  AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the\n",
      " > 029 [-0.03152   0.001936 -0.05264   0.04492   0.07715 ]  also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors\n",
      " > 030 [-0.0275   0.00521 -0.02652  0.0413   0.06024]  approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the\n",
      " > 031 [-0.01017   0.014694 -0.01256   0.0653    0.0662  ]  competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let\n",
      " > 032 [-0.02621 -0.02844 -0.02371  0.05603  0.06335]  the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote\n",
      " > 033 [-0.0992   0.0539  -0.03476  0.03885 -0.00565]  Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute\n",
      " > 034 [-0.0656    0.07275  -0.0486    0.0096   -0.012566]  denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of\n",
      " > 035 [-0.03418   0.02756  -0.01106   0.00684   0.001209]  attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of\n",
      " > 036 [-0.001559   0.02843   -0.03702    0.01942   -0.0002644]  of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in\n",
      " > 037 [ 0.001943  0.05664  -0.03018  -0.011765 -0.02719 ]  of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly,\n",
      " > 038 [-0.01747  0.0661  -0.01054 -0.00864 -0.04486]  Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector\n",
      " > 039 [-0.004223  0.04736  -0.03436   0.05505  -0.03293 ]  Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined\n",
      " > 040 [-0.0471   0.0189  -0.03513  0.0673   0.00687]  vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi).\n",
      " > 041 [-0.0668   0.02525 -0.01978  0.0968   0.02739]  as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated\n",
      " > 042 [-0.05286   0.03778  -0.009575  0.1057    0.04834 ]  d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its\n",
      " > 043 [-0.04294  0.01729 -0.03014  0.0802   0.04718]  well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where\n",
      " > 044 [-0.05838  0.04007 -0.0415   0.06036  0.0313 ]  its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the\n",
      " > 045 [-0.0783   0.0378  -0.03903  0.03607  0.07263]  where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\"\n",
      " > 046 [-0.05225  0.02419 -0.04633  0.05304  0.05048]  the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$\n",
      " > 047 [-0.04572  0.03067 -0.0517   0.01982  0.02187]  ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’\n",
      " > 048 [-0.04446   0.03796  -0.05634   0.01273   0.007046]  ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´\n",
      " > 049 [-0.02892 -0.00923 -0.0483   0.07007  0.05154]  âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion\n",
      " > 050 [-0.02972 -0.00907 -0.04892  0.0809   0.05508]  â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity\n",
      " > 051 [-0.01437  0.0195  -0.0728   0.1017   0.02997]  diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi,\n",
      " > 052 [-0.02393  0.0504  -0.0599   0.06506  0.01859]  similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at\n",
      " > 053 [-0.03348  0.05734 -0.0592   0.0442  -0.02623]  s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj).\n",
      " > 054 [-0.03897  0.05768 -0.05713  0.082   -0.02213]  at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below\n",
      " > 055 [-0.06033  0.0525  -0.0368   0.07404 -0.0238 ]  Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional\n",
      " > 056 [-0.03876  0.0385  -0.03598  0.06604 -0.00546]  process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š)\n",
      " > 057 [-0.02995  0.02963 -0.0225   0.0587   0.01808]  rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending\n",
      " > 058 [-0.03848  0.06476 -0.02754  0.06204  0.0359 ]  ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value\n",
      " > 059 [-0.0774   0.0709  -0.05862  0.0974   0.04083]  pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark.\n",
      " > 060 [-0.0661   0.04172 -0.03592  0.05176  0.04434]  value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other\n",
      " > 061 [-0.02403  0.02748 -0.03256  0.05063  0.06415]  Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï\n",
      " > 062 [ 0.005962  0.02255  -0.053     0.08185   0.06525 ]  other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the\n",
      " > 063 [ 0.012184  0.02432  -0.0445    0.07886   0.0462  ]  â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the\n",
      " > 064 [-0.01933  0.02722 -0.06094  0.0639   0.05774]  the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the\n",
      " > 065 [-0.02512  0.04242 -0.06128  0.0525   0.0667 ]  the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD\n",
      " > 066 [-0.0364   0.06055 -0.03888  0.00773  0.04993]  the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD\n",
      " > 067 [-0.0471   0.03644 -0.0641   0.03207  0.02406]  via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate\n",
      " > 068 [-0.02539  0.01559 -0.06042  0.04327  0.02615]  value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2\n",
      " > 069 [-0.01371  0.03473 -0.0246   0.01605  0.03067]  Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA. where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies\n",
      " > 070 [-0.02562  0.0374   0.01883  0.01287  0.03787]  ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA. where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely\n",
      " > 071 [-0.01724  0.04376 -0.02295  0.0321   0.03625]  implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’\n",
      " > 072 [-2.028e-02  5.051e-02 -6.282e-05  3.625e-02  2.838e-02]  immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed\n",
      " > 073 [-0.004448  0.05115   0.00049   0.02002   0.0239  ]  =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â·\n",
      " > 074 [-0.02435  0.03168 -0.03677  0.0558   0.05618]  seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time,\n",
      " > 075 [-0.04138  0.02603 -0.07275  0.02464  0.02666]  Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector\n",
      " > 076 [-0.01463   0.004272 -0.0725    0.0299    0.03038 ]  time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7\n",
      " > 077 [-0.00591  0.0296  -0.0224   0.0426   0.01874]  vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c)\n",
      " > 078 [-0.01116   0.03049  -0.0758    0.004726  0.0495  ]  ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector\n",
      " > 079 [-0.01348   0.02164  -0.0728   -0.004887  0.05484 ]  2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€\n",
      " > 080 [-0.00801  0.03598 -0.02829  0.03522  0.00917]  vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively,\n",
      " > 081 [ 2.383e-02 -9.519e-05 -5.359e-02  4.355e-02 -2.697e-03]  â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node\n",
      " > 082 [ 0.01325   0.001696 -0.03452   0.0487    0.07434 ]  respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD\n",
      " > 083 [-0.00435  0.00689 -0.03372  0.0688   0.03757]  node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12))\n",
      " > 084 [ 0.003395  0.02951  -0.0481    0.0346    0.0252  ]  BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic\n",
      " > 085 [ 0.01198   0.01802  -0.022     0.02112   0.008194]  to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the\n",
      " > 086 [ 0.002548  0.008606 -0.02547   0.02632  -0.0035  ]  algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges\n",
      " > 087 [-0.01098   0.005226 -0.0309    0.03044   0.01927 ]  complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f\n",
      " > 088 [ 0.00862   0.03653  -0.03586   0.0575   -0.007668]  edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and\n",
      " > 089 [-0.005768 -0.01506  -0.010216  0.03647   0.009544]  â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access\n",
      " > 090 [-0.002419 -0.04562  -0.0339    0.0216   -0.00898 ]  âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse\n",
      " > 091 [-0.01105 -0.0165  -0.05634  0.0454   0.04013]  access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed\n",
      " > 092 [-0.03546   -0.0001832 -0.0577     0.0506     0.02336  ]  GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative\n",
      " > 093 [-0.00735  0.01129 -0.06946  0.0431  -0.01018]  needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi)\n",
      " > 094 [-0.02159  0.0415  -0.03072  0.0597  -0.0127 ]  iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we\n",
      " > 095 [-0.0096   0.05347 -0.01212  0.04642 -0.00553]  i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly\n",
      " > 096 [-0.04358  0.03882 -0.01173  0.068    0.01736]  we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’\n",
      " > 097 [-0.03093   0.02756  -0.01607   0.0724    0.004936]  evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals\n",
      " > 098 [ 0.02396  0.02005 -0.0327   0.03635  0.02396]  Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010\n",
      " > 099 [ 0.01495  0.02733 -0.04404  0.0506   0.0175 ]  residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired\n",
      " > 100 [-0.00899 -0.02353 -0.051    0.02615  0.0399 ]  max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ\n",
      " > 101 [-0.01548   0.003729 -0.04813   0.04343   0.04214 ]  desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input\n",
      " > 102 [-0.002344  0.01242  -0.02608   0.0404    0.02795 ]  (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the\n",
      " > 103 [-0.00887  0.01141 -0.047    0.0689   0.03   ]  input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically,\n",
      " > 104 [-0.05304  0.0195  -0.05548  0.0867   0.01214]  the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus,\n",
      " > 105 [-0.0378   0.01323 -0.05215  0.08875  0.01013]  Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from\n",
      " > 106 [-0.02364  0.01396 -0.05286  0.066    0.02252]  Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns\n",
      " > 107 [-0.03305  0.00568 -0.03123  0.04465  0.02328]  from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused\n",
      " > 108 [-0.03683 -0.0232  -0.02017  0.03174  0.02838]  returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17)\n",
      " > 109 [-0.002996  0.00996  -0.03568   0.04178  -0.001425]  Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7)\n",
      " > 110 [-0.0323   -0.0065   -0.05814   0.0438    0.002844]  (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all\n",
      " > 111 [-0.03973  -0.010254 -0.0787    0.05115   0.02213 ]  5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading\n",
      " > 112 [-0.02107  -0.001347 -0.07245   0.0486    0.0225  ]  nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and\n",
      " > 113 [-0.02292  0.01602 -0.06854  0.0804   0.0057 ]  leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv\n",
      " > 114 [-0.03775  0.01727 -0.0748   0.09015  0.00628]  and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals\n",
      " > 115 [-0.03845  0.02399 -0.08875  0.06024  0.04446]  ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination.\n",
      " > 116 [-0.03375 -0.01743 -0.07135  0.04718  0.04712]  residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy\n",
      " > 117 [-0.0382   -0.002966 -0.08344   0.0389    0.05072 ]  termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49\n",
      " > 118 [-0.00913  0.01364 -0.07275  0.01645  0.06152]  greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast\n",
      " > 119 [ 0.01852  -0.015015 -0.04208   0.01784   0.03772 ]  12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through\n",
      " > 120 [-0.01221 -0.027   -0.0658   0.02034  0.0309 ]  contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of\n",
      " > 121 [-0.006382 -0.01402  -0.0674    0.03973   0.0501  ]  through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse,\n",
      " > 122 [-0.01938 -0.01543 -0.04895  0.03613  0.04272]  of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased\n",
      " > 123 [-0.00893 -0.01029 -0.06168  0.0165   0.0656 ]  GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14)\n",
      " > 124 [ 0.001194 -0.01189  -0.0661    0.03336   0.04276 ]  increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and\n",
      " > 125 [-0.02142  0.02591 -0.059    0.03772  0.0534 ]  (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD\n",
      " > 126 [ 0.004883  0.03192  -0.02843   0.0308    0.06586 ]  and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil\n",
      " > 127 [ 0.01353  0.02252 -0.03497 -0.01004  0.03983]  BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq. (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi,\n",
      " > 128 [-0.01163   0.02483  -0.01511  -0.008995  0.02138 ]  unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq. (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k\n",
      " > 129 [-0.00267   0.04846   0.007656 -0.003195 -0.002975]  f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼\n",
      " > 130 [-0.00662  0.05637  0.01582  0.01243  0.01206]  k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B\n",
      " > 131 [-0.05905  0.0317   0.02159  0.03857  0.03146]  âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional\n",
      " > 132 [-0.02574  0.04584 -0.02853  0.05414  0.02275]  II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i)\n",
      " > 133 [ 0.0003083  0.01907   -0.03027    0.04248    0.04584  ]  dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition\n",
      " > 134 [-0.047    0.02405 -0.03162  0.037    0.03168]  (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function,\n",
      " > 135 [-0.04083  0.04373  0.00813  0.02847  0.02742]  decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization,\n",
      " > 136 [ 0.00591  0.03497 -0.034    0.0524   0.02347]  function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates\n",
      " > 137 [-0.02457  0.02756 -0.03065  0.05896  0.01704]  factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo.\n",
      " > 138 [-0.0004673  0.02261   -0.01426    0.0337     0.02911  ]  a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1\n",
      " > 139 [ 0.009705   0.01988   -0.0132    -0.0004299  0.04306  ]  3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P,\n",
      " > 140 [-0.0222    0.04593  -0.02814  -0.007473  0.01095 ]  â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed.\n",
      " > 141 [-0.03091   0.03424  -0.02191   0.0562    0.007286]  Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator\n",
      " > 142 [-0.0339   0.03104 -0.07513  0.0592   0.0053 ]  distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs\n",
      " > 143 [-0.02408  0.0178  -0.0572   0.05234  0.01746]  estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete\n",
      " > 144 [ 0.02226  0.01212 -0.0549   0.0346   0.02962]  constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place,\n",
      " > 145 [ 0.008484  0.00737  -0.0517    0.00806   0.0412  ]  Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011\n",
      " > 146 [-0.00919   0.010544 -0.01159   0.02396   0.03714 ]  place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries\n",
      " > 147 [-0.01756  0.00963 -0.02814  0.04318  0.04486]  \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion\n",
      " > 148 [-0.001258  0.002903 -0.0249    0.03078   0.03073 ]  entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate\n",
      " > 149 [ 0.02554  0.03812 -0.02925  0.03207  0.06274]  diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€²\n",
      " > 150 [ 0.00989   0.011696 -0.01874   0.01535   0.0638  ]  approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï\n",
      " > 151 [ 0.01572  0.00395 -0.03574  0.01656  0.0363 ]  t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2.\n",
      " > 152 [-0.02277  0.0385  -0.0209   0.01532  0.0667 ]  â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines\n",
      " > 153 [-0.03108  0.02422 -0.00995  0.01982  0.05386]  The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical\n",
      " > 154 [-0.02083  -0.01255  -0.02419   0.013374  0.0525  ]  6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) .\n",
      " > 155 [-0.0632   -0.002838 -0.03558   0.0347    0.02846 ]  Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2\n",
      " > 156 [-0.007748  0.01158  -0.03427   0.0738    0.05493 ]  . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ\n",
      " > 157 [-0.0273   -0.002586 -0.03418   0.0828    0.04337 ]  Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final\n",
      " > 158 [-0.06186  0.03078 -0.0261   0.03128  0.04312]  âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51]\n",
      " > 159 [-0.05606   0.05228  -0.062     0.02434  -0.003428]  final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i)\n",
      " > 160 [-0.03766  -0.02327  -0.03918   0.03635   0.013626]  [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are\n",
      " > 161 [-0.006424 -0.008385 -0.0351    0.04456   0.02876 ]  â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs\n",
      " > 162 [-0.03687   0.005173 -0.01994   0.0693    0.0739  ]  are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the\n",
      " > 163 [-0.04703  -0.0133    0.005627  0.06058   0.06866 ]  from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets,\n",
      " > 164 [-0.01246    0.005306  -0.0004368  0.07465    0.0534   ]  the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis,\n",
      " > 165 [-0.00898  0.01517 -0.0218   0.0706   0.032  ]  datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca.\n",
      " > 166 [-0.03812 -0.0119  -0.01558  0.0202  -0.00943]  analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the\n",
      " > 167 [-0.0422   0.01326 -0.02272  0.06085 -0.04736]  A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed-\n",
      " > 168 [-0.06885   0.006264 -0.02042   0.09955  -0.02374 ]  the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively.\n",
      " > 169 [-0.05475  -0.015205 -0.02956   0.0962   -0.03214 ]  embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md)\n",
      " > 170 [-0.04715    0.0009723 -0.01994    0.04053   -0.015465 ]  respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n)\n",
      " > 171 [-0.03586   0.005043 -0.008606  0.05      0.01506 ]  O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of\n",
      " > 172 [-0.04456 -0.02298 -0.0393   0.08307  0.03424]  ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product\n",
      " > 173 [-0.02687 -0.00698 -0.036    0.07324  0.072  ]  of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine\n",
      " > 174 [-0.03406   0.003435 -0.02467   0.0743    0.05725 ]  product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm\n",
      " > 175 [-0.0568  -0.01237 -0.01215  0.00887  0.06604]  similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods:\n",
      " > 176 [-0.05652 -0.0351  -0.03387  0.02094  0.0301 ]  p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms,\n",
      " > 177 [-0.06464 -0.01197 -0.0399   0.0504   0.02324]  methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute\n",
      " > 178 [-0.07916  0.01099 -0.05444  0.05786  0.0321 ]  all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods\n",
      " > 179 [-0.0687   0.02333 -0.0563   0.06152  0.06107]  attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE\n",
      " > 180 [-0.06946  0.00993 -0.04602  0.0967   0.05695]  methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters.\n",
      " > 181 [-0.0653   0.02917 -0.02031  0.08276  0.02841]  [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar,\n",
      " > 182 [-0.05438  0.01135 -0.0288   0.06384 -0.00971]  clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid\n",
      " > 183 [-0.03815  -0.04898  -0.010994  0.02592   0.02391 ]  Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed\n",
      " > 184 [-0.05658 -0.02658 -0.0495   0.0305   0.04257]  grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters\n",
      " > 185 [-0.03268 -0.01188 -0.0518   0.07043  0.05383]  seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat-\n",
      " > 186 [-0.02025  -0.003027 -0.05893   0.04947   0.06494 ]  clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by\n",
      " > 187 [-0.0347   0.01084 -0.03992 -0.01123  0.0584 ]  sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets.\n",
      " > 188 [-0.0338   0.016   -0.02724  0.01095  0.09924]  by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE,\n",
      " > 189 [-0.02554  0.00608 -0.00798  0.05533  0.03033]  datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms\n",
      " > 190 [-0.01619  -0.01665  -0.007652  0.04095   0.03683 ]  PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and\n",
      " > 191 [-0.03906  0.01277 -0.01291  0.00958  0.08057]  outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit\n",
      " > 192 [-0.063     0.0411   -0.0169    0.002012  0.05005 ]  SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20]\n",
      " > 193 [-0.02501   0.01205   0.012245  0.04126   0.005554]  Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343\n",
      " > 194 [-0.003336  0.003069 -0.011024  0.00549  -0.007717]  [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154\n",
      " > 195 [-0.0371    0.001767 -0.0318    0.02303   0.0421  ]  0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC)\n",
      " > 196 [-0.05664  0.00414 -0.0104   0.03815  0.02917]  0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN)\n",
      " > 197 [-0.0363    0.01836   0.002352  0.077     0.02136 ]  (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422\n",
      " > 198 [-0.03986  0.02852  0.04144  0.05136  0.03848]  (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2\n",
      " > 199 [-0.01189  0.03036  0.0498   0.0192   0.0697 ]  0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000\n",
      " > 200 [ 0.000299  0.02559  -0.001233  0.05615   0.04175 ]  10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the\n",
      " > 201 [-0.01999 -0.01396  0.01929  0.0731   0.024  ]  1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure\n",
      " > 202 [-0.03497   0.007965  0.03072   0.05707   0.0371  ]  the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance\n",
      " > 203 [-0.01646   0.005512  0.02812   0.0549    0.05988 ]  structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the\n",
      " > 204 [-0.03375  -0.00818   0.010895  0.0648    0.05942 ]  performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their\n",
      " > 205 [-0.008705  0.01328  -0.01718   0.068     0.04755 ]  the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms,\n",
      " > 206 [ 0.01374   0.007153 -0.02477   0.08185   0.0593  ]  their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be.\n",
      " > 207 [-0.01459  -0.001059 -0.0216    0.0521    0.03992 ]  algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA\n",
      " > 208 [-0.01209 -0.01364 -0.00914  0.03558  0.04456]  be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes\n",
      " > 209 [-0.0363   0.01497 -0.03555  0.0563   0.0884 ]  LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank\n",
      " > 210 [-0.05032  0.01756 -0.041    0.05914  0.0426 ]  sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time\n",
      " > 211 [-0.0404    -0.0005484 -0.02307    0.0277     0.01819  ]  PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec\n",
      " > 212 [-0.02936  -0.007187 -0.0254    0.02467   0.02869 ]  (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M\n",
      " > 213 [-4.797e-02  2.480e-05  1.617e-02  2.071e-02  5.212e-02]  PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax\n",
      " > 214 [-0.02069  0.03314  0.06155  0.02863  0.0961 ]  Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS)\n",
      " > 215 [-0.0217   0.0571   0.02728  0.05338  0.07886]  Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than\n",
      " > 216 [-2.943e-02 -9.251e-05 -2.393e-02  3.983e-02  6.897e-02]  obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed.\n",
      " > 217 [-0.03326   0.011795 -0.03787   0.05978   0.04456 ]  than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed\n",
      " > 218 [-0.02551  0.02838 -0.05563  0.05527  0.038  ]  seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table\n",
      " > 219 [-0.0325  -0.00107 -0.03415  0.0223   0.03696]  non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the\n",
      " > 220 [-0.01549 -0.0078  -0.045    0.02563  0.02193]  (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage,\n",
      " > 221 [-0.01866 -0.02022 -0.04874  0.0562   0.0697 ]  the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA\n",
      " > 222 [-0.02711 -0.01865 -0.0559   0.08405  0.05835]  all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art\n",
      " > 223 [-0.02094   0.003452 -0.01932   0.05786   0.05988 ]  (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant,\n",
      " > 224 [-0.003435  0.01183  -0.02167   0.02391   0.1056  ]  state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage)\n",
      " > 225 [ 0.00658  0.01617 -0.03946 -0.00217  0.0894 ]  insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD\n",
      " > 226 [-0.01413   0.001809 -0.02882  -0.003422  0.0825  ]  stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary,\n",
      " > 227 [-0.02675  0.00664 -0.01918  0.02911  0.0834 ]  WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied\n",
      " > 228 [-0.04013 -0.01223 -0.01587  0.04834  0.05908]  summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang\n",
      " > 229 [-0.05237 -0.0335  -0.03564 -0.0138  -0.01582]  to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong\n",
      " > 230 [-4.013e-02 -1.190e-02 -8.575e-03 -2.754e-05 -2.147e-02]  Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the\n",
      " > 231 [-0.03415  -0.00501  -0.01706   0.01701   0.007668]  (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship\n",
      " > 232 [-0.02606   0.006588 -0.01587   0.03763   0.0072  ]  the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)).\n",
      " > 233 [ 0.000578  0.01303  -0.0417    0.0657    0.003735]  co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates\n",
      " > 234 [-0.02135  0.01123 -0.0343   0.06683  0.02667]  (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local\n",
      " > 235 [-0.06033  0.03055 -0.04837  0.0808   0.0585 ]  demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster.\n",
      " > 236 [-0.07635  -0.005695 -0.03522   0.06116   0.0708  ]  local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15]\n",
      " > 237 [-0.0852  -0.0231  -0.0294   0.04642  0.04266]  cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods\n",
      " > 238 [-0.04248 -0.0421  -0.04865  0.0593   0.05334]  [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on\n",
      " > 239 [-0.00329 -0.02399 -0.0523   0.03708  0.03156]  methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights\n",
      " > 240 [-0.01386  -0.001057 -0.0459    0.0597    0.007866]  running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly\n",
      " > 241 [-0.0466  -0.01566 -0.0687   0.0712   0.02612]  weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected\n",
      " > 242 [-0.0576    0.002647 -0.09296   0.0741    0.015015]  costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities,\n",
      " > 243 [-0.09265   0.04327  -0.05722   0.0585   -0.003994]  connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been\n",
      " > 244 [-0.0765   0.04538 -0.04663  0.0665   0.02768]  communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching\n",
      " > 245 [-0.064    0.05133 -0.06915  0.07855  0.0517 ]  been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the\n",
      " > 246 [-0.06915  0.0285  -0.04013  0.0574   0.0297 ]  the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints\n",
      " > 247 [-0.05664  0.0191  -0.0629   0.04993  0.0342 ]  objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach\n",
      " > 248 [-0.05228  0.02362 -0.06094  0.06946  0.0406 ]  constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA,\n",
      " > 249 [-0.05435  0.01464 -0.04886  0.05127  0.0471 ]  follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii)\n",
      " > 250 [-0.01833   0.006332 -0.045     0.03583   0.05856 ]  LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17\n",
      " > 251 [-0.01898  0.0085  -0.02837  0.0508   0.03162]  (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou,\n",
      " > 252 [-0.0753    0.007763 -0.03732   0.05795  -0.01449 ]  17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference\n",
      " > 253 [-0.04358 -0.00499 -0.052    0.0502  -0.0205 ]  Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za\n",
      " > 254 [-0.07306  0.01288 -0.04553  0.05463  0.0373 ]  (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities\n",
      " > 255 [-0.02664  0.02597 -0.04007  0.11035 -0.02142]  Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J.\n",
      " > 256 [ 0.0358   -0.002596 -0.05072   0.1383    0.007164]  in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz,\n",
      " > 257 [-0.002794 -0.0207   -0.03111   0.0767   -0.00964 ]  Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter-\n",
      " > 258 [-0.03232 -0.01524 -0.04434  0.0564   0.01779]  Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings\n",
      " > 259 [-0.04657  -0.001767 -0.01854   0.0636    0.04724 ]  Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery\n",
      " > 260 [-0.05002  -0.006645 -0.0306    0.0693    0.06213 ]  Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &\n",
      " > 261 [-0.03102   0.002884 -0.05258   0.08435   0.05386 ]  Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and\n",
      " > 262 [-0.03104   0.004654 -0.05667   0.0955    0.04428 ]  & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€\n",
      " > 263 [-0.04572  0.0285  -0.0493   0.098    0.00894]  and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486.\n",
      " > 264 [-0.0639   0.01843 -0.04575  0.07184  0.01031]  partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J.\n",
      " > 265 [-0.0786    0.00787  -0.0569    0.0796   -0.002981]  475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion\n",
      " > 266 [-0.08966   -0.00773   -0.0655     0.0899    -0.0001345]  J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical\n",
      " > 267 [-0.03928  -0.002499 -0.06433   0.07465   0.0291  ]  diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D.\n",
      " > 268 [-0.02228 -0.0377  -0.02115  0.07367  0.01607]  Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters\n",
      " > 269 [-0.039   -0.005   -0.02776  0.0804  -0.00542]  Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings\n",
      " > 270 [-0.0367   0.04083 -0.03336  0.0726  -0.00932]  Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference\n",
      " > 271 [-0.01426  0.03543 -0.0301   0.05548  0.0247 ]  Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023,\n",
      " > 272 [-0.01578  0.01485 -0.04288  0.07513  0.0426 ]  on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter:\n",
      " > 273 [-0.05145  0.00995 -0.06885  0.0506   0.01502]  2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions\n",
      " > 274 [-0.0834   0.02594 -0.063    0.04425  0.05444]  filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015.\n",
      " > 275 [-0.0453    0.0391   -0.02759   0.0417    0.013824]  Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled\n",
      " > 276 [-0.02159  0.02634 -0.058    0.0683  -0.01804]  2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and\n",
      " > 277 [-0.01451   0.003176 -0.0408    0.0726   -0.00638 ]  labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing\n",
      " > 278 [-0.00523   0.002424 -0.06158   0.03543   0.0314  ]  and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems\n",
      " > 279 [-0.01543  -0.004055 -0.0676    0.03918   0.03165 ]  constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017,\n",
      " > 280 [-0.0376  -0.03342 -0.0613   0.04385  0.00783]  systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€\n",
      " > 281 [-0.03976  -0.014824 -0.0613    0.05273   0.003597]  2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€\n",
      " > 282 [-0.0106  -0.01206 -0.02783  0.0142  -0.01933]  graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp. 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and\n",
      " > 283 [-0.00771    0.00641   -0.01678   -0.0002708  0.02597  ]  applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp. 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional\n",
      " > 284 [-0.03992  0.02525 -0.03705  0.0666   0.02007]  and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead,\n",
      " > 285 [-0.03928   0.005642 -0.04077   0.05746  -0.00916 ]  â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference\n",
      " > 286 [-0.0387  -0.0209  -0.03093  0.02443 -0.02061]  Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226.\n",
      " > 287 [-0.0511  -0.04276 -0.04062  0.01269 -0.02621]  Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge\n",
      " > 288 [-0.0707  -0.04272 -0.01056  0.01836 -0.02768]  1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational\n",
      " > 289 [-0.0587  -0.00948 -0.0322   0.00851 -0.02002]  Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings\n",
      " > 290 [-0.02754 -0.00945 -0.05402  0.02844 -0.01668]  â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in\n",
      " > 291 [-0.04654  -0.03644  -0.04456   0.003359 -0.0369  ]  Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations\n",
      " > 292 [-0.02629  -0.01889  -0.07166  -0.006042 -0.02054 ]  in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge\n",
      " > 293 [-0.0356   -0.001446 -0.03903  -0.000639 -0.02512 ]  Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559.\n",
      " > 294 [-0.03214   0.01892  -0.01069   0.002169 -0.002045]  knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han,\n",
      " > 295 [-0.02696   0.01108  -0.00362  -0.00592   0.004467]  556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1,\n",
      " > 296 [-0.03418 -0.03418 -0.01598  0.01021  0.01588]  â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of\n",
      " > 297 [-0.038   -0.03598 -0.0551   0.0401   0.03072]  2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment,\n",
      " > 298 [-0.03592  -0.001046 -0.0434    0.007553  0.00903 ]  of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J.\n",
      " > 299 [-0.0546   -0.02347  -0.0271    0.06247   0.013214]  Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring\n",
      " > 300 [-0.0855   -0.0278   -0.03705   0.0674    0.005737]  J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences\n",
      " > 301 [-0.074   -0.02396 -0.04077  0.02151 -0.01755]  â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H.\n",
      " > 302 [-0.0261    0.012375 -0.06      0.0481   -0.0186  ]  Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient\n",
      " > 303 [-0.0449    0.006535 -0.071     0.0703   -0.01619 ]  H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of\n",
      " > 304 [-0.04968   0.007298 -0.05133   0.06836  -0.011086]  efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen,\n",
      " > 305 [-0.0444   0.01472 -0.0266   0.06616  0.01419]  of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€\n",
      " > 306 [-0.03473  -0.01421  -0.04388   0.0523    0.009705]  Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms,\n",
      " > 307 [-0.03458   0.003601 -0.0602   -0.002686 -0.02405 ]  labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W.\n",
      " > 308 [-0.0637   0.02588 -0.0646   0.01014 -0.00616]  algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433,\n",
      " > 309 [-0.04675  0.02107 -0.05798  0.05078 -0.00964]  W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association\n",
      " > 310 [-0.01788  0.02081 -0.07886  0.06836 -0.00903]  1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient\n",
      " > 311 [-0.02737   0.011154 -0.05612   0.05542   0.00556 ]  Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye,\n",
      " > 312 [-0.02846   0.01493  -0.0467    0.04984   0.001856]  efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to\n",
      " > 313 [-0.04776 -0.01663 -0.04617  0.01416  0.01627]  Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018,\n",
      " > 314 [-0.0534   0.01156 -0.05402  0.00742  0.0062 ]  to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297.\n",
      " > 315 [-0.0683  -0.02193 -0.03854  0.02652 -0.04028]  2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the\n",
      " > 316 [-0.0414  -0.01807 -0.05746  0.04135 -0.03247]  281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not\n",
      " > 317 [-0.02315  0.02512 -0.06223  0.01706  0.00902]  the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence\n",
      " > 318 [-0.04916  -0.003975 -0.0638   -0.000464  0.02116 ]  available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      " > 319 [-0.02081   0.003202 -0.06354   0.01337   0.02121 ]  Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and\n",
      " > 320 [-0.008675  0.0455   -0.0608    0.04767   0.0082  ]  2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration.\n",
      " > 321 [-0.01642  0.05054 -0.04913  0.0454   0.01595]  and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’\n",
      " > 322 [-0.06396  0.0454  -0.04428  0.06033  0.03223]  iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V,\n",
      " > 323 [-0.0505   0.03268 -0.0281   0.04852  0.04596]  â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t\n",
      " > 324 [-0.0419   0.07855 -0.0392   0.0682   0.02153]  âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality\n",
      " > 325 [-0.02354  0.0653  -0.0857   0.0243   0.02748]  Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that\n",
      " > 326 [-0.0405   0.07135 -0.0659   0.02249  0.0373 ]  inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at\n",
      " > 327 [-0.02858  0.0614  -0.05145  0.05756  0.0359 ]  that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23)\n",
      " > 328 [-0.03647  0.0826  -0.04492  0.0785   0.0397 ]  at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1\n",
      " > 329 [-0.02855  0.05954 -0.05234  0.093    0.0323 ]  We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X\n",
      " > 330 [-0.03387  0.0401  -0.01881  0.0684   0.01926]  â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line\n",
      " > 331 [-0.00907  0.0363  -0.0355   0.06744  0.03093]  viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1.\n",
      " > 332 [-0.01523  0.02179 -0.04312  0.0429   0.0254 ]  Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’\n",
      " > 333 [-0.01227  -0.001817 -0.0189    0.06384   0.03757 ]  âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries\n",
      " > 334 [-0.01791  -0.015434 -0.03534   0.0723    0.02481 ]  multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration,\n",
      " > 335 [-0.01505  -0.012794 -0.0484    0.0302    0.0333  ]  entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The\n",
      " > 336 [-0.0003085  0.01869   -0.0654     0.03035    0.00647  ]  iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration\n",
      " > 337 [-0.00078   0.05927  -0.0515    0.03464   0.014656]  theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed\n",
      " > 338 [-0.01371  0.0324  -0.06573  0.03287  0.04718]  iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0. Therefore, the total amount of greedy operations\n",
      " > 339 [-0.01048   -0.007328  -0.0718    -0.0003147  0.05078  ]  entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it-\n",
      " > 340 [-0.00968    0.0008035 -0.0643     0.00693    0.02812  ]  operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and\n",
      " > 341 [ 0.000737  0.03119  -0.0419    0.0198    0.02278 ]  erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we\n",
      " > 342 [-0.02692  0.03001 -0.03857  0.0445   0.0255 ]  Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then,\n",
      " > 343 [-0.04135  0.03424 -0.0281   0.05188  0.01406]  have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we\n",
      " > 344 [-0.02303  0.01628 -0.03035  0.02507  0.01617]  Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes\n",
      " > 345 [-0.01575  0.01544 -0.02045  0.02798  0.0365 ]  we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1,\n",
      " > 346 [-0.03244   0.04825  -0.009705  0.05682   0.05084 ]  completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly,\n",
      " > 347 [-0.0647   0.04633 -0.00205  0.07196  0.0526 ]  Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that\n",
      " > 348 [-0.0732   0.0404  -0.0161   0.0718   0.04733]  Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U\n",
      " > 349 [-0.0521    0.03497  -0.007572  0.0754    0.0492  ]  that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ\n",
      " > 350 [-0.03055  0.03125 -0.01053  0.0665   0.05124]  are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp\n",
      " > 351 [-0.03111  0.04953 -0.02315  0.02861  0.04904]  âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via\n",
      " > 352 [-0.02687  0.06793 -0.01797  0.04428  0.01677]  exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma\n",
      " > 353 [-0.04843   0.0534   -0.007713  0.03488   0.04846 ]  via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d.\n",
      " > 354 [-0.00949    0.0628     0.0007463  0.01149    0.0408   ]  Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity\n",
      " > 355 [ 0.002222  0.0758   -0.02382  -0.01752   0.02621 ]  d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7)\n",
      " > 356 [ 0.004925  0.0721   -0.04816   0.00933   0.04468 ]  larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i\n",
      " > 357 [-0.01234  0.0475  -0.051    0.0246   0.03058]  7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line\n",
      " > 358 [-0.05048  0.05563 -0.03482  0.0392   0.03467]  i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice\n",
      " > 359 [-0.0331   0.04593 -0.05713  0.01949  0.0363 ]  (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi)\n",
      " > 360 [-0.03284  0.0503  -0.0579   0.01675  0.03372]  that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi)\n",
      " > 361 [-0.05005  0.06088 -0.0643   0.0632   0.03552]  vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â·\n",
      " > 362 [-0.0413   0.02167 -0.06354  0.05914  0.0304 ]  ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA\n",
      " > 363 [-0.065    0.0219  -0.04465  0.06885  0.01024]  Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three\n",
      " > 364 [-0.024    0.04718 -0.01472  0.02968  0.0287 ]  ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with\n",
      " > 365 [-0.002268  0.02054   0.01093  -0.004692  0.05844 ]  three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly\n",
      " > 366 [-0.00787 -0.01255 -0.02321 -0.02182  0.07153]  with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse\n",
      " > 367 [-0.02017  -0.006298 -0.04752  -0.012405  0.0512  ]  nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing\n",
      " > 368 [-0.0443   -0.009155 -0.0655    0.02286   0.0569  ]  AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0\n",
      " > 369 [-0.02678  -0.03049  -0.000994 -0.02278   0.0749  ]  balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0\n",
      " > 370 [-0.01933   0.02132   0.033    -0.008995  0.06067 ]  0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8\n",
      " > 371 [-0.0411   0.03632  0.02394 -0.02074  0.0793 ]  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance\n",
      " > 372 [-0.0511   0.0414   0.02669 -0.0033   0.08044]  8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with\n",
      " > 373 [-0.06647   0.014786 -0.00899   0.03333   0.06464 ]  performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show\n",
      " > 374 [-0.03452  0.0356   0.00256  0.03952  0.06198]  with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased\n",
      " > 375 [-0.01828    0.04803    0.01031   -0.0009985  0.04623  ]  show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason\n",
      " > 376 [-0.0701   0.04895 -0.0145   0.01646  0.05286]  increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest\n",
      " > 377 [-0.02792  0.0469  -0.01904  0.01581  0.03455]  reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)),\n",
      " > 378 [ 0.005836  0.01566  -0.00811   0.02419   0.03824 ]  manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented\n",
      " > 379 [-0.003187 -0.02197  -0.002235  0.0314    0.02289 ]  (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients,\n",
      " > 380 [ 0.000669 -0.01665  -0.010544  0.025     0.07    ]  implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust\n",
      " > 381 [-0.00453   0.012566 -0.01252   0.02994   0.066   ]  ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from\n",
      " > 382 [0.002172 0.012794 0.009834 0.02005  0.0798  ]  robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808\n",
      " > 383 [0.00427 0.02669 0.02956 0.00915 0.11206]  from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554\n",
      " > 384 [-0.007057  0.0477    0.001934  0.03836   0.0638  ]  0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster\n",
      " > 385 [-0.03616  0.0632  -0.02669  0.05804  0.0439 ]  0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates\n",
      " > 386 [-0.06274  0.0973  -0.04993  0.07556  0.04654]  within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best\n",
      " > 387 [-0.04373  0.0681  -0.02316  0.05475  0.04437]  evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA\n",
      " > 388 [-0.0395    0.03754  -0.010956  0.05634   0.04883 ]  best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness\n",
      " > 389 [-0.0532   0.0453  -0.0143   0.06903  0.05142]  LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in\n",
      " > 390 [-0.04755  0.03452 -0.01569  0.0253   0.0599 ]  cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“\n",
      " > 391 [-0.04797   0.002983 -0.004288  0.02173   0.0586  ]  in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237\n",
      " > 392 [-0.05927   0.05286   0.02103   0.0406    0.002258]  Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156\n",
      " > 393 [-0.04416   0.05447   0.03934   0.0352    0.006153]  0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251\n",
      " > 394 [ 0.0015135  0.0532     0.00341   -0.00074    0.0007095]  0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707\n",
      " > 395 [-0.02383   0.05466  -0.005997 -0.003782 -0.005146]  0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811\n",
      " > 396 [-0.05087   0.0474   -0.02339  -0.001264 -0.01456 ]  0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86\n",
      " > 397 [-0.03964   0.04282  -0.00923   0.011604 -0.01616 ]  0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562\n",
      " > 398 [-0.03085   0.0352    0.01715   0.06046   0.003223]  0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992\n",
      " > 399 [-0.02223   0.02255   0.009964  0.02895   0.05035 ]  0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32\n",
      " > 400 [-0.004738  0.00547  -0.02327   0.03784   0.04803 ]  0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion\n",
      " > 401 [-0.002249  0.0241   -0.00449   0.02641   0.0613  ]  64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an\n",
      " > 402 [-0.02507  0.05295 -0.02084  0.02292  0.0757 ]  threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe\n",
      " > 403 [-0.02257  0.02122 -0.01738  0.02258  0.05365]  order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains\n",
      " > 404 [-0.0263   0.02275 -0.0283   0.0664   0.00385]  that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making\n",
      " > 405 [-0.0244   0.01155 -0.01846  0.0614   0.02219]  contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result\n",
      " > 406 [-0.009     0.011734 -0.006687  0.03558   0.0472  ]  making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the\n",
      " > 407 [-0.0121    0.0383    0.007202  0.0241    0.03427 ]  in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The\n",
      " > 408 [-0.0118   0.0179   0.02116  0.01788  0.0388 ]  the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871\n",
      " > 409 [-0.05045   0.01854  -0.015144  0.00886   0.066   ]  The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs\n",
      " > 410 [-0.02867    0.0228     0.0003724  0.0634     0.01358  ]  0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes\n",
      " > 411 [-0.02614  0.01162 -0.0446   0.10913 -0.03662]  graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased\n",
      " > 412 [-0.05127  0.03293 -0.0527   0.1082  -0.00762]  nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions\n",
      " > 413 [-0.04852  0.03995 -0.00805  0.09265  0.03955]  co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA\n",
      " > 414 [-0.02565  0.0369  -0.0233   0.06793  0.0376 ]  precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement\n",
      " > 415 [-0.04697  0.01564 -0.0325   0.07056  0.05432]  LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked\n",
      " > 416 [-0.0393   0.01892 -0.0291   0.04977  0.06146]  improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181\n",
      " > 417 [-0.006996  0.04288   0.002956  0.003136  0.07025 ]  remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808\n",
      " > 418 [0.01996 0.0541  0.0688  0.0374  0.05222]  0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE\n",
      " > 419 [0.002682 0.03093  0.04718  0.01732  0.06064 ]  0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ—\n",
      " > 420 [-0.00342   0.012115 -0.00778   0.01158   0.03026 ]  TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the\n",
      " > 421 [-0.009445  0.00917  -0.01974   0.02577   0.0277  ]  âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs,\n",
      " > 422 [-0.02567  0.01295 -0.05276  0.0266   0.02625]  the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity\n",
      " > 423 [-0.04584  0.0273  -0.0644   0.04678  0.01094]  vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj)\n",
      " > 424 [-0.05856  0.03537 -0.055    0.02911  0.01979]  affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original\n",
      " > 425 [-0.03662  0.03253 -0.02782  0.052    0.02502]  Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most\n",
      " > 426 [-0.0196   0.04355 -0.0133   0.03726  0.0615 ]  original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the\n",
      " > 427 [-0.02522  0.036   -0.05115  0.02747  0.03075]  most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and\n",
      " > 428 [-0.03812  0.04465 -0.0931   0.08014  0.02039]  the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster\n",
      " > 429 [-0.0188    0.02159  -0.014435  0.05664   0.07416 ]  and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and\n",
      " > 430 [-0.01466  0.03876  0.02835  0.01405  0.04037]  cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient\n",
      " > 431 [ 0.002928  0.04877  -0.01032   0.0442    0.01994 ]  and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these\n",
      " > 432 [-0.01467  0.0346  -0.00515  0.02536  0.04956]  coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network\n",
      " > 433 [-0.0598   0.015   -0.02884  0.02684  0.06226]  two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering.\n",
      " > 434 [-0.10205  -0.002577 -0.03842   0.02167   0.0727  ]  Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional\n",
      " > 435 [-0.06976  -0.004925 -0.0479    0.03363   0.06995 ]  clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial\n",
      " > 436 [-0.03943   -0.0002513 -0.05435    0.0274     0.05777  ]  dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories.\n",
      " > 437 [-0.01797   0.003899 -0.0593    0.0412    0.10223 ]  initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip-\n",
      " > 438 [-0.05734  0.01379 -0.0594   0.0357   0.0926 ]  categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph. 21\n",
      " > 439 [-0.05966  0.0251  -0.03885  0.0747   0.03305] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering\n",
      " > 440 [-0.03027  0.03235 -0.0667   0.0965   0.0402 ]  size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded\n",
      " > 441 [-0.02742  0.03827 -0.08075  0.0866   0.03787]  clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph\n",
      " > 442 [-0.01859  0.01143 -0.06274  0.07745  0.03378]  theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks,\n",
      " > 443 [-0.02417   0.007774 -0.06183   0.09393   0.01334 ]  graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology\n",
      " > 444 [-0.0415  -0.01338 -0.0669   0.0969   0.0441 ]  networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly\n",
      " > 445 [-0.05252  0.0063  -0.04868  0.06238  0.0397 ]  methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal\n",
      " > 446 [-0.0278   0.02759 -0.0359   0.0568   0.02019]  mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are\n",
      " > 447 [-0.03029  0.03308 -0.03384  0.05762  0.02544]  sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33]\n",
      " > 448 [-0.04932  0.02055 -0.02606  0.0725   0.0336 ]  often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1\n",
      " > 449 [-0.0318   0.01287 -0.0486   0.08167  0.0484 ]  [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs\n",
      " > 450 [-0.02176  -0.009705 -0.04297   0.0732    0.0746  ]  arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large\n",
      " > 451 [-0.02028  -0.002424 -0.05496   0.0647    0.0517  ]  vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse,\n",
      " > 452 [-0.007168 -0.02188  -0.0664    0.02844   0.02696 ]  large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the\n",
      " > 453 [-0.01128 -0.0268  -0.04025  0.03183  0.0326 ]  AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the\n",
      " > 454 [-0.02255   -0.0001911 -0.0313     0.06088    0.06744  ]  approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote\n",
      " > 455 [-0.02684 -0.01135 -0.04037  0.03214  0.04706]  the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of\n",
      " > 456 [-0.02287   0.04373  -0.03165   0.002956 -0.009125]  denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in\n",
      " > 457 [-0.01189  0.03098 -0.03256  0.02235 -0.01729]  of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector\n",
      " > 458 [-0.03397   0.0401   -0.013695  0.008484 -0.01813 ]  Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi).\n",
      " > 459 [-0.0491   0.0276  -0.02985  0.0855   0.0346 ]  vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its\n",
      " > 460 [-0.05157  0.04013 -0.03001  0.0852   0.05164]  d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the\n",
      " > 461 [-0.0469   0.02794 -0.0662   0.05612  0.0483 ]  its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$\n",
      " > 462 [-0.05313  0.02457 -0.05472  0.02692  0.0214 ]  the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´\n",
      " > 463 [-0.02917  0.01516 -0.0625   0.06354  0.03674]  ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity\n",
      " > 464 [-0.03006  -0.004913 -0.06015   0.0938    0.0439  ]  â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at\n",
      " > 465 [-0.02887  0.05466 -0.0716   0.07837  0.01717]  similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below\n",
      " > 466 [-0.03784  0.04828 -0.05026  0.07227 -0.01701]  at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š)\n",
      " > 467 [-0.04395  0.03943 -0.0389   0.0643   0.02306]  process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value\n",
      " > 468 [-0.04697  0.05228 -0.02882  0.0556   0.0486 ]  ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other\n",
      " > 469 [-0.01195  0.02664 -0.05862  0.0762   0.06216]  value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the\n",
      " > 470 [ 0.005646  0.01817  -0.05148   0.08203   0.0653  ]  other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the\n",
      " > 471 [-0.0283   0.04105 -0.0627   0.04     0.058  ]  the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD\n",
      " > 472 [-0.01221  0.02496 -0.0627   0.02911  0.0278 ]  the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2\n",
      " > 473 [-0.009605  0.01955  -0.02663   0.02118   0.04587 ]  value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA. where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely\n",
      " > 474 [-0.01816  0.0432   0.01171  0.0125   0.02866]  ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA. where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed\n",
      " > 475 [-0.01058  0.04642 -0.01155  0.04068  0.03043]  immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time,\n",
      " > 476 [-0.00941  0.01395 -0.0639   0.0351   0.04498]  seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7\n",
      " > 477 [-0.00748   0.007225 -0.0901    0.009026  0.0451  ]  time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector\n",
      " > 478 [ 0.0003533  0.02426   -0.05356    0.02518    0.01823  ]  ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively,\n",
      " > 479 [ 0.01723   0.007515 -0.04684   0.04065   0.02277 ]  vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD\n",
      " > 480 [ 0.01672  0.01758 -0.04974  0.04312  0.0481 ]  respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic\n",
      " > 481 [ 0.00947  0.01802 -0.04382  0.02942  0.01495]  BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges\n",
      " > 482 [ 0.00978  0.01747 -0.02206  0.02821 -0.00983]  algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and\n",
      " > 483 [ 0.001304 -0.00421  -0.03665   0.0376   -0.00608 ]  edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse\n",
      " > 484 [-0.00885 -0.03099 -0.0539   0.0393   0.01552]  âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative\n",
      " > 485 [-0.02486  0.01917 -0.04266  0.04822  0.01233]  GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we\n",
      " > 486 [-0.02458   0.0336   -0.04004   0.0757   -0.010574]  iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’\n",
      " > 487 [ 0.00243   0.02808  -0.0494    0.0518    0.006756]  we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010\n",
      " > 488 [ 0.0128   0.00904 -0.04712  0.02869  0.02682]  Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ\n",
      " > 489 [-0.01027   0.001535 -0.03787   0.04715   0.019   ]  max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the\n",
      " > 490 [-0.01988  0.01398 -0.03006  0.06396  0.01203]  (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus,\n",
      " > 491 [-0.02448  0.0071  -0.07196  0.0875   0.01675]  the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns\n",
      " > 492 [-0.0242    0.010796 -0.0292    0.0559    0.01715 ]  Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17)\n",
      " > 493 [-0.02437  -0.015114 -0.0347    0.0395    0.017   ]  returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all\n",
      " > 494 [-0.0166   -0.001597 -0.0704    0.05112   0.006863]  (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and\n",
      " > 495 [-0.02132   0.01518  -0.07916   0.0694    0.008484]  nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals\n",
      " > 496 [-0.02536  -0.005363 -0.0871    0.0566    0.01723 ]  and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy\n",
      " > 497 [-0.0193    0.006207 -0.0861    0.04764   0.03357 ]  residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast\n",
      " > 498 [-0.01388   0.001596 -0.0824    0.02063   0.0451  ]  greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of\n",
      " > 499 [-0.004044 -0.0203   -0.0657    0.03433   0.02592 ]  contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased\n",
      " > 500 [-0.007   -0.00943 -0.06     0.03247  0.04276]  of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and\n",
      " > 501 [ 0.00787  -0.001864 -0.04294   0.02667   0.0513  ]  increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil\n",
      " > 502 [ 0.01004  0.0073  -0.0243   0.01022  0.04242]  and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq. (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k\n",
      " > 503 [-0.0002254  0.013596  -0.00279   -0.006493   0.01585  ]  unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq. (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B\n",
      " > 504 [-0.004738  0.0446    0.00422   0.01688   0.01743 ]  k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i)\n",
      " > 505 [-0.02351  0.04025 -0.0328   0.0447   0.0357 ]  II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function,\n",
      " > 506 [-0.014404  0.0226   -0.04956   0.0418    0.02934 ]  (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates\n",
      " > 507 [ 0.01883  0.02367 -0.03644  0.02786  0.02722]  function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1\n",
      " > 508 [ 0.00458  0.02213 -0.01607  0.02057  0.02766]  a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed.\n",
      " > 509 [-0.01843    0.03525   -0.05396    0.0001371  0.01265  ]  â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs\n",
      " > 510 [ 0.01256  0.01567 -0.07306  0.04507  0.0232 ]  distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place,\n",
      " > 511 [ 0.021    0.02748 -0.0407   0.0244   0.02992]  constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries\n",
      " > 512 [ 0.000856  0.02127  -0.0337    0.03583   0.02635 ]  place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate\n",
      " > 513 [ 0.006313  0.01819  -0.03326   0.01877   0.04883 ]  entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï\n",
      " > 514 [ 0.01226  0.02478 -0.03555  0.01648  0.05865]  approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines\n",
      " > 515 [-0.02467   0.002172 -0.02625   0.02347   0.04745 ]  â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) .\n",
      " > 516 [-0.0247  -0.01292 -0.0255   0.03262  0.04285]  6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ\n",
      " > 517 [-0.03732  0.03114 -0.0294   0.0602   0.0517 ]  . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51]\n",
      " > 518 [-0.03967  0.02184 -0.03833  0.03998  0.02771]  âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are\n",
      " > 519 [-0.02773 -0.01567 -0.02376  0.05487  0.0425 ]  [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the\n",
      " > 520 [-0.02184  0.01408 -0.03235  0.0851   0.0652 ]  are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis,\n",
      " > 521 [-0.01683   0.002851 -0.03412   0.07367   0.03622 ]  the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the\n",
      " > 522 [-0.03436   -0.01142   -0.01988    0.06195   -0.0005455]  analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively.\n",
      " > 523 [-0.05267  -0.007153 -0.0242    0.0839   -0.0084  ]  the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n)\n",
      " > 524 [-0.03503 -0.02817 -0.03256  0.07184  0.0113 ]  respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product\n",
      " > 525 [-0.04562 -0.01746 -0.0225   0.0615   0.06616]  ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm\n",
      " > 526 [-0.0364  -0.02051 -0.02644  0.07214  0.05722]  product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms,\n",
      " > 527 [-0.0688  -0.01807 -0.0479   0.04187  0.02458]  p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods\n",
      " > 528 [-0.07886  0.01646 -0.05475  0.07837  0.03552]  all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters.\n",
      " > 529 [-0.04852   0.002876 -0.04016   0.08234   0.01365 ]  methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid\n",
      " > 530 [-0.049   -0.01204 -0.03812  0.065    0.02124]  clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters\n",
      " > 531 [-0.02621 -0.02309 -0.05795  0.02141  0.03693]  grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by\n",
      " > 532 [-0.01924  0.00386 -0.0558   0.04047  0.0707 ]  clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE,\n",
      " > 533 [-0.0241     0.0009365 -0.02228    0.01515    0.09143  ]  by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and\n",
      " > 534 [-0.03986   0.002424 -0.01356   0.03027   0.04285 ]  PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20]\n",
      " > 535 [-0.05518   0.02917  -0.013664 -0.003122  0.0448  ]  SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154\n",
      " > 536 [-0.0252    -0.0003357 -0.003895   0.02888    0.01484  ]  [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN)\n",
      " > 537 [-0.03586  0.00766  0.01378  0.0337   0.05707]  0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2\n",
      " > 538 [-0.02484  0.00765  0.03192  0.0607   0.03023]  (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the\n",
      " > 539 [-0.005768  0.000605  0.0241    0.05914   0.03854 ]  10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance\n",
      " > 540 [-0.02518  -0.007668  0.01883   0.0644    0.04822 ]  the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their\n",
      " > 541 [-0.01035   0.0053   -0.008644  0.0737    0.0627  ]  performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be.\n",
      " > 542 [ 0.00817  0.00686 -0.02745  0.0658   0.05325]  their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes\n",
      " > 543 [-0.03014   -0.0001978 -0.01967    0.03726    0.0358   ]  be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time\n",
      " > 544 [-0.04755  0.01645 -0.0444   0.04562  0.0498 ]  sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M\n",
      " > 545 [-0.04242  0.01454  0.02293  0.02849  0.06155]  (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS)\n",
      " > 546 [-0.02751   0.00396   0.013695  0.03488   0.0871  ]  Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed.\n",
      " > 547 [-0.02112  0.01114 -0.05438  0.0481   0.04102]  obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table\n",
      " > 548 [-0.02267   0.011154 -0.05698   0.0519    0.04715 ]  seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage,\n",
      " > 549 [-0.013275 -0.0185   -0.0678    0.0647    0.0627  ]  (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art\n",
      " > 550 [-0.01752 -0.00918 -0.05182  0.0716   0.0693 ]  all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage)\n",
      " > 551 [-0.01358  0.00952 -0.02658  0.01539  0.0885 ]  state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary,\n",
      " > 552 [-0.02591 -0.00826 -0.03824  0.0442   0.0614 ]  stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang\n",
      " > 553 [-0.02606  -0.000682 -0.02864   0.04483   0.0437  ]  summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the\n",
      " > 554 [-0.02849  0.00523  0.00381  0.02715  0.01982]  Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)).\n",
      " > 555 [-0.02013   0.002918 -0.03041   0.0554    0.02531 ]  the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local\n",
      " > 556 [-0.03607   0.001306 -0.0537    0.0636    0.03934 ]  (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15]\n",
      " > 557 [-0.05045 -0.0311  -0.05142  0.0749   0.0661 ]  local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on\n",
      " > 558 [-0.02681 -0.02615 -0.05453  0.06757  0.02815]  [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly\n",
      " > 559 [-0.03192   0.003494 -0.08636   0.0676    0.009575]  running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities,\n",
      " > 560 [-0.04648  0.0212  -0.08     0.08264  0.02356]  costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching\n",
      " > 561 [-0.0716   0.0415  -0.04184  0.06586  0.02849]  communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints\n",
      " > 562 [-0.0472   0.02084 -0.0471   0.0667   0.03323]  the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA,\n",
      " > 563 [-0.03665  0.02205 -0.08264  0.0521   0.03058]  constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17\n",
      " > 564 [-0.0352   0.00645 -0.0412   0.0633   0.03537]  LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference\n",
      " > 565 [-0.0629   0.00986 -0.03995  0.06476 -0.01278]  17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities\n",
      " > 566 [-0.01064   0.004322 -0.0627    0.1003    0.003572]  (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz,\n",
      " > 567 [ 0.005173 -0.00956  -0.05167   0.1133    0.001737]  in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings\n",
      " > 568 [-0.05157 -0.02473 -0.03622  0.0768   0.04828]  Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &\n",
      " > 569 [-0.0366  -0.00545 -0.0429   0.09674  0.06122]  Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€\n",
      " > 570 [-0.05173  0.00569 -0.0618   0.08746  0.0294 ]  & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J.\n",
      " > 571 [-0.0711   0.01162 -0.07025  0.0819   0.0085 ]  partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical\n",
      " > 572 [-0.0651   -0.010735 -0.05707   0.0885    0.00798 ]  J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters\n",
      " > 573 [-0.01701  -0.018    -0.01923   0.07764   0.012314]  Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference\n",
      " > 574 [-0.02473  0.0273  -0.04297  0.0748   0.0201 ]  Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter:\n",
      " > 575 [-0.0437   0.02586 -0.0528   0.04987  0.0474 ]  on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015.\n",
      " > 576 [-0.05173  0.01816 -0.0635   0.05212  0.02509]  filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and\n",
      " > 577 [-0.014465  -0.00704   -0.0523     0.04837   -0.0008817]  2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems\n",
      " > 578 [-0.02061 -0.0343  -0.0775   0.04132  0.01537]  and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€\n",
      " > 579 [-0.02547 -0.02333 -0.059    0.02705 -0.00436]  systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp. 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and\n",
      " > 580 [-0.01602   0.003695 -0.03986   0.03806  -0.0074  ]  graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp. 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead,\n",
      " > 581 [-0.03778  -0.00675  -0.04526   0.04578  -0.003376]  and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226.\n",
      " > 582 [-0.0601  -0.02956 -0.03     0.0396  -0.0315 ]  Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational\n",
      " > 583 [-0.05414 -0.03015 -0.0382   0.03558 -0.01671]  1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in\n",
      " > 584 [-0.02364 -0.02992 -0.07446  0.02017 -0.02716]  â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge\n",
      " > 585 [-0.0352   -0.0135   -0.04865  -0.001491 -0.00885 ]  in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han,\n",
      " > 586 [-0.03677  -0.00828  -0.02438   0.003557  0.015396]  knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of\n",
      " > 587 [-0.02988 -0.02522 -0.0331   0.00666  0.01023]  â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J.\n",
      " > 588 [-0.0674  -0.02518 -0.05423  0.05182  0.00912]  of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences\n",
      " > 589 [-0.07306  -0.03375  -0.05316   0.06793   0.004814]  J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient\n",
      " > 590 [-0.03867   0.01181  -0.05737   0.066    -0.009796]  Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen,\n",
      " > 591 [-0.04617  -0.003315 -0.05246   0.07794   0.00503 ]  efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms,\n",
      " > 592 [-0.04605   0.000993 -0.065     0.04788  -0.001592]  Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433,\n",
      " > 593 [-0.04614   0.02028  -0.0729    0.04648  -0.005077]  algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient\n",
      " > 594 [-0.03543  0.01839 -0.0756   0.05896 -0.00509]  1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to\n",
      " > 595 [-0.0461   0.00627 -0.07275  0.0211  -0.00427]  efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297.\n",
      " > 596 [-0.04813 -0.01495 -0.0772   0.02568 -0.01211]  to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not\n",
      " > 597 [-0.03125 -0.01955 -0.06696  0.03458 -0.02374]  281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      " > 598 [-0.02336  0.04367 -0.06036  0.04642  0.02379]  available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration.\n",
      " > 599 [-0.02003  0.03802 -0.0681   0.04944  0.01654]  2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V,\n",
      " > 600 [-0.0488   0.06525 -0.0632   0.0649   0.02904]  iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality\n",
      " > 601 [-0.02705  0.05847 -0.0666   0.01762  0.0299 ]  âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at\n",
      " > 602 [-0.03668  0.0722  -0.0727   0.03143  0.0455 ]  inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1\n",
      " > 603 [-0.0342   0.0698  -0.04172  0.0817   0.01544]  at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line\n",
      " > 604 [-0.01686  0.02475 -0.0509   0.04752  0.0316 ]  â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’\n",
      " > 605 [-0.01744  0.00792 -0.03052  0.04916  0.03964]  Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration,\n",
      " > 606 [-0.01452  0.02705 -0.0671   0.04044  0.01472]  multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration\n",
      " > 607 [-0.01857  0.039   -0.066    0.03168  0.03638]  iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0. Therefore, the total amount of greedy operations\n",
      " > 608 [-0.01065  0.0239  -0.05826  0.0217   0.0374 ]  iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and\n",
      " > 609 [-0.01171  0.01031 -0.05878  0.0302   0.02092]  operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then,\n",
      " > 610 [-0.01567   0.01189  -0.03693   0.007343  0.02719 ]  Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes\n",
      " > 611 [-0.01514  0.02126 -0.01083  0.03084  0.0342 ]  Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly,\n",
      " > 612 [-0.04266   0.0637   -0.003775  0.05795   0.04962 ]  completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U\n",
      " > 613 [-0.0455   0.04196 -0.00487  0.0606   0.0506 ]  Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp\n",
      " > 614 [-0.03119   0.04578  -0.011345  0.0689    0.0468  ]  are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma\n",
      " > 615 [-0.010826  0.06366  -0.01581   0.006275  0.03937 ]  exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity\n",
      " > 616 [-0.00518    0.0683    -0.02605    0.0016365  0.0349   ]  Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i\n",
      " > 617 [-0.005226  0.08154  -0.0568    0.02007   0.0354  ]  larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice\n",
      " > 618 [-0.0278   0.05923 -0.05753  0.02925  0.02824]  i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi)\n",
      " > 619 [-0.03912  0.02486 -0.0684   0.04895  0.03044]  that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA\n",
      " > 620 [-0.03336  0.04025 -0.04294  0.04282  0.01953]  ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with\n",
      " > 621 [-0.01272   0.007072 -0.02289  -0.01422   0.03778 ]  ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse\n",
      " > 622 [-0.002945 -0.012344 -0.0565   -0.000802  0.04575 ]  with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0\n",
      " > 623 [-0.0266  -0.01071 -0.03586  0.02258  0.05463]  AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8\n",
      " > 624 [-0.0484    0.02017   0.014534 -0.006626  0.0779  ]  0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with\n",
      " > 625 [-0.04614   0.03662   0.02185   0.004845  0.0625  ]  8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased\n",
      " > 626 [-0.04904   0.03693   0.001612  0.03018   0.0418  ]  with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest\n",
      " > 627 [-0.02574  0.02173 -0.02898  0.0176   0.02745]  increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented\n",
      " > 628 [ 0.007675  0.0109   -0.02034   0.02292   0.04398 ]  manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust\n",
      " > 629 [ 0.012215  -0.0001095 -0.005566   0.02357    0.07697  ]  implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808\n",
      " > 630 [ 0.01374  0.01994 -0.005    0.0221   0.0797 ]  robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster\n",
      " > 631 [-0.03198  0.0703  -0.03128  0.04428  0.05176]  0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best\n",
      " > 632 [-0.06204  0.07275 -0.0408   0.076    0.03882]  within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness\n",
      " > 633 [-0.0501   0.03207 -0.02063  0.03607  0.0427 ]  best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“\n",
      " > 634 [-0.04922  0.0356  -0.00879  0.03117  0.04782]  cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156\n",
      " > 635 [-0.04352    0.03964    0.01617    0.0294    -0.0008626]  Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707\n",
      " > 636 [-0.02359   0.03455  -0.01613  -0.012215 -0.00814 ]  0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86\n",
      " > 637 [-0.03827   0.03452  -0.006973  0.03412  -0.01125 ]  0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992\n",
      " > 638 [-0.00915   0.007397 -0.01952   0.04953   0.02577 ]  0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion\n",
      " > 639 [ 4.852e-05  1.678e-02 -1.459e-02  2.652e-02  5.948e-02]  0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe\n",
      " > 640 [-0.02617  0.02695 -0.02834  0.0408   0.04004]  threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making\n",
      " > 641 [-0.005474  0.010826 -0.0091    0.03418   0.02864 ]  that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the\n",
      " > 642 [-0.0059   0.01227 -0.00563  0.02126  0.03198]  making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871\n",
      " > 643 [-0.02441   0.01845  -0.014336  0.04602   0.02182 ]  the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes\n",
      " > 644 [-0.03607   0.02002  -0.01296   0.0794    0.009186]  0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions\n",
      " > 645 [-0.0365   0.01752 -0.02408  0.0782   0.0147 ]  nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement\n",
      " > 646 [-0.03616  0.02682 -0.04028  0.05698  0.03763]  precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181\n",
      " > 647 [-0.02928  0.01945 -0.02054  0.0376   0.05817]  improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE\n",
      " > 648 [0.002232 0.0231   0.006737 0.02208  0.03244 ]  0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the\n",
      " > 649 [-0.01698  0.00879 -0.0346   0.02632  0.0204 ]  TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity\n",
      " > 650 [-0.02353  0.02052 -0.0664   0.03836  0.02771]  the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original\n",
      " > 651 [-0.0455   0.02916 -0.04477  0.04007  0.0504 ]  affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the\n",
      " > 652 [-0.02524  0.0538  -0.06573  0.0494   0.0361 ]  original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster\n",
      " > 653 [-0.003191  0.01761  -0.01117   0.03934   0.011826]  the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient\n",
      " > 654 [-0.0152    0.03165  -0.001853  0.02158   0.02101 ]  cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network\n",
      " > 655 [-0.0358    0.002813 -0.02524   0.02927   0.05273 ]  coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional\n",
      " > 656 [-0.0739   -0.011505 -0.0425    0.03018   0.0801  ]  Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories.\n",
      " > 657 [-0.03598 -0.01189 -0.0681   0.03314  0.07404]  dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph. 21\n",
      " > 658 [-0.02942  0.03647 -0.05655  0.0718   0.02843] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph\n",
      " > 659 [-0.02104  0.0179  -0.06366  0.0772   0.03207]  clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology\n",
      " > 660 [-0.03204   0.009155 -0.0658    0.0789    0.0235  ]  graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal\n",
      " > 661 [-0.03546  0.0361  -0.0522   0.05545  0.0328 ]  methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33]\n",
      " > 662 [-0.02902  0.01498 -0.05026  0.07605  0.03424]  sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs\n",
      " > 663 [-0.03384  0.01476 -0.05203  0.0787   0.0489 ]  [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse,\n",
      " > 664 [-0.02266  -0.006447 -0.05267   0.05637   0.04486 ]  vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the\n",
      " > 665 [-0.014114 -0.02342  -0.04565   0.0258    0.03415 ]  AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of\n",
      " > 666 [-0.01058  -0.009094 -0.02246   0.0147    0.02625 ]  the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector\n",
      " > 667 [-0.0296    0.02347  -0.014336  0.04834   0.00905 ]  of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its\n",
      " > 668 [-0.04797  0.02493 -0.03625  0.05557  0.04932]  vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$\n",
      " > 669 [-0.0305     0.0015745 -0.06744    0.0655     0.04684  ]  its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity\n",
      " > 670 [-0.0325   0.01289 -0.0584   0.07324  0.03065]  ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below\n",
      " > 671 [-0.0342   0.04065 -0.0382   0.07715  0.035  ]  similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value\n",
      " > 672 [-0.02313  0.02599 -0.03363  0.06274  0.0631 ]  process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the\n",
      " > 673 [-0.02057  0.02647 -0.0489   0.0644   0.06183]  value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD\n",
      " > 674 [-0.002039  0.01671  -0.0423    0.02464   0.03677 ]  the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA. where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely\n",
      " > 675 [-0.007057  0.00962  -0.03098   0.02235   0.04672 ]  value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA. where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time,\n",
      " > 676 [-0.014046  0.01889  -0.0677    0.02298   0.04077 ]  immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector\n",
      " > 677 [ 0.006725  -0.0001826 -0.0535     0.02437    0.04852  ]  time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD\n",
      " > 678 [ 0.002934  0.02394  -0.04184   0.02957   0.02536 ]  vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges\n",
      " > 679 [ 0.00827    0.0009336 -0.03186    0.0324     0.0147   ]  BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse\n",
      " > 680 [-0.011185 -0.002092 -0.03674   0.04355  -0.001942]  edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we\n",
      " > 681 [-0.0196    0.014465 -0.0481    0.0479    0.02812 ]  GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010\n",
      " > 682 [-0.00616   0.0073   -0.03754   0.0487    0.005768]  we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the\n",
      " > 683 [-0.01907 -0.01996 -0.05234  0.05414  0.01752]  max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns\n",
      " > 684 [-0.02292 -0.00097 -0.0506   0.0681   0.018  ]  the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all\n",
      " > 685 [-0.03677  -0.013855 -0.05457   0.06177   0.02065 ]  returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals\n",
      " > 686 [-0.0174    0.00751  -0.0822    0.04684   0.011566]  nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast\n",
      " > 687 [-0.01391  -0.000556 -0.0807    0.0477    0.02834 ]  residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased\n",
      " > 688 [ 0.0172  -0.00445 -0.04474  0.03494  0.046  ]  contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil\n",
      " > 689 [ 0.02313  -0.02167  -0.0352    0.014656  0.04126 ]  increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq. (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B\n",
      " > 690 [ 0.00953   0.0037   -0.0064    0.005493  0.02228 ]  unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq. (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function,\n",
      " > 691 [ 0.00544  0.03014 -0.02414  0.0476   0.0355 ]  II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1\n",
      " > 692 [ 0.01581  0.02023 -0.03674  0.02182  0.0334 ]  function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs\n",
      " > 693 [ 0.00805  0.02275 -0.0557   0.01616  0.02614]  â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries\n",
      " > 694 [ 0.01282  0.02014 -0.04965  0.01636  0.04388]  constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï\n",
      " > 695 [-0.01345  -0.006332 -0.02614   0.00833   0.0428  ]  entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) .\n",
      " > 696 [-0.01312  0.00973 -0.02818  0.02802  0.0554 ]  â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51]\n",
      " > 697 [-0.0259    0.010445 -0.011116  0.05716   0.06775 ]  . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the\n",
      " > 698 [-0.02359  -0.00879  -0.010155  0.0616    0.03836 ]  [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the\n",
      " > 699 [-0.02231  -0.007427 -0.01872   0.0867    0.03015 ]  the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n)\n",
      " > 700 [-0.0393  -0.02032 -0.02208  0.0778   0.04544]  the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm\n",
      " > 701 [-0.04776 -0.0217  -0.0294   0.05484  0.0648 ]  ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods\n",
      " > 702 [-0.05685  -0.005474 -0.04047   0.0704    0.005753]  p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid\n",
      " > 703 [-0.04138 -0.01213 -0.042    0.0816   0.0369 ]  methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by\n",
      " > 704 [-0.03363  -0.014824 -0.03793   0.01894   0.0664  ]  grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and\n",
      " > 705 [-0.04398   0.004215 -0.01747   0.01412   0.082   ]  by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154\n",
      " > 706 [-0.0587    0.01878   0.001186  0.010345  0.06494 ]  SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2\n",
      " > 707 [-0.013824  0.002958  0.02794   0.05222   0.0459  ]  0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance\n",
      " > 708 [-0.004063 -0.00208   0.007507  0.06396   0.05103 ]  10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be.\n",
      " > 709 [-0.02469   0.002707 -0.00873   0.05716   0.05585 ]  performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time\n",
      " > 710 [-0.03308    0.001169   0.0006247  0.0383     0.043    ]  be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS)\n",
      " > 711 [-0.03818   -0.0002458 -0.02254    0.02895    0.04208  ]  (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table\n",
      " > 712 [-0.02298 -0.01546 -0.05704  0.05347  0.0562 ]  obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art\n",
      " > 713 [-0.01481 -0.01796 -0.06506  0.0515   0.06476]  (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary,\n",
      " > 714 [-0.0145   0.01152 -0.03336  0.04095  0.06058]  state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the\n",
      " > 715 [-0.02708   0.002703 -0.02856   0.0539    0.0359  ]  summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local\n",
      " > 716 [-0.03513 -0.02258 -0.04303  0.0514   0.0324 ]  the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on\n",
      " > 717 [-0.04623 -0.02176 -0.075    0.0803   0.03397]  local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities,\n",
      " > 718 [-0.03586  0.02237 -0.0685   0.0703   0.02094]  running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints\n",
      " > 719 [-0.03757  0.02794 -0.06155  0.05753  0.03577]  communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17\n",
      " > 720 [-0.03708  0.00819 -0.05734  0.05527  0.01888]  constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities\n",
      " > 721 [-0.04987   0.002188 -0.06      0.07043  -0.005272]  17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings\n",
      " > 722 [-0.01151  -0.004272 -0.05823   0.12317   0.0267  ]  in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€\n",
      " > 723 [-6.018e-02 -7.576e-05 -6.848e-02  1.012e-01  2.875e-02]  Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical\n",
      " > 724 [-0.05502   0.01145  -0.0632    0.0805    0.004967]  partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference\n",
      " > 725 [-0.0287   0.01053 -0.04565  0.0668   0.03653]  Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015.\n",
      " > 726 [-0.02834  0.01819 -0.05643  0.052    0.0233 ]  on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems\n",
      " > 727 [-0.01166 -0.01622 -0.05563  0.03928 -0.01675]  2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp. 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and\n",
      " > 728 [-0.03534 -0.02077 -0.06042  0.04074 -0.01604]  systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp. 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226.\n",
      " > 729 [-0.04733 -0.00848 -0.04724  0.04132 -0.01917]  and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in\n",
      " > 730 [-0.05386 -0.0227  -0.04117  0.02266 -0.02336]  1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han,\n",
      " > 731 [-0.03014  -0.02061  -0.0532    0.00864  -0.001612]  in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J.\n",
      " > 732 [-0.0478   -0.02757  -0.04395   0.03041   0.001903]  â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient\n",
      " > 733 [-0.06384  -0.01797  -0.05435   0.07367  -0.001074]  J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms,\n",
      " > 734 [-0.03967   0.0145   -0.0679    0.06915  -0.005524]  efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient\n",
      " > 735 [-0.057    0.01828 -0.0742   0.02289 -0.00848]  algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297.\n",
      " > 736 [-0.05057  -0.00953  -0.0786    0.02998  -0.004063]  efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      " > 737 [-0.0386    0.03537  -0.0743    0.0571    0.004887]  281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V,\n",
      " > 738 [-0.01942  0.04526 -0.07654  0.02863  0.02481]  2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at\n",
      " > 739 [-0.02303  0.0689  -0.07214  0.02829  0.03088]  âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line\n",
      " > 740 [-0.01262  0.02962 -0.03717  0.05377  0.0398 ]  at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration,\n",
      " > 741 [-0.02142  0.04153 -0.05753  0.03467  0.03534]  Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0. Therefore, the total amount of greedy operations\n",
      " > 742 [-0.00732  0.01833 -0.05978  0.02863  0.02899]  iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then,\n",
      " > 743 [-0.01324  0.01581 -0.04904  0.0227   0.03827]  operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly,\n",
      " > 744 [-0.0126    0.02475  -0.008965  0.02373   0.03622 ]  Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp\n",
      " > 745 [-0.00823   0.0619   -0.003447  0.0198    0.04584 ]  Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity\n",
      " > 746 [-0.00943   0.0655   -0.04218  -0.006065  0.03726 ]  exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice\n",
      " > 747 [-0.010315  0.07196  -0.05795   0.0268    0.04053 ]  larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA\n",
      " > 748 [-0.02336   0.00968  -0.0351    0.004787  0.03406 ]  that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse\n",
      " > 749 [-0.02225   0.009636 -0.03735   0.02199   0.03192 ]  ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8\n",
      " > 750 [-0.04807   0.002335 -0.03314   0.0342    0.05988 ]  AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased\n",
      " > 751 [-0.01985   0.01695  -0.006363  0.00821   0.0314  ]  8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented\n",
      " > 752 [-0.008125  0.01631  -0.00877   0.01011   0.04208 ]  increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808\n",
      " > 753 [-0.005352  0.02846  -0.0206    0.03372   0.0638  ]  implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best\n",
      " > 754 [-0.0367   0.0522  -0.0195   0.03317  0.0546 ]  0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“\n",
      " > 755 [-0.04343   0.02441  -0.001577  0.03235   0.04578 ]  best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707\n",
      " > 756 [-0.04593   0.02129   0.013504  0.03442  -0.001228]  Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992\n",
      " > 757 [-0.01108  0.01195 -0.02444  0.01935  0.0383 ]  0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe\n",
      " > 758 [-0.005283  0.00889  -0.01726   0.03586   0.0265  ]  0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the\n",
      " > 759 [-0.01651   0.013824 -0.01394   0.0556    0.010345]  that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes\n",
      " > 760 [-0.0283   0.02611 -0.0081   0.05902  0.03143]  the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement\n",
      " > 761 [-0.0408    0.012474 -0.02582   0.05875   0.02295 ]  nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE\n",
      " > 762 [-0.021     0.007195 -0.03108   0.04172   0.02754 ]  improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity\n",
      " > 763 [-0.01717  0.02138 -0.0274   0.03287  0.02696]  TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the\n",
      " > 764 [-0.03815  0.02948 -0.04416  0.03952  0.0257 ]  affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient\n",
      " > 765 [-0.0226   0.00805 -0.02705  0.03705  0.03732]  the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional\n",
      " > 766 [-0.03412   0.003609 -0.02979   0.02853   0.06903 ]  coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph. 21\n",
      " > 767 [-0.0207   0.01086 -0.05365  0.06256  0.03195] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstractâ€”Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs âˆˆ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Termsâ€”local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial\n",
      " > 768 [-0.03604  0.02063 -0.05167  0.08044  0.03049]  NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]â€“[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]â€“[8] on e-commerce platforms, and many others [9]â€“[13]. Canonical solutions for LGC [14]â€“[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]â€“[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the â€œoptimalâ€ local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random\n",
      " > 769 [-0.02351   0.002737 -0.05292   0.0573    0.0361  ]  partial remedy, recent efforts [24]â€“[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]â€“[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]â€“[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and\n",
      " > 770 [-0.01155 -0.00741 -0.04184  0.04993  0.0443 ]  random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n Ã— n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector\n",
      " > 771 [-0.00848  0.02861 -0.05487  0.02985  0.04532]  and 152Ã— speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E âˆˆ V Ã— Vis a set of m edges. For each edge (vi, vj) âˆˆ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) âˆˆ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P viâˆˆC d(vi). supp(âˆ’ â†’x) The support of vector âˆ’ â†’x, i.e., {i : âˆ’ â†’xi Ì¸= 0}. Î± The restart factor in RWR, Î± âˆˆ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). Ï€(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).âˆ’ â†’Ï t, âˆ’ â†’Ï â€² t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, âˆ’ â†’z (i) The TNAM and its i-th row vector (See Eq. (10)). Ïµ, Ïƒ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors âˆ’ â†’z (i) âˆ€vi âˆˆ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by Dâˆ’1A, where Pi,j = 1 d(vi) if (vi, vj) âˆˆ E, otherwise Pi,j = 0. Accordingly, Pâ„“ i,j signifies the probability of a length- â„“ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector âˆ’ â†’x (i), which is the i-th row vector in the node attribute matrix X of G. We assume âˆ’ â†’x (i) is L2-normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Xi,j and âˆ’ â†’x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â·\n",
      " > 772 [-0.03616  0.0038  -0.04688  0.0719   0.04077]  âˆ’ â†’x (i), respectively. The support of vector âˆ’ â†’x (i) is defined as supp(âˆ’ â†’x (i)) ={j : âˆ’ â†’x (i) j Ì¸= 0}, which comprises the indices of non-zero entries in âˆ’ â†’x (i). The volume of a set C of nodes is defined as vol(C) =P viâˆˆC d(vi). Given a length-n vector âˆ’ â†’x , its volume vol(âˆ’ â†’x ) is defined as P iâˆˆsupp(âˆ’ â†’x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(âˆ’ â†’x(i), âˆ’ â†’x(j))qP vâ„“âˆˆV f(âˆ’ â†’x(i), âˆ’ â†’x(â„“)) qP âˆ’ â†’x(â„“)âˆˆV f(âˆ’ â†’x(j), âˆ’ â†’x(â„“)) , (1) where f(Â·, Â·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi âˆˆ Vto be symmetric and normalized to a comparable range ( 0 â‰¤ s(vi, vj) â‰¤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(Â·, Â·), i.e., cosine similarity and exponent cosine similarity [39]. When f(Â·, Â·) 2ð‘£! ð‘£! ð‘£\" ð‘£# ð‘£$ ð‘£% ð‘£& ð‘£' ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 ð‘£! ð‘£# ð‘£\" â‹® ð‘£$ ð‘£( ð‘£\" ð‘£& ð‘£% ð‘£) ð‘£% ð‘£& ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£$ â‹® ð‘£( SNASð’”(ð’—ð’Š,ð’—ð’‹) seed target ð‘£! ð‘£\" ð‘£! ð‘£\" ð‘£# ð‘£% ð‘£& ð‘£$ ð‘£)ð‘£' AttributedGraphð“– ð…(ð’—ð’”,ð’—ð’Š) ð…(ð’—ð’•,ð’—ð’‹) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = âˆ’ â†’x (i) Â· âˆ’ â†’x (j) since âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Then, the SNAS s(vi, vj) can be computed via âˆ’ â†’x(i) Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random\n",
      " > 773 [-0.01613   0.001336 -0.03882   0.08984   0.05942 ]  Â· âˆ’ â†’x(j) qP vâ„“âˆˆV âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“) qP vâ„“âˆˆV âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“) . (2) Analogously, when the exponent cosine similarity is adopted, f(âˆ’ â†’x (i), âˆ’ â†’x (j)) = exp \u0010âˆ’ â†’x (i) Â· âˆ’ â†’x (j)/Î´ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(j)/Î´ \u0001 qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(i) Â· âˆ’ â†’x(â„“)/Î´ \u0001qP vâ„“âˆˆV exp \u0000âˆ’ â†’x(j) Â· âˆ’ â†’x(â„“)/Î´ \u0001, (4) where Î´ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seedâ€™s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs âˆˆ V, for any target node vt âˆˆ V, the BDD âˆ’ â†’Ï t of node pair (vs, vt) is defined by âˆ’ â†’Ï t = X vi,vjâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and Ï€(vx, vy) =Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ Â· Pâ„“ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 âˆ’ Î± probability or navigates to one of its neighbors uniformly at random with probability Î±. In essence, Ï€(vs, vi) (resp. Ï€(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, Ï€(vs, vi) âˆ€vi âˆˆ V (resp. Ï€(vt, vj) âˆ€vj âˆˆ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vectorâˆ’ â†’a âˆˆ Rn, we refer to the below process as an RWR-based graph diffusion: X vxâˆˆV âˆ’ â†’a x Â· Ï€(vx, vy) âˆ€vy âˆˆ V, (7) where âˆ’ â†’a x Â· Ï€(vx, vy) can be interpreted as the amount of mass in âˆ’ â†’a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate\n",
      " > 774 [-0.006855  0.02226  -0.0297    0.05045   0.06238 ]  random walks with restart from seed node vs and target node vt. The BDD âˆ’ â†’Ï t of (vs, vt) in Eq. (5) is therefore ð‘£ð‘  ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£1 ð‘£3 ð‘£2 â‹® ð‘£4 ð‘£ð‘› 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 â‹¯ â‹¯ â‹¯ 0.1 0.1 0.2 ð‘£1 ð‘£2 ð‘£3 ð‘£4 â‹® ð‘£ð‘› seed ð…â€²(ð’—ð’”,ð’—ð’Š) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 â‹¯ 0.1 0.5 0.6 0.1 0.4 â‹¯ 0.1 0.4 0.3 0.1 0.5 â‹¯ 0.2 ð‘£1 ð‘£2 ð‘£4 ð‘£4 ð‘£5 ð‘£1 ð‘£2 ð‘£1 ð‘£4 ð‘£2 â‹® ð‘£5 ð‘£ð‘› âˆ™ ð‘£3ð‘£3 0.48 0.45 0.11 0.45 0 TNAM ð’ ð TNAM ð’ ð“â€² ð†â€² 0 1.0 0 0 0 0 0 ðŸ(ð‘ ) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value âˆ’ â†’Ï t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise âˆ€vi, vj âˆˆ V. The BDD Ï(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector âˆ’ â†’Ï (denoted as âˆ’ â†’Ï â€²), followed by a simple extraction of the nodes with |Cs| largest values in âˆ’ â†’Ï â€² as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of âˆ’ â†’Ï â€². More formally, given a diffusion threshold Ïµ, we aim to estimate a BDD vector âˆ’ â†’Ï â€² such that both its output volume vol(âˆ’ â†’Ï â€²) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/Ïµ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD âˆ’ â†’Ï in Eq. (5) requires the RWR scores Ï€(vs, vi) of all intermediate nodes vi âˆˆ Vw.r.t. the seed vs, the RWR scores Ï€(vt, vj) of all intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA. where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter\n",
      " > 775 [-0.01619  0.0236  -0.05777  0.03223  0.05167]  intermediate nodes vj âˆˆ V w.r.t. all possible target nodes vt âˆˆ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) âˆˆ V Ã— V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of âˆ’ â†’Ï particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for âˆ’ â†’Ï â€². III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., Ï€(vt, vj) Â· d(vt) = Ï€(vj, vt) Â· d(vj)), we can rewrite the BDD value âˆ’ â†’Ï t of any target node vt âˆˆ Vw.r.t. the seed node vs as âˆ’ â†’Ï t = 1 d(vt) X viâˆˆV âˆ’ â†’Ï•i Â· Ï€(vi, vt), (8) 3ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 âˆ™ âœ“ âœ“âœ“âœ“ð ð’ ð“â€² Step 2: Estimate RWR ð… ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 3: Compute ð“â€² ð‘£3 ð‘£1 ð‘£5 ð‘£2 ð‘£7 ð‘£8 ð‘£4 ð‘£6 Step 4: Estimate BDD ð† RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð†â€² ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’ð‘¿ âˆ€ð‘£ð‘–,ð‘£ð‘— âˆˆ ð’± ð‘  ð‘£ð‘—,ð‘£ð‘– = ð’›(ð‘—) âˆ™ð’›(ð‘–) ð’‡=e-cosine Orthogonal Random Features Reduce attribute dimension ð’… to ð’Œ ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð‘£1 ð‘£2 ð‘£3 ð‘£4 ð‘£5 ð‘£6 ð‘£7 ð‘£8 ð’Œ-SVD ð’‡=cosine ð…â€² RWR-based Graph Diffusion Fig. 3: Overview of LACA. where âˆ’ â†’Ï• is called the RWR-SNAS vector w.r.t. vs and âˆ’ â†’Ï•i = X vjâˆˆV Ï€(vs, vj) Â· s(vj, vi) Â· d(vi). (9) Intuitively, if the RWR-SNAS vector âˆ’ â†’Ï• is at hand, Eq. (8) implies that an approximate BDD vector âˆ’ â†’Ï â€² can be obtained via the RWR-based graph diffusion (see Eq. (7)) of âˆ’ â†’Ï• over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector âˆ’ â†’Ï• in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k â‰ª d and is a constant) vectors, i.e., s(vj, vi) =âˆ’ â†’z (j) Â· âˆ’ â†’z (i), (10) where Z âˆˆ RnÃ—k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design\n",
      " > 776 [ 0.005512  0.01279  -0.03378   0.01965   0.03967 ]  (hereafter TNAM). In doing so, the RWR-SNAS vector âˆ’ â†’Ï• in Eq. (9) can be reformulated as âˆ’ â†’Ï•i = âˆ’ â†’Ï€ Â· Z Â· âˆ’ â†’z (i) Â· d(vi), (11) where âˆ’ â†’Ï€ denotes the RWR vector w.r.t. seed node vs, i.e.,âˆ’ â†’Ï€ i = Ï€(vs, vi) âˆ€vi âˆˆ V. Given an estimation âˆ’ â†’Ï€ â€² of âˆ’ â†’Ï€ , the term âˆ’ â†’Ï€ Â· Z in Eq. (11) can be approximated by âˆ’ â†’Ïˆ = X iâˆˆsupp(âˆ’ â†’Ï€â€²) âˆ’ â†’Ï€ â€² i Â· âˆ’ â†’z (i) âˆˆ Rk, (12) and accordingly, we can estimate âˆ’ â†’Ï• by âˆ’ â†’Ï•â€² i = âˆ’ â†’Ïˆ Â· âˆ’ â†’z (i) Â· d(vi) âˆ€vi âˆˆ {vi|i âˆˆ supp(âˆ’ â†’Ï€ â€²)}. (13) Note that âˆ’ â†’Ïˆ is shared by the computations of all possibleâˆ’ â†’Ï•â€² i. If we can calculate an approximate RWR vector âˆ’ â†’Ï€ â€² with support size (i.e., the number of non-zero entries) supp(âˆ’ â†’Ï€ â€²) = O(1/Ïµ) in O(1/Ïµ) time, the construction times and support sizes of âˆ’ â†’Ïˆ and âˆ’ â†’Ï•â€² are also bounded by O(1/Ïµ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt âˆˆ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector âˆ’ â†’Ïˆ based on their RWR scores in âˆ’ â†’Ï€ â€², (ii) construction of the RWR-SNAS vector âˆ’ â†’Ï•â€² by Eq. (13), and (iii) RWR- based graph diffusion of âˆ’ â†’Ï•â€² over G to get âˆ’ â†’Ï â€². ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.4 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 0.08 ð‘£4 ð‘£6 ð‘£3 ð‘£5 ð‘£2 ð‘£1ð‘£7 ð‘£10ð‘£9ð‘£8 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 ð’’ð‘– ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.08 0 0 0 0.08 ð’—ðŸ ð’—ðŸ ð’—ðŸ‘ ð’—ðŸ’ ð’—ðŸ“ 0.24 ð’’ð‘– ð’’ð‘– ð’“ð‘– ð’“ð‘– ð’“ð‘–0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with Î± = 0.8 and Ïµ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector âˆ’ â†’Ï€ â€², RWR-SNAS vector âˆ’ â†’Ï•â€², and approximate BDD vector âˆ’ â†’Ï â€². Since âˆ’ â†’Ï•â€² can be properly computed using Eq. (13), our main tasks are to construct Z, âˆ’ â†’Ï€ â€², and âˆ’ â†’Ï â€². Let âˆ’ â†’1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score âˆ’ â†’Ï€ t of any node vt âˆˆ Vcan be represented as âˆ’ â†’Ï€ t = X viâˆˆV âˆ’ â†’1 (s) i Â· Ï€(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’\n",
      " > 777 [ 0.01132   0.000563 -0.02072   0.04257   0.02138 ]  a unified graph diffusion algorithm that obtains estimations âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² by diffusing âˆ’ â†’1 (s) andâˆ’ â†’Ï•â€² over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/Ïµ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs âˆˆ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR âˆ’ â†’Ï€ â€² using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with âˆ’ â†’1 (s) as input, while Step 2 aggregates their TNAM vectors as âˆ’ â†’Ïˆ (Eq. (12)) to build the RWR-SNAS vector âˆ’ â†’Ï•â€² (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with âˆ’ â†’Ï•â€² over G to derive the approximate BDD vector âˆ’ â†’Ï â€² (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector âˆ’ â†’Ï€ and BDD vector âˆ’ â†’Ï in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² as diffusing an input vector âˆ’ â†’f along edges over G to get âˆ’ â†’q satisfying âˆ€vt âˆˆ V, 0 â‰¤ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ âˆ’ â†’q t â‰¤ Ïµ Â· d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor Î±, diffusion threshold Ïµ, initial vector âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; 2 while true do 3 Compute sparse vector âˆ’ â†’Î³ ; â–· Eq. (15) 4 if âˆ’ â†’Î³ is 0 then break; 5 âˆ’ â†’r â† âˆ’ â†’r âˆ’ âˆ’ â†’Î³ ; 6 Update âˆ’ â†’q and âˆ’ â†’Î³ ; â–· Eq. (16) 7 âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector\n",
      " > 778 [-0.006996  0.001404 -0.0325    0.03503   0.006874]  âˆ’ â†’r â† âˆ’ â†’r + âˆ’ â†’Î³ ; 8 return âˆ’ â†’q ; which is âˆ’ â†’Ï€ â€² and âˆ’ â†’Ï â€² when âˆ’ â†’f is set to âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€², respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for â€œdiffusingâ€ any initial non-negative vector âˆ’ â†’f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector âˆ’ â†’r as âˆ’ â†’f and a reserve vector âˆ’ â†’q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in âˆ’ â†’r , which continuously transfers the residuals into the reserve vector âˆ’ â†’q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in âˆ’ â†’r whose corresponding âˆ’ â†’r i/d(vi) values are equal to or beyond ÏµÂ·âˆ¥âˆ’ â†’f âˆ¥1 and move them to a temporary vector âˆ’ â†’Î³ for subsequent diffusion. More precisely, we obtain a sparse vector âˆ’ â†’Î³ at Line 3 as follows: âˆ’ â†’Î³ i = (âˆ’ â†’r i if (âˆ’ â†’r Dâˆ’1)i = âˆ’ â†’r i d(vi) â‰¥ Ïµ, 0 otherwise. (15) Next, we update residual vector âˆ’ â†’r as âˆ’ â†’r âˆ’ âˆ’ â†’Î³ such thatâˆ’ â†’r contain the residuals below the threshold and then convert (1âˆ’Î±) portion of residuals in âˆ’ â†’Î³ into reserve vector âˆ’ â†’q (Lines 5-6). âˆ€vi âˆˆ V, its remaining Î± fraction of residual in âˆ’ â†’Î³ is later evenly scattered to its out-neighbors. That is, each node vj âˆˆ Vreceives a total of P viâˆˆN(vj) Î± Â· âˆ’ â†’Î³ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24\n",
      " > 779 [-0.00012  -0.003859 -0.04443   0.04422   0.02396 ]  multiplication as follows (Line 6): âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’Î³ , âˆ’ â†’Î³ â† Î±âˆ’ â†’Î³ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (a) PubMed (Î± = 0.8, Ïµ= 10âˆ’5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum âˆ¥âˆ’ â†’r âˆ¥1 (b) ArXiv (Î± = 0.8, Ïµ= 10âˆ’7) Fig. 5: Greedy v.s. Non-greedy. These residuals in âˆ’ â†’Î³ will be added back to âˆ’ â†’r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector âˆ’ â†’f , restart factor Î±, and diffusion threshold Ïµ, Algo. 1 outputs a diffused vector âˆ’ â†’q satisfying Eq. (14) using O \u0010 max n |supp(âˆ’ â†’f )|, âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting âˆ’ â†’Î³ turns to be a zero vector (Line 4), i.e., all non-converted residuals in âˆ’ â†’r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns âˆ’ â†’q as the diffused vector ofâˆ’ â†’f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of âˆ’ â†’f and 1 (1âˆ’Î±)Ïµ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR âˆ’ â†’Ï€ and âˆ’ â†’Ï if âˆ’ â†’1 (s) and âˆ’ â†’Ï•â€² are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector âˆ’ â†’f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor Î± = 0.8 and diffusion threshold Ïµ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in âˆ’ â†’f , while the reserves of all nodes are 0 as in Fig. 4(a). Since âˆ’ â†’r 1 d(v1) = 0.4/4 â‰¥ Ïµ and âˆ’ â†’r 2 d(v2) = 0.6/3 â‰¥ Ïµ, GreedyDiffuse converts the 1 âˆ’ Î± = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4Î± d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6Î± d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of\n",
      " > 780 [-0.015274  0.01694  -0.06616   0.06323   0.03073 ]  d(v4) = 0.12 â‰¥ Ïµ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 âˆ’ Î±) = 0.048 will be converted into their reserves, and a residual of 0.24Î± d(v3) = 0.24Î± d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than Ïµ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, Î±, Ïƒ, Ïµ, âˆ’ â†’f Output: Diffused vector âˆ’ â†’q 1 âˆ’ â†’r â† âˆ’ â†’f ; âˆ’ â†’q â† 0; Ctot â† 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(âˆ’ â†’Î³ )| |supp(âˆ’ â†’r )| > Ïƒand Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ then 5 Ctot â† Ctot + vol(âˆ’ â†’r ); 6 Update âˆ’ â†’q and âˆ’ â†’r ; â–· Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return âˆ’ â†’q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum âˆ¥âˆ’ â†’r âˆ¥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). âˆ’ â†’q â† âˆ’ â†’q + (1âˆ’ Î±)âˆ’ â†’r , âˆ’ â†’r â† Î±âˆ’ â†’r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2Ã— more iterations to terminate and near 4Ã— more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e.,\n",
      " > 781 [ 0.005848 -0.0121   -0.04214   0.0391    0.02469 ]  of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in âˆ’ â†’q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 âˆ’ Î± = 20% of residuals into reserves in each iteration, making âˆ¥âˆ’ â†’r âˆ¥1 decrease rapidly after a few iterations, e.g., âˆ¥âˆ’ â†’r âˆ¥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in Î±âˆ’ â†’r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (Ïµ = 10âˆ’7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter Ïƒ âˆˆ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating âˆ’ â†’Î³ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq. (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function\n",
      " > 782 [ 0.01723 -0.03723 -0.01596  0.01997  0.06186]  i.e., |supp(âˆ’ â†’Î³ )|/|supp(âˆ’ â†’r )|, outstrips Ïƒ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(âˆ’ â†’r ), is still less than the total cost using GreedyDiffuse, i.e., âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that the smaller Ïƒ is, the more non-greedy operations will be conducted. When Ïƒ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of âˆ’ â†’r , i.e., the amount of work needed in computing Î±âˆ’ â†’r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector âˆ’ â†’q such that Eq. (14) holds âˆ€vt âˆˆ Vusing O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector âˆ’ â†’q returned by Algo. 2 is bounded, solely dependent on âˆ¥âˆ’ â†’f âˆ¥1, Î±, and Ïµ. Lemma IV .3.Let âˆ’ â†’f and âˆ’ â†’q be the input and output of Algo. 2, respectively. Then, |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , where 1 â‰¤ Î² â‰¤ 2. In particular, when Ïƒ â‰¥ 1, Î² = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) intoâˆ’ â†’z (i) Â· âˆ’ â†’z (j) (Eq. (10)), the key is to find length- k vectorsâˆ’ â†’y (i) âˆ€vi âˆˆ V(i.e., an n Ã— k matrix Y ) such that f(vi, vj) =âˆ’ â†’y (i) Â· âˆ’ â†’y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = âˆ’ â†’y (i)Â·âˆ’ â†’y (j) âˆšâˆ’ â†’y (i)Â·âˆ’ â†’y âˆ—Â· âˆšâˆ’ â†’y (j)Â·âˆ’ â†’y âˆ— = âˆ’ â†’z (i) Â· âˆ’ â†’z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(Â·, Â·), and dimension k Output: The TNAM Z 1 U, Î›, V â† k-SVD(X); 2 switch f(Â·, Â·) do 3 case cosine similarity function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by\n",
      " > 783 [ 0.01924   0.01575  -0.02672   0.010506  0.04697 ]  function do 4 Y â† UÎ›; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G âˆ¼ N(0, 1)kÃ—k; 7 Q â† QRDecomposition(G); 8 Sample diagonal matrix Î£ii âˆ¼ Ï‡(k) âˆ€i âˆˆ {1, . . . , k}; 9 Compute Y ; â–· Eq. (19) 10 Compute âˆ’ â†’y âˆ— ; â–· Eq. (18) 11 for vi âˆˆ Vdo Compute âˆ’ â†’z (i); â–· Eq. (18) 12 return Z âˆ’ â†’y âˆ— = P vâ„“âˆˆV âˆ’ â†’y (â„“) and âˆ’ â†’z (i) = âˆ’ â†’y (i)/ pâˆ’ â†’y (i) Â· âˆ’ â†’y âˆ— . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product âˆ’ â†’x (i) Â· âˆ’ â†’x (j) âˆ€vi, vj âˆˆ V. Let U âˆˆ RnÃ—k and diagonal matrix Î› âˆˆ RkÃ—k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UÎ› can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let Î»k+1 be the (k+1)-th largest singular value of X. Then, (UÎ›) Â· (UÎ›)âŠ¤ âˆ’ XXâŠ¤ 2 â‰¤ Î»k+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors âˆ’ â†’y (i) and âˆ’ â†’z (i) for each node vi âˆˆ Vbased on the input node attribute matrix X, the metric functions f(Â·, Â·) in Section II-B, and a small integer k â‰ª d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Î› (Line 1). UÎ› then substitutes X for subsequent generation of vectors âˆ’ â†’y (i) âˆ€vi âˆˆ V. When f(Â·, Â·) is the cosine similarity function, it is straightforward to get Y = UÎ› (Lines 3-4). However, when f(Â·, Â·) is the exponential cosine similarity function (Eq. (3)), constructing âˆ’ â†’y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V Ã— Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators âˆ’ â†’y (i) âˆ€vi âˆˆ Vsuch thatâˆ’ â†’y (i) Â· âˆ’ â†’y (j) â‰ˆ f(vi, vj) âˆ€vi, vj âˆˆ V. More concretely, Algo. 3 first randomly generates a k Ã— k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support\n",
      " > 784 [-0.0008082  0.03433   -0.04376    0.02092    0.03726  ]  a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q âˆˆ RkÃ—k [44]. Algo. 3 further builds a kÃ—k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor Î±, parameter Ïƒ, diffusion threshold Ïµ Output: Approximate BDD vector âˆ’ â†’Ï â€² /* Step 1: Estimate RWR vector âˆ’ â†’Ï€ â€² */ 1 Create a unit vector âˆ’ â†’1 (s) âˆˆ Rn; 2 âˆ’ â†’Ï€ â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, Ïµ,âˆ’ â†’1 (s) ); /* Step 2: Compute RWR-SNAS vector âˆ’ â†’Ï•â€² */ 3 Compute âˆ’ â†’Ïˆ; â–· Eq. (12) 4 for i âˆˆ supp(âˆ’ â†’Ï€ â€²) do Compute âˆ’ â†’Ï•â€² i; â–· Eq. (13) /* Step 3: Estimate BDD vector âˆ’ â†’Ï â€² */ 5 âˆ’ â†’Ï â€² â† AdaptiveDiffuse(P, Î±, Ïƒ, ÏµÂ· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, âˆ’ â†’Ï•â€²) 6 for i âˆˆ supp(âˆ’ â†’Ï â€²) do âˆ’ â†’Ï â€² i â† âˆ’ â†’Ï â€² i d(vi) 7 return âˆ’ â†’Ï â€² Î£ with diagonal entries sampled i.i.d. from the Ï‡-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of Î£Q and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y â† q 2 exp(1/Î´) k Â· sin( bY ) âˆ¥ cos( bY ), (19) where bY â† 1 Î´ UÎ›Î£Q and âˆ¥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates thatâˆ’ â†’y (i) Â·âˆ’ â†’y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) âˆˆ V Ã— V. Theorem V .2. E \u0002âˆ’ â†’y (i) Â· âˆ’ â†’y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector âˆ’ â†’y (i) for each node vi âˆˆ V, Algo. 3 first computes the sum of these vectors, i.e., âˆ’ â†’y âˆ— , and finally constructs âˆ’ â†’z (i) for each node vi âˆˆ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold Ïµ, and parameters Î± and Ïƒ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector âˆ’ â†’1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear\n",
      " > 785 [-0.010826  0.00999  -0.02194   0.01848   0.0627  ]  support size |supp(âˆ’ â†’Ï€ â€²)| of the returned RWR vector âˆ’ â†’Ï€ â€² is bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Next, âˆ’ â†’Ï€ â€² is used for producing vector âˆ’ â†’Ïˆ â† âˆ’ â†’Ï€ â€² Â· Z at Line 3 and subsequently the RWR-SNAS vector âˆ’ â†’Ï•â€² at Line 4. In particular, in lieu of computing âˆ’ â†’Ï•â€² i for each node vi âˆˆ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector âˆ’ â†’Ï€ â€², i.e., i âˆˆ supp(âˆ’ â†’Ï â€²), whereby the number of non-zero entries in âˆ’ â†’Ï•â€² can be guaranteed to be bounded by 7O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector âˆ’ â†’Ï•â€² over graph G using the AdaptiveDiffuse with diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1, and parameters Î± and Ïƒ (Line 5). Let âˆ’ â†’Ï â€² be the output of the above diffusion process. LACA then gives âˆ’ â†’Ï â€² a final touch by dividing each non-zero entryâˆ’ â†’Ï â€² i in âˆ’ â†’Ï â€² by 1 d(vi) (Line 6) and returns âˆ’ â†’Ï â€² as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) âˆ€vi, vj âˆˆ Vsatisfy Eq. (10), âˆ’ â†’Ï â€² output by Algo. 4 ensures âˆ€vt âˆˆ V 0 â‰¤ âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ ï£« ï£­1 + X viâˆˆV d(vi) Â· max vjâˆˆV s(vi, vj) ï£¶ ï£¸ Â· Ïµ. Volume and Complexity Analysis. Recall that âˆ’ â†’Ï â€² is obtained by calling AdaptiveDiffuse with âˆ’ â†’f = âˆ’ â†’Ï•â€² and diffusion threshold Ïµ Â· âˆ¥âˆ’ â†’Ï•â€²âˆ¥1. Both the support size |supp(âˆ’ â†’Ï â€²)| and volume vol(âˆ’ â†’Ï â€²) of âˆ’ â†’Ï â€² are therefore bounded by O \u0010 1 (1âˆ’Î±)Ïµ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector âˆ’ â†’1 (s) , i.e., âˆ¥âˆ’ â†’1 (s) âˆ¥1 = 1, entailing O \u0010 1 (1âˆ’Î±)Ïµ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector âˆ’ â†’Ï€ â€², i.e., |supp(âˆ’ â†’Ï€ â€²)|, and the dimension k of Z, which is O \u0010 k (1âˆ’Î±)Ïµ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(âˆ’ â†’Ï•â€²) , âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 (1âˆ’Î±)ÏµÂ·âˆ¥âˆ’ â†’Ï•â€²âˆ¥1 o\u0011 = O \u0010 1 (1âˆ’Î±)Ïµ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1âˆ’Î±)Ïµ \u0011 , which equals O (1/Ïµ) when Î± and k are regarded as constants and is linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster\n",
      " > 786 [-0.02173  -0.007797 -0.01536   0.0699    0.0577  ]  linear to the volume of its output âˆ’ â†’Ï â€². C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and Hâ—¦ âˆˆ RnÃ—k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 âˆ’ Î±)âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤LH), (20) where âˆ¥ Â· âˆ¥F stands for the matrix Frobenius norm. The fitting term âˆ¥H âˆ’ Hâ—¦âˆ¥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix Hâ—¦, while the graph Laplacian regularization term trace(HâŠ¤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter Î± âˆˆ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =Pâˆž â„“=0(1 âˆ’ Î±)Î±â„“ ËœA â„“ Hâ—¦, where ËœA = Dâˆ’1 2 ADâˆ’1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation âˆ’ â†’h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi âˆˆ V can be formulated as âˆ’ â†’h(i) = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“ ËœA â„“âˆ’ â†’hâ—¦(i) , where the normalized adjacency matrix ËœA can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix Hâ—¦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = Pâˆž â„“=0 (1 âˆ’ Î±)Î±â„“Pâ„“Z, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to âˆ€vt âˆˆ V, âˆ’ â†’Ï t = âˆ’ â†’h(s) Â·âˆ’ â†’h(t), implying that âˆ’ â†’Ï â€² output by LACA essentially approximates âˆ’ â†’h(s)Â·HâŠ¤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ)\n",
      " > 787 [-0.03165 -0.01124 -0.01764  0.0732   0.03882]  cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of âˆ’ â†’h(s) among n GNN-like embeddings {âˆ’ â†’h(t)|vi âˆˆ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ËœO(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs âˆˆ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ËœO\u00001 Ïµ \u0001 APR-Nibble O(md) HK-Relax[16] - ËœO \u0010log(1/Ïµ) Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based\n",
      " > 788 [-0.0504   0.00801 -0.02481  0.06085  0.05304]  Ïµ \u0011 CRD [20] O\u00001 Ïµ \u0001 p-Norm FD[21] O max viâˆˆV d(vi)2 Ïµ ! WFD [33] O(md) Jaccard[54] Link Similarity - ËœO(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ËœO(nd) AttriRank[58] O(nd2 + m) ËœO(n) Node2Vec[59] Node Embedding O(n) ËœO(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) Â· d) CFANE[62] O((m+ n) Â· d) LACA Ours O(nd) ËœO\u00001 Ïµ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpointsâ€™ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision\n",
      " > 789 [-0.05005  0.00791 -0.02898  0.0723   0.05695]  attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs âˆ© Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16\n",
      " > 790 [-0.03442   -0.013725  -0.0009284  0.02423    0.0569   ]  precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition,\n",
      " > 791 [-0.02937   0.003515 -0.01067   0.04587   0.05234 ]  16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying Ïµ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2\n",
      " > 792 [-0.02325 -0.01516 -0.00455  0.0585   0.05362]  addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying Ïµ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold Ïµ and are bounded by O(1/Ïµ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing Ïµ from 1.0 to 10âˆ’8. For each seed node vs âˆˆ S, given the predicted local cluster Cs, we compute the recall by |Cs âˆ© Ys|/|Ys|. Intuitively, the smaller Ïµ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying Ïµ on six datasets. The x-axis and y-axis represent Ïµ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds Ïµ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when Ïµ â‰¥ 10âˆ’3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When Ïµ â‰¤ 10âˆ’6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10âˆ’3 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table\n",
      " > 793 [-0.02438  -0.003168 -0.03912   0.04242   0.05975 ]  10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10âˆ’2 10âˆ’1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10âˆ’1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10âˆ’1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10âˆ’1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when Ïµ is roughly 10âˆ’4 or 10âˆ’5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same Ïµ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when Ïµ is large and inferior ones when Ïµ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACAâ€™s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly\n",
      " > 794 [-0.02122 -0.00658 -0.04654  0.04877  0.0526 ]  (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196Ã—, 209Ã—, and 152Ã—, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such\n",
      " > 795 [-0.03568  -0.007133 -0.05545   0.0516    0.02919 ]  significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on â€œJian Peiâ€ Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on â€œJian Peiâ€ Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar â€œJian Peiâ€, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as â€œJiawei Hanâ€ (similarity: 33%) and â€œCharu Aggarwalâ€ (33%) as well as a subgroup centered around â€œJiawei Hanâ€ (e.g., â€œBolin Dingâ€ (25%)). In con- trast, PR-Nibbleâ€”an LGC baselineâ€”selected three schol- ars (e.g., â€œHang Li,â€ â€œDimitris Papadiasâ€) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness\n",
      " > 796 [-0.04462   0.003456 -0.06573   0.06885   0.04575 ]  such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]â€“[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]â€“[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]â€“[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]â€“[77] and k-truss [78]â€“[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan\n",
      " > 797 [-0.01392  -0.002087 -0.03665   0.0743    0.02473 ]  cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]â€“[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings\n",
      " > 798 [-0.0457   -0.02042  -0.05444   0.1112   -0.008934]  plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, â€œGlobal graph clustering and local graph exploration for community detection in twitter,â€ in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1â€“8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, â€œConstrained social community recommendation,â€ in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586â€“5596. [3] J. Chen, O. Za Â¨Ä±ane, and R. Goebel, â€œLocal community identification in social networks,â€ in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237â€“242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, â€œFinding local communities in protein networks,â€ BMC bioinformatics, vol. 10, pp. 1â€“14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, â€œIsorankn: spectral methods for global alignment of multiple protein networks,â€ Bioinformatics, vol. 25, pp. i253â€“i258, 2009. [6] J. Li, J. He, and Y . Zhu, â€œE-tail product return prediction via hypergraph- based local graph cut,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519â€“527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, â€œA local algorithm for product return prediction in e-commerce.â€ in IJCAI, 2018, pp. 3718â€“3724. [8] H. Yang, Y . Zhu, and J. He, â€œLocal algorithm for user action prediction towards display ads,â€ in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091â€“2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, â€œDiscovering polarization niches via dense subgraphs with attractors and repulsers,â€ Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883â€“3896, 2022. [10] D. F. Gleich and M. W. Mahoney, â€œUsing local spectral methods to robustify graph-based learning algorithms,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607.\n",
      " > 799 [-0.0578   0.02127 -0.0642   0.0888   0.01814]  Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359â€“368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, â€œActive search of connections for case building and combating human trafficking,â€ in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120â€“2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, â€œBiased normalized cuts.â€ IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, â€œA local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,â€ Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339â€“2365, 2012. [14] D. A. Spielman and S.-H. Teng, â€œA local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,â€ SIAM Journal on computing , vol. 42, no. 1, pp. 1â€“26, 2013. [15] R. Andersen, F. Chung, and K. Lang, â€œLocal graph partitioning using pagerank vectors,â€ in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCSâ€™06) . IEEE, 2006, pp. 475â€“486. [16] K. Kloster and D. F. Gleich, â€œHeat kernel based community detection,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386â€“1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, â€œEfficient estimation of heat kernel pagerank for local clustering,â€ in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339â€“1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, â€œRandom walks and diffusion on networks,â€ Physics reports, vol. 716, pp. 1â€“58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, â€œThink locally, act locally: Detection of small, medium-sized, and large communities in large networks,â€ Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, â€œCapacity releasing diffusion for speed and locality,â€ in International Conference on Machine Learning . PMLR, 2017, pp. 3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs:\n",
      " > 800 [-0.02686   0.00876  -0.06097   0.06158   0.003712]  3598â€“3607. [21] K. Fountoulakis, D. Wang, and S. Yang, â€œp-norm flow diffusion for local graph clustering,â€ in International Conference on Machine Learning . PMLR, 2020, pp. 3222â€“3232. [22] A. Jung and Y . SarcheshmehPour, â€œLocal graph clustering with network lasso,â€ IEEE Signal Processing Letters , vol. 28, pp. 106â€“110, 2020. [23] L. Lov Â´asz, â€œRandom walks on graphs,â€ Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, â€œLocal higher- order graph clustering,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555â€“564. [25] D. Fu, D. Zhou, and J. He, â€œLocal motif clustering on time-evolving graphs,â€ in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390â€“400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, â€œLocal motif clustering via (hyper) graph partitioning,â€ in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96â€“109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, â€œIndex-free triangle-based graph local clustering,â€ Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, â€œCross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,â€ in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803â€“814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, â€œAttributed social network embedding,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257â€“2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, â€œClustering attributed graphs: models, measures and methods,â€ Network Science , vol. 3, no. 3, pp. 408â€“444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, â€œLocal partition in rich graphs,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001â€“1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, â€œLocal clustering over labeled graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp. 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to\n",
      " > 801 [-0.02274 -0.01298 -0.05453  0.04895 -0.03075]  graphs: An index-free approach,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805â€“2817. [33] S. Yang and K. Fountoulakis, â€œWeighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,â€ in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252â€“39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, â€œFinding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,â€ SIAM review, vol. 53, no. 2, pp. 217â€“288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, â€œOrthogonal random features,â€ Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, â€œFora: simple and effective approximate single-source personalized pagerank,â€ in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505â€“514. [37] R. Yang, â€œEfficient and effective similarity search over bipartite graphs,â€ in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308â€“318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ vol. 30, 2017, pp. 1024â€“1034. [39] B. Li, B. Jing, and H. Tong, â€œGraph communal contrastive learning,â€ Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, â€œFast random walk with restart and its applications,â€ in Sixth international conference on data mining (ICDMâ€™06). IEEE, 2006, pp. 613â€“622. [41] G. Jeh and J. Widom, â€œScaling personalized web search,â€ in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271â€“ 279. [42] S. Rothe and H. Sch Â¨utze, â€œCosimrank: A flexible & efficient graph- theoretic similarity measure,â€ in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392â€“ 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, â€œBidirectional pagerank estima- tion: From average-case to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J.\n",
      " > 802 [-0.04416 -0.01111 -0.0487   0.03763 -0.02736]  to worst-case,â€ in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164â€“176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, â€œA unified view on graph neural networks as graph signal denoising,â€ in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202â€“1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, â€œInterpreting and unifying graph neural networks with an optimization framework,â€ in Proceedings of the Web Conference 2021 , 2021, pp. 1215â€“1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R Â´ozemberczki, M. Lukasik, and S. G Â¨unnemann, â€œScaling graph neural networks with approximate pagerank,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464â€“2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, â€œCollective classification in network data,â€ AI magazine , vol. 29, no. 3, pp. 93â€“93, 2008. [49] L. Tang and H. Liu, â€œRelational learning via latent social dimensions,â€ in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817â€“826. [50] X. Huang, J. Li, and X. Hu, â€œLabel informed attributed network embedding,â€ in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731â€“739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, â€œOpen graph benchmark: Datasets for machine learning on graphs,â€ Advances in neural information processing systems , vol. 33, pp. 22 118â€“22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, â€œGraphsaint: Graph sampling based inductive learning method,â€ in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab.(LANL), Los Alamos, NM (United\n",
      " > 803 [-0.0386   -0.006947 -0.05457   0.03445   0.00345 ]  Hsieh, â€œCluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257â€“266. [54] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [55] G. Jeh and J. Widom, â€œSimrank: a measure of structural-context similarity,â€ in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538â€“ 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, â€œA unified framework for link recommendation using random walks.â€ IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, â€œSemantic cosine similarity,â€ in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, â€œUnsu- pervised ranking using graph structures and node attributes.â€ ACM, 2017. [59] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., â€œScaling attributed network embedding to massive graphs,â€ Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37â€“49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, â€œPane: scalable and effective attributed network embedding,â€ The VLDB Journal, vol. 32, pp. 1237â€“1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, â€œUnsupervised attributed network embedding via cross fusion.â€ ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, â€œA short introduction to local graph clustering methods and software,â€ 2018. [64] A. Hagberg, P. Swart, and D. S Chult, â€œExploring network struc- ture, dynamics, and function using networkx,â€ Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient\n",
      " > 804 [-0.0796  -0.01446 -0.0745   0.05063 -0.01982]  States), Tech. Rep., 2008. [65] F. Chung, â€œThe heat kernel as the pagerank of a graph,â€ Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735â€“19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, â€œEffective community search for large attributed graphs,â€ vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233â€“1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, â€œEffective and efficient community search over large directed graphs,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093â€“2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, â€œEffective and efficient community search over large heterogeneous information networks,â€ vol. 13, no. 6. VLDB Endowment, 2020, pp. 854â€“867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, â€œPanther: Fast top-k similarity search on large networks,â€ in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445â€“1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, â€œLocal community detection: A survey,â€ IEEE Access, vol. 10, pp. 110 701â€“110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, â€œAlmost optimal local graph clustering using evolving sets,â€ Journal of the ACM (JACM), vol. 63, no. 2, pp. 1â€“31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, â€œLocal graph clustering with noisy labels,â€ in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] â€”â€”, â€œCommunity search over big graphs: Models, algorithms, and op- portunities,â€ in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451â€“1454. [75] M. Sozio and A. Gionis, â€œThe community-search problem and how to plan a successful cocktail party.â€ ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, â€œLocal search of communities in large graphs.â€ ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X.\n",
      " > 805 [-0.03918   0.054    -0.0787    0.04376   0.002855]  â€œEfficient and effective community search,â€ Data Mining and Knowledge Discovery , vol. 29, pp. 1406â€“1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, â€œQuerying k-truss community in large and dynamic graphs.â€ ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, â€œApproximate closest community search in networks,â€ vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276â€“287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, â€œVac: Vertex- centric attributed community search.â€ IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, â€œEffective and efficient attributed community search,â€ The VLDB Journal, vol. 26, pp. 803â€“828, 2017. [82] X. Huang and L. V . S. Lakshmanan, â€œAttribute-driven community search,â€ vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949â€“960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, â€œWhen structure meets keywords: Cohesive attributed community search.â€ ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, â€œMatrix computations,â€ Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, â€œArbitrary- order proximity preserved network embedding,â€ in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778â€“2786. [87] J. MacQueen et al. , â€œSome methods for classification and analysis of multivariate observations,â€ in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281â€“297. [88] J. Yang and J. Leskovec, â€œDefining and evaluating network communities based on ground-truth.â€ IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, â€œNetwork representation learning with rich text information,â€ in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111â€“2117. [90] X. Huang, J. Li, and X. Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi)\n",
      " > 806 [-0.00887  0.0507  -0.0786   0.02332  0.03662]  Hu, â€œAccelerated attributed network embedding,â€ vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633â€“641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, â€œLow-bit quantization for attributed network representation learning.â€ International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, â€œAnrl: Attributed network representation learning via deep neural networks.â€ International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, â€œDeep attributed network embedding.â€ Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDistributed representations of words and phrases and their composi- tionality,â€ vol. 26, 2013, pp. 3111â€“3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 and by âˆ’ â†’b â„“ the remaining residual at Line 5 in â„“-th iteration. Then, according to Line 6, âˆ’ â†’q can be represented by âˆ’ â†’q = (1âˆ’ Î±) âˆžX â„“ âˆ’ â†’Î³ â„“. (21) Next, we consider {âˆ’ â†’Î³ 1, âˆ’ â†’Î³ 2, . . . ,âˆ’ â†’Î³ â„“, âˆ’ â†’Î³ â„“+1, . . . ,âˆ’ â†’Î³ âˆž}. By Lines 3 and 7-8, we have âˆ’ â†’Î³ 1 = âˆ’ â†’f âˆ’ âˆ’ â†’b 1, âˆ’ â†’Î³ 1 = âˆ’ â†’b 1 + Î±âˆ’ â†’Î³ 1P âˆ’ âˆ’ â†’b 2 Â·Â·Â· âˆ’ â†’Î³ â„“ = âˆ’ â†’b â„“âˆ’1 + Î±âˆ’ â†’Î³ â„“âˆ’1P âˆ’ âˆ’ â†’b â„“, âˆ’ â†’Î³ â„“+1 = âˆ’ â†’b â„“ + Î±âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b â„“+1 Â·Â·Â· . Then, Eq. (21) can be rewritten as âˆ’ â†’q 1 âˆ’ Î± = âˆ’ â†’f + Î± âˆžX â„“=1 âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’b âˆž = âˆžX â„“=0 Î±tâˆ’ â†’f Pâ„“ âˆ’ âˆžX â„“=0 Î±tâˆ’ â†’b â„“Pâ„“. (22) Recall that âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i/d(vi) < Ïµ(see Lines 3 and 5). Let âˆ’ â†’b be a length- n vector where each i- th entry is Ïµ Â· d(vi). Together with Eq. (22) and the matrix definition of Ï€(vi, vj) defined in Eq. (6), we can bound each entry âˆ’ â†’q t by âˆ’ â†’q t â‰¥ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’f Pâ„“)t âˆ’ (1 âˆ’ Î±) âˆžX â„“=0 Î±t(âˆ’ â†’b Pâ„“)t = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV âˆ’ â†’b i Â· Ï€(vi, vt) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ X viâˆˆV Ïµ Â· d(vi) Â· Ï€(vi, vt). By the fact of d(vi) Â· Ï€(vi, vt) = d(vt) Â· Ï€(vt, vi) (Lemma 1 in [43]) and P viâˆˆV Ï€(vt, vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P.\n",
      " > 807 [-0.01724  0.04932 -0.0721   0.02872  0.0458 ]  vi) = 1, the above inequality can be simplified as âˆ’ â†’q t â‰¥ X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt) Â· X viâˆˆV Ï€(vt, vi) = X viâˆˆV âˆ’ â†’f i Â· Ï€(vi, vt) âˆ’ Ïµ Â· d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration â„“. For ease of exposition, we denote âˆ’ â†’Î³ â„“ as the vector âˆ’ â†’Î³ obtained at Line 3 in â„“-th iteration and by âˆ’ â†’r â„“ and âˆ’ â†’q â„“ the residual and reserve vectors âˆ’ â†’r and âˆ’ â†’q at the beginning of â„“-th iteration, respectively. Accordingly, for each non-zero entry âˆ’ â†’Î³ â„“ i in âˆ’ â†’Î³ â„“,âˆ’ â†’Î³ â„“ i â‰¥ d(vi) Â· Ïµ. First, we prove that at the beginning of any â„“-th iteration, the following equation holds: âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. (23) We prove this by induction. For the base case where â„“ = 1, i.e., at the beginning of the first iteration, we have âˆ’ â†’r 1 = âˆ’ â†’f and âˆ’ â†’q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of â„“-th (â„“ >0) iteration, i.e., âˆ¥âˆ’ â†’r â„“âˆ¥1 + âˆ¥âˆ’ â†’q â„“âˆ¥1 = âˆ¥âˆ’ â†’f âˆ¥1. According to Lines 5-7, we have âˆ’ â†’q â„“+1 = âˆ’ â†’q â„“ + (1âˆ’ Î±) Â· âˆ’ â†’Î³ â„“ âˆ’ â†’r â„“+1 = âˆ’ â†’r â„“ âˆ’ âˆ’ â†’Î³ + Î± Â· âˆ’ â†’Î³ â„“P. As such, âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 = âˆ¥âˆ’ â†’q â„“âˆ¥1 + âˆ¥âˆ’ â†’r âˆ¥1 + Î± Â· âˆ¥âˆ’ â†’Î³ â„“P âˆ’ âˆ’ â†’Î³ â„“âˆ¥1. (24) Note that âˆ¥âˆ’ â†’Î³ Pâˆ’ âˆ’ â†’Î³ â„“âˆ¥1 = X viâˆˆV X vjâˆˆV âˆ’ â†’Î³ â„“ j Â· Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆV Pj,i âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = X vjâˆˆV âˆ’ â†’Î³ â„“ j X viâˆˆN(vj) 1 d(vj) âˆ’ X vjâˆˆV âˆ’ â†’Î³ â„“ j = 0, implying that âˆ¥âˆ’ â†’r â„“+1âˆ¥1 + âˆ¥âˆ’ â†’q â„“+1âˆ¥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each â„“-th iteration, Algo. 1 converts (1âˆ’Î±) fraction of âˆ’ â†’Î³ â„“ into âˆ’ â†’q â„“, i.e., at least (1âˆ’Î±)Â·ÏµÂ·d(vi) out of each non-zero entries in âˆ’ â†’Î³ â„“ is passed to âˆ’ â†’q â„“. After L iterations, we obtain âˆ’ â†’q L. Recall that by Eq. (23), âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. We obtain LX â„“=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“) (1 âˆ’ Î±) Â· Ïµ Â· d(vi) â‰¤ âˆ¥âˆ’ â†’q Lâˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1, (25) which leads to PL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ . Notice that in Line 6, each non-zero entry in âˆ’ â†’Î³ â„“ will result in d(vi) operations in the matrix-vector multiplicationâˆ’ â†’Î³ â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’\n",
      " > 808 [ 0.003288  0.0135   -0.0387    0.02713   0.04446 ]  â„“P. Thus, the total cost of Line 6 for L iterations isPL â„“=1 P iâˆˆsupp(âˆ’ â†’Î³ â„“) d(vi), which is bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries inâˆ’ â†’Î³ â„“. Their total cost for L iterations can then be bounded by âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ as well. As for Line 3, in the first iteration, we need to inspect every entry in âˆ’ â†’r 1, and hence, the time cost is âˆ¥âˆ’ â†’r 1âˆ¥0 = supp(âˆ’ â†’f ) . In any subsequent â„“-th iteration, we solely need to inspect the entries in âˆ’ â†’r â„“ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in âˆ’ â†’Î³ â„“âˆ’1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we letâˆ’ â†’b â„“ be the remaining residual in the â„“-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that âˆ’ â†’b â„“ is 0 when â„“-th iteration runs Lines 5-6. As such, âˆ’ â†’b â„“ always satisfies âˆ€vi âˆˆ V, âˆ’ â†’b â„“ i d(vi) < Ïµand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when âˆ’ â†’Î³ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(âˆ’ â†’f ) , âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors âˆ’ â†’q , âˆ’ â†’r , and âˆ’ â†’Î³ in each â„“-th iteration as in the proof of Theorem IV .1. Further, we assume that in â„“1-th, â„“2-th, . . ., and â„“T -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) â‰¤ âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is\n",
      " > 809 [ 0.01433    0.0392    -0.03363    0.0005026  0.04376  ]  âˆ¥âˆ’ â†’q â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ . Let L = {1, 2, . . . , L} and T = {â„“1, â„“2, . . . , â„“T }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(âˆ’ â†’r ) < âˆ¥âˆ’ â†’f âˆ¥1 (1âˆ’Î±)Ïµ , meaning that X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final âˆ’ â†’q are from non-zero entries in âˆ’ â†’Î³ j âˆ€j âˆˆ T and âˆ’ â†’r j âˆ€j âˆˆ L \\ T. Then, vol(âˆ’ â†’q ) = X iâˆˆsupp(âˆ’ â†’q ) d(vi) â‰¤ TX j=1 X iâˆˆsupp(âˆ’ â†’Î³ â„“j ) d(vi) + X jâˆˆL\\T X iâˆˆsupp(âˆ’ â†’r j) d(vi) â‰¤ 2âˆ¥âˆ’ â†’f âˆ¥1 âˆ’ âˆ¥âˆ’ â†’r â„“T âˆ¥1 (1 âˆ’ Î±)Ïµ = Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, where Î² stands for a constant in the range [1, 2] since 0 â‰¤ âˆ¥âˆ’ â†’r â„“T âˆ¥1 â‰¤ âˆ¥âˆ’ â†’f âˆ¥1. Similarly, we obtain |supp(âˆ’ â†’q )| â‰¤ X iâˆˆsupp(âˆ’ â†’q ) d(vi) =vol(âˆ’ â†’q ) â‰¤ Î²âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ. When Ïƒ â‰¥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(âˆ’ â†’q )| â‰¤vol(âˆ’ â†’q ) â‰¤ âˆ¥âˆ’ â†’f âˆ¥1 (1 âˆ’ Î±)Ïµ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckartâ€“Young Theorem [84]). Suppose that Mk âˆˆ RnÃ—k is the rank- k approximation to M âˆˆ RnÃ—n obtained by exact SVD, then minrank(cM)â‰¤k âˆ¥M âˆ’ cMâˆ¥2 = âˆ¥M âˆ’ Mkâˆ¥2 = Î»k+1, where Î»k+1 stands for the (k + 1)-th largest singular value of M. Suppose that UÎ›V âŠ¤ is the exact k-SVD of X, by Theorem A.1, we have âˆ¥UÎ›V âŠ¤âˆ’Xâˆ¥2 â‰¤ Î»k+1, where Î»k+1 is the (k+ 1)-th largest singular value of X. Let bU bÎ› bV âŠ¤ be the k-SVD of XXâŠ¤. Similarly, from Theorem A.1, we get âˆ¥ bU bÎ› bV âŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ bÎ»k+1, where bÎ»k+1 is the (k+1)-th largest singular value of XXâŠ¤. According to [85], columns in U are the eigenvectors of matrix XXâŠ¤ and the squared singular values of X are the eigenvalues of XXâŠ¤. Given that singular values are non- negative, all the eigenvalues of XXâŠ¤ are also non-negative. Î›2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XXâŠ¤, and Î»k+1 2 = bÎ»k+1. Further, by Theorem 4.1 in [86], it can be verified that Î›2 and U are the top-k singular values and left/right singular vectors of XXâŠ¤, respectively, and Î»k+1 2 is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’\n",
      " > 810 [-0.00814  0.03232 -0.0171   0.02826  0.04245]  is the (k +1)-th largest singular value of XXâŠ¤. Consequently, âˆ¥UÎ›2UâŠ¤ âˆ’ XXâŠ¤âˆ¥2 â‰¤ Î»k+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi âˆˆ V, vector âˆ’ â†’x (i) is L2 normalized, i.e., âˆ¥âˆ’ â†’x (i)âˆ¥2 = 1. Thus, we can derive âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 = 2(1 âˆ’ cos (âˆ’ â†’x (i), âˆ’ â†’x (j)) = 2(1âˆ’ âˆ’ â†’x (i) Â· âˆ’ â†’x (j)) âˆˆ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012âˆ’ â†’x (i) Â· âˆ’ â†’x (j) Î´ \u0013 = exp 1 âˆ’ 1 2 âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 Î´ ! = exp \u00121 Î´ âˆ’ âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 = exp \u00121 Î´ \u0013 Â· exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 Î´ XÎ£Q = 1 Î´ UÎ›Î£Q via Lines 6-9, we have E h K Â· KâŠ¤ i = exp \u0012 âˆ’âˆ¥âˆ’ â†’x (i) âˆ’ âˆ’ â†’x (j)âˆ¥2 2 2Î´ \u0013 , where K = 1âˆš d Â· sin(âˆ’ â†’by (i)) âˆ¥ cos(âˆ’ â†’by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of âˆ’ â†’y âˆ— and âˆ’ â†’z (i) âˆ€vi âˆˆ Vat Lines 10-11 need O(nk) time. Thus, when f(Â·, Â·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(Â·, Â·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let âˆ’ â†’Ï â—¦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, âˆ€vj âˆˆ V, X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt) âˆ’ âˆ’ â†’Ï â—¦ t â‰¥ 0 and X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) Â· Ï€(vj, vt)âˆ’âˆ’ â†’Ï â—¦ t â‰¤ ÏµÂ·d(vt), yielding 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· d(vj) d(vt) Â· Ï€(vj, vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step\n",
      " > 811 [-0.02347  0.0309  -0.038    0.03244  0.03992]  vt)âˆ’ âˆ’ â†’Ï â—¦ t d(vt) â‰¤ Ïµ. By the fact of Ï€(vt, vj) =d(vj) d(vt) Â·Ï€(vj, vt) (Lemma 1 in [43]) and âˆ’ â†’Ï â€² t = âˆ’ â†’Ï â—¦ t d(vt) (Line 6), we have 0 â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ. Further, we obtain âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV âˆ’ â†’Ï€ â€² i Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ. Notice that âˆ’ â†’Ï€ obtained at Line 2 in Algo. 4 satisfies 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ Vaccording to Theorem IV .2. We then derive âˆ’ â†’Ï â€² t â‰¤ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ âˆ’ â†’Ï t and âˆ’ â†’Ï â€² t â‰¥ X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV (Ï€(vs, vi) âˆ’ Ïµd(vi)) Â· s(vi, vj) Â· Ï€(vt, vj) âˆ’ Ïµ, where the latter leads to âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X jâˆˆsupp(âˆ’ â†’Ï€â€²) X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) + X jâˆˆ{1,...,n}\\supp(âˆ’ â†’Ï€â€²) X viâˆˆV Ï€(vs, vi) Â· s(vi, vj) Â· Ï€(vt, vj). Recall that 0 â‰¤ Ï€(vs, vi)âˆ’âˆ’ â†’Ï€ â€² i â‰¤ ÏµÂ·d(vi) âˆ€vi âˆˆ V. Thus, âˆ€i /âˆˆ supp(âˆ’ â†’Ï€ â€²), Ï€(vs, vi) â‰¤ âˆ’ â†’Ï€ â€² i + Ïµ Â·d(vi) =Ïµ Â·d(vi). Accordingly, âˆ’ â†’Ï t âˆ’ âˆ’ â†’Ï â€² t â‰¤ Ïµ + X vjâˆˆV X viâˆˆV Ïµd(vi) Â· s(vi, vj) Â· Ï€(vt, vj) â‰¤ Ïµ + X viâˆˆV Ïµd(vi) Â· X vjâˆˆV s(vi, vj) Â· Ï€(vt, vj) â‰¤ \u0000 1 +P viâˆˆV d(vi) Â· maxvjâˆˆV s(vi, vj) \u0001 Â· Ïµ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: âˆ‚{(1 âˆ’ Î±) Â· âˆ¥H âˆ’ Hâ—¦âˆ¥2 F + Î± Â· trace(HâŠ¤(I âˆ’ ËœA)H)} âˆ‚H = 0 =â‡’ (1 âˆ’ Î±) Â· (H âˆ’ Hâ—¦) +Î±(I âˆ’ ËœA)H = 0 =â‡’ H = (1âˆ’ Î±) Â· \u0010 I âˆ’ Î± ËœA \u0011âˆ’1 Hâ—¦. (27) By the property of the Neumann series, we have(Iâˆ’Î± ËœA)âˆ’1 =Pâˆž â„“=0 Î±t ËœA â„“ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor Î±, parameter Ïƒ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying Î±. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when Î± is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section\n",
      " > 812 [-0.00908  -0.011505 -0.02374   0.009056  0.0344  ]  step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying Î±. That is, the precision scores increase conspicuously with Î± increasing. The only exception is on BlogCL, where the best result is attained when Î± = 0.8. Recall that in Algo. 2, when Î± is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying Ïƒ. Next, we study the parameter Ïƒ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large Ïƒ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when Ïƒ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing Ïƒ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying Î± in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying Î± in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying Ïƒ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying Ïƒ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to Ïƒ on Cora and PubMed, (ii) undergo a significant performance downturn when Ïƒ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when Ïƒ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small Ïƒ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in âˆ’ â†’q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations\n",
      " > 813 [-0.01396  0.03174 -0.01562  0.02275  0.03635]  Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuseâ€™s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability\n",
      " > 814 [-0.0336    0.03897  -0.004436  0.03955   0.04663 ]  observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964\n",
      " > 815 [-0.0206   -0.005043 -0.009735  0.02165   0.02629 ]  Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Cond.â†“ WCSSâ†“ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when\n",
      " > 816 [-0.014824  0.00919  -0.01056   0.0556    0.03497 ]  0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (a) Varying Ïµ in LACA (C) 10âˆ’8 10âˆ’6 10âˆ’4 10âˆ’2 110âˆ’2 10âˆ’1 1 101 102 running time (sec) (b) Varying Ïµ in LACA (E) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10âˆ’1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying Ïµ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold Ïµ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing Ïµ from 1 to 10âˆ’8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in Ïµ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same Ïµ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99Ã— more edges than ArXiv, their running times are comparable when Ïµ â‰¥ 10âˆ’3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10âˆ’7 â‰¤ Ïµ â‰¤ 10âˆ’4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph\n",
      " > 817 [-0.02089  0.01508 -0.01808  0.04074  0.04276]  when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/Ïµ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into\n",
      " > 818 [-0.02843  0.02278 -0.03156  0.0471   0.01984]  graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 âœ— 0.304 0.28 âœ— âœ— âœ— âœ— LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj), where Ï(vi, vj) =( Ï€(vi, vj) Â· s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M\n",
      " > 819 [-0.044    0.01476 -0.0378   0.03152  0.0564 ]  into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï€(vs, vi) Â· Ï(vi, vj) Â· Ï(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï€(vi, vj) Â· Ï(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vjâˆˆV Ï(vs, vi) Â· Ï(vi, vj) Â· Ï€(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]â€“[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n Ã— n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph. 21\n"
     ]
    }
   ],
   "source": [
    "from rolling.paper import create_paper, print_paper\n",
    "paper = create_paper(\n",
    "    title=pdfs[0],\n",
    "    text=text,\n",
    "    embedding_function=model.encode\n",
    ")\n",
    "print_paper(paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
