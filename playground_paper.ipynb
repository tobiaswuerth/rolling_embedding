{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./arxiv_downloads\\\\2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf',\n",
       " './arxiv_downloads\\\\2503.20491v1.VPO__Aligning_Text_to_Video_Generation_Models_with_Prompt_Optimization.pdf']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rolling.pdf import list_pdfs, read_pdf\n",
    "pdfs = list_pdfs()\n",
    "pdfs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adaptive Local Clustering over Attributed Graphs\\nTechnical Report\\nHaoran Zheng\\nHong Kong Baptist University\\nHong Kong SAR, China\\ncshrzheng@comp.hkbu.edu.hk\\nRenchi Yang\\nHong Kong Baptist University\\nHong Kong SAR, China\\nrenchi@hkbu.edu.hk\\nJianliang Xu\\nHong Kong Baptist University\\nHong Kong SAR, China\\nxujl@hkbu.edu.hk\\nAbstract—Given a graph G and a seed node vs, the objective\\nof local graph clustering (LGC) is to identify a subgraph Cs ∈ G\\n(a.k.a. local cluster) surrounding vs in time roughly linear with\\nthe size of Cs. This approach yields personalized clusters without\\nneeding to access the entire graph, which makes it highly suitable\\nfor numerous applications involving large graphs. However, most\\nexisting solutions merely rely on the topological connectivity\\nbetween nodes in G, rendering them vulnerable to missing or\\nnoisy links that are commonly present in real-world graphs.\\nTo address this issue, this paper resorts to leveraging the\\ncomplementary nature of graph topology and node attr'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = read_pdf(pdfs[0])\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rolling.embedding import GTEEmbeddingModel\n",
    "model = GTEEmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper: [-0.02779  0.01419 -0.0361   0.04477  0.0311 ] ./arxiv_downloads\\2503.20488v1.Adaptive_Local_Clustering_over_Attributed_Graphs.pdf\n",
      " > 000 [-0.06433   0.008675 -0.04153   0.07434   0.05066 ] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size\n",
      " > 001 [-0.06604   0.012856 -0.0344    0.0818    0.05963 ]  Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between\n",
      " > 002 [-0.0586   0.02893 -0.03262  0.0936   0.02443]  size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering\n",
      " > 003 [-0.0486   0.03452 -0.0625   0.112    0.04013]  between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes.\n",
      " > 004 [-0.02919  0.03049 -0.07434  0.08276  0.05405]  clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded\n",
      " > 005 [-0.012665  0.01805  -0.05014   0.06256   0.0642  ]  attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments,\n",
      " > 006 [-0.0347   0.01418 -0.04953  0.0645   0.0605 ]  theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph\n",
      " > 007 [-0.01552  0.0166  -0.03864  0.0898   0.02663]  experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts,\n",
      " > 008 [-1.958e-02 -6.139e-05 -5.530e-02  8.838e-02  2.081e-02]  graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks,\n",
      " > 009 [-0.02066    0.0002027 -0.04794    0.0787     0.02551  ]  counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce\n",
      " > 010 [-0.03787 -0.02257 -0.04324  0.0827   0.02315]  networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology\n",
      " > 011 [-0.0534  -0.02104 -0.0234   0.0877   0.02707]  e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this\n",
      " > 012 [-0.0622  -0.01028 -0.061    0.0935   0.0793 ]  methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly\n",
      " > 013 [-0.0661  -0.01639 -0.05038  0.05884  0.0684 ]  this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed\n",
      " > 014 [-0.05035  0.01588 -0.02812  0.03934  0.0248 ]  mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal\n",
      " > 015 [-0.02182  0.0269  -0.03308  0.0486   0.03607]  constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely\n",
      " > 016 [-0.02625  0.0405  -0.0504   0.07434  0.04755]  sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are\n",
      " > 017 [-0.046     0.02943  -0.01125   0.05872  -0.002268]  solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively\n",
      " > 018 [-0.044     0.00848  -0.004364  0.04434   0.01152 ]  often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33]\n",
      " > 019 [-0.0542   0.02528 -0.02402  0.06323  0.05698]  effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections.\n",
      " > 020 [-0.05542  0.02766 -0.02625  0.079    0.04785]  [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1\n",
      " > 021 [-0.03592  0.00904 -0.05545  0.0829   0.0441 ]  connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to\n",
      " > 022 [-0.0172  -0.0161  -0.0395   0.05472  0.0664 ]  arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs\n",
      " > 023 [-0.05017   0.006844 -0.04892   0.09454   0.07416 ]  node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the\n",
      " > 024 [-0.03592  0.01484 -0.05627  0.0989   0.069  ]  vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large\n",
      " > 025 [-0.02629 -0.00294 -0.0444   0.01976  0.05457]  the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into\n",
      " > 026 [-0.0126   -0.01727  -0.0543    0.010925  0.04813 ]  large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse,\n",
      " > 027 [-0.000406 -0.01469  -0.0621    0.04138   0.015175]  low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also\n",
      " > 028 [-0.01529 -0.02103 -0.05463  0.02989  0.02673]  AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the\n",
      " > 029 [-0.03152   0.001936 -0.05264   0.04492   0.07715 ]  also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors\n",
      " > 030 [-0.0275   0.00521 -0.02652  0.0413   0.06024]  approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the\n",
      " > 031 [-0.01017   0.014694 -0.01256   0.0653    0.0662  ]  competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let\n",
      " > 032 [-0.02621 -0.02844 -0.02371  0.05603  0.06335]  the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote\n",
      " > 033 [-0.0992   0.0539  -0.03476  0.03885 -0.00565]  Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute\n",
      " > 034 [-0.0656    0.07275  -0.0486    0.0096   -0.012566]  denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of\n",
      " > 035 [-0.03418   0.02756  -0.01106   0.00684   0.001209]  attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of\n",
      " > 036 [-0.001559   0.02843   -0.03702    0.01942   -0.0002644]  of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in\n",
      " > 037 [ 0.001943  0.05664  -0.03018  -0.011765 -0.02719 ]  of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly,\n",
      " > 038 [-0.01747  0.0661  -0.01054 -0.00864 -0.04486]  Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector\n",
      " > 039 [-0.004223  0.04736  -0.03436   0.05505  -0.03293 ]  Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined\n",
      " > 040 [-0.0471   0.0189  -0.03513  0.0673   0.00687]  vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi).\n",
      " > 041 [-0.0668   0.02525 -0.01978  0.0968   0.02739]  as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated\n",
      " > 042 [-0.05286   0.03778  -0.009575  0.1057    0.04834 ]  d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its\n",
      " > 043 [-0.04294  0.01729 -0.03014  0.0802   0.04718]  well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where\n",
      " > 044 [-0.05838  0.04007 -0.0415   0.06036  0.0313 ]  its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the\n",
      " > 045 [-0.0783   0.0378  -0.03903  0.03607  0.07263]  where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\"\n",
      " > 046 [-0.05225  0.02419 -0.04633  0.05304  0.05048]  the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$\n",
      " > 047 [-0.04572  0.03067 -0.0517   0.01982  0.02187]  𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · −\n",
      " > 048 [-0.04446   0.03796  -0.05634   0.01273   0.007046]  𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ\n",
      " > 049 [-0.02892 -0.00923 -0.0483   0.07007  0.05154]  − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion\n",
      " > 050 [-0.02972 -0.00907 -0.04892  0.0809   0.05508]  →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity\n",
      " > 051 [-0.01437  0.0195  -0.0728   0.1017   0.02997]  diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi,\n",
      " > 052 [-0.02393  0.0504  -0.0599   0.06506  0.01859]  similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at\n",
      " > 053 [-0.03348  0.05734 -0.0592   0.0442  -0.02623]  s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj).\n",
      " > 054 [-0.03897  0.05768 -0.05713  0.082   -0.02213]  at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below\n",
      " > 055 [-0.06033  0.0525  -0.0368   0.07404 -0.0238 ]  Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional\n",
      " > 056 [-0.03876  0.0385  -0.03598  0.06604 -0.00546]  process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊)\n",
      " > 057 [-0.02995  0.02963 -0.0225   0.0587   0.01808]  rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending\n",
      " > 058 [-0.03848  0.06476 -0.02754  0.06204  0.0359 ]  𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value\n",
      " > 059 [-0.0774   0.0709  -0.05862  0.0974   0.04083]  pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark.\n",
      " > 060 [-0.0661   0.04172 -0.03592  0.05176  0.04434]  value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other\n",
      " > 061 [-0.02403  0.02748 -0.03256  0.05063  0.06415]  Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ\n",
      " > 062 [ 0.005962  0.02255  -0.053     0.08185   0.06525 ]  other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the\n",
      " > 063 [ 0.012184  0.02432  -0.0445    0.07886   0.0462  ]  →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the\n",
      " > 064 [-0.01933  0.02722 -0.06094  0.0639   0.05774]  the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the\n",
      " > 065 [-0.02512  0.04242 -0.06128  0.0525   0.0667 ]  the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD\n",
      " > 066 [-0.0364   0.06055 -0.03888  0.00773  0.04993]  the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD\n",
      " > 067 [-0.0471   0.03644 -0.0641   0.03207  0.02406]  via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate\n",
      " > 068 [-0.02539  0.01559 -0.06042  0.04327  0.02615]  value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2\n",
      " > 069 [-0.01371  0.03473 -0.0246   0.01605  0.03067]  Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA. where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies\n",
      " > 070 [-0.02562  0.0374   0.01883  0.01287  0.03787]  𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA. where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely\n",
      " > 071 [-0.01724  0.04376 -0.02295  0.0321   0.03625]  implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =−\n",
      " > 072 [-2.028e-02  5.051e-02 -6.282e-05  3.625e-02  2.838e-02]  immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed\n",
      " > 073 [-0.004448  0.05115   0.00049   0.02002   0.0239  ]  =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) ·\n",
      " > 074 [-0.02435  0.03168 -0.03677  0.0558   0.05618]  seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time,\n",
      " > 075 [-0.04138  0.02603 -0.07275  0.02464  0.02666]  · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector\n",
      " > 076 [-0.01463   0.004272 -0.0725    0.0299    0.03038 ]  time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7\n",
      " > 077 [-0.00591  0.0296  -0.0224   0.0426   0.01874]  vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c)\n",
      " > 078 [-0.01116   0.03049  -0.0758    0.004726  0.0495  ]  𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector\n",
      " > 079 [-0.01348   0.02164  -0.0728   -0.004887  0.05484 ]  2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π\n",
      " > 080 [-0.00801  0.03598 -0.02829  0.03522  0.00917]  vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively,\n",
      " > 081 [ 2.383e-02 -9.519e-05 -5.359e-02  4.355e-02 -2.697e-03]  →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node\n",
      " > 082 [ 0.01325   0.001696 -0.03452   0.0487    0.07434 ]  respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD\n",
      " > 083 [-0.00435  0.00689 -0.03372  0.0688   0.03757]  node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12))\n",
      " > 084 [ 0.003395  0.02951  -0.0481    0.0346    0.0252  ]  BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic\n",
      " > 085 [ 0.01198   0.01802  -0.022     0.02112   0.008194]  to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the\n",
      " > 086 [ 0.002548  0.008606 -0.02547   0.02632  -0.0035  ]  algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges\n",
      " > 087 [-0.01098   0.005226 -0.0309    0.03044   0.01927 ]  complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f\n",
      " > 088 [ 0.00862   0.03653  -0.03586   0.0575   -0.007668]  edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and\n",
      " > 089 [-0.005768 -0.01506  -0.010216  0.03647   0.009544]  →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access\n",
      " > 090 [-0.002419 -0.04562  -0.0339    0.0216   -0.00898 ]  − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse\n",
      " > 091 [-0.01105 -0.0165  -0.05634  0.0454   0.04013]  access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed\n",
      " > 092 [-0.03546   -0.0001832 -0.0577     0.0506     0.02336  ]  GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative\n",
      " > 093 [-0.00735  0.01129 -0.06946  0.0431  -0.01018]  needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi)\n",
      " > 094 [-0.02159  0.0415  -0.03072  0.0597  -0.0127 ]  iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we\n",
      " > 095 [-0.0096   0.05347 -0.01212  0.04642 -0.00553]  i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly\n",
      " > 096 [-0.04358  0.03882 -0.01173  0.068    0.01736]  we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)−\n",
      " > 097 [-0.03093   0.02756  -0.01607   0.0724    0.004936]  evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals\n",
      " > 098 [ 0.02396  0.02005 -0.0327   0.03635  0.02396]  α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010\n",
      " > 099 [ 0.01495  0.02733 -0.04404  0.0506   0.0175 ]  residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired\n",
      " > 100 [-0.00899 -0.02353 -0.051    0.02615  0.0399 ]  max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ\n",
      " > 101 [-0.01548   0.003729 -0.04813   0.04343   0.04214 ]  desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input\n",
      " > 102 [-0.002344  0.01242  -0.02608   0.0404    0.02795 ]  (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the\n",
      " > 103 [-0.00887  0.01141 -0.047    0.0689   0.03   ]  input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically,\n",
      " > 104 [-0.05304  0.0195  -0.05548  0.0867   0.01214]  the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus,\n",
      " > 105 [-0.0378   0.01323 -0.05215  0.08875  0.01013]  Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from\n",
      " > 106 [-0.02364  0.01396 -0.05286  0.066    0.02252]  Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns\n",
      " > 107 [-0.03305  0.00568 -0.03123  0.04465  0.02328]  from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused\n",
      " > 108 [-0.03683 -0.0232  -0.02017  0.03174  0.02838]  returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17)\n",
      " > 109 [-0.002996  0.00996  -0.03568   0.04178  -0.001425]  Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7)\n",
      " > 110 [-0.0323   -0.0065   -0.05814   0.0438    0.002844]  (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all\n",
      " > 111 [-0.03973  -0.010254 -0.0787    0.05115   0.02213 ]  5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading\n",
      " > 112 [-0.02107  -0.001347 -0.07245   0.0486    0.0225  ]  nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and\n",
      " > 113 [-0.02292  0.01602 -0.06854  0.0804   0.0057 ]  leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv\n",
      " > 114 [-0.03775  0.01727 -0.0748   0.09015  0.00628]  and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals\n",
      " > 115 [-0.03845  0.02399 -0.08875  0.06024  0.04446]  ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination.\n",
      " > 116 [-0.03375 -0.01743 -0.07135  0.04718  0.04712]  residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy\n",
      " > 117 [-0.0382   -0.002966 -0.08344   0.0389    0.05072 ]  termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49\n",
      " > 118 [-0.00913  0.01364 -0.07275  0.01645  0.06152]  greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast\n",
      " > 119 [ 0.01852  -0.015015 -0.04208   0.01784   0.03772 ]  12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through\n",
      " > 120 [-0.01221 -0.027   -0.0658   0.02034  0.0309 ]  contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of\n",
      " > 121 [-0.006382 -0.01402  -0.0674    0.03973   0.0501  ]  through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse,\n",
      " > 122 [-0.01938 -0.01543 -0.04895  0.03613  0.04272]  of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased\n",
      " > 123 [-0.00893 -0.01029 -0.06168  0.0165   0.0656 ]  GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14)\n",
      " > 124 [ 0.001194 -0.01189  -0.0661    0.03336   0.04276 ]  increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and\n",
      " > 125 [-0.02142  0.02591 -0.059    0.03772  0.0534 ]  (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD\n",
      " > 126 [ 0.004883  0.03192  -0.02843   0.0308    0.06586 ]  and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil\n",
      " > 127 [ 0.01353  0.02252 -0.03497 -0.01004  0.03983]  BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq. (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi,\n",
      " > 128 [-0.01163   0.02483  -0.01511  -0.008995  0.02138 ]  unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq. (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k\n",
      " > 129 [-0.00267   0.04846   0.007656 -0.003195 -0.002975]  f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼\n",
      " > 130 [-0.00662  0.05637  0.01582  0.01243  0.01206]  k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B\n",
      " > 131 [-0.05905  0.0317   0.02159  0.03857  0.03146]  ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional\n",
      " > 132 [-0.02574  0.04584 -0.02853  0.05414  0.02275]  II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i)\n",
      " > 133 [ 0.0003083  0.01907   -0.03027    0.04248    0.04584  ]  dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition\n",
      " > 134 [-0.047    0.02405 -0.03162  0.037    0.03168]  (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function,\n",
      " > 135 [-0.04083  0.04373  0.00813  0.02847  0.02742]  decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization,\n",
      " > 136 [ 0.00591  0.03497 -0.034    0.0524   0.02347]  function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates\n",
      " > 137 [-0.02457  0.02756 -0.03065  0.05896  0.01704]  factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo.\n",
      " > 138 [-0.0004673  0.02261   -0.01426    0.0337     0.02911  ]  a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1\n",
      " > 139 [ 0.009705   0.01988   -0.0132    -0.0004299  0.04306  ]  3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P,\n",
      " > 140 [-0.0222    0.04593  -0.02814  -0.007473  0.01095 ]  →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed.\n",
      " > 141 [-0.03091   0.03424  -0.02191   0.0562    0.007286]  α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator\n",
      " > 142 [-0.0339   0.03104 -0.07513  0.0592   0.0053 ]  distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs\n",
      " > 143 [-0.02408  0.0178  -0.0572   0.05234  0.01746]  estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete\n",
      " > 144 [ 0.02226  0.01212 -0.0549   0.0346   0.02962]  constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place,\n",
      " > 145 [ 0.008484  0.00737  -0.0517    0.00806   0.0412  ]  Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011\n",
      " > 146 [-0.00919   0.010544 -0.01159   0.02396   0.03714 ]  place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries\n",
      " > 147 [-0.01756  0.00963 -0.02814  0.04318  0.04486]  \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion\n",
      " > 148 [-0.001258  0.002903 -0.0249    0.03078   0.03073 ]  entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate\n",
      " > 149 [ 0.02554  0.03812 -0.02925  0.03207  0.06274]  diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′\n",
      " > 150 [ 0.00989   0.011696 -0.01874   0.01535   0.0638  ]  approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ\n",
      " > 151 [ 0.01572  0.00395 -0.03574  0.01656  0.0363 ]  t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2.\n",
      " > 152 [-0.02277  0.0385  -0.0209   0.01532  0.0667 ]  ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines\n",
      " > 153 [-0.03108  0.02422 -0.00995  0.01982  0.05386]  The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical\n",
      " > 154 [-0.02083  -0.01255  -0.02419   0.013374  0.0525  ]  6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) .\n",
      " > 155 [-0.0632   -0.002838 -0.03558   0.0347    0.02846 ]  Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2\n",
      " > 156 [-0.007748  0.01158  -0.03427   0.0738    0.05493 ]  . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈\n",
      " > 157 [-0.0273   -0.002586 -0.03418   0.0828    0.04337 ]  H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final\n",
      " > 158 [-0.06186  0.03078 -0.0261   0.03128  0.04312]  ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51]\n",
      " > 159 [-0.05606   0.05228  -0.062     0.02434  -0.003428]  final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i)\n",
      " > 160 [-0.03766  -0.02327  -0.03918   0.03635   0.013626]  [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are\n",
      " > 161 [-0.006424 -0.008385 -0.0351    0.04456   0.02876 ]  →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs\n",
      " > 162 [-0.03687   0.005173 -0.01994   0.0693    0.0739  ]  are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the\n",
      " > 163 [-0.04703  -0.0133    0.005627  0.06058   0.06866 ]  from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets,\n",
      " > 164 [-0.01246    0.005306  -0.0004368  0.07465    0.0534   ]  the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis,\n",
      " > 165 [-0.00898  0.01517 -0.0218   0.0706   0.032  ]  datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca.\n",
      " > 166 [-0.03812 -0.0119  -0.01558  0.0202  -0.00943]  analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the\n",
      " > 167 [-0.0422   0.01326 -0.02272  0.06085 -0.04736]  A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed-\n",
      " > 168 [-0.06885   0.006264 -0.02042   0.09955  -0.02374 ]  the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively.\n",
      " > 169 [-0.05475  -0.015205 -0.02956   0.0962   -0.03214 ]  embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md)\n",
      " > 170 [-0.04715    0.0009723 -0.01994    0.04053   -0.015465 ]  respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n)\n",
      " > 171 [-0.03586   0.005043 -0.008606  0.05      0.01506 ]  O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of\n",
      " > 172 [-0.04456 -0.02298 -0.0393   0.08307  0.03424]  ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product\n",
      " > 173 [-0.02687 -0.00698 -0.036    0.07324  0.072  ]  of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine\n",
      " > 174 [-0.03406   0.003435 -0.02467   0.0743    0.05725 ]  product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm\n",
      " > 175 [-0.0568  -0.01237 -0.01215  0.00887  0.06604]  similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods:\n",
      " > 176 [-0.05652 -0.0351  -0.03387  0.02094  0.0301 ]  p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms,\n",
      " > 177 [-0.06464 -0.01197 -0.0399   0.0504   0.02324]  methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute\n",
      " > 178 [-0.07916  0.01099 -0.05444  0.05786  0.0321 ]  all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods\n",
      " > 179 [-0.0687   0.02333 -0.0563   0.06152  0.06107]  attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE\n",
      " > 180 [-0.06946  0.00993 -0.04602  0.0967   0.05695]  methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters.\n",
      " > 181 [-0.0653   0.02917 -0.02031  0.08276  0.02841]  [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar,\n",
      " > 182 [-0.05438  0.01135 -0.0288   0.06384 -0.00971]  clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid\n",
      " > 183 [-0.03815  -0.04898  -0.010994  0.02592   0.02391 ]  Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed\n",
      " > 184 [-0.05658 -0.02658 -0.0495   0.0305   0.04257]  grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters\n",
      " > 185 [-0.03268 -0.01188 -0.0518   0.07043  0.05383]  seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat-\n",
      " > 186 [-0.02025  -0.003027 -0.05893   0.04947   0.06494 ]  clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by\n",
      " > 187 [-0.0347   0.01084 -0.03992 -0.01123  0.0584 ]  sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets.\n",
      " > 188 [-0.0338   0.016   -0.02724  0.01095  0.09924]  by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE,\n",
      " > 189 [-0.02554  0.00608 -0.00798  0.05533  0.03033]  datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms\n",
      " > 190 [-0.01619  -0.01665  -0.007652  0.04095   0.03683 ]  PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and\n",
      " > 191 [-0.03906  0.01277 -0.01291  0.00958  0.08057]  outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit\n",
      " > 192 [-0.063     0.0411   -0.0169    0.002012  0.05005 ]  SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20]\n",
      " > 193 [-0.02501   0.01205   0.012245  0.04126   0.005554]  Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343\n",
      " > 194 [-0.003336  0.003069 -0.011024  0.00549  -0.007717]  [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154\n",
      " > 195 [-0.0371    0.001767 -0.0318    0.02303   0.0421  ]  0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC)\n",
      " > 196 [-0.05664  0.00414 -0.0104   0.03815  0.02917]  0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN)\n",
      " > 197 [-0.0363    0.01836   0.002352  0.077     0.02136 ]  (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422\n",
      " > 198 [-0.03986  0.02852  0.04144  0.05136  0.03848]  (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2\n",
      " > 199 [-0.01189  0.03036  0.0498   0.0192   0.0697 ]  0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000\n",
      " > 200 [ 0.000299  0.02559  -0.001233  0.05615   0.04175 ]  10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the\n",
      " > 201 [-0.01999 -0.01396  0.01929  0.0731   0.024  ]  1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure\n",
      " > 202 [-0.03497   0.007965  0.03072   0.05707   0.0371  ]  the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance\n",
      " > 203 [-0.01646   0.005512  0.02812   0.0549    0.05988 ]  structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the\n",
      " > 204 [-0.03375  -0.00818   0.010895  0.0648    0.05942 ]  performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their\n",
      " > 205 [-0.008705  0.01328  -0.01718   0.068     0.04755 ]  the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms,\n",
      " > 206 [ 0.01374   0.007153 -0.02477   0.08185   0.0593  ]  their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be.\n",
      " > 207 [-0.01459  -0.001059 -0.0216    0.0521    0.03992 ]  algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA\n",
      " > 208 [-0.01209 -0.01364 -0.00914  0.03558  0.04456]  be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes\n",
      " > 209 [-0.0363   0.01497 -0.03555  0.0563   0.0884 ]  LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank\n",
      " > 210 [-0.05032  0.01756 -0.041    0.05914  0.0426 ]  sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time\n",
      " > 211 [-0.0404    -0.0005484 -0.02307    0.0277     0.01819  ]  PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec\n",
      " > 212 [-0.02936  -0.007187 -0.0254    0.02467   0.02869 ]  (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M\n",
      " > 213 [-4.797e-02  2.480e-05  1.617e-02  2.071e-02  5.212e-02]  PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax\n",
      " > 214 [-0.02069  0.03314  0.06155  0.02863  0.0961 ]  Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS)\n",
      " > 215 [-0.0217   0.0571   0.02728  0.05338  0.07886]  Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than\n",
      " > 216 [-2.943e-02 -9.251e-05 -2.393e-02  3.983e-02  6.897e-02]  obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed.\n",
      " > 217 [-0.03326   0.011795 -0.03787   0.05978   0.04456 ]  than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed\n",
      " > 218 [-0.02551  0.02838 -0.05563  0.05527  0.038  ]  seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table\n",
      " > 219 [-0.0325  -0.00107 -0.03415  0.0223   0.03696]  non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the\n",
      " > 220 [-0.01549 -0.0078  -0.045    0.02563  0.02193]  (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage,\n",
      " > 221 [-0.01866 -0.02022 -0.04874  0.0562   0.0697 ]  the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA\n",
      " > 222 [-0.02711 -0.01865 -0.0559   0.08405  0.05835]  all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art\n",
      " > 223 [-0.02094   0.003452 -0.01932   0.05786   0.05988 ]  (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant,\n",
      " > 224 [-0.003435  0.01183  -0.02167   0.02391   0.1056  ]  state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage)\n",
      " > 225 [ 0.00658  0.01617 -0.03946 -0.00217  0.0894 ]  insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD\n",
      " > 226 [-0.01413   0.001809 -0.02882  -0.003422  0.0825  ]  stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary,\n",
      " > 227 [-0.02675  0.00664 -0.01918  0.02911  0.0834 ]  WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied\n",
      " > 228 [-0.04013 -0.01223 -0.01587  0.04834  0.05908]  summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang\n",
      " > 229 [-0.05237 -0.0335  -0.03564 -0.0138  -0.01582]  to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong\n",
      " > 230 [-4.013e-02 -1.190e-02 -8.575e-03 -2.754e-05 -2.147e-02]  Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the\n",
      " > 231 [-0.03415  -0.00501  -0.01706   0.01701   0.007668]  (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship\n",
      " > 232 [-0.02606   0.006588 -0.01587   0.03763   0.0072  ]  the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)).\n",
      " > 233 [ 0.000578  0.01303  -0.0417    0.0657    0.003735]  co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates\n",
      " > 234 [-0.02135  0.01123 -0.0343   0.06683  0.02667]  (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local\n",
      " > 235 [-0.06033  0.03055 -0.04837  0.0808   0.0585 ]  demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster.\n",
      " > 236 [-0.07635  -0.005695 -0.03522   0.06116   0.0708  ]  local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15]\n",
      " > 237 [-0.0852  -0.0231  -0.0294   0.04642  0.04266]  cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods\n",
      " > 238 [-0.04248 -0.0421  -0.04865  0.0593   0.05334]  [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on\n",
      " > 239 [-0.00329 -0.02399 -0.0523   0.03708  0.03156]  methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights\n",
      " > 240 [-0.01386  -0.001057 -0.0459    0.0597    0.007866]  running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly\n",
      " > 241 [-0.0466  -0.01566 -0.0687   0.0712   0.02612]  weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected\n",
      " > 242 [-0.0576    0.002647 -0.09296   0.0741    0.015015]  costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities,\n",
      " > 243 [-0.09265   0.04327  -0.05722   0.0585   -0.003994]  connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been\n",
      " > 244 [-0.0765   0.04538 -0.04663  0.0665   0.02768]  communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching\n",
      " > 245 [-0.064    0.05133 -0.06915  0.07855  0.0517 ]  been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the\n",
      " > 246 [-0.06915  0.0285  -0.04013  0.0574   0.0297 ]  the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints\n",
      " > 247 [-0.05664  0.0191  -0.0629   0.04993  0.0342 ]  objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach\n",
      " > 248 [-0.05228  0.02362 -0.06094  0.06946  0.0406 ]  constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA,\n",
      " > 249 [-0.05435  0.01464 -0.04886  0.05127  0.0471 ]  follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii)\n",
      " > 250 [-0.01833   0.006332 -0.045     0.03583   0.05856 ]  LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17\n",
      " > 251 [-0.01898  0.0085  -0.02837  0.0508   0.03162]  (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou,\n",
      " > 252 [-0.0753    0.007763 -0.03732   0.05795  -0.01449 ]  17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference\n",
      " > 253 [-0.04358 -0.00499 -0.052    0.0502  -0.0205 ]  Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za\n",
      " > 254 [-0.07306  0.01288 -0.04553  0.05463  0.0373 ]  (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities\n",
      " > 255 [-0.02664  0.02597 -0.04007  0.11035 -0.02142]  Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J.\n",
      " > 256 [ 0.0358   -0.002596 -0.05072   0.1383    0.007164]  in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz,\n",
      " > 257 [-0.002794 -0.0207   -0.03111   0.0767   -0.00964 ]  Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter-\n",
      " > 258 [-0.03232 -0.01524 -0.04434  0.0564   0.01779]  Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings\n",
      " > 259 [-0.04657  -0.001767 -0.01854   0.0636    0.04724 ]  Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery\n",
      " > 260 [-0.05002  -0.006645 -0.0306    0.0693    0.06213 ]  Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &\n",
      " > 261 [-0.03102   0.002884 -0.05258   0.08435   0.05386 ]  Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and\n",
      " > 262 [-0.03104   0.004654 -0.05667   0.0955    0.04428 ]  & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,”\n",
      " > 263 [-0.04572  0.0285  -0.0493   0.098    0.00894]  and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486.\n",
      " > 264 [-0.0639   0.01843 -0.04575  0.07184  0.01031]  partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J.\n",
      " > 265 [-0.0786    0.00787  -0.0569    0.0796   -0.002981]  475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion\n",
      " > 266 [-0.08966   -0.00773   -0.0655     0.0899    -0.0001345]  J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical\n",
      " > 267 [-0.03928  -0.002499 -0.06433   0.07465   0.0291  ]  diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D.\n",
      " > 268 [-0.02228 -0.0377  -0.02115  0.07367  0.01607]  Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters\n",
      " > 269 [-0.039   -0.005   -0.02776  0.0804  -0.00542]  Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings\n",
      " > 270 [-0.0367   0.04083 -0.03336  0.0726  -0.00932]  Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference\n",
      " > 271 [-0.01426  0.03543 -0.0301   0.05548  0.0247 ]  Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023,\n",
      " > 272 [-0.01578  0.01485 -0.04288  0.07513  0.0426 ]  on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter:\n",
      " > 273 [-0.05145  0.00995 -0.06885  0.0506   0.01502]  2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions\n",
      " > 274 [-0.0834   0.02594 -0.063    0.04425  0.05444]  filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015.\n",
      " > 275 [-0.0453    0.0391   -0.02759   0.0417    0.013824]  Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled\n",
      " > 276 [-0.02159  0.02634 -0.058    0.0683  -0.01804]  2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and\n",
      " > 277 [-0.01451   0.003176 -0.0408    0.0726   -0.00638 ]  labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing\n",
      " > 278 [-0.00523   0.002424 -0.06158   0.03543   0.0314  ]  and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems\n",
      " > 279 [-0.01543  -0.004055 -0.0676    0.03918   0.03165 ]  constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017,\n",
      " > 280 [-0.0376  -0.03342 -0.0613   0.04385  0.00783]  systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,”\n",
      " > 281 [-0.03976  -0.014824 -0.0613    0.05273   0.003597]  2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,”\n",
      " > 282 [-0.0106  -0.01206 -0.02783  0.0142  -0.01933]  graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp. 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and\n",
      " > 283 [-0.00771    0.00641   -0.01678   -0.0002708  0.02597  ]  applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp. 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional\n",
      " > 284 [-0.03992  0.02525 -0.03705  0.0666   0.02007]  and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead,\n",
      " > 285 [-0.03928   0.005642 -0.04077   0.05746  -0.00916 ]  “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference\n",
      " > 286 [-0.0387  -0.0209  -0.03093  0.02443 -0.02061]  Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226.\n",
      " > 287 [-0.0511  -0.04276 -0.04062  0.01269 -0.02621]  Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge\n",
      " > 288 [-0.0707  -0.04272 -0.01056  0.01836 -0.02768]  1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational\n",
      " > 289 [-0.0587  -0.00948 -0.0322   0.00851 -0.02002]  Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings\n",
      " > 290 [-0.02754 -0.00945 -0.05402  0.02844 -0.01668]  “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in\n",
      " > 291 [-0.04654  -0.03644  -0.04456   0.003359 -0.0369  ]  Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations\n",
      " > 292 [-0.02629  -0.01889  -0.07166  -0.006042 -0.02054 ]  in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge\n",
      " > 293 [-0.0356   -0.001446 -0.03903  -0.000639 -0.02512 ]  Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559.\n",
      " > 294 [-0.03214   0.01892  -0.01069   0.002169 -0.002045]  knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han,\n",
      " > 295 [-0.02696   0.01108  -0.00362  -0.00592   0.004467]  556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1,\n",
      " > 296 [-0.03418 -0.03418 -0.01598  0.01021  0.01588]  “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of\n",
      " > 297 [-0.038   -0.03598 -0.0551   0.0401   0.03072]  2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment,\n",
      " > 298 [-0.03592  -0.001046 -0.0434    0.007553  0.00903 ]  of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J.\n",
      " > 299 [-0.0546   -0.02347  -0.0271    0.06247   0.013214]  Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring\n",
      " > 300 [-0.0855   -0.0278   -0.03705   0.0674    0.005737]  J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences\n",
      " > 301 [-0.074   -0.02396 -0.04077  0.02151 -0.01755]  “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H.\n",
      " > 302 [-0.0261    0.012375 -0.06      0.0481   -0.0186  ]  Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient\n",
      " > 303 [-0.0449    0.006535 -0.071     0.0703   -0.01619 ]  H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of\n",
      " > 304 [-0.04968   0.007298 -0.05133   0.06836  -0.011086]  efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen,\n",
      " > 305 [-0.0444   0.01472 -0.0266   0.06616  0.01419]  of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,”\n",
      " > 306 [-0.03473  -0.01421  -0.04388   0.0523    0.009705]  Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms,\n",
      " > 307 [-0.03458   0.003601 -0.0602   -0.002686 -0.02405 ]  labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W.\n",
      " > 308 [-0.0637   0.02588 -0.0646   0.01014 -0.00616]  algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433,\n",
      " > 309 [-0.04675  0.02107 -0.05798  0.05078 -0.00964]  W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association\n",
      " > 310 [-0.01788  0.02081 -0.07886  0.06836 -0.00903]  1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient\n",
      " > 311 [-0.02737   0.011154 -0.05612   0.05542   0.00556 ]  Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye,\n",
      " > 312 [-0.02846   0.01493  -0.0467    0.04984   0.001856]  efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to\n",
      " > 313 [-0.04776 -0.01663 -0.04617  0.01416  0.01627]  Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018,\n",
      " > 314 [-0.0534   0.01156 -0.05402  0.00742  0.0062 ]  to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297.\n",
      " > 315 [-0.0683  -0.02193 -0.03854  0.02652 -0.04028]  2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the\n",
      " > 316 [-0.0414  -0.01807 -0.05746  0.04135 -0.03247]  281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not\n",
      " > 317 [-0.02315  0.02512 -0.06223  0.01706  0.00902]  the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence\n",
      " > 318 [-0.04916  -0.003975 -0.0638   -0.000464  0.02116 ]  available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      " > 319 [-0.02081   0.003202 -0.06354   0.01337   0.02121 ]  Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and\n",
      " > 320 [-0.008675  0.0455   -0.0608    0.04767   0.0082  ]  2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration.\n",
      " > 321 [-0.01642  0.05054 -0.04913  0.0454   0.01595]  and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· −\n",
      " > 322 [-0.06396  0.0454  -0.04428  0.06033  0.03223]  iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V,\n",
      " > 323 [-0.0505   0.03268 -0.0281   0.04852  0.04596]  →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t\n",
      " > 324 [-0.0419   0.07855 -0.0392   0.0682   0.02153]  − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality\n",
      " > 325 [-0.02354  0.0653  -0.0857   0.0243   0.02748]  Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that\n",
      " > 326 [-0.0405   0.07135 -0.0659   0.02249  0.0373 ]  inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at\n",
      " > 327 [-0.02858  0.0614  -0.05145  0.05756  0.0359 ]  that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23)\n",
      " > 328 [-0.03647  0.0826  -0.04492  0.0785   0.0397 ]  at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1\n",
      " > 329 [-0.02855  0.05954 -0.05234  0.093    0.0323 ]  We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X\n",
      " > 330 [-0.03387  0.0401  -0.01881  0.0684   0.01926]  ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line\n",
      " > 331 [-0.00907  0.0363  -0.0355   0.06744  0.03093]  vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1.\n",
      " > 332 [-0.01523  0.02179 -0.04312  0.0429   0.0254 ]  Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication−\n",
      " > 333 [-0.01227  -0.001817 -0.0189    0.06384   0.03757 ]  ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries\n",
      " > 334 [-0.01791  -0.015434 -0.03534   0.0723    0.02481 ]  multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration,\n",
      " > 335 [-0.01505  -0.012794 -0.0484    0.0302    0.0333  ]  entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The\n",
      " > 336 [-0.0003085  0.01869   -0.0654     0.03035    0.00647  ]  iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration\n",
      " > 337 [-0.00078   0.05927  -0.0515    0.03464   0.014656]  theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed\n",
      " > 338 [-0.01371  0.0324  -0.06573  0.03287  0.04718]  iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0. Therefore, the total amount of greedy operations\n",
      " > 339 [-0.01048   -0.007328  -0.0718    -0.0003147  0.05078  ]  entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it-\n",
      " > 340 [-0.00968    0.0008035 -0.0643     0.00693    0.02812  ]  operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and\n",
      " > 341 [ 0.000737  0.03119  -0.0419    0.0198    0.02278 ]  erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we\n",
      " > 342 [-0.02692  0.03001 -0.03857  0.0445   0.0255 ]  Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then,\n",
      " > 343 [-0.04135  0.03424 -0.0281   0.05188  0.01406]  have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we\n",
      " > 344 [-0.02303  0.01628 -0.03035  0.02507  0.01617]  Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes\n",
      " > 345 [-0.01575  0.01544 -0.02045  0.02798  0.0365 ]  we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1,\n",
      " > 346 [-0.03244   0.04825  -0.009705  0.05682   0.05084 ]  completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly,\n",
      " > 347 [-0.0647   0.04633 -0.00205  0.07196  0.0526 ]  λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that\n",
      " > 348 [-0.0732   0.0404  -0.0161   0.0718   0.04733]  Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U\n",
      " > 349 [-0.0521    0.03497  -0.007572  0.0754    0.0492  ]  that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈\n",
      " > 350 [-0.03055  0.03125 -0.01053  0.0665   0.05124]  are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp\n",
      " > 351 [-0.03111  0.04953 -0.02315  0.02861  0.04904]  ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via\n",
      " > 352 [-0.02687  0.06793 -0.01797  0.04428  0.01677]  exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma\n",
      " > 353 [-0.04843   0.0534   -0.007713  0.03488   0.04846 ]  via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d.\n",
      " > 354 [-0.00949    0.0628     0.0007463  0.01149    0.0408   ]  Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity\n",
      " > 355 [ 0.002222  0.0758   -0.02382  -0.01752   0.02621 ]  d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7)\n",
      " > 356 [ 0.004925  0.0721   -0.04816   0.00933   0.04468 ]  larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i\n",
      " > 357 [-0.01234  0.0475  -0.051    0.0246   0.03058]  7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line\n",
      " > 358 [-0.05048  0.05563 -0.03482  0.0392   0.03467]  i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice\n",
      " > 359 [-0.0331   0.04593 -0.05713  0.01949  0.0363 ]  (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi)\n",
      " > 360 [-0.03284  0.0503  -0.0579   0.01675  0.03372]  that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi)\n",
      " > 361 [-0.05005  0.06088 -0.0643   0.0632   0.03552]  vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 ·\n",
      " > 362 [-0.0413   0.02167 -0.06354  0.05914  0.0304 ]  ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A\n",
      " > 363 [-0.065    0.0219  -0.04465  0.06885  0.01024]  · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three\n",
      " > 364 [-0.024    0.04718 -0.01472  0.02968  0.0287 ]  ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with\n",
      " > 365 [-0.002268  0.02054   0.01093  -0.004692  0.05844 ]  three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly\n",
      " > 366 [-0.00787 -0.01255 -0.02321 -0.02182  0.07153]  with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse\n",
      " > 367 [-0.02017  -0.006298 -0.04752  -0.012405  0.0512  ]  nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing\n",
      " > 368 [-0.0443   -0.009155 -0.0655    0.02286   0.0569  ]  AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0\n",
      " > 369 [-0.02678  -0.03049  -0.000994 -0.02278   0.0749  ]  balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0\n",
      " > 370 [-0.01933   0.02132   0.033    -0.008995  0.06067 ]  0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8\n",
      " > 371 [-0.0411   0.03632  0.02394 -0.02074  0.0793 ]  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance\n",
      " > 372 [-0.0511   0.0414   0.02669 -0.0033   0.08044]  8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with\n",
      " > 373 [-0.06647   0.014786 -0.00899   0.03333   0.06464 ]  performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show\n",
      " > 374 [-0.03452  0.0356   0.00256  0.03952  0.06198]  with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased\n",
      " > 375 [-0.01828    0.04803    0.01031   -0.0009985  0.04623  ]  show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason\n",
      " > 376 [-0.0701   0.04895 -0.0145   0.01646  0.05286]  increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest\n",
      " > 377 [-0.02792  0.0469  -0.01904  0.01581  0.03455]  reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)),\n",
      " > 378 [ 0.005836  0.01566  -0.00811   0.02419   0.03824 ]  manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented\n",
      " > 379 [-0.003187 -0.02197  -0.002235  0.0314    0.02289 ]  (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients,\n",
      " > 380 [ 0.000669 -0.01665  -0.010544  0.025     0.07    ]  implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust\n",
      " > 381 [-0.00453   0.012566 -0.01252   0.02994   0.066   ]  ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from\n",
      " > 382 [0.002172 0.012794 0.009834 0.02005  0.0798  ]  robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808\n",
      " > 383 [0.00427 0.02669 0.02956 0.00915 0.11206]  from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554\n",
      " > 384 [-0.007057  0.0477    0.001934  0.03836   0.0638  ]  0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster\n",
      " > 385 [-0.03616  0.0632  -0.02669  0.05804  0.0439 ]  0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates\n",
      " > 386 [-0.06274  0.0973  -0.04993  0.07556  0.04654]  within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best\n",
      " > 387 [-0.04373  0.0681  -0.02316  0.05475  0.04437]  evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA\n",
      " > 388 [-0.0395    0.03754  -0.010956  0.05634   0.04883 ]  best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness\n",
      " > 389 [-0.0532   0.0453  -0.0143   0.06903  0.05142]  LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in\n",
      " > 390 [-0.04755  0.03452 -0.01569  0.0253   0.0599 ]  cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓\n",
      " > 391 [-0.04797   0.002983 -0.004288  0.02173   0.0586  ]  in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237\n",
      " > 392 [-0.05927   0.05286   0.02103   0.0406    0.002258]  Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156\n",
      " > 393 [-0.04416   0.05447   0.03934   0.0352    0.006153]  0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251\n",
      " > 394 [ 0.0015135  0.0532     0.00341   -0.00074    0.0007095]  0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707\n",
      " > 395 [-0.02383   0.05466  -0.005997 -0.003782 -0.005146]  0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811\n",
      " > 396 [-0.05087   0.0474   -0.02339  -0.001264 -0.01456 ]  0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86\n",
      " > 397 [-0.03964   0.04282  -0.00923   0.011604 -0.01616 ]  0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562\n",
      " > 398 [-0.03085   0.0352    0.01715   0.06046   0.003223]  0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992\n",
      " > 399 [-0.02223   0.02255   0.009964  0.02895   0.05035 ]  0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32\n",
      " > 400 [-0.004738  0.00547  -0.02327   0.03784   0.04803 ]  0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion\n",
      " > 401 [-0.002249  0.0241   -0.00449   0.02641   0.0613  ]  64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an\n",
      " > 402 [-0.02507  0.05295 -0.02084  0.02292  0.0757 ]  threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe\n",
      " > 403 [-0.02257  0.02122 -0.01738  0.02258  0.05365]  order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains\n",
      " > 404 [-0.0263   0.02275 -0.0283   0.0664   0.00385]  that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making\n",
      " > 405 [-0.0244   0.01155 -0.01846  0.0614   0.02219]  contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result\n",
      " > 406 [-0.009     0.011734 -0.006687  0.03558   0.0472  ]  making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the\n",
      " > 407 [-0.0121    0.0383    0.007202  0.0241    0.03427 ]  in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The\n",
      " > 408 [-0.0118   0.0179   0.02116  0.01788  0.0388 ]  the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871\n",
      " > 409 [-0.05045   0.01854  -0.015144  0.00886   0.066   ]  The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs\n",
      " > 410 [-0.02867    0.0228     0.0003724  0.0634     0.01358  ]  0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes\n",
      " > 411 [-0.02614  0.01162 -0.0446   0.10913 -0.03662]  graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased\n",
      " > 412 [-0.05127  0.03293 -0.0527   0.1082  -0.00762]  nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions\n",
      " > 413 [-0.04852  0.03995 -0.00805  0.09265  0.03955]  co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA\n",
      " > 414 [-0.02565  0.0369  -0.0233   0.06793  0.0376 ]  precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement\n",
      " > 415 [-0.04697  0.01564 -0.0325   0.07056  0.05432]  LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked\n",
      " > 416 [-0.0393   0.01892 -0.0291   0.04977  0.06146]  improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181\n",
      " > 417 [-0.006996  0.04288   0.002956  0.003136  0.07025 ]  remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808\n",
      " > 418 [0.01996 0.0541  0.0688  0.0374  0.05222]  0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE\n",
      " > 419 [0.002682 0.03093  0.04718  0.01732  0.06064 ]  0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗\n",
      " > 420 [-0.00342   0.012115 -0.00778   0.01158   0.03026 ]  TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the\n",
      " > 421 [-0.009445  0.00917  -0.01974   0.02577   0.0277  ]  ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs,\n",
      " > 422 [-0.02567  0.01295 -0.05276  0.0266   0.02625]  the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity\n",
      " > 423 [-0.04584  0.0273  -0.0644   0.04678  0.01094]  vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj)\n",
      " > 424 [-0.05856  0.03537 -0.055    0.02911  0.01979]  affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original\n",
      " > 425 [-0.03662  0.03253 -0.02782  0.052    0.02502]  · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most\n",
      " > 426 [-0.0196   0.04355 -0.0133   0.03726  0.0615 ]  original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the\n",
      " > 427 [-0.02522  0.036   -0.05115  0.02747  0.03075]  most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and\n",
      " > 428 [-0.03812  0.04465 -0.0931   0.08014  0.02039]  the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster\n",
      " > 429 [-0.0188    0.02159  -0.014435  0.05664   0.07416 ]  and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and\n",
      " > 430 [-0.01466  0.03876  0.02835  0.01405  0.04037]  cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient\n",
      " > 431 [ 0.002928  0.04877  -0.01032   0.0442    0.01994 ]  and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these\n",
      " > 432 [-0.01467  0.0346  -0.00515  0.02536  0.04956]  coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network\n",
      " > 433 [-0.0598   0.015   -0.02884  0.02684  0.06226]  two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering.\n",
      " > 434 [-0.10205  -0.002577 -0.03842   0.02167   0.0727  ]  Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional\n",
      " > 435 [-0.06976  -0.004925 -0.0479    0.03363   0.06995 ]  clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial\n",
      " > 436 [-0.03943   -0.0002513 -0.05435    0.0274     0.05777  ]  dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories.\n",
      " > 437 [-0.01797   0.003899 -0.0593    0.0412    0.10223 ]  initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip-\n",
      " > 438 [-0.05734  0.01379 -0.0594   0.0357   0.0926 ]  categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph. 21\n",
      " > 439 [-0.05966  0.0251  -0.03885  0.0747   0.03305] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering\n",
      " > 440 [-0.03027  0.03235 -0.0667   0.0965   0.0402 ]  size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded\n",
      " > 441 [-0.02742  0.03827 -0.08075  0.0866   0.03787]  clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph\n",
      " > 442 [-0.01859  0.01143 -0.06274  0.07745  0.03378]  theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks,\n",
      " > 443 [-0.02417   0.007774 -0.06183   0.09393   0.01334 ]  graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology\n",
      " > 444 [-0.0415  -0.01338 -0.0669   0.0969   0.0441 ]  networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly\n",
      " > 445 [-0.05252  0.0063  -0.04868  0.06238  0.0397 ]  methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal\n",
      " > 446 [-0.0278   0.02759 -0.0359   0.0568   0.02019]  mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are\n",
      " > 447 [-0.03029  0.03308 -0.03384  0.05762  0.02544]  sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33]\n",
      " > 448 [-0.04932  0.02055 -0.02606  0.0725   0.0336 ]  often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1\n",
      " > 449 [-0.0318   0.01287 -0.0486   0.08167  0.0484 ]  [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs\n",
      " > 450 [-0.02176  -0.009705 -0.04297   0.0732    0.0746  ]  arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large\n",
      " > 451 [-0.02028  -0.002424 -0.05496   0.0647    0.0517  ]  vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse,\n",
      " > 452 [-0.007168 -0.02188  -0.0664    0.02844   0.02696 ]  large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the\n",
      " > 453 [-0.01128 -0.0268  -0.04025  0.03183  0.0326 ]  AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the\n",
      " > 454 [-0.02255   -0.0001911 -0.0313     0.06088    0.06744  ]  approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote\n",
      " > 455 [-0.02684 -0.01135 -0.04037  0.03214  0.04706]  the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of\n",
      " > 456 [-0.02287   0.04373  -0.03165   0.002956 -0.009125]  denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in\n",
      " > 457 [-0.01189  0.03098 -0.03256  0.02235 -0.01729]  of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector\n",
      " > 458 [-0.03397   0.0401   -0.013695  0.008484 -0.01813 ]  Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi).\n",
      " > 459 [-0.0491   0.0276  -0.02985  0.0855   0.0346 ]  vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its\n",
      " > 460 [-0.05157  0.04013 -0.03001  0.0852   0.05164]  d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the\n",
      " > 461 [-0.0469   0.02794 -0.0662   0.05612  0.0483 ]  its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$\n",
      " > 462 [-0.05313  0.02457 -0.05472  0.02692  0.0214 ]  the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ\n",
      " > 463 [-0.02917  0.01516 -0.0625   0.06354  0.03674]  𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity\n",
      " > 464 [-0.03006  -0.004913 -0.06015   0.0938    0.0439  ]  →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at\n",
      " > 465 [-0.02887  0.05466 -0.0716   0.07837  0.01717]  similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below\n",
      " > 466 [-0.03784  0.04828 -0.05026  0.07227 -0.01701]  at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊)\n",
      " > 467 [-0.04395  0.03943 -0.0389   0.0643   0.02306]  process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value\n",
      " > 468 [-0.04697  0.05228 -0.02882  0.0556   0.0486 ]  𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other\n",
      " > 469 [-0.01195  0.02664 -0.05862  0.0762   0.06216]  value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the\n",
      " > 470 [ 0.005646  0.01817  -0.05148   0.08203   0.0653  ]  other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the\n",
      " > 471 [-0.0283   0.04105 -0.0627   0.04     0.058  ]  the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD\n",
      " > 472 [-0.01221  0.02496 -0.0627   0.02911  0.0278 ]  the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2\n",
      " > 473 [-0.009605  0.01955  -0.02663   0.02118   0.04587 ]  value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA. where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely\n",
      " > 474 [-0.01816  0.0432   0.01171  0.0125   0.02866]  𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA. where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed\n",
      " > 475 [-0.01058  0.04642 -0.01155  0.04068  0.03043]  immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time,\n",
      " > 476 [-0.00941  0.01395 -0.0639   0.0351   0.04498]  seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7\n",
      " > 477 [-0.00748   0.007225 -0.0901    0.009026  0.0451  ]  time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector\n",
      " > 478 [ 0.0003533  0.02426   -0.05356    0.02518    0.01823  ]  𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively,\n",
      " > 479 [ 0.01723   0.007515 -0.04684   0.04065   0.02277 ]  vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD\n",
      " > 480 [ 0.01672  0.01758 -0.04974  0.04312  0.0481 ]  respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic\n",
      " > 481 [ 0.00947  0.01802 -0.04382  0.02942  0.01495]  BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges\n",
      " > 482 [ 0.00978  0.01747 -0.02206  0.02821 -0.00983]  algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and\n",
      " > 483 [ 0.001304 -0.00421  -0.03665   0.0376   -0.00608 ]  edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse\n",
      " > 484 [-0.00885 -0.03099 -0.0539   0.0393   0.01552]  − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative\n",
      " > 485 [-0.02486  0.01917 -0.04266  0.04822  0.01233]  GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we\n",
      " > 486 [-0.02458   0.0336   -0.04004   0.0757   -0.010574]  iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)−\n",
      " > 487 [ 0.00243   0.02808  -0.0494    0.0518    0.006756]  we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010\n",
      " > 488 [ 0.0128   0.00904 -0.04712  0.02869  0.02682]  α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ\n",
      " > 489 [-0.01027   0.001535 -0.03787   0.04715   0.019   ]  max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the\n",
      " > 490 [-0.01988  0.01398 -0.03006  0.06396  0.01203]  (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus,\n",
      " > 491 [-0.02448  0.0071  -0.07196  0.0875   0.01675]  the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns\n",
      " > 492 [-0.0242    0.010796 -0.0292    0.0559    0.01715 ]  Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17)\n",
      " > 493 [-0.02437  -0.015114 -0.0347    0.0395    0.017   ]  returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all\n",
      " > 494 [-0.0166   -0.001597 -0.0704    0.05112   0.006863]  (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and\n",
      " > 495 [-0.02132   0.01518  -0.07916   0.0694    0.008484]  nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals\n",
      " > 496 [-0.02536  -0.005363 -0.0871    0.0566    0.01723 ]  and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy\n",
      " > 497 [-0.0193    0.006207 -0.0861    0.04764   0.03357 ]  residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast\n",
      " > 498 [-0.01388   0.001596 -0.0824    0.02063   0.0451  ]  greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of\n",
      " > 499 [-0.004044 -0.0203   -0.0657    0.03433   0.02592 ]  contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased\n",
      " > 500 [-0.007   -0.00943 -0.06     0.03247  0.04276]  of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and\n",
      " > 501 [ 0.00787  -0.001864 -0.04294   0.02667   0.0513  ]  increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil\n",
      " > 502 [ 0.01004  0.0073  -0.0243   0.01022  0.04242]  and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq. (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k\n",
      " > 503 [-0.0002254  0.013596  -0.00279   -0.006493   0.01585  ]  unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq. (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B\n",
      " > 504 [-0.004738  0.0446    0.00422   0.01688   0.01743 ]  k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i)\n",
      " > 505 [-0.02351  0.04025 -0.0328   0.0447   0.0357 ]  II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function,\n",
      " > 506 [-0.014404  0.0226   -0.04956   0.0418    0.02934 ]  (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates\n",
      " > 507 [ 0.01883  0.02367 -0.03644  0.02786  0.02722]  function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1\n",
      " > 508 [ 0.00458  0.02213 -0.01607  0.02057  0.02766]  a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed.\n",
      " > 509 [-0.01843    0.03525   -0.05396    0.0001371  0.01265  ]  →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs\n",
      " > 510 [ 0.01256  0.01567 -0.07306  0.04507  0.0232 ]  distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place,\n",
      " > 511 [ 0.021    0.02748 -0.0407   0.0244   0.02992]  constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries\n",
      " > 512 [ 0.000856  0.02127  -0.0337    0.03583   0.02635 ]  place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate\n",
      " > 513 [ 0.006313  0.01819  -0.03326   0.01877   0.04883 ]  entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ\n",
      " > 514 [ 0.01226  0.02478 -0.03555  0.01648  0.05865]  approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines\n",
      " > 515 [-0.02467   0.002172 -0.02625   0.02347   0.04745 ]  ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) .\n",
      " > 516 [-0.0247  -0.01292 -0.0255   0.03262  0.04285]  6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈\n",
      " > 517 [-0.03732  0.03114 -0.0294   0.0602   0.0517 ]  . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51]\n",
      " > 518 [-0.03967  0.02184 -0.03833  0.03998  0.02771]  ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are\n",
      " > 519 [-0.02773 -0.01567 -0.02376  0.05487  0.0425 ]  [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the\n",
      " > 520 [-0.02184  0.01408 -0.03235  0.0851   0.0652 ]  are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis,\n",
      " > 521 [-0.01683   0.002851 -0.03412   0.07367   0.03622 ]  the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the\n",
      " > 522 [-0.03436   -0.01142   -0.01988    0.06195   -0.0005455]  analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively.\n",
      " > 523 [-0.05267  -0.007153 -0.0242    0.0839   -0.0084  ]  the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n)\n",
      " > 524 [-0.03503 -0.02817 -0.03256  0.07184  0.0113 ]  respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product\n",
      " > 525 [-0.04562 -0.01746 -0.0225   0.0615   0.06616]  ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm\n",
      " > 526 [-0.0364  -0.02051 -0.02644  0.07214  0.05722]  product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms,\n",
      " > 527 [-0.0688  -0.01807 -0.0479   0.04187  0.02458]  p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods\n",
      " > 528 [-0.07886  0.01646 -0.05475  0.07837  0.03552]  all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters.\n",
      " > 529 [-0.04852   0.002876 -0.04016   0.08234   0.01365 ]  methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid\n",
      " > 530 [-0.049   -0.01204 -0.03812  0.065    0.02124]  clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters\n",
      " > 531 [-0.02621 -0.02309 -0.05795  0.02141  0.03693]  grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by\n",
      " > 532 [-0.01924  0.00386 -0.0558   0.04047  0.0707 ]  clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE,\n",
      " > 533 [-0.0241     0.0009365 -0.02228    0.01515    0.09143  ]  by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and\n",
      " > 534 [-0.03986   0.002424 -0.01356   0.03027   0.04285 ]  PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20]\n",
      " > 535 [-0.05518   0.02917  -0.013664 -0.003122  0.0448  ]  SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154\n",
      " > 536 [-0.0252    -0.0003357 -0.003895   0.02888    0.01484  ]  [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN)\n",
      " > 537 [-0.03586  0.00766  0.01378  0.0337   0.05707]  0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2\n",
      " > 538 [-0.02484  0.00765  0.03192  0.0607   0.03023]  (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the\n",
      " > 539 [-0.005768  0.000605  0.0241    0.05914   0.03854 ]  10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance\n",
      " > 540 [-0.02518  -0.007668  0.01883   0.0644    0.04822 ]  the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their\n",
      " > 541 [-0.01035   0.0053   -0.008644  0.0737    0.0627  ]  performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be.\n",
      " > 542 [ 0.00817  0.00686 -0.02745  0.0658   0.05325]  their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes\n",
      " > 543 [-0.03014   -0.0001978 -0.01967    0.03726    0.0358   ]  be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time\n",
      " > 544 [-0.04755  0.01645 -0.0444   0.04562  0.0498 ]  sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M\n",
      " > 545 [-0.04242  0.01454  0.02293  0.02849  0.06155]  (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS)\n",
      " > 546 [-0.02751   0.00396   0.013695  0.03488   0.0871  ]  Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed.\n",
      " > 547 [-0.02112  0.01114 -0.05438  0.0481   0.04102]  obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table\n",
      " > 548 [-0.02267   0.011154 -0.05698   0.0519    0.04715 ]  seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage,\n",
      " > 549 [-0.013275 -0.0185   -0.0678    0.0647    0.0627  ]  (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art\n",
      " > 550 [-0.01752 -0.00918 -0.05182  0.0716   0.0693 ]  all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage)\n",
      " > 551 [-0.01358  0.00952 -0.02658  0.01539  0.0885 ]  state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary,\n",
      " > 552 [-0.02591 -0.00826 -0.03824  0.0442   0.0614 ]  stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang\n",
      " > 553 [-0.02606  -0.000682 -0.02864   0.04483   0.0437  ]  summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the\n",
      " > 554 [-0.02849  0.00523  0.00381  0.02715  0.01982]  Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)).\n",
      " > 555 [-0.02013   0.002918 -0.03041   0.0554    0.02531 ]  the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local\n",
      " > 556 [-0.03607   0.001306 -0.0537    0.0636    0.03934 ]  (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15]\n",
      " > 557 [-0.05045 -0.0311  -0.05142  0.0749   0.0661 ]  local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on\n",
      " > 558 [-0.02681 -0.02615 -0.05453  0.06757  0.02815]  [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly\n",
      " > 559 [-0.03192   0.003494 -0.08636   0.0676    0.009575]  running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities,\n",
      " > 560 [-0.04648  0.0212  -0.08     0.08264  0.02356]  costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching\n",
      " > 561 [-0.0716   0.0415  -0.04184  0.06586  0.02849]  communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints\n",
      " > 562 [-0.0472   0.02084 -0.0471   0.0667   0.03323]  the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA,\n",
      " > 563 [-0.03665  0.02205 -0.08264  0.0521   0.03058]  constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17\n",
      " > 564 [-0.0352   0.00645 -0.0412   0.0633   0.03537]  LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference\n",
      " > 565 [-0.0629   0.00986 -0.03995  0.06476 -0.01278]  17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities\n",
      " > 566 [-0.01064   0.004322 -0.0627    0.1003    0.003572]  (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz,\n",
      " > 567 [ 0.005173 -0.00956  -0.05167   0.1133    0.001737]  in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings\n",
      " > 568 [-0.05157 -0.02473 -0.03622  0.0768   0.04828]  Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &\n",
      " > 569 [-0.0366  -0.00545 -0.0429   0.09674  0.06122]  Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,”\n",
      " > 570 [-0.05173  0.00569 -0.0618   0.08746  0.0294 ]  & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J.\n",
      " > 571 [-0.0711   0.01162 -0.07025  0.0819   0.0085 ]  partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical\n",
      " > 572 [-0.0651   -0.010735 -0.05707   0.0885    0.00798 ]  J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters\n",
      " > 573 [-0.01701  -0.018    -0.01923   0.07764   0.012314]  Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference\n",
      " > 574 [-0.02473  0.0273  -0.04297  0.0748   0.0201 ]  Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter:\n",
      " > 575 [-0.0437   0.02586 -0.0528   0.04987  0.0474 ]  on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015.\n",
      " > 576 [-0.05173  0.01816 -0.0635   0.05212  0.02509]  filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and\n",
      " > 577 [-0.014465  -0.00704   -0.0523     0.04837   -0.0008817]  2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems\n",
      " > 578 [-0.02061 -0.0343  -0.0775   0.04132  0.01537]  and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,”\n",
      " > 579 [-0.02547 -0.02333 -0.059    0.02705 -0.00436]  systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp. 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and\n",
      " > 580 [-0.01602   0.003695 -0.03986   0.03806  -0.0074  ]  graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp. 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead,\n",
      " > 581 [-0.03778  -0.00675  -0.04526   0.04578  -0.003376]  and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226.\n",
      " > 582 [-0.0601  -0.02956 -0.03     0.0396  -0.0315 ]  Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational\n",
      " > 583 [-0.05414 -0.03015 -0.0382   0.03558 -0.01671]  1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in\n",
      " > 584 [-0.02364 -0.02992 -0.07446  0.02017 -0.02716]  “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge\n",
      " > 585 [-0.0352   -0.0135   -0.04865  -0.001491 -0.00885 ]  in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han,\n",
      " > 586 [-0.03677  -0.00828  -0.02438   0.003557  0.015396]  knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of\n",
      " > 587 [-0.02988 -0.02522 -0.0331   0.00666  0.01023]  “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J.\n",
      " > 588 [-0.0674  -0.02518 -0.05423  0.05182  0.00912]  of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences\n",
      " > 589 [-0.07306  -0.03375  -0.05316   0.06793   0.004814]  J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient\n",
      " > 590 [-0.03867   0.01181  -0.05737   0.066    -0.009796]  Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen,\n",
      " > 591 [-0.04617  -0.003315 -0.05246   0.07794   0.00503 ]  efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms,\n",
      " > 592 [-0.04605   0.000993 -0.065     0.04788  -0.001592]  Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433,\n",
      " > 593 [-0.04614   0.02028  -0.0729    0.04648  -0.005077]  algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient\n",
      " > 594 [-0.03543  0.01839 -0.0756   0.05896 -0.00509]  1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to\n",
      " > 595 [-0.0461   0.00627 -0.07275  0.0211  -0.00427]  efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297.\n",
      " > 596 [-0.04813 -0.01495 -0.0772   0.02568 -0.01211]  to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not\n",
      " > 597 [-0.03125 -0.01955 -0.06696  0.03458 -0.02374]  281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      " > 598 [-0.02336  0.04367 -0.06036  0.04642  0.02379]  available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration.\n",
      " > 599 [-0.02003  0.03802 -0.0681   0.04944  0.01654]  2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V,\n",
      " > 600 [-0.0488   0.06525 -0.0632   0.0649   0.02904]  iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality\n",
      " > 601 [-0.02705  0.05847 -0.0666   0.01762  0.0299 ]  − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at\n",
      " > 602 [-0.03668  0.0722  -0.0727   0.03143  0.0455 ]  inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1\n",
      " > 603 [-0.0342   0.0698  -0.04172  0.0817   0.01544]  at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line\n",
      " > 604 [-0.01686  0.02475 -0.0509   0.04752  0.0316 ]  ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication−\n",
      " > 605 [-0.01744  0.00792 -0.03052  0.04916  0.03964]  Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration,\n",
      " > 606 [-0.01452  0.02705 -0.0671   0.04044  0.01472]  multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration\n",
      " > 607 [-0.01857  0.039   -0.066    0.03168  0.03638]  iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0. Therefore, the total amount of greedy operations\n",
      " > 608 [-0.01065  0.0239  -0.05826  0.0217   0.0374 ]  iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and\n",
      " > 609 [-0.01171  0.01031 -0.05878  0.0302   0.02092]  operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then,\n",
      " > 610 [-0.01567   0.01189  -0.03693   0.007343  0.02719 ]  Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes\n",
      " > 611 [-0.01514  0.02126 -0.01083  0.03084  0.0342 ]  Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly,\n",
      " > 612 [-0.04266   0.0637   -0.003775  0.05795   0.04962 ]  completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U\n",
      " > 613 [-0.0455   0.04196 -0.00487  0.0606   0.0506 ]  Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp\n",
      " > 614 [-0.03119   0.04578  -0.011345  0.0689    0.0468  ]  are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma\n",
      " > 615 [-0.010826  0.06366  -0.01581   0.006275  0.03937 ]  exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity\n",
      " > 616 [-0.00518    0.0683    -0.02605    0.0016365  0.0349   ]  Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i\n",
      " > 617 [-0.005226  0.08154  -0.0568    0.02007   0.0354  ]  larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice\n",
      " > 618 [-0.0278   0.05923 -0.05753  0.02925  0.02824]  i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi)\n",
      " > 619 [-0.03912  0.02486 -0.0684   0.04895  0.03044]  that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A\n",
      " > 620 [-0.03336  0.04025 -0.04294  0.04282  0.01953]  ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with\n",
      " > 621 [-0.01272   0.007072 -0.02289  -0.01422   0.03778 ]  ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse\n",
      " > 622 [-0.002945 -0.012344 -0.0565   -0.000802  0.04575 ]  with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0\n",
      " > 623 [-0.0266  -0.01071 -0.03586  0.02258  0.05463]  AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8\n",
      " > 624 [-0.0484    0.02017   0.014534 -0.006626  0.0779  ]  0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with\n",
      " > 625 [-0.04614   0.03662   0.02185   0.004845  0.0625  ]  8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased\n",
      " > 626 [-0.04904   0.03693   0.001612  0.03018   0.0418  ]  with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest\n",
      " > 627 [-0.02574  0.02173 -0.02898  0.0176   0.02745]  increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented\n",
      " > 628 [ 0.007675  0.0109   -0.02034   0.02292   0.04398 ]  manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust\n",
      " > 629 [ 0.012215  -0.0001095 -0.005566   0.02357    0.07697  ]  implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808\n",
      " > 630 [ 0.01374  0.01994 -0.005    0.0221   0.0797 ]  robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster\n",
      " > 631 [-0.03198  0.0703  -0.03128  0.04428  0.05176]  0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best\n",
      " > 632 [-0.06204  0.07275 -0.0408   0.076    0.03882]  within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness\n",
      " > 633 [-0.0501   0.03207 -0.02063  0.03607  0.0427 ]  best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓\n",
      " > 634 [-0.04922  0.0356  -0.00879  0.03117  0.04782]  cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156\n",
      " > 635 [-0.04352    0.03964    0.01617    0.0294    -0.0008626]  Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707\n",
      " > 636 [-0.02359   0.03455  -0.01613  -0.012215 -0.00814 ]  0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86\n",
      " > 637 [-0.03827   0.03452  -0.006973  0.03412  -0.01125 ]  0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992\n",
      " > 638 [-0.00915   0.007397 -0.01952   0.04953   0.02577 ]  0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion\n",
      " > 639 [ 4.852e-05  1.678e-02 -1.459e-02  2.652e-02  5.948e-02]  0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe\n",
      " > 640 [-0.02617  0.02695 -0.02834  0.0408   0.04004]  threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making\n",
      " > 641 [-0.005474  0.010826 -0.0091    0.03418   0.02864 ]  that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the\n",
      " > 642 [-0.0059   0.01227 -0.00563  0.02126  0.03198]  making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871\n",
      " > 643 [-0.02441   0.01845  -0.014336  0.04602   0.02182 ]  the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes\n",
      " > 644 [-0.03607   0.02002  -0.01296   0.0794    0.009186]  0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions\n",
      " > 645 [-0.0365   0.01752 -0.02408  0.0782   0.0147 ]  nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement\n",
      " > 646 [-0.03616  0.02682 -0.04028  0.05698  0.03763]  precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181\n",
      " > 647 [-0.02928  0.01945 -0.02054  0.0376   0.05817]  improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE\n",
      " > 648 [0.002232 0.0231   0.006737 0.02208  0.03244 ]  0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the\n",
      " > 649 [-0.01698  0.00879 -0.0346   0.02632  0.0204 ]  TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity\n",
      " > 650 [-0.02353  0.02052 -0.0664   0.03836  0.02771]  the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original\n",
      " > 651 [-0.0455   0.02916 -0.04477  0.04007  0.0504 ]  affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the\n",
      " > 652 [-0.02524  0.0538  -0.06573  0.0494   0.0361 ]  original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster\n",
      " > 653 [-0.003191  0.01761  -0.01117   0.03934   0.011826]  the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient\n",
      " > 654 [-0.0152    0.03165  -0.001853  0.02158   0.02101 ]  cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network\n",
      " > 655 [-0.0358    0.002813 -0.02524   0.02927   0.05273 ]  coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional\n",
      " > 656 [-0.0739   -0.011505 -0.0425    0.03018   0.0801  ]  Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories.\n",
      " > 657 [-0.03598 -0.01189 -0.0681   0.03314  0.07404]  dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph. 21\n",
      " > 658 [-0.02942  0.03647 -0.05655  0.0718   0.02843] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph\n",
      " > 659 [-0.02104  0.0179  -0.06366  0.0772   0.03207]  clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology\n",
      " > 660 [-0.03204   0.009155 -0.0658    0.0789    0.0235  ]  graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal\n",
      " > 661 [-0.03546  0.0361  -0.0522   0.05545  0.0328 ]  methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33]\n",
      " > 662 [-0.02902  0.01498 -0.05026  0.07605  0.03424]  sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs\n",
      " > 663 [-0.03384  0.01476 -0.05203  0.0787   0.0489 ]  [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse,\n",
      " > 664 [-0.02266  -0.006447 -0.05267   0.05637   0.04486 ]  vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the\n",
      " > 665 [-0.014114 -0.02342  -0.04565   0.0258    0.03415 ]  AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of\n",
      " > 666 [-0.01058  -0.009094 -0.02246   0.0147    0.02625 ]  the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector\n",
      " > 667 [-0.0296    0.02347  -0.014336  0.04834   0.00905 ]  of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its\n",
      " > 668 [-0.04797  0.02493 -0.03625  0.05557  0.04932]  vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$\n",
      " > 669 [-0.0305     0.0015745 -0.06744    0.0655     0.04684  ]  its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity\n",
      " > 670 [-0.0325   0.01289 -0.0584   0.07324  0.03065]  𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below\n",
      " > 671 [-0.0342   0.04065 -0.0382   0.07715  0.035  ]  similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value\n",
      " > 672 [-0.02313  0.02599 -0.03363  0.06274  0.0631 ]  process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the\n",
      " > 673 [-0.02057  0.02647 -0.0489   0.0644   0.06183]  value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD\n",
      " > 674 [-0.002039  0.01671  -0.0423    0.02464   0.03677 ]  the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA. where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely\n",
      " > 675 [-0.007057  0.00962  -0.03098   0.02235   0.04672 ]  value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA. where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time,\n",
      " > 676 [-0.014046  0.01889  -0.0677    0.02298   0.04077 ]  immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector\n",
      " > 677 [ 0.006725  -0.0001826 -0.0535     0.02437    0.04852  ]  time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD\n",
      " > 678 [ 0.002934  0.02394  -0.04184   0.02957   0.02536 ]  vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges\n",
      " > 679 [ 0.00827    0.0009336 -0.03186    0.0324     0.0147   ]  BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse\n",
      " > 680 [-0.011185 -0.002092 -0.03674   0.04355  -0.001942]  edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we\n",
      " > 681 [-0.0196    0.014465 -0.0481    0.0479    0.02812 ]  GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010\n",
      " > 682 [-0.00616   0.0073   -0.03754   0.0487    0.005768]  we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the\n",
      " > 683 [-0.01907 -0.01996 -0.05234  0.05414  0.01752]  max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns\n",
      " > 684 [-0.02292 -0.00097 -0.0506   0.0681   0.018  ]  the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all\n",
      " > 685 [-0.03677  -0.013855 -0.05457   0.06177   0.02065 ]  returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals\n",
      " > 686 [-0.0174    0.00751  -0.0822    0.04684   0.011566]  nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast\n",
      " > 687 [-0.01391  -0.000556 -0.0807    0.0477    0.02834 ]  residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased\n",
      " > 688 [ 0.0172  -0.00445 -0.04474  0.03494  0.046  ]  contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil\n",
      " > 689 [ 0.02313  -0.02167  -0.0352    0.014656  0.04126 ]  increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq. (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B\n",
      " > 690 [ 0.00953   0.0037   -0.0064    0.005493  0.02228 ]  unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq. (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function,\n",
      " > 691 [ 0.00544  0.03014 -0.02414  0.0476   0.0355 ]  II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1\n",
      " > 692 [ 0.01581  0.02023 -0.03674  0.02182  0.0334 ]  function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs\n",
      " > 693 [ 0.00805  0.02275 -0.0557   0.01616  0.02614]  →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries\n",
      " > 694 [ 0.01282  0.02014 -0.04965  0.01636  0.04388]  constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ\n",
      " > 695 [-0.01345  -0.006332 -0.02614   0.00833   0.0428  ]  entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) .\n",
      " > 696 [-0.01312  0.00973 -0.02818  0.02802  0.0554 ]  ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51]\n",
      " > 697 [-0.0259    0.010445 -0.011116  0.05716   0.06775 ]  . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the\n",
      " > 698 [-0.02359  -0.00879  -0.010155  0.0616    0.03836 ]  [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the\n",
      " > 699 [-0.02231  -0.007427 -0.01872   0.0867    0.03015 ]  the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n)\n",
      " > 700 [-0.0393  -0.02032 -0.02208  0.0778   0.04544]  the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm\n",
      " > 701 [-0.04776 -0.0217  -0.0294   0.05484  0.0648 ]  ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods\n",
      " > 702 [-0.05685  -0.005474 -0.04047   0.0704    0.005753]  p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid\n",
      " > 703 [-0.04138 -0.01213 -0.042    0.0816   0.0369 ]  methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by\n",
      " > 704 [-0.03363  -0.014824 -0.03793   0.01894   0.0664  ]  grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and\n",
      " > 705 [-0.04398   0.004215 -0.01747   0.01412   0.082   ]  by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154\n",
      " > 706 [-0.0587    0.01878   0.001186  0.010345  0.06494 ]  SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2\n",
      " > 707 [-0.013824  0.002958  0.02794   0.05222   0.0459  ]  0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance\n",
      " > 708 [-0.004063 -0.00208   0.007507  0.06396   0.05103 ]  10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be.\n",
      " > 709 [-0.02469   0.002707 -0.00873   0.05716   0.05585 ]  performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time\n",
      " > 710 [-0.03308    0.001169   0.0006247  0.0383     0.043    ]  be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS)\n",
      " > 711 [-0.03818   -0.0002458 -0.02254    0.02895    0.04208  ]  (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table\n",
      " > 712 [-0.02298 -0.01546 -0.05704  0.05347  0.0562 ]  obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art\n",
      " > 713 [-0.01481 -0.01796 -0.06506  0.0515   0.06476]  (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary,\n",
      " > 714 [-0.0145   0.01152 -0.03336  0.04095  0.06058]  state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the\n",
      " > 715 [-0.02708   0.002703 -0.02856   0.0539    0.0359  ]  summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local\n",
      " > 716 [-0.03513 -0.02258 -0.04303  0.0514   0.0324 ]  the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on\n",
      " > 717 [-0.04623 -0.02176 -0.075    0.0803   0.03397]  local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities,\n",
      " > 718 [-0.03586  0.02237 -0.0685   0.0703   0.02094]  running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints\n",
      " > 719 [-0.03757  0.02794 -0.06155  0.05753  0.03577]  communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17\n",
      " > 720 [-0.03708  0.00819 -0.05734  0.05527  0.01888]  constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities\n",
      " > 721 [-0.04987   0.002188 -0.06      0.07043  -0.005272]  17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings\n",
      " > 722 [-0.01151  -0.004272 -0.05823   0.12317   0.0267  ]  in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,”\n",
      " > 723 [-6.018e-02 -7.576e-05 -6.848e-02  1.012e-01  2.875e-02]  Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical\n",
      " > 724 [-0.05502   0.01145  -0.0632    0.0805    0.004967]  partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference\n",
      " > 725 [-0.0287   0.01053 -0.04565  0.0668   0.03653]  Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015.\n",
      " > 726 [-0.02834  0.01819 -0.05643  0.052    0.0233 ]  on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems\n",
      " > 727 [-0.01166 -0.01622 -0.05563  0.03928 -0.01675]  2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp. 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and\n",
      " > 728 [-0.03534 -0.02077 -0.06042  0.04074 -0.01604]  systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp. 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226.\n",
      " > 729 [-0.04733 -0.00848 -0.04724  0.04132 -0.01917]  and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in\n",
      " > 730 [-0.05386 -0.0227  -0.04117  0.02266 -0.02336]  1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han,\n",
      " > 731 [-0.03014  -0.02061  -0.0532    0.00864  -0.001612]  in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J.\n",
      " > 732 [-0.0478   -0.02757  -0.04395   0.03041   0.001903]  “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient\n",
      " > 733 [-0.06384  -0.01797  -0.05435   0.07367  -0.001074]  J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms,\n",
      " > 734 [-0.03967   0.0145   -0.0679    0.06915  -0.005524]  efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient\n",
      " > 735 [-0.057    0.01828 -0.0742   0.02289 -0.00848]  algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297.\n",
      " > 736 [-0.05057  -0.00953  -0.0786    0.02998  -0.004063]  efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018.\n",
      " > 737 [-0.0386    0.03537  -0.0743    0.0571    0.004887]  281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V,\n",
      " > 738 [-0.01942  0.04526 -0.07654  0.02863  0.02481]  2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at\n",
      " > 739 [-0.02303  0.0689  -0.07214  0.02829  0.03088]  − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line\n",
      " > 740 [-0.01262  0.02962 -0.03717  0.05377  0.0398 ]  at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration,\n",
      " > 741 [-0.02142  0.04153 -0.05753  0.03467  0.03534]  Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0. Therefore, the total amount of greedy operations\n",
      " > 742 [-0.00732  0.01833 -0.05978  0.02863  0.02899]  iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then,\n",
      " > 743 [-0.01324  0.01581 -0.04904  0.0227   0.03827]  operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly,\n",
      " > 744 [-0.0126    0.02475  -0.008965  0.02373   0.03622 ]  Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp\n",
      " > 745 [-0.00823   0.0619   -0.003447  0.0198    0.04584 ]  Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity\n",
      " > 746 [-0.00943   0.0655   -0.04218  -0.006065  0.03726 ]  exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice\n",
      " > 747 [-0.010315  0.07196  -0.05795   0.0268    0.04053 ]  larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A\n",
      " > 748 [-0.02336   0.00968  -0.0351    0.004787  0.03406 ]  that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse\n",
      " > 749 [-0.02225   0.009636 -0.03735   0.02199   0.03192 ]  ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8\n",
      " > 750 [-0.04807   0.002335 -0.03314   0.0342    0.05988 ]  AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased\n",
      " > 751 [-0.01985   0.01695  -0.006363  0.00821   0.0314  ]  8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented\n",
      " > 752 [-0.008125  0.01631  -0.00877   0.01011   0.04208 ]  increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808\n",
      " > 753 [-0.005352  0.02846  -0.0206    0.03372   0.0638  ]  implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best\n",
      " > 754 [-0.0367   0.0522  -0.0195   0.03317  0.0546 ]  0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓\n",
      " > 755 [-0.04343   0.02441  -0.001577  0.03235   0.04578 ]  best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707\n",
      " > 756 [-0.04593   0.02129   0.013504  0.03442  -0.001228]  Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992\n",
      " > 757 [-0.01108  0.01195 -0.02444  0.01935  0.0383 ]  0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe\n",
      " > 758 [-0.005283  0.00889  -0.01726   0.03586   0.0265  ]  0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the\n",
      " > 759 [-0.01651   0.013824 -0.01394   0.0556    0.010345]  that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes\n",
      " > 760 [-0.0283   0.02611 -0.0081   0.05902  0.03143]  the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement\n",
      " > 761 [-0.0408    0.012474 -0.02582   0.05875   0.02295 ]  nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE\n",
      " > 762 [-0.021     0.007195 -0.03108   0.04172   0.02754 ]  improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity\n",
      " > 763 [-0.01717  0.02138 -0.0274   0.03287  0.02696]  TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the\n",
      " > 764 [-0.03815  0.02948 -0.04416  0.03952  0.0257 ]  affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient\n",
      " > 765 [-0.0226   0.00805 -0.02705  0.03705  0.03732]  the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional\n",
      " > 766 [-0.03412   0.003609 -0.02979   0.02853   0.06903 ]  coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph. 21\n",
      " > 767 [-0.0207   0.01086 -0.05365  0.06256  0.03195] Adaptive Local Clustering over Attributed Graphs Technical Report Haoran Zheng Hong Kong Baptist University Hong Kong SAR, China cshrzheng@comp.hkbu.edu.hk Renchi Yang Hong Kong Baptist University Hong Kong SAR, China renchi@hkbu.edu.hk Jianliang Xu Hong Kong Baptist University Hong Kong SAR, China xujl@hkbu.edu.hk Abstract—Given a graph G and a seed node vs, the objective of local graph clustering (LGC) is to identify a subgraph Cs ∈ G (a.k.a. local cluster) surrounding vs in time roughly linear with the size of Cs. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in G, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs. To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an esti- mation of the bidirectional diffusion distribution(BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components ofLACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over G with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. Index Terms—local cluster, attributes, graph diffusion, adap- tive algorithm I. I NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial\n",
      " > 768 [-0.03604  0.02063 -0.05167  0.08044  0.03049]  NTRODUCTION Local graph clustering (LGC) seeks to identify a single cluster (a.k.a. local cluster) pertinent to a given seed node by exploring a small region around it in the input graph. Com- pared to its global counterparts, LGC runs in time proportional to the size of its resulting cluster, regardless of the overall size of the input graph, making it particularly suitable for analyzing large-scale graphs, such as social networks, online shopping graphs, biological networks, and more. In practice, LGC finds extensive applications in various domains, including commu- nity detection or recommendation on social media [1]–[3], protein grouping in bioinformatics [4], [5], product return or user action prediction [6]–[8] on e-commerce platforms, and many others [9]–[13]. Canonical solutions for LGC [14]–[17] are predominantly based on random walk-based graph diffusion models [18], where the main idea is to spread mass from the seed node to other nodes along the edges. This methodology offers high scalability and rigorous theoretical guarantees for complex- ity and output cluster size, but it is extremely sensitive to structural heterogeneities (e.g., high-degree nodes) in real networks, as pinpointed in [19], [20]. To mitigate this is- sue, subsequent LGC works [20]–[22] formulate LGC as a combinatorial optimization problem and leverage max flow algorithms to derive the “optimal” local clusters. Although these approaches improve the theoretical results of previous works, they are mostly of theoretical interest only and struggle to cope with sizable graphs due to poor practical efficiency. Most importantly, the above algorithms primarily focus on optimizing connectivity-based clustering quality metrics [23]. Real graphs are often constructed from data riddled with noise and thus encompass substantial noisy or missing links, leading to high ground-truth conductance [23], e.g., 0.765 for Flickr and 0.649 for Yelp. Consequently, applying classic LGC techniques to such graphs leads to sub-optimal performance, e.g, often low precisions below 30% (see Table V). As a partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random\n",
      " > 769 [-0.02351   0.002737 -0.05292   0.0573    0.0361  ]  partial remedy, recent efforts [24]–[27] incorporate higher- order connectivity patterns (e.g., motifs) into the LGC frame- works for improved clustering. However, they still rely solely on graph topology and are therefore vulnerable to missing or noisy nodes/edges. Moreover, these methods suffer from severe efficiency issues as they require enumerating the motifs in graphs in the pre-processing or on the fly. In real life, graphs are often endowed with rich nodal attributes, e.g., user profiles in social networks and paper ab- stracts in citation graphs. These are termed attributed graphs. Recent studies [28]–[30] have corroborated that node attributes provide information that can effectively complement the graph topology for better performance in various tasks. Inspired by this, a straightforward idea is to exploit the attribute information to enhance LGC performance whilst retaining the locality. Very recently, several attempts [31]–[33] have been made to extend classic LGC techniques to attributed graphs by simply re-weighting each edge by the attribute similarity of its endpoints. However, this strategy merely accounts for connected nodes and still fails to deal with missing and noisy connections. To tackle the foregoing issues, we propose LACA, a novel solution that seamlessly integrates node attributes into graph structures for effective LGC computation, while offering strong theoretical guarantees on locality and high empirical 1 arXiv:2503.20488v1 [cs.SI] 26 Mar 2025efficiency. Specifically, LACA formulates the LGC task as a seeded random walk diffusion over graphs based on the novel notion of bidirectional diffusion distribution (BDD), an attribute-aware affinity measure dedicated to node pairs in attributed graphs. The main idea is to model the affinity between any node pair (vs, vt) as the expected normalized (exponential) cosine similarity (hereafter SNAS) between the attributes of ending node pairs of random walks originating from vs and vt, which essentially evaluates their meeting probability from the perspectives of both of them through random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and\n",
      " > 770 [-0.01155 -0.00741 -0.04184  0.04993  0.0443 ]  random walks and attribute-based transitions. Accordingly, given a seed node vs, the goal of LGC is to find a local cluster containing nodes with the highest BDD values w.r.t. vs. The exact computation of BDD incurs a significant cost of up to O(n3) as there are n × n possible ending node pairs from random walks starting at the seed node and any of the n nodes. This is non-local and infeasible for large graphs. In response, we develop an adaptive framework for an approximate solution with a runtime linear to the output size and independent of the size of the graph. First, we obtain a fast and theoretically grounded decomposition of the SNAS matrix into low-dimensional feature vectors through randomized tech- niques [34], [35]. This step enables the decoupling of the BDD computation and transforms the problem into the diffusion of vectors over input graphs. On top of that, we devise a new algorithm AdaptiveDiffuse, which diffuses non-negative vectors on graphs using efficient matrix operations in an adaptive fashion. It not only alleviates the intensive memory access patterns in previous traversal/sampling-based diffusion approaches [15], [36], [37] but also overcomes their limitations of sensitivity to high-degree nodes and slow convergence, without compromising the theoretical assurance in approxima- tion and asymptotic performance. Lastly, a carefully designed three-step scheme is employed to construct the approximate BDD. We further conduct non-trivial theoretical analyses to reveal the approximation and complexity bounds of LACA, as well as its connection to graph neural networks (GNNs) [38]. Our empirical studies, which evaluate LACA against 17 competitors on 8 real attributed graph datasets with ground- truth local clusters, demonstrate that LACA is consistently superior or comparable with the state-of-the-art baseline ap- proaches in terms of result quality at a fraction of their cost. In particular, on the largest Amazon2M dataset with 61.9 million edges, LACA is able to recover the target local clusters with an average of 1.8% improvement in precision and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector\n",
      " > 771 [-0.00848  0.02861 -0.05487  0.02985  0.04532]  and 152× speedup compared to the best competitor. II. P ROBLEM FORMULATION A. Notations and Terminology Let G = (V, E) be a connected, undirected, and unweighted graph (a.k.a. network), where V = {v1, v2, . . . , vn} is a set of n nodes and E ∈ V × Vis a set of m edges. For each edge (vi, vj) ∈ E, we say vi and vj are neighbors to each other. We use N(vi) to denote the set of neighbors of vi and d(vi) =|N(vi)| as its degree. Let A be the adjacency matrix of G where Ai,j = 1if (vi, vj) ∈ E, otherwise Ai,j = 0. The TABLE I: Frequently used notations. Notation Description V, E, X The node set, edge set, and node attribute matrix X of attributed graph G, respectively. n, m, d The numbers of nodes, edges, and distinct attributes, respectively. N(vi), d(vi) The set of neighbors and degree of node vi, respectively. A, D, P The adjacency, degree, and transition matrices of the graph G, respectively. vol(C) The volume of a set C of nodes, i.e., P vi∈C d(vi). supp(− →x) The support of vector − →x, i.e., {i : − →xi ̸= 0}. α The restart factor in RWR, α ∈ (0, 1). s(vi, vj) The SNAS defined by Eq. (1). π(vx, vy) The RWR score of vy w.r.t. vx (See Eq. (6)).− →ρ t, − →ρ ′ t The exact and approximate BDD values of vt w.r.t. vs respectively (See definition in Eq. (5)). Z, − →z (i) The TNAM and its i-th row vector (See Eq. (10)). ϵ, σ The diffusion threshold and balancing parameter in Algo. 2. k The dimension of TNAM vectors − →z (i) ∀vi ∈ V. diagonal matrix D is used to symbolize the degree matrix of G where Di,i = d(vi). The transition matrix of G is defined by D−1A, where Pi,j = 1 d(vi) if (vi, vj) ∈ E, otherwise Pi,j = 0. Accordingly, Pℓ i,j signifies the probability of a length- ℓ random walk originating from node vi ending at node vj. A graph is referred to as an attributed graph if each node vi in V is endowed with a d-dimensional attribute vector − →x (i), which is the i-th row vector in the node attribute matrix X of G. We assume − →x (i) is L2-normalized, i.e., ∥− →x (i)∥2 = 1. Xi,j and − →x (i) j represent the (i, j)-th element in matrix X and j-th entry in vector − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) ·\n",
      " > 772 [-0.03616  0.0038  -0.04688  0.0719   0.04077]  − →x (i), respectively. The support of vector − →x (i) is defined as supp(− →x (i)) ={j : − →x (i) j ̸= 0}, which comprises the indices of non-zero entries in − →x (i). The volume of a set C of nodes is defined as vol(C) =P vi∈C d(vi). Given a length-n vector − →x , its volume vol(− →x ) is defined as P i∈supp(− →x ) d(vi). Table I lists notations frequently used throughout this paper. A local cluster Cs w.r.t. a seed node vs in graph G is defined as a subset of V containing vs. Intuitively, in an attributed graph G, a good local cluster should be internally cohesive and well-separated from the remainder of G in terms of both topological connections and attribute similarity. Given the seed node vs, the general goal of LGC over attributed graph G is to identify such a local cluster Cs with the runtime cost roughly linear to its volume vol(Cs). B. Symmetric Normalized Attribute Similarity We quantify the the similarity of two nodes vi and vj in G in terms of attributes as follows: s(vi, vj) = f(− →x(i), − →x(j))qP vℓ∈V f(− →x(i), − →x(ℓ)) qP − →x(ℓ)∈V f(− →x(j), − →x(ℓ)) , (1) where f(·, ·) can be any metric function defined over two vectors. The denominator in Eq. (1) is to ensure the s(vi, vj) values w.r.t. any node vi ∈ Vto be symmetric and normalized to a comparable range ( 0 ≤ s(vi, vj) ≤ 1), which facilitates the design of the BDD in subsequent section. Particularly, s(vi, vj) is referred to as the SNAS of nodes vi and vj. We adopt two classic metric functions for f(·, ·), i.e., cosine similarity and exponent cosine similarity [39]. When f(·, ·) 2𝑣! 𝑣! 𝑣\" 𝑣# 𝑣$ 𝑣% 𝑣& 𝑣' 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 1.0 0.3 0.1 0.2 0.7 0.1 0.3 1.0 0.8 0.5 0.3 0.2 0.1 0.8 1.0 0.4 0.2 0.6 0.2 0.5 0.4 1.0 0.1 0.3 0.7 0.3 0.2 0.1 1.0 0.5 0.1 0.2 0.6 0.3 0.5 1.0 𝑣! 𝑣# 𝑣\" ⋮ 𝑣$ 𝑣( 𝑣\" 𝑣& 𝑣% 𝑣) 𝑣% 𝑣& 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣$ ⋮ 𝑣( SNAS𝒔(𝒗𝒊,𝒗𝒋) seed target 𝑣! 𝑣\" 𝑣! 𝑣\" 𝑣# 𝑣% 𝑣& 𝑣$ 𝑣)𝑣' AttributedGraph𝓖 𝝅(𝒗𝒔,𝒗𝒊) 𝝅(𝒗𝒕,𝒗𝒋) Fig. 1: Figurative Illustration of BDD. is the cosine similarity, f(− →x (i), − →x (j)) = − →x (i) · − →x (j) since ∥− →x (i)∥2 = 1. Then, the SNAS s(vi, vj) can be computed via − →x(i) · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random\n",
      " > 773 [-0.01613   0.001336 -0.03882   0.08984   0.05942 ]  · − →x(j) qP vℓ∈V − →x(i) · − →x(ℓ) qP vℓ∈V − →x(j) · − →x(ℓ) . (2) Analogously, when the exponent cosine similarity is adopted, f(− →x (i), − →x (j)) = exp \u0010− →x (i) · − →x (j)/δ \u0011 (3) and the SNAS of nodes vi, vj is formulated as exp \u0000− →x(i) · − →x(j)/δ \u0001 qP vℓ∈V exp \u0000− →x(i) · − →x(ℓ)/δ \u0001qP vℓ∈V exp \u0000− →x(j) · − →x(ℓ)/δ \u0001, (4) where δ (typically 1 or 2) is the sensitivity factor. Essentially, Eq. (4) can be deemed as a variant of the softmax function. C. Bidirectional Diffusion Distribution We further propose the bidirectional diffusion (BDD) to model the likelihood of two nodes in G to be in the same cluster. In particular, Unlike previous works that are based on biased graph proximity from the seed’s view, BDD inte- grates the strength of topological connections and the attribute similarity of the node pair in a coherent framework from the perspectives of both the seed and target. More precisely, given a seed node vs ∈ V, for any target node vt ∈ V, the BDD − →ρ t of node pair (vs, vt) is defined by − →ρ t = X vi,vj∈V π(vs, vi) · s(vi, vj) · π(vt, vj), (5) where s(vi, vj) is the SNAS of intermediate nodes vi, vj and π(vx, vy) =P∞ ℓ=0 (1 − α)αℓ · Pℓ x,y (6) stands for the RWR ( random walk with restart [40], [41]) score of node vy w.r.t. node vx. At each step, an RWR over G either stops at the current node with 1 − α probability or navigates to one of its neighbors uniformly at random with probability α. In essence, π(vs, vi) (resp. π(vt, vj)) is the probability that an RWR originating from vs (resp. vt) termi- nates at node vi (resp. vj). Put differently, π(vs, vi) ∀vi ∈ V (resp. π(vt, vj) ∀vj ∈ V) describe the distribution of mass (1.0 in total) disseminated from vs (resp. vt) to all the nodes in G via random walks, respectively. Particularly, given a vector− →a ∈ Rn, we refer to the below process as an RWR-based graph diffusion: X vx∈V − →a x · π(vx, vy) ∀vy ∈ V, (7) where − →a x · π(vx, vy) can be interpreted as the amount of mass in − →a spread from vx to vy via random walks on G. Let nodes (vi, vj) be the ending node pair of two bidi- rectional random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate\n",
      " > 774 [-0.006855  0.02226  -0.0297    0.05045   0.06238 ]  random walks with restart from seed node vs and target node vt. The BDD − →ρ t of (vs, vt) in Eq. (5) is therefore 𝑣𝑠 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣1 𝑣3 𝑣2 ⋮ 𝑣4 𝑣𝑛 0.4 0.5 0.4 0.3 0.6 0.3 0.1 0.1 0.1 0.3 0.4 0.5 ⋯ ⋯ ⋯ 0.1 0.1 0.2 𝑣1 𝑣2 𝑣3 𝑣4 ⋮ 𝑣𝑛 seed 𝝅′(𝒗𝒔,𝒗𝒊) 0.3 0.4 0.4 0.4 0.3 0.1 0.3 ⋯ 0.1 0.5 0.6 0.1 0.4 ⋯ 0.1 0.4 0.3 0.1 0.5 ⋯ 0.2 𝑣1 𝑣2 𝑣4 𝑣4 𝑣5 𝑣1 𝑣2 𝑣1 𝑣4 𝑣2 ⋮ 𝑣5 𝑣𝑛 ∙ 𝑣3𝑣3 0.48 0.45 0.11 0.45 0 TNAM 𝒁 𝝍 TNAM 𝒁 𝝓′ 𝝆′ 0 1.0 0 0 0 0 0 𝟏(𝑠) Fig. 2: Basic Idea of LACA. the overall SNAS of all such ending pairs, as illustrated in Fig. 1. Intuitively, if two nodes vs and vt are located in the same cluster, their proximal nodes (i.e., nodes with high RWR scores) are more likely to have high SNAS (i.e., high attribute homogeneity), resulting in a high BDD value − →ρ t. Particularly, the injection of the SNAS reduces the likelihood of transiting to undesired nodes caused by noisy links, while increasing the affinity to desired nodes with high attribute similarities but low connectivity due to missing links. Remark. When the input graph G is non-attributed, we set the SNAS s(vi, vj) = 1if vi = vj and 0 otherwise ∀vi, vj ∈ V. The BDD ρ(vq, vt) is then a variant of the CoSimRank [42] metric, which measures the likelihood of random walks from two nodes meeting each other over the graph. D. Problem Statement Based thereon, the LGC task for seed nodevs over attributed graph G is framed as an approximation of the BDD vector − →ρ (denoted as − →ρ ′), followed by a simple extraction of the nodes with |Cs| largest values in − →ρ ′ as the predicted local cluster Cs. In turn, our major focus of the LGC problem lies in the computation of − →ρ ′. More formally, given a diffusion threshold ϵ, we aim to estimate a BDD vector − →ρ ′ such that both its output volume vol(− →ρ ′) (i.e., the size of the explored region on G) and entailed computation cost are bounded by O \u0000 1/ϵ \u0001 , i.e., achieving the locality. Notice that the direct computation of BDD − →ρ in Eq. (5) requires the RWR scores π(vs, vi) of all intermediate nodes vi ∈ Vw.r.t. the seed vs, the RWR scores π(vt, vj) of all intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA. where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter\n",
      " > 775 [-0.01619  0.0236  -0.05777  0.03223  0.05167]  intermediate nodes vj ∈ V w.r.t. all possible target nodes vt ∈ V, and the SNAS values s(vi, vj) of all possible node pairs (vi, vj) ∈ V × V. The latter two ingredients involve up to O(n2) node pairs, rendering the local estimation of − →ρ particularly challenging. Additionally, it remains unclear how to provide approximation accuracy assurance for − →ρ ′. III. S OLUTION OVERVIEW To address the preceding technical challenges, we stream- line the approximation of BDD via a three-step framework through our careful analyses as follows. A. Problem Transformation Firstly, by the definition of BDD in Eq. (5) and the sym- metric property of RWR scores [43] (i.e., π(vt, vj) · d(vt) = π(vj, vt) · d(vj)), we can rewrite the BDD value − →ρ t of any target node vt ∈ Vw.r.t. the seed node vs as − →ρ t = 1 d(vt) X vi∈V − →ϕi · π(vi, vt), (8) 3𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 ∙ ✓ ✓✓✓𝝍 𝒁 𝝓′ Step 2: Estimate RWR 𝝅 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 3: Compute 𝝓′ 𝑣3 𝑣1 𝑣5 𝑣2 𝑣7 𝑣8 𝑣4 𝑣6 Step 4: Estimate BDD 𝝆 RWR-based Graph Diffusion Step 1: Pre-processing Stage Filtered 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝝆′ 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒁𝑿 ∀𝑣𝑖,𝑣𝑗 ∈ 𝒱 𝑠 𝑣𝑗,𝑣𝑖 = 𝒛(𝑗) ∙𝒛(𝑖) 𝒇=e-cosine Orthogonal Random Features Reduce attribute dimension 𝒅 to 𝒌 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝑣1 𝑣2 𝑣3 𝑣4 𝑣5 𝑣6 𝑣7 𝑣8 𝒌-SVD 𝒇=cosine 𝝅′ RWR-based Graph Diffusion Fig. 3: Overview of LACA. where − →ϕ is called the RWR-SNAS vector w.r.t. vs and − →ϕi = X vj∈V π(vs, vj) · s(vj, vi) · d(vi). (9) Intuitively, if the RWR-SNAS vector − →ϕ is at hand, Eq. (8) implies that an approximate BDD vector − →ρ ′ can be obtained via the RWR-based graph diffusion (see Eq. (7)) of − →ϕ over the graph G with a diffusion threshold. However, the direct and exact computation of the RWR- SNAS vector − →ϕ in Eq. (9) is still immensely expensive as it requires calculating the SNAS s(vj, vi) for all possible node pairs (up to O(n2)) in G. Towards this end, we propose to decompose each SNAS s(vj, vi) as the product of two length- k (k ≪ d and is a constant) vectors, i.e., s(vj, vi) =− →z (j) · − →z (i), (10) where Z ∈ Rn×k is a transformed node attribute matrix X (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design\n",
      " > 776 [ 0.005512  0.01279  -0.03378   0.01965   0.03967 ]  (hereafter TNAM). In doing so, the RWR-SNAS vector − →ϕ in Eq. (9) can be reformulated as − →ϕi = − →π · Z · − →z (i) · d(vi), (11) where − →π denotes the RWR vector w.r.t. seed node vs, i.e.,− →π i = π(vs, vi) ∀vi ∈ V. Given an estimation − →π ′ of − →π , the term − →π · Z in Eq. (11) can be approximated by − →ψ = X i∈supp(− →π′) − →π ′ i · − →z (i) ∈ Rk, (12) and accordingly, we can estimate − →ϕ by − →ϕ′ i = − →ψ · − →z (i) · d(vi) ∀vi ∈ {vi|i ∈ supp(− →π ′)}. (13) Note that − →ψ is shared by the computations of all possible− →ϕ′ i. If we can calculate an approximate RWR vector − →π ′ with support size (i.e., the number of non-zero entries) supp(− →π ′) = O(1/ϵ) in O(1/ϵ) time, the construction times and support sizes of − →ψ and − →ϕ′ are also bounded by O(1/ϵ). As illustrated in Fig. 2, our above idea transforms the computation of the BDD for each target node vt ∈ V in Fig. 1 into (i) aggregation of TNAM vectors of nodes into vector − →ψ based on their RWR scores in − →π ′, (ii) construction of the RWR-SNAS vector − →ϕ′ by Eq. (13), and (iii) RWR- based graph diffusion of − →ϕ′ over G to get − →ρ ′. 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.4 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 0.08 𝑣4 𝑣6 𝑣3 𝑣5 𝑣2 𝑣1𝑣7 𝑣10𝑣9𝑣8 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 0.08 0.24 0.08 0.272 0.352 0 0 0 0 0 0 0 00.4 𝒒𝑖 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.08 0 0 0 0.08 𝒗𝟏 𝒗𝟐 𝒗𝟑 𝒗𝟒 𝒗𝟓 0.24 𝒒𝑖 𝒒𝑖 𝒓𝑖 𝒓𝑖 𝒓𝑖0.6 0.6 0.12 0.080.16 0.24 0.16 0.08 0 0.272 0.12 0.080.352 0.0480.048 0 0 (a) Initially (b) 1st Iteration (c) 2nd Iteration Fig. 4: GreedyDiffuse with α = 0.8 and ϵ = 0.1. B. A Three-Step Framework The BDD approximation thus involves four subtasks, i.e., the computations of the TNAM Z, approximate RWR vector − →π ′, RWR-SNAS vector − →ϕ′, and approximate BDD vector − →ρ ′. Since − →ϕ′ can be properly computed using Eq. (13), our main tasks are to construct Z, − →π ′, and − →ρ ′. Let − →1 (s) be a vector with 1 at s-th entry and 0 everywhere else. The exact RWR score − →π t of any node vt ∈ Vcan be represented as − →π t = X vi∈V − →1 (s) i · π(vi, vt), which can also be regarded as an RWR-based graph diffusion. This inspires us to design a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 −\n",
      " > 777 [ 0.01132   0.000563 -0.02072   0.04257   0.02138 ]  a unified graph diffusion algorithm that obtains estimations − →π ′ and − →ρ ′ by diffusing − →1 (s) and− →ϕ′ over G based on RWR, respectively, while fulfilling the volume and runtime bounds (i.e., O(1/ϵ)), and high practical efficiency. As for TNAM Z, we propose to generate it through a preprocessing of the input node attribute matrix X as Z can be reused in the LGC task of any seed node vs ∈ V. As summarized in Fig. 3, our proposed LACA includes a preprocessing algorithm that converts X into a k-dimensional TNAM Z thereby enabling the problem transformation in Eq. (11) (Algo. 3), and a three-step scheme for the online approximation of BDD vector. Therein, Step 1 identifies a small set of nodes around the seed node by estimating their RWR − →π ′ using an RWR-based graph diffusion algorithm (Algo. 1 or 2) with − →1 (s) as input, while Step 2 aggregates their TNAM vectors as − →ψ (Eq. (12)) to build the RWR-SNAS vector − →ϕ′ (Eq. (11)). In the final step (Step 3), LACA conducts another RWR-based graph diffusion with − →ϕ′ over G to derive the approximate BDD vector − →ρ ′ (Algo. 4). In succeeding sections, we first elaborate on the algorithmic design of our generalized RWR-based graph diffusion ap- proaches (Algo. 1 and 2) for the estimations of RWR vector − →π and BDD vector − →ρ in Section IV-C. After that, Section V de- scribes the technical details of Algo. 3 for constructing TNAM Z and the complete LACA algorithm (Algo. 4), followed by theoretical analyses in terms of accuracy approximation, volume, and runtime. IV. RWR- BASED GRAPH DIFFUSION ALGORITHMS We unify the computations of − →π ′ and − →ρ ′ as diffusing an input vector − →f along edges over G to get − →q satisfying ∀vt ∈ V, 0 ≤ X vi∈V − →f i · π(vi, vt) − − →q t ≤ ϵ · d(vt), (14) 4Algorithm 1: GreedyDiffuse Input: Transition matrix P, restart factor α, diffusion threshold ϵ, initial vector − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; 2 while true do 3 Compute sparse vector − →γ ; ▷ Eq. (15) 4 if − →γ is 0 then break; 5 − →r ← − →r − − →γ ; 6 Update − →q and − →γ ; ▷ Eq. (16) 7 − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector\n",
      " > 778 [-0.006996  0.001404 -0.0325    0.03503   0.006874]  − →r ← − →r + − →γ ; 8 return − →q ; which is − →π ′ and − →ρ ′ when − →f is set to − →1 (s) and − →ϕ′, respectively. Instead of using deterministic graph traversals or random walk samples for diffusion as in prior algorithms [15], [36], we first develop GreedyDiffuse by leveraging matrix operations and a greedy strategy for cache-friendly memory access patterns and higher efficiency. Further, we upgrade GreedyDiffuse to AdaptiveDiffuse for faster ter- mination without compromising the theoretical guarantees through an adaptive scheme. A. The GreedyDiffuse Approach Algo. 1 presents the pseudo-code of GreedyDiffuse for “diffusing” any initial non-negative vector − →f . Algo. 1 is greedy in the sense that the diffusion operations are solely conducted for nodes whose residues are beyond a certain threshold so as to minimize the total amount of operations needed to satisfy the desired accuracy guarantee. Given the transition matrix P of the input graph G and the diffusion threshold, Algo. 1 begins by initializing a residual vector − →r as − →f and a reserve vector − →q as 0 at Line 1. Afterward, it starts an iterative process for diffusing and converting the residuals in − →r , which continuously transfers the residuals into the reserve vector − →q (Lines 2-7). Specifically, in each iteration, we first identify the residuals in − →r whose corresponding − →r i/d(vi) values are equal to or beyond ϵ·∥− →f ∥1 and move them to a temporary vector − →γ for subsequent diffusion. More precisely, we obtain a sparse vector − →γ at Line 3 as follows: − →γ i = (− →r i if (− →r D−1)i = − →r i d(vi) ≥ ϵ, 0 otherwise. (15) Next, we update residual vector − →r as − →r − − →γ such that− →r contain the residuals below the threshold and then convert (1−α) portion of residuals in − →γ into reserve vector − →q (Lines 5-6). ∀vi ∈ V, its remaining α fraction of residual in − →γ is later evenly scattered to its out-neighbors. That is, each node vj ∈ Vreceives a total of P vi∈N(vj) α · − →γ i d(vi) residual from its incoming neighbors N(vj), which can be written as a sparse matrix-vector multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24\n",
      " > 779 [-0.00012  -0.003859 -0.04443   0.04422   0.02396 ]  multiplication as follows (Line 6): − →q ← − →q + (1− α)− →γ , − →γ ← α− →γ P (16) Greedy Non-greedy 2 5 10 15 20 250 0.2 0.4 0.6 0.8 Residual Sum ∥− →r ∥1 (a) PubMed (α = 0.8, ϵ= 10−5) 5 15 25 35 45 550 0.05 0.1 0.15 0.2 Residual Sum ∥− →r ∥1 (b) ArXiv (α = 0.8, ϵ= 10−7) Fig. 5: Greedy v.s. Non-greedy. These residuals in − →γ will be added back to − →r for the next round of conversion and diffusion (Line 7). Theorem IV .1. Given initial vector − →f , restart factor α, and diffusion threshold ϵ, Algo. 1 outputs a diffused vector − →q satisfying Eq. (14) using O \u0010 max n |supp(− →f )|, ∥− →f ∥1 (1−α)ϵ o\u0011 time. Proof. All missing proofs can be found in Appendix A. Algo. 1 repeats the above procedure until the resulting − →γ turns to be a zero vector (Line 4), i.e., all non-converted residuals in − →r fall below the desired threshold in Eq. (15). Eventually, Algo. 1 returns − →q as the diffused vector of− →f . Theorem IV .1 establishes the approximation accuracy guarantees of Algo. 1 and indicates that GreedyDiffuse runs in time proportional to the size of − →f and 1 (1−α)ϵ , but independent of the size of the input graph G. This indicates that GreedyDiffuse enables the local estimation of RWR − →π and − →ρ if − →1 (s) and − →ϕ′ are given as input, respectively. A Running Example. Consider the example G in Fig. 4. The input vector − →f is a length-10 vector in which the first and second entries are 0.4 and 0.6. We conduct GreedyDiffuse over G with restart factor α = 0.8 and diffusion threshold ϵ = 0.1. Initially, the residuals of v1 and v2 are 0.4 and 0.6 as in − →f , while the reserves of all nodes are 0 as in Fig. 4(a). Since − →r 1 d(v1) = 0.4/4 ≥ ϵ and − →r 2 d(v2) = 0.6/3 ≥ ϵ, GreedyDiffuse converts the 1 − α = 0.2 portion of their residuals into their reserves and distributes the rest to their neighbors evenly. Specifically, nodes v2-v5 receive a residual of 0.4α d(v1) = 0.08 from v1, respectively, while each of nodes v1, v3, and v4 receives 0.6α d(v2) = 0.16 residual. Notably, both v3 and v4 have a total residual of 0.24, which satisfy 0.24 d(v3) = 0.24 d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of\n",
      " > 780 [-0.015274  0.01694  -0.06616   0.06323   0.03073 ]  d(v4) = 0.12 ≥ ϵ. Thus, in the second iteration, GreedyDiffuse merely performs diffusion operations on v3 and v4. A residual of 0.24(1 − α) = 0.048 will be converted into their reserves, and a residual of 0.24α d(v3) = 0.24α d(v4) = 0.096 will be transferred to v1 and v2 from v3 and v4, respectively. The residuals at v1, v2, and v5 are updated to 0.352, 0.272, and 0.08, respectively, leading to 0.352 d(v1) = 0.088, 0.272 d(v2) = 0.0907, 0.08 d(v5) = 0.016, all of which are less than ϵ = 0.1. GreedyDiffuse then terminates and returns the reserve values as the result. B. An Empirical Study of GreedyDiffuse Although GreedyDiffuse enjoys favorable theoretical properties in Theorem IV .1, it suffers from slow convergence 5Algorithm 2: AdaptiveDiffuse Input: P, α, σ, ϵ, − →f Output: Diffused vector − →q 1 − →r ← − →f ; − →q ← 0; Ctot ← 0; 2 while true do 3 Line 3 is the same as Line 3 in Algo. 1; 4 if |supp(− →γ )| |supp(− →r )| > σand Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ then 5 Ctot ← Ctot + vol(− →r ); 6 Update − →q and − →r ; ▷ Eq. (17) 7 else Lines 8-11 are the same as Lines 4-7 in Algo. 1; 12 return − →q ; on real graphs due to its aggressive strategy in Eq. (16). To exemplify, we evaluate the residual sum ∥− →r ∥1 at the end of each iteration in Algo. 1 when adopting greedy (Lines 5-7) and non-greedy operations (Eq. (17)) on PubMed and ArXiv datasets (Table III). − →q ← − →q + (1− α)− →r , − →r ← α− →r P (17) Distinct from the greedy way in Eq. (16), non-greedy opera- tions in Eq. (17) directly convert and diffuse the residuals of all nodes in one shot in each iteration. From Fig. 5, we can observe that Algo. 1 using the greedy strategy needs 2× more iterations to terminate and near 4× more iterations to attain the same residual sum compared to its non-greedy variant on both datasets, leading to inferior empirical efficiency. The reason is that GreedyDiffuse always attempts to sift out a small moiety of low-degree nodes (Eq. (15)) for residual conversion and diffusion in each iteration (Line 5), making it sensitive to high-degree nodes and leaving the bulk of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e.,\n",
      " > 781 [ 0.005848 -0.0121   -0.04214   0.0391    0.02469 ]  of residual untouched. Such a way tends to trigger relentless residual accumulation and propagation among a minority of nodes, causing numerous iterations but fewer non-zero entries in − →q . As reported in Table II, on real datasets on ArXiv and Yelp, the average node degrees of local clusters output by GreedyDiffuse are notably lower than the average node degrees of the entire graphs and those by the non-greedy strategy. In contrast, non-greedy operations transform 1 − α = 20% of residuals into reserves in each iteration, making ∥− →r ∥1 decrease rapidly after a few iterations, e.g., ∥− →r ∥1 = 0.107 after only 10 iterations. Meanwhile, the residual is evenly dis- tributed across more nodes, each with a small value, enabling early termination. However, due to its brute-force nature, the non-greedy strategy entails up to O(m) cost in α− →r Pof each iteration in the worst case, especially on dense graphs. C. The AdaptiveDiffuse Approach Inspired by the preceding analysis, we propose combining greedy and non-greedy operations in an adaptive way to overcome the limitations of both, ensuring fast termination and high locality. TABLE II: Average node degrees of local clusters (ϵ = 10−7). Dataset Global avg. degree Greedy Non-greedy PubMed 13.77 12.49 14.39 Yelp 20.47 17.83 22.65 The pseudo-code of this AdaptiveDiffuse method is provided in Algo. 2, which additionally requires inputting a parameter σ ∈ [0, 1] and a variable Ctot tracking the total cost incurred by non-greedy diffusion operations. In contrast to Algo. 1, in each iteration, after calculating − →γ by Eq. (15) at Line 3, Algo. 2 adaptively selects the greedy strategy (Lines 8-11) or non-greedy way (Lines 5-6) based on the following conditions. The rationale is to first deplete the residuals through non-greedy diffusion as much as possible (for faster convergence) and then disseminate the rest care- fully with greedy operations (for rigorous guarantees). To be specific, AdaptiveDiffuse conducts non-greedy diffusion operations when the fraction of nodes with residues above the threshold (Eq. (15)), i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq. (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function\n",
      " > 782 [ 0.01723 -0.03723 -0.01596  0.01997  0.06186]  i.e., |supp(− →γ )|/|supp(− →r )|, outstrips σ, and in the meantime, the total cost Ctot after conducting such non-greedy operations, i.e., Ctot + vol(− →r ), is still less than the total cost using GreedyDiffuse, i.e., ∥− →f ∥1 (1−α)ϵ . Notice that the smaller σ is, the more non-greedy operations will be conducted. When σ = 0, AdaptiveDiffuse prioritize executing Lines 5-6 over Lines 8-11. Once the non-greedy (Lines 5-6) strategy is chosen, Ctot is increased by the volume of − →r , i.e., the amount of work needed in computing α− →r P. In turn, the same theoretical properties as GreedyDiffuse can be proved in the following lemma for AdaptiveDiffuse: Theorem IV .2.Algo. 2 outputs a vector − →q such that Eq. (14) holds ∀vt ∈ Vusing O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 time. In addition, Lemma IV .3 states that the support size and the volume of the vector − →q returned by Algo. 2 is bounded, solely dependent on ∥− →f ∥1, α, and ϵ. Lemma IV .3.Let − →f and − →q be the input and output of Algo. 2, respectively. Then, |supp(− →q )| ≤vol(− →q ) ≤ β∥− →f ∥1 (1−α)ϵ , where 1 ≤ β ≤ 2. In particular, when σ ≥ 1, β = 1. V. T HE LACA APPROACH This section presents our local algorithm LACA for the estimation of the BDD vector for LGC. We first elucidate the algorithmic details of Algo. 3 for constructing TNAM Z. The complete algorithmic details of LACA (Algo. 4) and related analyses are provided in Section V-B. Lastly, we conduct an in-depth theoretical analysis to unveil the connection between LACA and GNNs [38]. A. Construction of TNAM Z Basic Idea. To realize the idea of transforming s(vi, vj) into− →z (i) · − →z (j) (Eq. (10)), the key is to find length- k vectors− →y (i) ∀vi ∈ V(i.e., an n × k matrix Y ) such that f(vi, vj) =− →y (i) · − →y (j). Accordingly, Eq. (1) can be rewritten as s(vi, vj) = − →y (i)·− →y (j) √− →y (i)·− →y ∗· √− →y (j)·− →y ∗ = − →z (i) · − →z (j), where 6Algorithm 3: TNAM Construction Input: Attribute matrix X, function f(·, ·), and dimension k Output: The TNAM Z 1 U, Λ, V ← k-SVD(X); 2 switch f(·, ·) do 3 case cosine similarity function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by\n",
      " > 783 [ 0.01924   0.01575  -0.02672   0.010506  0.04697 ]  function do 4 Y ← UΛ; 5 case exponential cosine similarity function do 6 Sample a Gaussian matrix G ∼ N(0, 1)k×k; 7 Q ← QRDecomposition(G); 8 Sample diagonal matrix Σii ∼ χ(k) ∀i ∈ {1, . . . , k}; 9 Compute Y ; ▷ Eq. (19) 10 Compute − →y ∗ ; ▷ Eq. (18) 11 for vi ∈ Vdo Compute − →z (i); ▷ Eq. (18) 12 return Z − →y ∗ = P vℓ∈V − →y (ℓ) and − →z (i) = − →y (i)/ p− →y (i) · − →y ∗ . (18) Recall that the SNAS metrics in Section II-B are defined upon the dot product − →x (i) · − →x (j) ∀vi, vj ∈ V. Let U ∈ Rn×k and diagonal matrix Λ ∈ Rk×k consist of the top- k left singular vectors and top- k singular values of X, respec- tively. Lemma V .1 connotes that UΛ can be used as the k- dimensional approximation of X for the construction of Y . Lemma V .1.Let λk+1 be the (k+1)-th largest singular value of X. Then, (UΛ) · (UΛ)⊤ − XX⊤ 2 ≤ λk+1 2. Details. In Algo. 3, we describe the pseudo-code for con- structing vectors − →y (i) and − →z (i) for each node vi ∈ Vbased on the input node attribute matrix X, the metric functions f(·, ·) in Section II-B, and a small integer k ≪ d (typically 32) to cope with the high-dimension d of X. That is, Algo. 3 first applies a k-truncated singular value decomposition (k- SVD) [34] over X to obtain its top- k left and right singular vectors U, V , and the diagonal singular value matrix Λ (Line 1). UΛ then substitutes X for subsequent generation of vectors − →y (i) ∀vi ∈ V. When f(·, ·) is the cosine similarity function, it is straightforward to get Y = UΛ (Lines 3-4). However, when f(·, ·) is the exponential cosine similarity function (Eq. (3)), constructing − →y (i) exactly involves the materialization of f(vi, vj) for all node pairs in V × Vand a matrix factorization, which is prohibitive for large graphs. As a workaround, we capitalize on the orthogonal random features [35] to create estimators − →y (i) ∀vi ∈ Vsuch that− →y (i) · − →y (j) ≈ f(vi, vj) ∀vi, vj ∈ V. More concretely, Algo. 3 first randomly generates a k × k random Gaussian matrix G with every entry sampled from the standard normal distribution independently at Line 6, followed by a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support\n",
      " > 784 [-0.0008082  0.03433   -0.04376    0.02092    0.03726  ]  a QR decomposition of G at Line 7. This step produces a uniformly distributed random orthogonal matrix Q ∈ Rk×k [44]. Algo. 3 further builds a k×k diagonal matrix Algorithm 4: LACA Input: G = (V, E), TNAM Z, seed node vs, restart factor α, parameter σ, diffusion threshold ϵ Output: Approximate BDD vector − →ρ ′ /* Step 1: Estimate RWR vector − →π ′ */ 1 Create a unit vector − →1 (s) ∈ Rn; 2 − →π ′ ← AdaptiveDiffuse(P, α, σ, ϵ,− →1 (s) ); /* Step 2: Compute RWR-SNAS vector − →ϕ′ */ 3 Compute − →ψ; ▷ Eq. (12) 4 for i ∈ supp(− →π ′) do Compute − →ϕ′ i; ▷ Eq. (13) /* Step 3: Estimate BDD vector − →ρ ′ */ 5 − →ρ ′ ← AdaptiveDiffuse(P, α, σ, ϵ· ∥− →ϕ′∥1, − →ϕ′) 6 for i ∈ supp(− →ρ ′) do − →ρ ′ i ← − →ρ ′ i d(vi) 7 return − →ρ ′ Σ with diagonal entries sampled i.i.d. from the χ-distribution with k degrees of freedom (Line 8), enforcing the norms of the rows of ΣQ and G identically distributed. Based thereon, we construct matrix Y at Line 9 as follows: Y ← q 2 exp(1/δ) k · sin( bY ) ∥ cos( bY ), (19) where bY ← 1 δ UΛΣQ and ∥ stands for a horizontal concatenation of two matrices. Theorem V .2 indicates that− →y (i) ·− →y (j) is an unbiased estimator of f(vi, vj) for any node pair (vi, vj) ∈ V × V. Theorem V .2. E \u0002− →y (i) · − →y (j)\u0003 = f(vi, vj) in Eq. (3). After computing vector − →y (i) for each node vi ∈ V, Algo. 3 first computes the sum of these vectors, i.e., − →y ∗ , and finally constructs − →z (i) for each node vi ∈ Vby Eq. (18) (Lines 10- 11). The total processing cost entailed by Algo. 3 is linear to the size of the input node attribute matrix X, as proved in the following lemma: Lemma V .3. The runtime cost of Algo. 3 is O(nd). B. Complete Algorithm and Analysis In Algo. 4, we present the complete pseudo-code of LACA, which takes as input the attributed graph G, the TNAM Z obtained in the preprocessing stage, seed node vs, diffusion threshold ϵ, and parameters α and σ. In the first place, Algo. 4 invokes AdaptiveDiffuse (Algo. 2) with a unit vector − →1 (s) as input, which has value 1 at entry s and 0 everywhere else (Lines 1-2). By Lemma IV .3, the support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear\n",
      " > 785 [-0.010826  0.00999  -0.02194   0.01848   0.0627  ]  support size |supp(− →π ′)| of the returned RWR vector − →π ′ is bounded by O \u0010 1 (1−α)ϵ \u0011 . Next, − →π ′ is used for producing vector − →ψ ← − →π ′ · Z at Line 3 and subsequently the RWR-SNAS vector − →ϕ′ at Line 4. In particular, in lieu of computing − →ϕ′ i for each node vi ∈ V by Eq. (13), LACA merely accounts for nodes with non-zero entries in vector − →π ′, i.e., i ∈ supp(− →ρ ′), whereby the number of non-zero entries in − →ϕ′ can be guaranteed to be bounded by 7O \u0010 1 (1−α)ϵ \u0011 . After that, LACA starts to diffuse the RWR-SNAS vector − →ϕ′ over graph G using the AdaptiveDiffuse with diffusion threshold ϵ · ∥− →ϕ′∥1, and parameters α and σ (Line 5). Let − →ρ ′ be the output of the above diffusion process. LACA then gives − →ρ ′ a final touch by dividing each non-zero entry− →ρ ′ i in − →ρ ′ by 1 d(vi) (Line 6) and returns − →ρ ′ as the approximate BDD vector at Line 7. On the basis of Theorem IV .2, we can establish the accuracy guarantee of LACA as follows: Theorem V .4. When the TNAM Z and SNAS s(vi, vj) ∀vi, vj ∈ Vsatisfy Eq. (10), − →ρ ′ output by Algo. 4 ensures ∀vt ∈ V 0 ≤ − →ρ t − − →ρ ′ t ≤  1 + X vi∈V d(vi) · max vj∈V s(vi, vj)   · ϵ. Volume and Complexity Analysis. Recall that − →ρ ′ is obtained by calling AdaptiveDiffuse with − →f = − →ϕ′ and diffusion threshold ϵ · ∥− →ϕ′∥1. Both the support size |supp(− →ρ ′)| and volume vol(− →ρ ′) of − →ρ ′ are therefore bounded by O \u0010 1 (1−α)ϵ \u0011 using Lemma IV .3. Next, we analyze the time complexity of LACA. First, Line 2 invokes Algo. 1 with a one-hot vector − →1 (s) , i.e., ∥− →1 (s) ∥1 = 1, entailing O \u0010 1 (1−α)ϵ \u0011 time as per Theorem IV .2. The cost of Lines 3 and 4 is dependent on the number of non-zero elements in vector − →π ′, i.e., |supp(− →π ′)|, and the dimension k of Z, which is O \u0010 k (1−α)ϵ \u0011 time by Lemma IV .3. Analogously, we can derive that the computational complexities of Lines 6- 7 are O \u0010 max n supp(− →ϕ′) , ∥− →ϕ′∥1 (1−α)ϵ·∥− →ϕ′∥1 o\u0011 = O \u0010 1 (1−α)ϵ \u0011 . Overall, the time complexity of LACA is O \u0010 k (1−α)ϵ \u0011 , which equals O (1/ϵ) when α and k are regarded as constants and is linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster\n",
      " > 786 [-0.02173  -0.007797 -0.01536   0.0699    0.0577  ]  linear to the volume of its output − →ρ ′. C. Theoretical Connection to GNNs Recent studies [45], [46] demystify that learning node repre- sentations H via existing canonical GNN architectures can be characterized by a graph smoothing process in Definition V .5. Definition V .5 (Graph Signal Denoising [45]) . Let L be the normalized Laplacian matrix of G and H◦ ∈ Rn×k be a feature matrix. The graph signal denoising is to optimize H: arg min H (1 − α)∥H − H◦∥2 F + α · trace(H⊤LH), (20) where ∥ · ∥F stands for the matrix Frobenius norm. The fitting term ∥H − H◦∥2 F in Eq. (20) seeks to make the final node representations H close to the initial feature matrix H◦, while the graph Laplacian regularization term trace(H⊤LH) forces learned representations of two adjacent nodes over G to be similar. The hyperparameter α ∈ [0, 1] controls the smoothness of H through graph regularization. Lemma V .6. The closed-form solution to Eq. (20) is H =P∞ ℓ=0(1 − α)αℓ ˜A ℓ H◦, where ˜A = D−1 2 AD−1 2 . By applying the gradient descent to solve Eq. (20), Lemma V .6 states that the final representation − →h(i) of TABLE III: Statistics of Datasets. Dataset n m m/n d |Ys| Cora [48] 2,708 5,429 2.01 1,433 488 PubMed [48] 19,717 44,338 2.25 500 7,026 BlogCL [49] 5,196 343,486 66.11 8,189 869 Flickr [50] 7,575 479,476 63.30 12,047 846 ArXiv [51] 169,343 1,166,243 6.89 128 12,828 Yelp [52] 716,847 7,335,833 10.23 300 476,555 Reddit [52] 232,965 11,606,919 49.82 602 9,418 Amazon2M [53] 2,449,029 61,859,140 25.26 100 260,129 any node vi ∈ V can be formulated as − →h(i) = P∞ ℓ=0 (1 − α)αℓ ˜A ℓ− →h◦(i) , where the normalized adjacency matrix ˜A can also be replaced by the transition matrix P in popular GNN models [47]. If we let TNAM Z be the initial feature matrix H◦ input to GNN models, the eventual smoothed node representations (a.k.a. embeddings) are H = P∞ ℓ=0 (1 − α)αℓPℓZ, When Eq. (10) holds, combining Eq. (5) and Eq. (6) leads to ∀vt ∈ V, − →ρ t = − →h(s) ·− →h(t), implying that − →ρ ′ output by LACA essentially approximates − →h(s)·H⊤. In this view, our LGC task that extracts a local cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ)\n",
      " > 787 [-0.03165 -0.01124 -0.01764  0.0732   0.03882]  cluster Cs from G based on BDD values is equivalent to identifying the K-NN (K = |Cs|) of − →h(s) among n GNN-like embeddings {− →h(t)|vi ∈ V}. Distinctly, our LACA approach fulfills this goal without explicitly materializing the GNN-like embeddings H and incurring the ˜O(n) cost by the K-NN search, but undergoes a local exploration of G in time linear to the volume of Cs, regardless of n and m. VI. E XPERIMENTS This section experimentally evaluates our proposed LACA against 17 alternative solutions to LGC on 8 real datasets, in terms of both local clustering quality and efficiency. All experiments are conducted on a Linux machine powered by Intel Xeon(R) Gold 6330 2.00GHz CPUs and 2TB memory. Due to space limits, additional experimental results regarding the parameter analysis, ablation study, scalability tests, and the LGC quality of LACA on non-attributed graphs are deferred to Appendix B. For reproducibility, the source code, datasets, and detailed parameter settings are available at https://github. com/HaoranZ99/laca. A. Experimental Setup Datasets. Table III lists the statistics of the datasets used in the experiments. The numbers of nodes, edges, and distinct attributes of the graph data are denoted as n, m, and d, re- spectively. |Ys| stands for the average size of the ground-truth local clusters of all nodes in the graph. Cora, PubMed [48], and ArXiv [51] are citation networks, where nodes and edges represent publications and citation links among them, respec- tively. The attributes of each node are bag-of-words embed- dings of the corresponding publication. The ground-truth local cluster Ys of each publication contains the publications in its same subject areas. BlogCL [49] and Flickr [50] are social networks extracted from the BlogCatalog and Flickr websites, respectively. Ys of each user vs ∈ Vincludes users who are in the same topic categories or interest groups. Yelp and Reddit 8TABLE IV: Evaluated methods. Method Category Preprocessing Cost Online Cost PR-Nibble[15] Local Graph Clustering - ˜O\u00001 ϵ \u0001 APR-Nibble O(md) HK-Relax[16] - ˜O \u0010log(1/ϵ) ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based\n",
      " > 788 [-0.0504   0.00801 -0.02481  0.06085  0.05304]  ϵ \u0011 CRD [20] O\u00001 ϵ \u0001 p-Norm FD[21] O max vi∈V d(vi)2 ϵ ! WFD [33] O(md) Jaccard[54] Link Similarity - ˜O(n)Adamic-Adar[54] Common-Nbrs[54] SimRank[55] SimAttr[56], [57] Attribute Similarity - ˜O(nd) AttriRank[58] O(nd2 + m) ˜O(n) Node2Vec[59] Node Embedding O(n) ˜O(n)SAGE[38] O(nd2) PANE[60], [61] O((m+ n) · d) CFANE[62] O((m+ n) · d) LACA Ours O(nd) ˜O\u00001 ϵ \u0001 datasets are collected from in [52]. Yelp contains friendships between Yelp users, those who have been to the same types of business constitute local clusters. Reddit connects online posts if the same user comments on both and the communities of posts are used as local clusters. Amazon2M [53] is a co-purchasing network of Amazon products wherein each node corresponds to a product and each edge represents that two products are purchased together. The ground-truth local clusters are generated based on the categories of products. Competitors. We dub our LACA algorithms using metric functions cosine similarity and exponential cosine similarity as LACA (C) and LACA (E), respectively. We experimentally compare LACA (C) and LACA (E) against 17 methods adopted for LGC, which can be categorized into four groups: 1) LGC-based methods: PR-Nibble [15], APR-Nibble, HK-Relax [16], CRD [20], p-Norm FD [21], and WFD [33]; 2) Link Similarity-based methods: Jaccard [54], Adamic-Adar [54], Common-Nbrs [54], and SimRank [55]; 3) Attribute Similarity-based methods: SimAttr (C) [56], SimAttr (E) [57], and AttriRank [58]; 4) Network Embedding-based methods: Node2Vec [59], SAGE [38], PANE [60], [61], and CFANE [62]. Amid LGC-based approaches, PR-Nibble [15], APR-Nibble, and HK-Relax [16] are based on random walk graph diffusion, while CRD [20], p-Norm FD [21], and WFD [33] leverages maximum flow algorithms, all of which are local algorithms. APR-Nibble is a variant of PR-Nibble wherein edges are weighted by the Gaussian kernel of their endpoints’ attribute vectors, similar to WFD [33]. Groups 2)-4) comprise all global methods. Link similarity-based and attribute similarity-based methods calculate the link-based and attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision\n",
      " > 789 [-0.05005  0.00791 -0.02898  0.0723   0.05695]  attribute-based similarities between the seed node and all nodes, respectively. The local clusters are then generated by sorting all nodes according to the similarity scores. The fourth category of methods first encodes all nodes in the input graph into low-dimensional embedding vectors and then obtains the local clusters for given seed nodes through the K-NN, spectral clustering (SC), or DBSCAN over the embedding vectors. Particularly, SAGE [38], PANE [61], and CFANE [62] incorporate both topology and attribute semantics into the embeddings, whereas Node2Vec [59] disregard nodal attributes. Table IV summarizes the preprocessing cost and average complexity of these algorithms for generating local clusters. Implementations and Parameter Settings. For PR-Nibble, CRD, and APR-Nibble, we use their implementations pro- vided by [63]. We also employ the well-known NetworkX [64] package for the computation of link similarities of nodes, including Jaccard, Adamic-Adar, Common-Nbrs, and SimRank. As for other competitors, we obtain their source codes from the respective authors. All competitors are imple- mented in Python, except p-Norm FD and WFD, which have been implemented in Julia. For a fair comparison, we run grid searches for parameters and report the results corresponding to the best precision for LGC-based methods, LACA (C), and LACA (E). The parameters in global methods are set as suggested in their respective papers. On each dataset, we randomly select 500 seed nodes S from the graph for LGC tasks. All evaluation results reported in the experiments are averaged over seed nodes in S. B. Quality Evaluation 1) Precision: In this set of experiments, we empirically evaluate the average precisions of the local clusters returned by LACA (C), LACA (E), and the 17 competitors in four cat- egories, respectively, based on the ground-truth local clusters. More concretely, for each seed node vs in S, we run all the evaluated methods such that the predicted local cluster Cs sat- isfies |Cs| = |Ys| and calculate the precision as |Cs ∩ Ys|/|Cs|. Table V reports the average precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16\n",
      " > 790 [-0.03442   -0.013725  -0.0009284  0.02423    0.0569   ]  precision scores achieved by all the evaluated approaches on 8 datasets. The best results among all methods are highlighted in bold, and the best performance by the competitors is underlined. We exclude any method with a preprocessing time exceeding 3 days or an average running time for LGC over 2 hours. Overall, our methods LACA (C) and LACA (E) achieve the top-2 best average ranks in terms of precision on all datasets. Specifically, on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C) and LACA (E) consistently outperform all the competitors, often by a large margin. For instance, on Cora and Flickr, compared to the state-of-the-art baseline CFANE and PANE, LACA (C) is able to take a lead by 6.1% and 11.5%, respectively. Similar observations can be made on the medium-sized graph ArXiv with over one million edges and the largest dataset Amazon2M with 61.9 million edges, where both LACA (C) or LACA (E) outperforms the best competitor by a margin of up to 2.6% and 1.8%, respectively. Over other large graphs Yelp and Reddit, LACA (C) and LACA (E) also obtain comparable or superior prediction precision. Note that on Yelp LACA is slightly inferior to SimAttr (C) and SimAttr (E), with merely a 0.4% decline in precision, but significantly 9TABLE V: The average precision evaluated with ground-truth. Best is bolded and best baseline underlined . Method Cora [48] PubMed [48] BlogCL [49] Flickr [50] ArXiv [52] Yelp [51] Reddit [51] Amazon2M [53] Rank Local Graph Clustering PR-Nibble[15] 0.413 0.481 0.263 0.198 0.299 0.214 0.651 0.364 9.25 APR-Nibble 0.396 0.479 0.252 0.173 0.282 0.093 0.435 0.129 13.13 HK-Relax [16] 0.477 0.476 0.284 0.219 0.351 0.214 0.751 0.116 8.63 CRD [20] 0.149 0.112 0.236 0.192 0.166 0.098 0.641 0.072 16.38 p-Norm FD [21] 0.263 0.131 0.159 0.132 0.225 0.034 0.806 0.441 16.5 WFD [33] 0.298 0.17 0.181 0.133 0.253 0.043 0.589 0.503 15.63 Link Similarity Jaccard [54] 0.231 0.358 0.282 0.209 0.116 0.662 0.343 0.142 13.13 Adamic-Adar[54] 0.231 0.358 0.265 0.163 0.117 0.662 0.33 0.142 15 Common-Nbrs[54] 0.231 0.358 0.259 0.156 0.115 0.662 0.318 0.142 16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition,\n",
      " > 791 [-0.02937   0.003515 -0.01067   0.04587   0.05234 ]  16 SimRank [55] 0.436 0.519 0.273 0.186 - - - - 9.25 Attribute Similarity SimAttr (C)[56] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 SimAttr (E)[57] 0.288 0.469 0.306 0.182 0.154 0.758 0.035 0.194 10.88 AttriRank[58] 0.181 0.363 0.184 0.124 0.076 0.666 0.047 0.122 18.25 Network Embedding Node2Vec (K-NN) 0.181 0.363 0.167 0.111 0.074 0.665 - - 20.5 Node2Vec (SC) 0.41 0.488 0.263 0.182 - - - - 12.25 Node2Vec (DBSCAN) 0.419 0.488 0.263 0.182 0.316 0.679 - - 9.5 SAGE (K-NN) 0.423 0.434 0.209 0.143 - - - - 18 SAGE (SC) 0.403 0.399 0.226 0.155 - - - - 18.5 SAGE (DBSCAN) 0.326 0.359 0.226 0.155 - - - - 20 CFANE (K-NN) 0.495 0.531 0.505 0.2 - - - - 4 CFANE (SC) 0.494 0.531 0.505 0.198 - - - - 4.5 CFANE (DBSCAN) 0.383 0.44 0.36 0.164 - - - - 14.25 PANE (K-NN) 0.445 0.497 0.456 0.332 0.147 0.708 0.263 0.197 6.88 PANE (SC) 0.445 0.497 0.456 0.332 - - - - 5 PANE (DBSCAN) 0.422 0.477 0.267 0.264 0.145 0.635 0.232 0.177 10.88 Ours LACA (C) 0.556 0.552 0.51 0.447 0.377 0.754 0.808 0.465 1.63 LACA (E) 0.552 0.555 0.493 0.39 0.377 0.739 0.808 0.521 2 LACA (C) LACA (E) LACA (w/o SNAS) PR-Nibble HK-Relax APR-Nibble 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (a) Cora 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6Recall (b) PubMed 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (c) BlogCL 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5Recall (d) Flickr 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4Recall (e) ArXiv 10−8 10−6 10−4 10−2 1000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Recall (f) Yelp Fig. 6: Recall when varying ϵ. dominates other competitors by a substantial margin of at least 4.6%. The reason is that the ground-truth clusters in the Yelp dataset are more relevant to node attributes than graph structures. LACA is the only LGC solution that can handle such graphs without downgrading the precision, exhibiting the effectiveness of our algorithmic designs in combining attribute and structure information. This also underlines a limitation of LACA on graph datasets with high-quality attributes but substantial poor/corrupted structures, e.g., heterophilic graphs. In addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2\n",
      " > 792 [-0.02325 -0.01516 -0.00455  0.0585   0.05362]  addition, we can observe that LACA (C) attains comparable or superior performance to LACA (E) in most datasets. The only exception is on Amazon2M, where LACA (E) obtains an improvement of 5.9% over LACA (C). 2) Recall when varying ϵ: This set of experiments studies the effectiveness of LACA and other LGC-based methods in recovering the ground-truth local clusters when the sizes of their predicted ones (i.e., runtime budget) are varied. For a fair comparison, we only compare LACA (C) and LACA (E) with graph diffusion-based baselines PR-Nibble, APR-Nibble, and HK-Relax as the sizes of their output local clusters can be controlled by diffusion threshold ϵ and are bounded by O(1/ϵ). We additionally include an ablated version of LACA, dubbed as LACA (w/o SNAS), for comparison, which disables attribute information in LACA. For all evaluated algorithms, we vary the size of the output local clusters by decreasing ϵ from 1.0 to 10−8. For each seed node vs ∈ S, given the predicted local cluster Cs, we compute the recall by |Cs ∩ Ys|/|Ys|. Intuitively, the smaller ϵ is, the higher the recall should be. Fig. 6 depicts the average recall scores by all six methods when varying ϵ on six datasets. The x-axis and y-axis represent ϵ and the average recall, respectively. From Fig. 6, we can make the following observations. In most cases, both LACA (C) and LACA (E) consistently outperform other methods under the same diffusion thresholds ϵ. In particular, on all datasets, the superiority of LACA (C) and LACA (E) is pronounced when ϵ ≥ 10−3, indicating that LACA is effective and favorable when the budget for the sizes of local clusters and costs is low (e.g., only a small portion of the graph is allowed to be explored). When ϵ ≤ 10−6, we can make qualitatively analogous observations on all datasets 10Online Stage Preprocessing Stage LACA (C) LACA (E) CFANEHK-Relax PANESimRank 10−3 10−2 10−1 100 101 102 running time (sec) (a) Cora LACA (C) LACA (E) CFANESimRank PANE PR-Nibble 10−2 10−1 100 101 102 103 running time (sec) (b) PubMed LACA (C) LACA (E) CFANE PANESimAttrHK-Relax 10−3 10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table\n",
      " > 793 [-0.02438  -0.003168 -0.03912   0.04242   0.05975 ]  10−2 10−1 100 101 102 103 running time (sec) (c) BlogCL LACA (C) LACA (E) PANE HK-RelaxJaccardCFANE 10−2 10−1 100 101 102 103 running time (sec) (d) Flickr LACA (C) LACA (E) HK-RelaxPR-NibbleAPR-Nibble WFD 10−1 100 101 102 103 running time (sec) (e) ArXiv LACA (C) LACA (E) SimAttr PANEAttrRankNode2Vec 10−1 100 101 102 103 104 105 running time (sec) (f) Yelp LACA (C) LACA (E) p-Norm FDHK-RelaxPR-Nibble CRD 10−1 100 101 102 103 running time (sec) (g) Reddit LACA (C) LACA (E) WFD p-Norm FDPR-Nibble PANE 100 101 102 103 104 running time (sec) (h) Amazon2M Fig. 7: Running times. Best method and competitor (in terms of precision) are bolded and underlined , respectively. except ArXiv, where LACA (C), LACA (E), LACA (w/o SNAS), and HK-Relax perform comparably. Notice that on BlogCL, Flickr, and ArXiv, Hk-Relax outmatches LACA (C) and LACA (E) when ϵ is roughly 10−4 or 10−5. The reason is that HK-Relax produces Cs with larger sizes than that by LACA under the same ϵ owing to its higher running time (Table IV). Furthermore, it can be seen that LACA (w/o SNAS) obtains similar results to LACA (C) and LACA (E) when ϵ is large and inferior ones when ϵ is small, which are still superior to those by other methods on most datasets. This phenomenon implies that (i) our BDD (even without attributes) is more effective than existing graph diffusion metrics (personalized PageRank [41] and heat kernel PageRank [65]) in exploiting topological features for LGC, and (ii) the SNAS (attribute information) in LACA is crucial for identifying the nodes in Cs that are far from the seed. Further, in Appendix B, we evaluate the average external connectivity (i.e., conductance) and attribute variance (a.k.a. WCSS) of nodes in local clusters output by all methods. We also showcase that our LACA’s framework remains effective on non-attributed graphs in Appendix B. C. Efficiency Evaluation For ease of comparison, on each dataset, we assess the empirical efficiency of LACA (C) and LACA (E) only against the methods that yield the top- 4 best results among all com- petitors in terms of precision (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly\n",
      " > 794 [-0.02122 -0.00658 -0.04654  0.04877  0.0526 ]  (Table V). In Fig. 7, we show the running times (measured in wall-clock time) required by the preprocessing phase and the online phase (i.e., the procedure generating a local cluster for a single seed node) of each evaluated approach. The y-axis represents the running time in seconds on a log scale. The first observation we can make from Fig. 7 is that on small datasets Cora, PubMed, BlogCL, and Flickr, LACA (C), LACA (E) and the-state-of the-art solutions ( CFANE or PANE) are highly fast in the online stage, all of which take less than 0.1 seconds to finish a LGC task on average. However, CFANE and PANE obtain such a high online efficiency at the cost of up to 19.4 and 2.5 minutes for constructing node embeddings of all nodes in the preprocessing, whereas LACA (C) and LACA (E) require at most 2.8 seconds. On larger graphs ArXiv, Reddit, and Amazon2M compris- ing millions or tens of millions of edges, CFANE fails to report the results within 3 days and LGC-based methods HK-Relax, p-Norm FD and WFD are the state-of-the-art solutions. On such datasets, LACA (C) and LACA (E) are able to gain a significant speedup of 196×, 209×, and 152×, respectively, in the online stage on average. Note that the total preprocessing costs of LACA (C) and LACA (E) are still insignificant, which often take a few seconds, even less than the average cost for a single LGC task. For instance, on the largest dataset Amazon2M, with 62 million edges, LACA (E) attains a precision of 0.521 using an average of 36.7 seconds for Algo. 4 (online stage) and 13.4 seconds for TNAM construction (preprocessing stage). In comparison, the best competitor WFD consumes 92.9 minutes to get a precision of 0.503. The empirical observations are consistent with the theoretical evidence that HK-Relax, p-Norm FD and WFD have worse asymptotic complexities than LACA as in Table IV. On Yelp, where the best methods SimAttr (C) and SimAttr (E) are simply based on attributes, LACA (C) and LACA (E) have comparable prediction precision using slightly higher running time. In summary, our LACA methods can achieve significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such\n",
      " > 795 [-0.03568  -0.007133 -0.05545   0.0516    0.02919 ]  significantly higher efficiency for LGC on most attributed graphs with various volumes and meanwhile yield state-of-the-art result quality. D. Real-world Example Our proposed adaptive local clustering method can be applied to real-world scenarios, including the following three representative applications: game club recommenda- tion [2], social network analysis (e.g., Twitter community 11Jian Pei (100%) Dong Xin (11%) Bolin Ding (25%) Jiawei Han (33%)Charu Aggarwal (33%) Ke Wang (18%) Ming Hua (18%) Cindy Xide Lin (5%) Hector Gonzalez (11%) Bin Zhou (25%) Tianyi Wu (11%) (a) Run LACA on “Jian Pei” Jian Pei (100%) Xuemin Lin (33%) Jiawei Han (33%) Yufei Tao (11%)Jeffrey Xu Yu (5%) Ke Wang (18%) Hang Li (0%) Raymond Chi-Wing Wong (11%)Dimitris Papadias (0%) L. Wong (0%) P S Yu (25%) (b) Run PR-Nibble on “Jian Pei” Fig. 8: A real-world scenario on an academic network. detection) [17], [66], [67], and academic collaboration net- works [68]. A specific validation is conducted using the academic collaboration graph from the AMiner Coauthor dataset [69], which contains 1.7M scholars with co-authorships and keyword-based research interests. Applying LACA starting from the seed scholar “Jian Pei”, we identified 10 scholars with both strong co-authorship ties and aligned research interests, as shown in Fig. 8(a). This group includes di- rect co-authors such as “Jiawei Han” (similarity: 33%) and “Charu Aggarwal” (33%) as well as a subgroup centered around “Jiawei Han” (e.g., “Bolin Ding” (25%)). In con- trast, PR-Nibble—an LGC baseline—selected three schol- ars (e.g., “Hang Li,” “Dimitris Papadias”) with 0% similarity despite direct co-authorships (Fig. 8(b)). This stark difference (3/10 zero-similarity nodes in PR-Nibble vs. 0/10 in ours) demonstrates how ignoring attributes, as in LGC methods, risks recommending collaborators with mismatched expertise, even when strong structural ties exist. VII. R ELATED WORK A. Local Graph Clustering Local graph clustering (LGC) aims to find a high-quality local cluster without traversing the whole graph. The common characteristic of such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness\n",
      " > 796 [-0.04462   0.003456 -0.06573   0.06885   0.04575 ]  such methods is to optimize the conductance of the local cluster so that the target cluster is internally tightly connected while loosely connected to nodes outside the target cluster. Literature on LGC methods commonly distinguishes between two main types: random walk-based and flow-based methods (see [70] for a systematic survey). Among random walk-based methods [2], [14]–[17], [24], [26], [27], [32], [71], Nibble [14], PR-Nibble [15] and subsequent [2], [24], [26], [27], [32], [71] are proposed to perform local clustering via approximate personalized PageRank. Similarly, [16], [17] discuss the use of approximate heat kernel, including an LGC approach HK-Relax [16]. The flow-based methods include [20]–[22], [33], [72]. CRD [20] converts capacity diffusion into a maximum-flow problem. p-Norm FD [21] incorporates spectral and maximum-flow-based methods by choosing different p values. These methods usually provide theoretical guarantees on running time and approximation quality. However, these guar- antees may not be useful in practical efficacy. Considering this, recent works [31]–[33], [72] utilized additional graph resources besides connectivity More specifically, WFD [33] adjusts edge weights based on attribute similarity using the Gaussian kernel, followed by local diffusion through a flow- based approach. [72] extracts noisy node labels from additional graph resources for diffusion on reweighed graphs. Unlike these methods requiring costly preprocessing as reported in Section VI, ours seamlessly integrate node attribute-derived affinities into the diffusion process, speeding up preprocessing and improving attribute utilization. B. Community Search Community search aims to search densely connected com- munities for a user-specified query [73], [74]. Similar to local clustering, community search is query-dependent and does not need to compute the graph globally. Researchers have devised different community models to define densely connected communities, including the most popular ones: k- core [75]–[77] and k-truss [78]–[80]. To incorporate attribute cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan\n",
      " > 797 [-0.01392  -0.002087 -0.03665   0.0743    0.02473 ]  cohesiveness apart from structure cohesiveness, in recent years, a series of works [66], [80]–[83] on community search over attributed graphs have been developed. These methods usually consider keywords as the attributes of the nodes. Among these, ACQ [66], [81] and ATC [82] apply a core- based and a truss-based approach, respectively, both of which aimed at maximizing the number of keywords matching the query in the resulting community. Our approach differs as we do not use the intersections of keywords as the metric of attribute similarity. Although VAC [80] relaxes restrictions on using keywords as attributes, it still diverges from ours in both the objective and the method for calculating attribute similarity, especially in its repetitive computation to derive the minimum attribute similarity in the best k-truss. Another key difference is that community search literature imposes rigid topological constraints on the resulting community, which can lead to inferior outcomes. Moreover, these methods usually require offline index construction to enhance query processing speed, resulting in significant time and space overheads [32]. In contrast, our approach follows the LGC methods and designs an efficient attribute similarity computation method, eliminating the need for indices. Therefore, the problem setting of our work is orthogonal to that of community search. VIII. C ONCLUSION In this paper, we present LACA, an effective approach that leverages node attributes to improve the LGC quality on attributed graphs. LACA achieves remarkable performance through four major contributions: (i) a novel problem formu- lation based on the novel node affinity measure BDD, (ii) an adaptive RWR-based graph diffusion algorithm with faster convergence, (iii) a highly scalable preprocessing technique that enables problem reduction, and (iv) a well-thought-out three-step scheme for BDD approximation. The superiority of LACA over 17 baselines is experimentally validated over 8 real datasets in terms of both clustering quality and practical efficiency. Regarding future work, we plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings\n",
      " > 798 [-0.0457   -0.02042  -0.05444   0.1112   -0.008934]  plan to study the local clustering on heterophilic graphs. 12REFERENCES [1] C. Karras, A. Karras, I. Giannoukou, K. C. Giotopoulos, D. Tsolis, and S. Sioutas, “Global graph clustering and local graph exploration for community detection in twitter,” in 2022 7th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM) . IEEE, 2022, pp. 1–8. [2] X. Zhang, S. Xu, W. Lin, and S. Wang, “Constrained social community recommendation,” in Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining , 2023, pp. 5586–5596. [3] J. Chen, O. Za ¨ıane, and R. Goebel, “Local community identification in social networks,” in 2009 international conference on advances in social network analysis and mining . IEEE, 2009, pp. 237–242. [4] K. V oevodski, S.-H. Teng, and Y . Xia, “Finding local communities in protein networks,” BMC bioinformatics, vol. 10, pp. 1–14, 2009. [5] C.-S. Liao, K. Lu, M. Baym, R. Singh, and B. Berger, “Isorankn: spectral methods for global alignment of multiple protein networks,” Bioinformatics, vol. 25, pp. i253–i258, 2009. [6] J. Li, J. He, and Y . Zhu, “E-tail product return prediction via hypergraph- based local graph cut,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 519–527. [7] Y . Zhu, J. Li, J. He, B. L. Quanz, and A. A. Deshpande, “A local algorithm for product return prediction in e-commerce.” in IJCAI, 2018, pp. 3718–3724. [8] H. Yang, Y . Zhu, and J. He, “Local algorithm for user action prediction towards display ads,” in Proceedings of the 23rd ACM SIGKDD Inter- national Conference on Knowledge Discovery and Data Mining , 2017, pp. 2091–2099. [9] A. Fazzone, T. Lanciano, R. Denni, C. E. Tsourakakis, and F. Bonchi, “Discovering polarization niches via dense subgraphs with attractors and repulsers,” Proceedings of the VLDB Endowment , vol. 15, no. 13, pp. 3883–3896, 2022. [10] D. F. Gleich and M. W. Mahoney, “Using local spectral methods to robustify graph-based learning algorithms,” in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607.\n",
      " > 799 [-0.0578   0.02127 -0.0642   0.0888   0.01814]  Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pp. 359–368. [11] R. Rabbany, D. Bayani, and A. Dubrawski, “Active search of connections for case building and combating human trafficking,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2018, pp. 2120–2129. [12] S. Maji, N. K. Vishnoi, and J. Malik, “Biased normalized cuts.” IEEE, 2011. [13] M. W. Mahoney, L. Orecchia, and N. K. Vishnoi, “A local spectral method for graphs: With applications to improving graph partitions and exploring data graphs locally,” Journal of Machine Learning Research , vol. 13, no. 77, pp. 2339–2365, 2012. [14] D. A. Spielman and S.-H. Teng, “A local clustering algorithm for mas- sive graphs and its application to nearly linear time graph partitioning,” SIAM Journal on computing , vol. 42, no. 1, pp. 1–26, 2013. [15] R. Andersen, F. Chung, and K. Lang, “Local graph partitioning using pagerank vectors,” in 2006 47th Annual IEEE Symposium on Founda- tions of Computer Science (FOCS’06) . IEEE, 2006, pp. 475–486. [16] K. Kloster and D. F. Gleich, “Heat kernel based community detection,” in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 1386–1395. [17] R. Yang, X. Xiao, Z. Wei, S. S. Bhowmick, J. Zhao, and R.-H. Li, “Efficient estimation of heat kernel pagerank for local clustering,” in Proceedings of the 2019 International Conference on Management of Data, 2019, pp. 1339–1356. [18] N. Masuda, M. A. Porter, and R. Lambiotte, “Random walks and diffusion on networks,” Physics reports, vol. 716, pp. 1–58, 2017. [19] L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney, “Think locally, act locally: Detection of small, medium-sized, and large communities in large networks,” Physical Review E , vol. 91, 2015. [20] D. Wang, K. Fountoulakis, M. Henzinger, M. W. Mahoney, and S. Rao, “Capacity releasing diffusion for speed and locality,” in International Conference on Machine Learning . PMLR, 2017, pp. 3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs:\n",
      " > 800 [-0.02686   0.00876  -0.06097   0.06158   0.003712]  3598–3607. [21] K. Fountoulakis, D. Wang, and S. Yang, “p-norm flow diffusion for local graph clustering,” in International Conference on Machine Learning . PMLR, 2020, pp. 3222–3232. [22] A. Jung and Y . SarcheshmehPour, “Local graph clustering with network lasso,” IEEE Signal Processing Letters , vol. 28, pp. 106–110, 2020. [23] L. Lov ´asz, “Random walks on graphs,” Combinatorics, Paul erdos is eighty, vol. 2, no. 1-46, p. 4, 1993. [24] H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich, “Local higher- order graph clustering,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 555–564. [25] D. Fu, D. Zhou, and J. He, “Local motif clustering on time-evolving graphs,” in Proceedings of the 26th ACM SIGKDD International con- ference on knowledge discovery & data mining , 2020, pp. 390–400. [26] A. Chhabra, M. F. Faraj, and C. Schulz, “Local motif clustering via (hyper) graph partitioning,” in 2023 Proceedings of the Symposium on Algorithm Engineering and Experiments (ALENEX) . SIAM, 2023, pp. 96–109. [27] Z. Yuan, Z. Wei, F. Lv, and J.-R. Wen, “Index-free triangle-based graph local clustering,” Frontiers of Computer Science , vol. 18, no. 3, p. 183404, 2024. [28] C. Huang, H. Li, Y . Zhang, W. Lei, and J. Lv, “Cross-space adaptive filter: Integrating graph topology and node attributes for alleviating the over-smoothing problem,” in Proceedings of the ACM on Web Conference 2024, 2024, pp. 803–814. [29] L. Liao, X. He, H. Zhang, and T.-S. Chua, “Attributed social network embedding,” IEEE Transactions on Knowledge and Data Engineering , vol. 30, no. 12, pp. 2257–2270, 2018. [30] C. Bothorel, J. D. Cruz, M. Magnani, and B. Micenkova, “Clustering attributed graphs: models, measures and methods,” Network Science , vol. 3, no. 3, pp. 408–444, 2015. [31] S. Freitas, N. Cao, Y . Xia, D. H. P. Chau, and H. Tong, “Local partition in rich graphs,” in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1001–1008. [32] Y . Niu, Y . Li, J. Fan, and Z. Bao, “Local clustering over labeled graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp. 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to\n",
      " > 801 [-0.02274 -0.01298 -0.05453  0.04895 -0.03075]  graphs: An index-free approach,” in 2022 IEEE 38th International Conference on Data Engineering (ICDE) . IEEE, 2022, pp. 2805–2817. [33] S. Yang and K. Fountoulakis, “Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees,” in Proceedings of the 40th International Conference on Machine Learn- ing, vol. 202, 2023, pp. 39 252–39 276. [34] N. Halko, P.-G. Martinsson, and J. A. Tropp, “Finding structure with randomness: Probabilistic algorithms for constructing approximate ma- trix decompositions,” SIAM review, vol. 53, no. 2, pp. 217–288, 2011. [35] F. X. X. Yu, A. T. Suresh, K. M. Choromanski, D. N. Holtmann- Rice, and S. Kumar, “Orthogonal random features,” Advances in neural information processing systems , vol. 29, 2016. [36] S. Wang, R. Yang, X. Xiao, Z. Wei, and Y . Yang, “Fora: simple and effective approximate single-source personalized pagerank,” in Proceed- ings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2017, pp. 505–514. [37] R. Yang, “Efficient and effective similarity search over bipartite graphs,” in Proceedings of the ACM Web Conference 2022 , 2022, pp. 308–318. [38] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” vol. 30, 2017, pp. 1024–1034. [39] B. Li, B. Jing, and H. Tong, “Graph communal contrastive learning,” Proceedings of the ACM Web Conference 2022 , 2022. [40] H. Tong, C. Faloutsos, and J.-Y . Pan, “Fast random walk with restart and its applications,” in Sixth international conference on data mining (ICDM’06). IEEE, 2006, pp. 613–622. [41] G. Jeh and J. Widom, “Scaling personalized web search,” in Proceedings of the 12th international conference on World Wide Web, 2003, pp. 271– 279. [42] S. Rothe and H. Sch ¨utze, “Cosimrank: A flexible & efficient graph- theoretic similarity measure,” in Proceedings of the 52nd Annual Meet- ing of the Association for Computational Linguistics , 2014, pp. 1392– 1402. [43] P. Lofgren, S. Banerjee, and A. Goel, “Bidirectional pagerank estima- tion: From average-case to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J.\n",
      " > 802 [-0.04416 -0.01111 -0.0487   0.03763 -0.02736]  to worst-case,” in Algorithms and Models for the Web Graph: 12th International Workshop, WAW 2015, Eindhoven, The Netherlands, December 10-11, 2015, Proceedings 12 . Springer, 2015, pp. 164–176. [44] R. J. Muirhead, Aspects of multivariate statistical theory . John Wiley & Sons, 2009. [45] Y . Ma, X. Liu, T. Zhao, Y . Liu, J. Tang, and N. Shah, “A unified view on graph neural networks as graph signal denoising,” in Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 2021, pp. 1202–1211. 13[46] M. Zhu, X. Wang, C. Shi, H. Ji, and P. Cui, “Interpreting and unifying graph neural networks with an optimization framework,” in Proceedings of the Web Conference 2021 , 2021, pp. 1215–1226. [47] A. Bojchevski, J. Gasteiger, B. Perozzi, A. Kapoor, M. Blais, B. R ´ozemberczki, M. Lukasik, and S. G ¨unnemann, “Scaling graph neural networks with approximate pagerank,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2464–2473. [48] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi- Rad, “Collective classification in network data,” AI magazine , vol. 29, no. 3, pp. 93–93, 2008. [49] L. Tang and H. Liu, “Relational learning via latent social dimensions,” in Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining , 2009, pp. 817–826. [50] X. Huang, J. Li, and X. Hu, “Label informed attributed network embedding,” in Proceedings of the tenth ACM international conference on web search and data mining , 2017, pp. 731–739. [51] W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec, “Open graph benchmark: Datasets for machine learning on graphs,” Advances in neural information processing systems , vol. 33, pp. 22 118–22 133, 2020. [52] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . K. Prasanna, “Graphsaint: Graph sampling based inductive learning method,” in The 8th International Conference on Learning Representations , 2023. [53] W.-L. Chiang, X. Liu, S. Si, Y . Li, S. Bengio, and C.-J. Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab.(LANL), Los Alamos, NM (United\n",
      " > 803 [-0.0386   -0.006947 -0.05457   0.03445   0.00345 ]  Hsieh, “Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks,” in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining , 2019, pp. 257–266. [54] D. Liben-Nowell and J. Kleinberg, “The link prediction problem for social networks,” in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556–559. [55] G. Jeh and J. Widom, “Simrank: a measure of structural-context similarity,” in Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining , 2002, pp. 538– 543. [56] Z. Yin, M. Gupta, T. Weninger, and J. Han, “A unified framework for link recommendation using random walks.” IEEE, 2010. [57] F. Rahutomo, T. Kitasuka, and M. Aritsugi, “Semantic cosine similarity,” in The 7th international student conference on advanced science and technology ICAST, vol. 4, no. 1, 2012, p. 1. [58] C.-C. Hsu, Y .-A. Lai, W.-H. Chen, M.-H. Feng, and S.-D. Lin, “Unsu- pervised ranking using graph structures and node attributes.” ACM, 2017. [59] A. Grover and J. Leskovec, “node2vec: Scalable feature learning for networks,” Proceedings of the 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , 2016. [60] R. Yang, J. Shi, X. Xiao, Y . Yang, J. Liu, S. S. Bhowmicket al., “Scaling attributed network embedding to massive graphs,” Proceedings of the VLDB Endowment, vol. 14, no. 1, pp. 37–49, 2020. [61] R. Yang, J. Shi, X. Xiao, Y . Yang, S. S. Bhowmick, and J. Liu, “Pane: scalable and effective attributed network embedding,” The VLDB Journal, vol. 32, pp. 1237–1262, 2023. [62] G. Pan, Y . Yao, H. Tong, F. Xu, and J. Lu, “Unsupervised attributed network embedding via cross fusion.” ACM, 2021. [63] K. Fountoulakis, D. F. Gleich, and M. W. Mahoney, “A short introduction to local graph clustering methods and software,” 2018. [64] A. Hagberg, P. Swart, and D. S Chult, “Exploring network struc- ture, dynamics, and function using networkx,” Los Alamos National Lab.(LANL), Los Alamos, NM (United States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient\n",
      " > 804 [-0.0796  -0.01446 -0.0745   0.05063 -0.01982]  States), Tech. Rep., 2008. [65] F. Chung, “The heat kernel as the pagerank of a graph,” Proceedings of the National Academy of Sciences , vol. 104, no. 50, pp. 19 735–19 740, 2007. [66] Y . Fang, R. Cheng, S. Luo, and J. Hu, “Effective community search for large attributed graphs,” vol. 9. Association for Computing Machinery (ACM), 2016, pp. 1233–1244. [67] Y . Fang, Z. Wang, R. Cheng, H. Wang, and J. Hu, “Effective and efficient community search over large directed graphs,” IEEE Transactions on Knowledge and Data Engineering, vol. 31, no. 11, pp. 2093–2107, 2018. [68] Y . Fang, Y . Yang, W. Zhang, X. Lin, and X. Cao, “Effective and efficient community search over large heterogeneous information networks,” vol. 13, no. 6. VLDB Endowment, 2020, pp. 854–867. [69] J. Zhang, J. Tang, C. Ma, H. Tong, Y . Jing, and J. Li, “Panther: Fast top-k similarity search on large networks,” in Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, 2015, pp. 1445–1454. [70] G. Baltsou, K. Christopoulos, and K. Tsichlas, “Local community detection: A survey,” IEEE Access, vol. 10, pp. 110 701–110 726, 2022. [71] R. Andersen, S. O. Gharan, Y . Peres, and L. Trevisan, “Almost optimal local graph clustering using evolving sets,” Journal of the ACM (JACM), vol. 63, no. 2, pp. 1–31, 2016. [72] A. B. de Luca, K. Fountoulakis, and S. Yang, “Local graph clustering with noisy labels,” in The Twelfth International Conference on Learning Representations, 2023. [73] X. Huang, L. V . Lakshmanan, and J. Xu, Community Search over Big Graphs. Morgan & Claypool Publishers, 2019. [74] ——, “Community search over big graphs: Models, algorithms, and op- portunities,” in Proceedings of the 33rd IEEE International Conference on Data Engineering (ICDE) , 2017, pp. 1451–1454. [75] M. Sozio and A. Gionis, “The community-search problem and how to plan a successful cocktail party.” ACM, 2010. [76] W. Cui, Y . Xiao, H. Wang, and W. Wang, “Local search of communities in large graphs.” ACM, 2014. [77] N. Barbieri, F. Bonchi, E. Galimberti, and F. Gullo, “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X.\n",
      " > 805 [-0.03918   0.054    -0.0787    0.04376   0.002855]  “Efficient and effective community search,” Data Mining and Knowledge Discovery , vol. 29, pp. 1406–1433, 2015. [78] X. Huang, H. Cheng, L. Qin, W. Tian, and J. X. Yu, “Querying k-truss community in large and dynamic graphs.” ACM, 2014. [79] X. Huang, L. V . S. Lakshmanan, J. X. Yu, and H. Cheng, “Approximate closest community search in networks,” vol. 9. Association for Computing Machinery (ACM), 2015, pp. 276–287. [80] Q. Liu, Y . Zhu, M. Zhao, X. Huang, J. Xu, and Y . Gao, “Vac: Vertex- centric attributed community search.” IEEE, 2020. [81] Y . Fang, R. Cheng, Y . Chen, S. Luo, and J. Hu, “Effective and efficient attributed community search,” The VLDB Journal, vol. 26, pp. 803–828, 2017. [82] X. Huang and L. V . S. Lakshmanan, “Attribute-driven community search,” vol. 10. Association for Computing Machinery (ACM), 2017, pp. 949–960. [83] Y . Zhu, J. He, J. Ye, L. Qin, X. Huang, and J. X. Yu, “When structure meets keywords: Cohesive attributed community search.” ACM, 2020. [84] G. H. Gloub and C. F. Van Loan, “Matrix computations,” Johns Hopkins Universtiy Press, 3rd edtion , 1996. [85] G. Strang, Introduction to linear algebra . SIAM, 2022. [86] Z. Zhang, P. Cui, X. Wang, J. Pei, X. Yao, and W. Zhu, “Arbitrary- order proximity preserved network embedding,” in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining , 2018, pp. 2778–2786. [87] J. MacQueen et al. , “Some methods for classification and analysis of multivariate observations,” in Proceedings of the fifth Berkeley sympo- sium on mathematical statistics and probability, vol. 1, no. 14. Oakland, CA, USA, 1967, pp. 281–297. [88] J. Yang and J. Leskovec, “Defining and evaluating network communities based on ground-truth.” IEEE, 2012. [89] C. Yang, Z. Liu, D. Zhao, M. Sun, and E. Y . Chang, “Network representation learning with rich text information,” in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization, 2015, pp. 2111–2117. [90] X. Huang, J. Li, and X. Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi)\n",
      " > 806 [-0.00887  0.0507  -0.0786   0.02332  0.03662]  Hu, “Accelerated attributed network embedding,” vol. Not available. Society for Industrial and Applied Mathematics, 2017, pp. 633–641. [91] H. Yang, S. Pan, L. Chen, C. Zhou, and P. Zhang, “Low-bit quantization for attributed network representation learning.” International Joint Conferences on Artificial Intelligence Organization, 2019. [92] Z. Zhang, H. Yang, J. Bu, S. Zhou, P. Yu, J. Zhang, M. Ester, and C. Wang, “Anrl: Attributed network representation learning via deep neural networks.” International Joint Conferences on Artificial Intelligence Organization, 2018. [93] H. Gao and H. Huang, “Deep attributed network embedding.” Interna- tional Joint Conferences on Artificial Intelligence Organization, 2018. [94] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and their composi- tionality,” vol. 26, 2013, pp. 3111–3119. 14APPENDIX A. Theoretical Proofs 1) Proof of Theorem IV .1: Proof. We denote − →γ ℓ as the vector − →γ obtained at Line 3 and by − →b ℓ the remaining residual at Line 5 in ℓ-th iteration. Then, according to Line 6, − →q can be represented by − →q = (1− α) ∞X ℓ − →γ ℓ. (21) Next, we consider {− →γ 1, − →γ 2, . . . ,− →γ ℓ, − →γ ℓ+1, . . . ,− →γ ∞}. By Lines 3 and 7-8, we have − →γ 1 = − →f − − →b 1, − →γ 1 = − →b 1 + α− →γ 1P − − →b 2 ··· − →γ ℓ = − →b ℓ−1 + α− →γ ℓ−1P − − →b ℓ, − →γ ℓ+1 = − →b ℓ + α− →γ ℓP − − →b ℓ+1 ··· . Then, Eq. (21) can be rewritten as − →q 1 − α = − →f + α ∞X ℓ=1 − →γ ℓP − − →b ∞ = ∞X ℓ=0 αt− →f Pℓ − ∞X ℓ=0 αt− →b ℓPℓ. (22) Recall that − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i/d(vi) < ϵ(see Lines 3 and 5). Let − →b be a length- n vector where each i- th entry is ϵ · d(vi). Together with Eq. (22) and the matrix definition of π(vi, vj) defined in Eq. (6), we can bound each entry − →q t by − →q t ≥ (1 − α) ∞X ℓ=0 αt(− →f Pℓ)t − (1 − α) ∞X ℓ=0 αt(− →b Pℓ)t = X vi∈V − →f i · π(vi, vt) − X vi∈V − →b i · π(vi, vt) = X vi∈V − →f i · π(vi, vt) − X vi∈V ϵ · d(vi) · π(vi, vt). By the fact of d(vi) · π(vi, vt) = d(vt) · π(vt, vi) (Lemma 1 in [43]) and P vi∈V π(vt, vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP.\n",
      " > 807 [-0.01724  0.04932 -0.0721   0.02872  0.0458 ]  vi) = 1, the above inequality can be simplified as − →q t ≥ X vi∈V − →f i · π(vi, vt) − ϵ · d(vt) · X vi∈V π(vt, vi) = X vi∈V − →f i · π(vi, vt) − ϵ · d(vt), which finishes the proof of Eq. (14). In what follows, we analyze the time complexity of Algo. (1). First, suppose that Lines 3-7 in Algo. 1 are executed for L iterations. Now, we consider any iteration ℓ. For ease of exposition, we denote − →γ ℓ as the vector − →γ obtained at Line 3 in ℓ-th iteration and by − →r ℓ and − →q ℓ the residual and reserve vectors − →r and − →q at the beginning of ℓ-th iteration, respectively. Accordingly, for each non-zero entry − →γ ℓ i in − →γ ℓ,− →γ ℓ i ≥ d(vi) · ϵ. First, we prove that at the beginning of any ℓ-th iteration, the following equation holds: ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. (23) We prove this by induction. For the base case where ℓ = 1, i.e., at the beginning of the first iteration, we have − →r 1 = − →f and − →q 1 = 0, and thus, Eq. (23) holds. Next, we assume Eq. (23) holds at the beginning of ℓ-th (ℓ >0) iteration, i.e., ∥− →r ℓ∥1 + ∥− →q ℓ∥1 = ∥− →f ∥1. According to Lines 5-7, we have − →q ℓ+1 = − →q ℓ + (1− α) · − →γ ℓ − →r ℓ+1 = − →r ℓ − − →γ + α · − →γ ℓP. As such, ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 = ∥− →q ℓ∥1 + ∥− →r ∥1 + α · ∥− →γ ℓP − − →γ ℓ∥1. (24) Note that ∥− →γ P− − →γ ℓ∥1 = X vi∈V X vj∈V − →γ ℓ j · Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈V Pj,i − X vj∈V − →γ ℓ j = X vj∈V − →γ ℓ j X vi∈N(vj) 1 d(vj) − X vj∈V − →γ ℓ j = 0, implying that ∥− →r ℓ+1∥1 + ∥− →q ℓ+1∥1 in Eq. (24) equals 0, namely, Eq. (23) still holds. As per Line 6, in each ℓ-th iteration, Algo. 1 converts (1−α) fraction of − →γ ℓ into − →q ℓ, i.e., at least (1−α)·ϵ·d(vi) out of each non-zero entries in − →γ ℓ is passed to − →q ℓ. After L iterations, we obtain − →q L. Recall that by Eq. (23), ∥− →q L∥1 ≤ ∥− →f ∥1. We obtain LX ℓ=1 X i∈supp(− →γ ℓ) (1 − α) · ϵ · d(vi) ≤ ∥− →q L∥1 ≤ ∥− →f ∥1, (25) which leads to PL ℓ=1 P i∈supp(− →γ ℓ) d(vi) ≤ ∥− →f ∥1 (1−α)ϵ . Notice that in Line 6, each non-zero entry in − →γ ℓ will result in d(vi) operations in the matrix-vector multiplication− →γ ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥−\n",
      " > 808 [ 0.003288  0.0135   -0.0387    0.02713   0.04446 ]  ℓP. Thus, the total cost of Line 6 for L iterations isPL ℓ=1 P i∈supp(− →γ ℓ) d(vi), which is bounded by ∥− →f ∥1 (1−α)ϵ as aforementioned. In addition, in each iteration, Lines 5 and 7 also merely involve operations on the non-zero entries in− →γ ℓ. Their total cost for L iterations can then be bounded by ∥− →f ∥1 (1−α)ϵ as well. As for Line 3, in the first iteration, we need to inspect every entry in − →r 1, and hence, the time cost is ∥− →r 1∥0 = supp(− →f ) . In any subsequent ℓ-th iteration, we solely need to inspect the entries in − →r ℓ affected by Line 7 in the previous iteration, which is also bounded by the non- zero entries in − →γ ℓ−1. Hence, the overall time complexity of Algo. 1 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . The theorem is proved. 152) Proof of Theorem IV .2: Proof. Similar to the proof of Theorem IV .1, we let− →b ℓ be the remaining residual in the ℓ-th iteration in Algo. 2. By Lines 6 and Lines 8-11, Eq. (22) still holds. Notice that − →b ℓ is 0 when ℓ-th iteration runs Lines 5-6. As such, − →b ℓ always satisfies ∀vi ∈ V, − →b ℓ i d(vi) < ϵand then Eq. (14) follows as in the proof of Theorem IV .1. Next, we analyze the time complexity of Algo. 2. Notice that according to Line 4 in Algo. (2), the total cost Ctot entailed by the non-greedy operations (Lines 4-6) is bounded by O \u0010 ∥− →f ∥1 (1−α)ϵ \u0011 . As for greedy operations (Lines 8-11), they will be conducted as in Algo. (1). Note that Algo. 2 also terminates when − →γ is 0. Therefore, the total amount of greedy operations in AdaptiveDiffuse is at most that needed in GreedyDiffuse (see Theorem IV .1). In sum, the overall complexity of Algo. 2 is O \u0010 max n supp(− →f ) , ∥− →f ∥1 (1−α)ϵ o\u0011 . 3) Proof of Lemma IV .3: Proof. We assume Algo. 2 conducts Lines 3-11 for L it- erations. and we refer to the vectors − →q , − →r , and − →γ in each ℓ-th iteration as in the proof of Theorem IV .1. Further, we assume that in ℓ1-th, ℓ2-th, . . ., and ℓT -th iterations, AdaptiveDiffuse executes Lines 8-11. Based on Inequal- ity (25) and Eq. (23), we can get TX j=1 X i∈supp(− →γ ℓj ) d(vi) ≤ ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is\n",
      " > 809 [ 0.01433    0.0392    -0.03363    0.0005026  0.04376  ]  ∥− →q ℓT ∥1 (1 − α)ϵ ≤ ∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ . Let L = {1, 2, . . . , L} and T = {ℓ1, ℓ2, . . . , ℓT }. As for the L \\ Titerations, Algo. (2) conducts Lines 5-6. Notice that in such cases, we have Ctot + vol(− →r ) < ∥− →f ∥1 (1−α)ϵ , meaning that X j∈L\\T X i∈supp(− →r j) d(vi) ≤ ∥− →f ∥1 (1 − α)ϵ. According to Lines 5-6 and Lines 8-11, all the non-zero elements in final − →q are from non-zero entries in − →γ j ∀j ∈ T and − →r j ∀j ∈ L \\ T. Then, vol(− →q ) = X i∈supp(− →q ) d(vi) ≤ TX j=1 X i∈supp(− →γ ℓj ) d(vi) + X j∈L\\T X i∈supp(− →r j) d(vi) ≤ 2∥− →f ∥1 − ∥− →r ℓT ∥1 (1 − α)ϵ = β∥− →f ∥1 (1 − α)ϵ, where β stands for a constant in the range [1, 2] since 0 ≤ ∥− →r ℓT ∥1 ≤ ∥− →f ∥1. Similarly, we obtain |supp(− →q )| ≤ X i∈supp(− →q ) d(vi) =vol(− →q ) ≤ β∥− →f ∥1 (1 − α)ϵ. When σ ≥ 1, AdaptiveDiffuse only executes Lines 8- 11, and thus, L \\ Tis an empty set. The above inequalities become |supp(− →q )| ≤vol(− →q ) ≤ ∥− →f ∥1 (1 − α)ϵ, which completes the proof. 4) Proof of Lemma V .1: Proof. We first need the following theorem. Theorem A.1 (Eckart–Young Theorem [84]). Suppose that Mk ∈ Rn×k is the rank- k approximation to M ∈ Rn×n obtained by exact SVD, then minrank(cM)≤k ∥M − cM∥2 = ∥M − Mk∥2 = λk+1, where λk+1 stands for the (k + 1)-th largest singular value of M. Suppose that UΛV ⊤ is the exact k-SVD of X, by Theorem A.1, we have ∥UΛV ⊤−X∥2 ≤ λk+1, where λk+1 is the (k+ 1)-th largest singular value of X. Let bU bΛ bV ⊤ be the k-SVD of XX⊤. Similarly, from Theorem A.1, we get ∥ bU bΛ bV ⊤ − XX⊤∥2 ≤ bλk+1, where bλk+1 is the (k+1)-th largest singular value of XX⊤. According to [85], columns in U are the eigenvectors of matrix XX⊤ and the squared singular values of X are the eigenvalues of XX⊤. Given that singular values are non- negative, all the eigenvalues of XX⊤ are also non-negative. Λ2 and U contain the k-largest eigenvalues and corresponding eigenvectors of XX⊤, and λk+1 2 = bλk+1. Further, by Theorem 4.1 in [86], it can be verified that Λ2 and U are the top-k singular values and left/right singular vectors of XX⊤, respectively, and λk+1 2 is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)−\n",
      " > 810 [-0.00814  0.03232 -0.0171   0.02826  0.04245]  is the (k +1)-th largest singular value of XX⊤. Consequently, ∥UΛ2U⊤ − XX⊤∥2 ≤ λk+1 2 and the proof is done. 5) Proof of Theorem V .2: Proof. Recall that for vi ∈ V, vector − →x (i) is L2 normalized, i.e., ∥− →x (i)∥2 = 1. Thus, we can derive ∥− →x (i) − − →x (j)∥2 2 = 2(1 − cos (− →x (i), − →x (j)) = 2(1− − →x (i) · − →x (j)) ∈ [0, 4]. On its basis, f(vi, vj) in Eq. (3) can be transformed as follows: f(vi, vj) = exp \u0012− →x (i) · − →x (j) δ \u0013 = exp 1 − 1 2 ∥− →x (i) − − →x (j)∥2 2 δ ! = exp \u00121 δ − ∥− →x (i) − − →x (j)∥2 2 2δ \u0013 = exp \u00121 δ \u0013 · exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 . (26) According to Theorem 1 in [35] and the mathematical form of bY = 1 δ XΣQ = 1 δ UΛΣQ via Lines 6-9, we have E h K · K⊤ i = exp \u0012 −∥− →x (i) − − →x (j)∥2 2 2δ \u0013 , where K = 1√ d · sin(− →by (i)) ∥ cos(− →by (j)). Plugging the definitions of f(vi, vj) in Eq. (26) and Y in Eq. (19) into the above equation proves the theorem. 166) Proof of Lemma V .3: Proof. According to [34], the invocation of k-SVD over X runs in O(ndk+nk2) time. Note that the number of iterations in k-SVD is regarded as a constant and thus is omitted since it is set to small integers, e.g., 7, in practice, and k is less than d. By Eq. (18), the computations of − →y ∗ and − →z (i) ∀vi ∈ Vat Lines 10-11 need O(nk) time. Thus, when f(·, ·) is the cosine similarity function, the construction of TNAM Z requires O(ndk) time. In comparison, when f(·, ·) is the exponential cosine simi- larity function, the QR decomposition at Line 7 and construct- ing Y at Line 9 take O(k3) and O(nk2) time, respectively. In turn, the overall time complexity of Algo. 3 is bounded by O(ndk), which can be reduced to O(nd) since k is regarded as a constant. 7) Proof of Theorem V .4: Proof. Let − →ρ ◦ be the vector returned by Algo. 2 invoked at Line 5 in Algo. 4. According to Theorem IV .2, ∀vj ∈ V, X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt) − − →ρ ◦ t ≥ 0 and X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) · π(vj, vt)−− →ρ ◦ t ≤ ϵ·d(vt), yielding 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · d(vj) d(vt) · π(vj, vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step\n",
      " > 811 [-0.02347  0.0309  -0.038    0.03244  0.03992]  vt)− − →ρ ◦ t d(vt) ≤ ϵ. By the fact of π(vt, vj) =d(vj) d(vt) ·π(vj, vt) (Lemma 1 in [43]) and − →ρ ′ t = − →ρ ◦ t d(vt) (Line 6), we have 0 ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − − →ρ ′ t ≤ ϵ. Further, we obtain − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V − →π ′ i · s(vi, vj) · π(vt, vj) − ϵ. Notice that − →π obtained at Line 2 in Algo. 4 satisfies 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ Vaccording to Theorem IV .2. We then derive − →ρ ′ t ≤ X j∈supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj) ≤ − →ρ t and − →ρ ′ t ≥ X j∈supp(− →π′) X vi∈V (π(vs, vi) − ϵd(vi)) · s(vi, vj) · π(vt, vj) − ϵ, where the latter leads to − →ρ t − − →ρ ′ t ≤ ϵ + X j∈supp(− →π′) X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) + X j∈{1,...,n}\\supp(− →π′) X vi∈V π(vs, vi) · s(vi, vj) · π(vt, vj). Recall that 0 ≤ π(vs, vi)−− →π ′ i ≤ ϵ·d(vi) ∀vi ∈ V. Thus, ∀i /∈ supp(− →π ′), π(vs, vi) ≤ − →π ′ i + ϵ ·d(vi) =ϵ ·d(vi). Accordingly, − →ρ t − − →ρ ′ t ≤ ϵ + X vj∈V X vi∈V ϵd(vi) · s(vi, vj) · π(vt, vj) ≤ ϵ + X vi∈V ϵd(vi) · X vj∈V s(vi, vj) · π(vt, vj) ≤ \u0000 1 +P vi∈V d(vi) · maxvj∈V s(vi, vj) \u0001 · ϵ, which proves the theorem. 8) Proof of Lemma V .6: Proof. By setting its derivative w.r.t.H to zero and , we obtain the optimal H as: ∂{(1 − α) · ∥H − H◦∥2 F + α · trace(H⊤(I − ˜A)H)} ∂H = 0 =⇒ (1 − α) · (H − H◦) +α(I − ˜A)H = 0 =⇒ H = (1− α) · \u0010 I − α ˜A \u0011−1 H◦. (27) By the property of the Neumann series, we have(I−α ˜A)−1 =P∞ ℓ=0 αt ˜A ℓ . Plugging it into Eq. (27) completes the proof. B. Additional Experiments 1) Parameter Analysis: In this set of experiments, we empirically investigate the impact of three key parameters in LACA (C) and LACA (E): the restart factor α, parameter σ, and the dimension k of TNAM vectors. For each of them, we run LACA (C) and LACA (E) over Cora, PubMed, BlogCalaog, Flickr, and ArXiv, respectively, by varying the parameter with others fixed. Varying α. Figs. 9(a) and 9(b) show the precision achieved by LACA (C) and LACA (E) on five datasets, respectively, when α is varied from 0.0 to 0.9 with step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section\n",
      " > 812 [-0.00908  -0.011505 -0.02374   0.009056  0.0344  ]  step size 0.1. It can be clearly observed that both LACA (C) and LACA (E) present nearly identical behaviors on all datasets when varying α. That is, the precision scores increase conspicuously with α increasing. The only exception is on BlogCL, where the best result is attained when α = 0.8. Recall that in Algo. 2, when α is small, AdaptiveDiffuse will convert substantial residuals into reserves of nearby nodes and distribute only a few to far-reaching neighbors, yielding local clusters with diminutive size, and hence, sub-par result quality. Varying σ. Next, we study the parameter σ for balancing greedy and non-greedy operations in AdaptiveDiffuse. Recall that a large σ indicates running more greedy operations in AdaptiveDiffuse, which degrades to GreedyDiffuse when σ = 1. Figs. 9(c) and 9(d) plot the precision scores when increasing σ from 0.0 to 1.0 in LACA (C) and LACA (E), respectively, on five datasets. We can observe that both LACA (C) and LACA (E) (i) are not 17Cora PubMed BlogCL Flickr ArXiv 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (a) Varying α in LACA (C) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.1 0.2 0.3 0.4 0.5 0.6Precision (b) Varying α in LACA (E) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (c) Varying σ in LACA (C) 0 0.2 0.4 0.6 0.8 1.0 0.1 0.2 0.3 0.4 0.5 0.6Precision (d) Varying σ in LACA (E) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (e) Varying k in LACA (C) 8 16 32 64 128 d0.1 0.2 0.3 0.4 0.5 0.6Precision (f) Varying k in LACA (E) Fig. 9: Precision when varying parameters. sensitive to σ on Cora and PubMed, (ii) undergo a significant performance downturn when σ is beyond 0.1 on BlogCatlog and Flickr, and (iii) on ArXiv, see a considerable uptick in precision when σ rises from 0 to 0.2 and invariant results afterward. LACA (C) and LACA (E) favor a small σ on BlogCatlog and Flickr datasets with high average degrees (m/n >60) because greedy operations are sensitive to high- degree nodes and tend to return small local clusters on such graphs (i.e., fewer non-zero entries in − →q ), as analyzed in Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations\n",
      " > 813 [-0.01396  0.03174 -0.01562  0.02275  0.03635]  Section IV-A. Varying k. Figs. 9(e) and 9(f) show the precision values of LACA (C) and LACA (E) when varying the dimension k of TNAM vectors Z in {8, 16, 32, 64, 128, d}. On citation networks Cora, PubMed, and ArXiv, we can see that the performance of both LACA (C) and LACA (E) remains stable when k is increased from 8 to d, except a slight drop on Cora when k = 8. In comparison, on social networks BlogCL and Flickr, a remarkable improvement and reduction in performance can be observed when k increases from 8 to 32 and from 32 to d, respectively. The reason is that both BlogCL and Flickr have numerous distinct attributes in X (d = 8, 189 and d = 12, 047), which embody substantial noisy information. Meanwhile, our k-SVD essentially denoises attribute data by extracting k key components. The observa- tions manifest the effectiveness of our TNAM construction technique (Section V-A) in capturing the attribute similarity of node pairs with a small dimension k, e.g., 16 or 32. 2) Ablation Study: To study the effectiveness of the SNAS in our BDD definition (Eq. (5)), our graph diffusion algorithm AdaptiveDiffuse in Section IV-C, as well as the k- SVD in Section V-A, we create three ablated versions for LACA (C) and LACA (E), respectively. Particularly, the variants of LACA (C) and LACA (E) without AdaptiveDiffuse are implemented using GreedyDiffuse as the diffusion component. Table VI reports the best precision scores attained by each method on 8 datasets. We can see remarkable performance decreases in both LACA (C) and LACA (E) after disabling any of these three ingredients, especially the SNAS. The only exception is on Amazon2M, where LACA (C) achieves a better result when SNAS is removed from BDD, whereas LACA (E) exhibits a radically different phenomenon. This indicates that the exponential cosine similarity is more robust in modelling the similarity of node attributes. From Table VI, AdaptiveDiffuse is another key component affecting the resulting quality, which accords with our analysis of GreedyDiffuse’s deficiencies in Section IV-C. Consistent with the observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability\n",
      " > 814 [-0.0336    0.03897  -0.004436  0.03955   0.04663 ]  observations from Figs. 9(e) and 9(f), the k-SVD in Algo. 3 improves the performance of LACA due to its denoising ability. TABLE VI: Ablation study. Darker shades indicate better results. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2M LACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465w/ok-SVD 0.551 0.552 0.426 0.2810.3770.7540.808 0.465w/oAdaptiveDiffuse0.544 0.551 0.48 0.4260.3290.7540.213 0.287w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521w/ok-SVD 0.546 0.554 0.395 0.2510.3760.7370.808 0.514w/oAdaptiveDiffuse0.54 0.553 0.469 0.3840.3360.7350.214 0.365w/o SNAS 0.486 0.537 0.302 0.2 0.3430.6870.779 0.495 3) Conductance and WCSS: This set of experiments studies the average conductance [23] and the average within-cluster sum of squares (WCSS) [87] of ground-truth and the local clusters output by LACA (C), LACA (E), and 17 competitors. Given a local cluster Cs, conductance only measures the connectivity between nodes in Cs and nodes outside Cs, while WCSS merely evaluates the variance of attribute vectors of nodes in Cs. Intuitively, lower conductance and WCSS indicate a higher clustering quality. The conductance and WCSS values of all methods on the eight datasets are reported in Table VII. We highlight the top 3 best results (with the smallest differences from the ground truth) on each dataset in blue, with darker shades indicating higher quality. From Table VII, we can see that none of the evaluated methods perform best on all datasets. But notably, LACA (C) and LACA (E) obtain the top 3 conductance or WCSS results on eight and seven datasets, respectively, whereas the best competitor PR-Nibble is ranked in the top 3 on six datasets, meaning that local clusters by LACA achieve a good balance of structure cohesiveness and attribute homogeneity. Another observation is that compared to WCSS, conductance values by different methods vary markedly. This is because nodes in a graph have divergent degrees, and hence, a small change in a cluster can lead to a large difference in conductance. 4) Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964\n",
      " > 815 [-0.0206   -0.005043 -0.009735  0.02165   0.02629 ]  Scalability Evaluation: This section experimentally eval- uates the scalability of LACA (C) and LACA (E) on four 18TABLE VII: Conductance and WCSS. Method Cora PubMed BlogCL Flickr ArXiv Yelp Reddit Amazon2M Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Cond.↓ WCSS↓ Ground-truth0.188 0.979 0.204 0.976 0.608 0.963 0.765 0.997 0.408 0.663 0.649 0.55 0.226 0.594 0.173 0.981 PR-Nibble[15] 0.337 0.966 0.199 0.974 0.569 0.969 0.733 0.998 0.518 0.667 0.237 0.55 0.368 0.595 0.369 0.984 APR-Nibble 0.323 0.964 0.196 0.974 0.671 0.964 0.799 0.99 0.345 0.475 0.345 0.475 0.583 0.428 0.409 0.937 HK-Relax[16] 0.138 0.966 0.096 0.971 0.481 0.967 0.748 0.998 0.222 0.663 0.13 0.556 0.196 0.583 0.132 0.946 CRD[20] 0.156 0.942 0.173 0.947 0.61 0.967 0.787 0.974 0.28 0.644 0.139 0.563 0.275 0.588 0.168 0.928 p-Norm FD[21] 0.131 0.954 0.178 0.958 0.709 0.967 0.845 0.996 0.252 0.656 0.35 0.404 0.235 0.593 0.118 0.971 WFD[33] 0.127 0.956 0.17 0.96 0.713 0.964 0.845 0.996 0.251 0.657 0.339 0.405 0.468 0.508 0.145 0.971 Jaccard[54] 0.617 0.984 0.637 0.981 0.655 0.969 0.854 0.998 0.846 0.682 0.696 0.58 0.744 0.66 0.707 0.993 Adamic-Adar[54] 0.617 0.984 0.637 0.981 0.609 0.969 0.584 0.998 0.834 0.682 0.696 0.58 0.771 0.654 0.707 0.993 Common-Nbrs[54] 0.617 0.984 0.637 0.981 0.594 0.969 0.573 0.998 0.834 0.682 0.696 0.58 0.777 0.654 0.707 0.993 SimRank[55] 0.265 0.98 0.226 0.978 0.723 0.968 0.954 0.998 - - - - - - - - SimAttr (C)[56] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 SimAttr (E)[57] 0.714 0.975 0.469 0.974 0.816 0.965 0.729 0.998 0.811 0.683 0.673 0.522 0.704 0.573 0.702 0.986 AttriRank[58] 0.816 0.985 0.654 0.98 0.818 0.968 0.891 0.998 0.925 0.684 0.683 0.576 0.951 0.669 0.86 0.995 Node2Vec[59] 0.194 0.984 0.182 0.979 0.589 0.968 0.739 0.998 0.36 0.67 0.407 0.579 - - - - GraphSAGE[38] 0.262 0.982 0.338 0.98 0.531 0.969 0.757 0.999 - - - - - - - - PANE[61] 0.465 0.98 0.376 0.976 0.548 0.967 0.715 0.998 0.841 0.673 0.592 0.562 0.736 0.573 0.748 0.986 CFANE[62] 0.369 0.979 0.264 0.975 0.754 0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when\n",
      " > 816 [-0.014824  0.00919  -0.01056   0.0556    0.03497 ]  0.964 0.876 0.998 - - - - - - - - LACA(C) 0.227 0.977 0.106 0.975 0.669 0.962 0.824 0.998 0.248 0.664 0.642 0.518 0.254 0.597 0.244 0.985 LACA(E) 0.228 0.977 0.108 0.975 0.661 0.962 0.849 0.992 0.247 0.665 0.494 0.534 0.254 0.598 0.155 0.981 ArXiv Yelp Reddit Amazon2M 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (a) Varying ϵ in LACA (C) 10−8 10−6 10−4 10−2 110−2 10−1 1 101 102 running time (sec) (b) Varying ϵ in LACA (E) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (c) Varying k in LACA (C) 8 16 32 64 128 d 10−1 1 101 102 running time (sec) (d) Varying k in LACA (E) Fig. 10: Efficiency when varying ϵ and k. large graphs ArXiv, Yelp, Reddit, and Amazon2M by varying diffusion threshold ϵ and dimension k. Fig. 10(a) and 10(b) depict the running times of LACA (C) and LACA (E) on the four datasets when decreasing ϵ from 1 to 10−8, respectively. Specifically, the average runtime of both LACA (C) and LACA (E) increases by roughly an order of magnitude when there is a tenfold decrease in ϵ, which is consistent with our complexity analysis of LACA in Section V-B. Although LACA (C) and LACA (E) are local algorithms, i.e., their time complexities are independent of n and m, we can observe that under the same ϵ settings, their empirical times vary on datasets with varied sizes. Note that this dif- ference is caused by their diverse graph structures that lead to different data locality and memory access patterns. For example, although Reddit contains more nodes and 99× more edges than ArXiv, their running times are comparable when ϵ ≥ 10−3 and the cost for ArXiv turns to be markedly higher than that for Reddit when 10−7 ≤ ϵ ≤ 10−4. The reason is that nodes in ArXiv are sparsely connected, making the matrix-vector multiplications in Algo. 2 inefficient. As for Yelp and Amazon2M, they encompass significantly more nodes and edges than ArXiv and Reddit, and, hence, are more likely to yield cache misses and intensive memory access patterns that result in lower efficiency. In Fig. 10(c) and 10(d), we display the running times of LACA (C) and LACA (E) when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph\n",
      " > 817 [-0.02089  0.01508 -0.01808  0.04074  0.04276]  when increasing k from 8 to d. Notably, the time required by LACA (C) and LACA (E) remains stable when varying k in {8, 16, 32, 64, 128}, indicating that the time cost of LACA is dominated by 1/ϵ when k is not large. TABLE VIII: Statistics of Datasets without Attributes. Dataset n m |Ys| com-DBLP [88] 317,080 1,049,866 1,862 com-Amazon [88] 334,863 925,872 47 com-Orkut [88] 3,072,441 117,185,083 621 TABLE IX: The average precision evaluated with ground- truth. Best is bolded and best baseline underlined . Method com-DBLP[88] com-Amazon[88] com-Orkut[88] PR-Nibble[15] 0.374 0.835 0.251 HK-Relax[16] 0.305 0.865 0.233 CRD[20] 0.247 0.696 0.021 p-Norm FD[21] 0.331 0.871 0.199 LACA(w/o SNAS) 0.399 0.919 0.253 5) Quality Evaluation on Graphs without Attributes: This set of experiments evaluates the performance of LACA (i.e., LACA (w/o SNAS)) against 4 strong LGC base- lines (PR-Nibble, HK-Relax, CRD, and p-Norm FD) on graphs without node attributes in terms of local clustering quality, using the same evaluation protocol in Section VI-B. Table VIII lists the statistics of the non-attributed graph datasets of various volumes and types for evaluation [88]. In com-DBLP, the nodes represent authors, and the edges repre- sent co-authorship between published authors. The ground- truth clusters are formed based on the publication venue. com-Amazon is a co-purchasing network where products are nodes, and edges represent frequently co-purchased items. 19The ground-truth clusters are determined by product cate- gories. com-Orkut is a social network where users establish friendships and join groups. The groups created by users are considered the ground-truth clusters. The average precisions of the local clusters produced by LACA and 4 competitors are presented in Table IX. It can be observed that LACA consistently delivers the best performance across all three datasets. Specifically, on the medium-sized datasets com-DBLP and com-Amazon, LACA surpasses the best competitors, PR-Nibble and p-Norm FD, by a significant gain of 2.5% and 4.8% in precision, respectively. On the large graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into\n",
      " > 818 [-0.02843  0.02278 -0.03156  0.0471   0.01984]  graph com-Orkut with 117.2 million edges, LACA still outperforms the state-of-the-art baseline PR-Nibble with an improvement of 0.2%. The results manifest that our proposed BDD in LACA can accurately capture the affinity between nodes even without node attributes through the bidirectional random walks that consider the node importance from the perspectives of both, as remarked in Section II-C. C. Alternative Implementation of LACA TABLE X: The average precision comparing with alternative BDD implementations. Method CoraPubMedBlogCLFlickrArXivYelpRedditAmazon2mLACA(C) 0.556 0.552 0.51 0.4470.3770.7540.808 0.465LACA(C)-RS-RS-RS0.181 0.383 0.167 0.1210.0910.7370.054 0.227LACA(C)-R-RS-RS0.224 0.446 0.184 0.1520.2090.7370.194 0.223LACA(C)-RS-R-RS0.222 0.441 0.18 0.1420.1690.7370.114 0.225LACA(C)-RS-RS-R0.194 0.360 0.174 0.1450.0820.7200.065 0.237LACA(E) 0.552 0.555 0.493 0.39 0.3770.7390.808 0.521LACA(E)-RS-RS-RS0.17 0.358 0.167 0.11 0.0910.7370.058 0.133LACA(E)-R-RS-RS0.179 0.364 0.178 0.1130.2080.7370.191 0.396LACA(E)-RS-R-RS0.181 0.365 0.177 0.11 0.1670.719 0.11 0.243LACA(E)-RS-RS-R0.183 0.352 0.172 0.1130.0820.7190.064 0.133 TABLE XI: Ablation study on various similarity measures. Method Cora PubMedBlogCLFlickrArXiv Yelp RedditAmazon2m LACA(C) 0.556 0.552 0.51 0.447 0.3770.754 0.808 0.465 LACA(E) 0.552 0.555 0.493 0.39 0.3770.739 0.808 0.521 LACA(Jaccard)0.524 ✗ 0.304 0.28 ✗ ✗ ✗ ✗ LACA(Pearson)0.518 0.551 0.289 0.115 - - - - 1) Alternative Implementation of BDD: To rigorously val- idate the effectiveness of our BDD, we have implemented the new graph diffusion algorithms for local clustering based on the suggested formulation in the comment and the other three alternative formulations as follows: 1) RS-RS-RS: Integrating attribute similarity (i.e., SNAS) into all three random walk steps as suggested. Specifically, for each node pair (vs, vt), the affinity is defined byP vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · ρ(vt, vj), where ρ(vi, vj) =( π(vi, vj) · s(vi, vj) if vi is connected to vj via an edge 1 vi = vj .. 2) R-RS-RS: Integrating attribute similarity (i.e., SNAS) into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M\n",
      " > 819 [-0.044    0.01476 -0.0378   0.03152  0.0564 ]  into the second and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V π(vs, vi) · ρ(vi, vj) · ρ(vt, vj). 3) RS-R-RS: Integrating attribute similarity (i.e., SNAS) into the first and third random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · π(vi, vj) · ρ(vt, vj). 4) RS-RS-R: Integrating attribute similarity (i.e., SNAS) into the first and second random walk steps. For each node pair (vs, vt), the affinity is defined by P vi,vj∈V ρ(vs, vi) · ρ(vi, vj) · π(vt, vj). and have compared them against our original BDD. Table X reports the local clustering performance of LACA (C) and LACA (E) based on our BDD and the above four alternative definitions. It can be observed that all these four variants undergo severe performance degradation compared to the BDD on most datasets. For instance, on Cora and Amazon, LACA (C)-BDD is able to yield 55.6% and 46.5% in precision, whereas the four alternatives achieve at most 22.4% and 23.7%, respectively. The remarkable superiority of the BDD over the alternatives is due to the fact that these alternatives overly incorporate the attribute similarity (at least two attribute-only transitions) and topological connectivity (three random walk steps) into the random walk diffusion process, rendering the graph traversal rather biased and easier to jump to the nodes that are distant or even disconnected from the seed node vs via the intermediate nodes with high attribute similarities and long random walks. In turn, it is more likely to produce nodes that are far-reaching from the local cluster around vs. 2) Alternative Choices on Similarity Measurements: To further demonstrate the superiority of LACA (C) and LACA (E), we have conducted an ablation study that employs the Jaccard and Pearson correlation coefficients as the SNAS in LACA (C) and LACA (E) on all datasets. Note that the Jaccard coefficient requires the attribute values to be binary and thus are not applicable to datasets with continuous attributes, i.e., PubMed, ArXiv, Yelp, Reddit, and Amazon2M and the Pearson correla- tion coefficient is unable to report the results on large graphs within three days due to the high complexity ( O(n2d)) needed for calculating the similarities of the intermediate node pairs. As presented in Table XI,LACA (C) and LACA (E) consistently outperform these two variants with considerable gains. For example, on Flickr, LACA (C) can obtain a precision of 44.7%, while the precision scores attained by Jaccard and Pearson correlation coefficients are merely 28% and 11.5%. D. Other Related Work 1) Attributed Network Embedding: Attributed network em- bedding (ANE) is to embed each node graph G into a low- dimensional feature vector, preserving both topology and attribute information. The obtained embeddings can be used in many downstream tasks, including graph clustering. ANE methods can be categorized into two types: factorization- based and learning-based. Factorization-based methods [61], [89]–[91] construct and factorize node proximity matrices that integrate graph topology and node attributes to derive low- dimensional vectors. TADW [89] leverages a second-order ad- jacency matrix, AANE [90] matches node representations with attribute proximities, PANE [60], [61] optimizes forward and backward affinity matrices via random walks and optimizes convergence with greedy initial technique. However, these methods suffer from scalability issues due to the necessity 20of factorization of an n × n proximity matrix. Learning- based methods [38], [62], [92], [93] are further classified into encoder-decoder and propagation categories. Encoder-decoder methods, including DANE [93] and ANRL [92], integrate attribute features and graph topology by utilizing multiple au- toencoders to minimize input reconstruction loss. Additionally, ANRL develops an attribute-aware model based on the Skip- gram model [94]. CFANE [62] integrates propagation-based and encoder-decoder methods, utilizing self-attention to refine its model. In contrast to our local-based approach, these ANE methods typically require processing nodes across the entire graph. 21\n"
     ]
    }
   ],
   "source": [
    "from rolling.paper import create_paper, print_paper\n",
    "paper = create_paper(\n",
    "    title=pdfs[0],\n",
    "    text=text,\n",
    "    embedding_function=model.encode\n",
    ")\n",
    "print_paper(paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
